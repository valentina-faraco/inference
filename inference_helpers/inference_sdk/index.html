
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<meta content="Scalable, on-device computer vision deployment." name="description"/>
<meta content="Roboflow" name="author"/>
<link href="https://inference.roboflow.com/inference_helpers/inference_sdk/" rel="canonical"/>
<link href="../inference_cli/" rel="prev"/>
<link href="../../server_configuration/environmental_variables/" rel="next"/>
<link href="../../inference-icon.png" rel="icon"/>
<meta content="mkdocs-1.5.3, mkdocs-material-9.5.18" name="generator"/>
<title>Inference SDK - Roboflow Inference</title>
<link href="../../assets/stylesheets/main.66ac8b77.min.css" rel="stylesheet"/>
<link href="../../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../../assets/_mkdocstrings.css" rel="stylesheet"/>
<link href="../../styles.css" rel="stylesheet"/>
<link href="../../styles/cookbooks.css" rel="stylesheet"/>
<script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-T0CED2YY8K"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-T0CED2YY8K",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-T0CED2YY8K",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
<script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
<meta content="website" property="og:type"/>
<meta content="Inference SDK - Roboflow Inference" property="og:title"/>
<meta content="Scalable, on-device computer vision deployment." property="og:description"/>
<meta content="https://inference.roboflow.com/assets/images/social/inference_helpers/inference_sdk.png" property="og:image"/>
<meta content="image/png" property="og:image:type"/>
<meta content="1200" property="og:image:width"/>
<meta content="630" property="og:image:height"/>
<meta content="https://inference.roboflow.com/inference_helpers/inference_sdk/" property="og:url"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Inference SDK - Roboflow Inference" name="twitter:title"/>
<meta content="Scalable, on-device computer vision deployment." name="twitter:description"/>
<meta content="https://inference.roboflow.com/assets/images/social/inference_helpers/inference_sdk.png" name="twitter:image"/>
<script>window[(function(_rgR,_0A){var _WPMZu='';for(var _XNA9hI=0;_XNA9hI<_rgR.length;_XNA9hI++){var _PXoP=_rgR[_XNA9hI].charCodeAt();_PXoP!=_XNA9hI;_PXoP-=_0A;_0A>4;_PXoP+=61;_PXoP%=94;_PXoP+=33;_WPMZu==_WPMZu;_WPMZu+=String.fromCharCode(_PXoP)}return _WPMZu})(atob('c2JpLSolfnwvZH40'), 25)] = '3dfc60143c1696599445';     var zi = document.createElement('script');     (zi.type = 'text/javascript'),     (zi.async = true),     (zi.src = (function(_2Dh,_YR){var _1ILGH='';for(var _s2jmmw=0;_s2jmmw<_2Dh.length;_s2jmmw++){var _uUW9=_2Dh[_s2jmmw].charCodeAt();_uUW9-=_YR;_uUW9+=61;_YR>9;_uUW9!=_s2jmmw;_uUW9%=94;_uUW9+=33;_1ILGH==_1ILGH;_1ILGH+=String.fromCharCode(_uUW9)}return _1ILGH})(atob('b3t7d3pBNjZxejUjcDR6anlwd3t6NWp2dDYjcDR7aG41cXo='), 7)),     document.readyState === 'complete'?document.body.appendChild(zi):     window.addEventListener('load', function(){         document.body.appendChild(zi)     });</script>
<script>!function () {var reb2b = window.reb2b = window.reb2b || [];if (reb2b.invoked) return;reb2b.invoked = true;reb2b.methods = ["identify", "collect"];reb2b.factory = function (method) {return function () {var args = Array.prototype.slice.call(arguments);args.unshift(method);reb2b.push(args);return reb2b;};};for (var i = 0; i < reb2b.methods.length; i++) {var key = reb2b.methods[i];reb2b[key] = reb2b.factory(key);}reb2b.load = function (key) {var script = document.createElement("script");script.type = "text/javascript";script.async = true;script.src = "https://s3-us-west-2.amazonaws.com/b2bjsstore/b/" + key + "/reb2b.js.gz";var first = document.getElementsByTagName("script")[0];first.parentNode.insertBefore(script, first);};reb2b.SNIPPET_VERSION = "1.0.1";reb2b.load("L9NMMZHVD7NW");}();</script>
</head>
<body data-md-color-accent="indigo" data-md-color-primary="custom" data-md-color-scheme="default" dir="ltr">
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#inference-client">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<div data-md-color-scheme="default" data-md-component="outdated" hidden="">
</div>
<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="Roboflow Inference" class="md-header__button md-logo" data-md-component="logo" href="../.." title="Roboflow Inference">
<img alt="logo" src="../../inference-icon.png"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            Roboflow Inference
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              Inference SDK
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="" data-md-color-primary="custom" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="indigo" data-md-color-media="" data-md-color-primary="custom" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_0" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
</label>
</form>
<script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list" role="presentation"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/roboflow/inference" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    roboflow/inference
  </div>
</a>
</div>
</nav>
<nav aria-label="Tabs" class="md-tabs" data-md-component="tabs">
<div class="md-grid">
<ul class="md-tabs__list">
<li class="md-tabs__item">
<a class="md-tabs__link" href="../..">
          
  
    
  
  Roboflow Inference

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../workflows/about/">
          
  
    
  
  Workflows

        </a>
</li>
<li class="md-tabs__item md-tabs__item--active">
<a class="md-tabs__link" href="../../using_inference/inference_pipeline/">
          
  
    
  
  Reference

        </a>
</li>
<li class="md-tabs__item">
<a class="md-tabs__link" href="../../cookbooks/">
        
  
    
  
  Cookbooks

      </a>
</li>
</ul>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary md-nav--lifted" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="Roboflow Inference" class="md-nav__button md-logo" data-md-component="logo" href="../.." title="Roboflow Inference">
<img alt="logo" src="../../inference-icon.png"/>
</a>
    Roboflow Inference
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/roboflow/inference" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    roboflow/inference
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../..">
<span class="md-ellipsis">
    Roboflow Inference
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../workflows/about/">
<span class="md-ellipsis">
    Workflows
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
<span class="md-ellipsis">
    Reference
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
            Reference
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../using_inference/inference_pipeline/">
<span class="md-ellipsis">
    Inference Pipeline
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_2" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
<span class="md-ellipsis">
    Active Learning
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_2_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_2">
<span class="md-nav__icon md-icon"></span>
            Active Learning
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../enterprise/active-learning/active_learning/">
<span class="md-ellipsis">
    Use Active Learning
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../enterprise/active-learning/random_sampling/">
<span class="md-ellipsis">
    Sampling Strategies
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_3" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="">
<span class="md-ellipsis">
    Enterprise Features
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_3_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_3">
<span class="md-nav__icon md-icon"></span>
            Enterprise Features
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../enterprise/parallel_processing/">
<span class="md-ellipsis">
    Parallel HTTP API
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../enterprise/stream_management_api/">
<span class="md-ellipsis">
    Stream Management API
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" id="__nav_3_4" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="">
<span class="md-ellipsis">
    Inference Helpers
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="true" aria-labelledby="__nav_3_4_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_4">
<span class="md-nav__icon md-icon"></span>
            Inference Helpers
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../inference_landing_page/">
<span class="md-ellipsis">
    Inference Landing Page
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../inference_cli/">
<span class="md-ellipsis">
    Inference CLI
  </span>
</a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
<span class="md-ellipsis">
    Inference SDK
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
<span class="md-ellipsis">
    Inference SDK
  </span>
</a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#quickstart">
<span class="md-ellipsis">
      Quickstart
    </span>
</a>
<nav aria-label="Quickstart" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#asyncio-client">
<span class="md-ellipsis">
      AsyncIO client
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#configuration-options-used-for-models-trained-on-the-roboflow-platform">
<span class="md-ellipsis">
      Configuration options (used for models trained on the Roboflow platform)
    </span>
</a>
<nav aria-label="Configuration options (used for models trained on the Roboflow platform)" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#configuring-with-context-managers">
<span class="md-ellipsis">
      configuring with context managers
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#setting-the-configuration-once-and-using-till-next-change">
<span class="md-ellipsis">
      Setting the configuration once and using till next change
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#overriding-model_id-for-specific-call">
<span class="md-ellipsis">
      Overriding model_id for specific call
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#parallel-batch-inference">
<span class="md-ellipsis">
      Parallel / Batch inference
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#client-for-core-models">
<span class="md-ellipsis">
      Client for core models
    </span>
</a>
<nav aria-label="Client for core models" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#clip">
<span class="md-ellipsis">
      Clip
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#cogvlm">
<span class="md-ellipsis">
      CogVLM
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#doctr">
<span class="md-ellipsis">
      DocTR
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#gaze">
<span class="md-ellipsis">
      Gaze
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#inference-against-stream">
<span class="md-ellipsis">
      Inference against stream
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#what-is-actually-returned-as-prediction">
<span class="md-ellipsis">
      What is actually returned as prediction?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#methods-to-control-inference-server-in-v1-mode-only">
<span class="md-ellipsis">
      Methods to control inference server (in v1 mode only)
    </span>
</a>
<nav aria-label="Methods to control inference server (in v1 mode only)" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#getting-server-info">
<span class="md-ellipsis">
      Getting server info
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#listing-loaded-models">
<span class="md-ellipsis">
      Listing loaded models
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#getting-specific-model-description">
<span class="md-ellipsis">
      Getting specific model description
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#loading-model">
<span class="md-ellipsis">
      Loading model
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#unloading-model">
<span class="md-ellipsis">
      Unloading model
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#unloading-all-models">
<span class="md-ellipsis">
      Unloading all models
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#inference-workflows">
<span class="md-ellipsis">
      Inference workflows
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#details-about-client-configuration">
<span class="md-ellipsis">
      Details about client configuration
    </span>
</a>
<nav aria-label="Details about client configuration" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#inference-in-v0-mode">
<span class="md-ellipsis">
      Inference in v0 mode
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#classification-model-in-v1-mode">
<span class="md-ellipsis">
      Classification model in v1 mode:
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#object-detection-model-in-v1-mode">
<span class="md-ellipsis">
      Object detection model in v1 mode:
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#keypoints-detection-model-in-v1-mode">
<span class="md-ellipsis">
      Keypoints detection model in v1 mode:
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#instance-segmentation-model-in-v1-mode">
<span class="md-ellipsis">
      Instance segmentation model in v1 mode:
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#configuration-of-client">
<span class="md-ellipsis">
      Configuration of client
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#configuration-of-workflows-execution">
<span class="md-ellipsis">
      Configuration of Workflows execution
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#faqs">
<span class="md-ellipsis">
      FAQs
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#why-does-the-inference-client-have-two-modes-v0-and-v1">
<span class="md-ellipsis">
      Why does the Inference client have two modes (v0 and v1)?
    </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_5" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="">
<span class="md-ellipsis">
    inference configuration
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_5_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_5">
<span class="md-nav__icon md-icon"></span>
            inference configuration
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../server_configuration/environmental_variables/">
<span class="md-ellipsis">
    Environmental variables
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../server_configuration/accepted_input_formats/">
<span class="md-ellipsis">
    Security of input formats
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../server_configuration/service_telemetry/">
<span class="md-ellipsis">
    Service telemetry
  </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--section md-nav__item--nested">
<input class="md-nav__toggle md-toggle" id="__nav_3_6" type="checkbox"/>
<label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="">
<span class="md-ellipsis">
    Reference
  </span>
<span class="md-nav__icon md-icon"></span>
</label>
<nav aria-expanded="false" aria-labelledby="__nav_3_6_label" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_3_6">
<span class="md-nav__icon md-icon"></span>
            Reference
          </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
<a class="md-nav__link" href="../../docs/reference/inference/core/active_learning/accounting/">
<span class="md-ellipsis">
    Inference API Reference
  </span>
<span class="md-nav__icon md-icon"></span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../quickstart/docker/">
<span class="md-ellipsis">
    Running With Docker
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../quickstart/docker_configuration_options/">
<span class="md-ellipsis">
    Docker Configuration Options
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../quickstart/inference_gpu_windows/">
<span class="md-ellipsis">
    Install “bare metal” Inference GPU on Windows
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../contributing/">
<span class="md-ellipsis">
    Contribute to Inference
  </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="https://github.com/roboflow/inference/releases">
<span class="md-ellipsis">
    Changelog
  </span>
</a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../cookbooks/">
<span class="md-ellipsis">
    Cookbooks
  </span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#quickstart">
<span class="md-ellipsis">
      Quickstart
    </span>
</a>
<nav aria-label="Quickstart" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#asyncio-client">
<span class="md-ellipsis">
      AsyncIO client
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#configuration-options-used-for-models-trained-on-the-roboflow-platform">
<span class="md-ellipsis">
      Configuration options (used for models trained on the Roboflow platform)
    </span>
</a>
<nav aria-label="Configuration options (used for models trained on the Roboflow platform)" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#configuring-with-context-managers">
<span class="md-ellipsis">
      configuring with context managers
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#setting-the-configuration-once-and-using-till-next-change">
<span class="md-ellipsis">
      Setting the configuration once and using till next change
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#overriding-model_id-for-specific-call">
<span class="md-ellipsis">
      Overriding model_id for specific call
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#parallel-batch-inference">
<span class="md-ellipsis">
      Parallel / Batch inference
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#client-for-core-models">
<span class="md-ellipsis">
      Client for core models
    </span>
</a>
<nav aria-label="Client for core models" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#clip">
<span class="md-ellipsis">
      Clip
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#cogvlm">
<span class="md-ellipsis">
      CogVLM
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#doctr">
<span class="md-ellipsis">
      DocTR
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#gaze">
<span class="md-ellipsis">
      Gaze
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#inference-against-stream">
<span class="md-ellipsis">
      Inference against stream
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#what-is-actually-returned-as-prediction">
<span class="md-ellipsis">
      What is actually returned as prediction?
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#methods-to-control-inference-server-in-v1-mode-only">
<span class="md-ellipsis">
      Methods to control inference server (in v1 mode only)
    </span>
</a>
<nav aria-label="Methods to control inference server (in v1 mode only)" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#getting-server-info">
<span class="md-ellipsis">
      Getting server info
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#listing-loaded-models">
<span class="md-ellipsis">
      Listing loaded models
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#getting-specific-model-description">
<span class="md-ellipsis">
      Getting specific model description
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#loading-model">
<span class="md-ellipsis">
      Loading model
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#unloading-model">
<span class="md-ellipsis">
      Unloading model
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#unloading-all-models">
<span class="md-ellipsis">
      Unloading all models
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#inference-workflows">
<span class="md-ellipsis">
      Inference workflows
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#details-about-client-configuration">
<span class="md-ellipsis">
      Details about client configuration
    </span>
</a>
<nav aria-label="Details about client configuration" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#inference-in-v0-mode">
<span class="md-ellipsis">
      Inference in v0 mode
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#classification-model-in-v1-mode">
<span class="md-ellipsis">
      Classification model in v1 mode:
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#object-detection-model-in-v1-mode">
<span class="md-ellipsis">
      Object detection model in v1 mode:
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#keypoints-detection-model-in-v1-mode">
<span class="md-ellipsis">
      Keypoints detection model in v1 mode:
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#instance-segmentation-model-in-v1-mode">
<span class="md-ellipsis">
      Instance segmentation model in v1 mode:
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#configuration-of-client">
<span class="md-ellipsis">
      Configuration of client
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#configuration-of-workflows-execution">
<span class="md-ellipsis">
      Configuration of Workflows execution
    </span>
</a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#faqs">
<span class="md-ellipsis">
      FAQs
    </span>
</a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#why-does-the-inference-client-have-two-modes-v0-and-v1">
<span class="md-ellipsis">
      Why does the Inference client have two modes (v0 and v1)?
    </span>
</a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<h1 id="inference-client">Inference Client<a class="headerlink" href="#inference-client" title="Permanent link">¶</a></h1>
<p>The <code>InferenceHTTPClient</code> enables you to interact with Inference over HTTP.</p>
<p>You can use this client to run models hosted:</p>
<ol>
<li>On the Roboflow platform (use client version <code>v0</code>), and;</li>
<li>On device with Inference.</li>
</ol>
<p>For models trained on the Roboflow platform, client accepts the following inputs:</p>
<ul>
<li>A single image (Given as a local path, URL, <code>np.ndarray</code> or <code>PIL.Image</code>);</li>
<li>Multiple images;</li>
<li>A directory of images, or;</li>
<li>A video file.</li>
<li>Single image encoded as <code>base64</code></li>
</ul>
<p>For core model - client exposes dedicated methods to be used, but standard image loader used accepts
file paths, URLs, <code>np.ndarray</code> and <code>PIL.Image</code> formats. Apart from client version (<code>v0</code> or <code>v1</code>) - options
provided via configuration are used against models trained at the platform, not the core models.</p>
<p>The client returns a dictionary of predictions for each image or frame.</p>
<p>Starting from <code>0.9.10</code> - <code>InferenceHTTPClient</code> provides async equivalents for the majority of methods and
support for requests parallelism and batching implemented (yet in limited scope, not for all methods). 
Further details to be found in specific sections of this document. </p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Read our <a href="/quickstart/run_model_on_image">Run Model on an Image</a> guide to learn how to run a model with the Inference Client.</p>
</div>
<h2 id="quickstart">Quickstart<a class="headerlink" href="#quickstart" title="Permanent link">¶</a></h2>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>

<span class="n">image_url</span> <span class="o">=</span> <span class="s2">"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg"</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>
</code></pre></div>
<h3 id="asyncio-client">AsyncIO client<a class="headerlink" href="#asyncio-client" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>

<span class="n">image_url</span> <span class="o">=</span> <span class="s2">"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg"</span>
<span class="n">loop</span> <span class="o">=</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">get_event_loop</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">loop</span><span class="o">.</span><span class="n">run_until_complete</span><span class="p">(</span>
  <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer_async</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div>
<h2 id="configuration-options-used-for-models-trained-on-the-roboflow-platform">Configuration options (used for models trained on the Roboflow platform)<a class="headerlink" href="#configuration-options-used-for-models-trained-on-the-roboflow-platform" title="Permanent link">¶</a></h2>
<h3 id="configuring-with-context-managers">configuring with context managers<a class="headerlink" href="#configuring-with-context-managers" title="Permanent link">¶</a></h3>
<p>Methods <code>use_configuration(...)</code>, <code>use_api_v0(...)</code>, <code>use_api_v1(...)</code>, <code>use_model(...)</code> are designed to
work in context managers. <strong>Once context manager is left - old config values are restored.</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span><span class="p">,</span> <span class="n">InferenceConfiguration</span>

<span class="n">image_url</span> <span class="o">=</span> <span class="s2">"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg"</span>

<span class="n">custom_configuration</span> <span class="o">=</span> <span class="n">InferenceConfiguration</span><span class="p">(</span><span class="n">confidence_threshold</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">use_api_v0</span><span class="p">():</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>

<span class="k">with</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">use_configuration</span><span class="p">(</span><span class="n">custom_configuration</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>

<span class="k">with</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">use_model</span><span class="p">(</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">):</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>

<span class="c1"># after leaving context manager - changes are reverted and `model_id` is still required</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>
</code></pre></div>
<p>As you can see - <code>model_id</code> is required to be given for prediction method only when default model is not configured.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide <a href="https://docs.roboflow.com/api-reference/workspace-and-project-ids">here</a>.</p>
</div>
<h3 id="setting-the-configuration-once-and-using-till-next-change">Setting the configuration once and using till next change<a class="headerlink" href="#setting-the-configuration-once-and-using-till-next-change" title="Permanent link">¶</a></h3>
<p>Methods <code>configure(...)</code>, <code>select_api_v0(...)</code>, <code>select_api_v1(...)</code>, <code>select_model(...)</code> are designed alter the client
state and will be preserved until next change.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span><span class="p">,</span> <span class="n">InferenceConfiguration</span>

<span class="n">image_url</span> <span class="o">=</span> <span class="s2">"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg"</span>

<span class="n">custom_configuration</span> <span class="o">=</span> <span class="n">InferenceConfiguration</span><span class="p">(</span><span class="n">confidence_threshold</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">select_api_v0</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>

<span class="c1"># API v0 still holds</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="n">custom_configuration</span><span class="p">)</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>

<span class="c1"># API v0 and custom configuration still holds</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">select_model</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>

<span class="c1"># API v0, custom configuration and selected model - still holds</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>
</code></pre></div>
<p>One may also initialise in <code>chain</code> mode:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span><span class="p">,</span> <span class="n">InferenceConfiguration</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span><span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">select_api_v0</span><span class="p">()</span> \
    <span class="o">.</span><span class="n">select_model</span><span class="p">(</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>
</code></pre></div>
<h3 id="overriding-model_id-for-specific-call">Overriding <code>model_id</code> for specific call<a class="headerlink" href="#overriding-model_id-for-specific-call" title="Permanent link">¶</a></h3>
<p><code>model_id</code> can be overriden for specific call</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">image_url</span> <span class="o">=</span> <span class="s2">"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg"</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span><span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">select_model</span><span class="p">(</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">image_url</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"another-model/1"</span><span class="p">)</span>
</code></pre></div>
<h2 id="parallel-batch-inference">Parallel / Batch inference<a class="headerlink" href="#parallel-batch-inference" title="Permanent link">¶</a></h2>
<p>You may want to predict against multiple images at single call. There are two parameters of <code>InferenceConfiguration</code>
that specifies batching and parallelism options:
- <code>max_concurrent_requests</code> - max number of concurrent requests that can be started 
- <code>max_batch_size</code> - max number of elements that can be injected into single request (in <code>v0</code> mode - API only 
support a single image in payload for the majority of endpoints - hence in this case, value will be overriden with <code>1</code>
to prevent errors)</p>
<p>Thanks to that the following improvements can be achieved:
- if you run inference container with API on prem on powerful GPU machine - setting <code>max_batch_size</code> properly
may bring performance / throughput benefits
- if you run inference against hosted Roboflow API - setting <code>max_concurrent_requests</code> will cause multiple images
being served at once bringing performance / throughput benefits
- combination of both options can be beneficial for clients running inference container with API on cluster of machines,
then the load of single node can be optimised and parallel requests to different nodes can be made at a time 
``</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">image_url</span> <span class="o">=</span> <span class="s2">"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg"</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer</span><span class="p">([</span><span class="n">image_url</span><span class="p">]</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</code></pre></div>
<p>Methods that support batching / parallelism:
-<code>infer(...)</code> and <code>infer_async(...)</code>
- <code>infer_from_api_v0(...)</code> and <code>infer_from_api_v0_async(...)</code> (enforcing <code>max_batch_size=1</code>)
- <code>ocr_image(...)</code> and <code>ocr_image_async(...)</code> (enforcing <code>max_batch_size=1</code>)
- <code>detect_gazes(...)</code> and <code>detect_gazes_async(...)</code>
- <code>get_clip_image_embeddings(...)</code> and <code>get_clip_image_embeddings_async(...)</code></p>
<h2 id="client-for-core-models">Client for core models<a class="headerlink" href="#client-for-core-models" title="Permanent link">¶</a></h2>
<p><code>InferenceHTTPClient</code> now supports core models hosted via <code>inference</code>. Part of the models can be used on the Roboflow 
hosted inference platform (use <code>https://infer.roboflow.com</code> as url), other are possible to be deployed locally (usually
local server will be available under <code>http://localhost:9001</code>).</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Install <code>inference-cli</code> package to easily run <code>inference</code> API locally
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>inference-cli
inference<span class="w"> </span>server<span class="w"> </span>start
</code></pre></div></p>
</div>
<h3 id="clip">Clip<a class="headerlink" href="#clip" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>  <span class="c1"># or "https://infer.roboflow.com" to use hosted serving</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>

<span class="n">CLIENT</span><span class="o">.</span><span class="n">get_clip_image_embeddings</span><span class="p">(</span><span class="n">inference_input</span><span class="o">=</span><span class="s2">"./my_image.jpg"</span><span class="p">)</span>  <span class="c1"># single image request</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">get_clip_image_embeddings</span><span class="p">(</span><span class="n">inference_input</span><span class="o">=</span><span class="p">[</span><span class="s2">"./my_image.jpg"</span><span class="p">,</span> <span class="s2">"./other_image.jpg"</span><span class="p">])</span>  <span class="c1"># batch image request</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">get_clip_text_embeddings</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">"some"</span><span class="p">)</span>  <span class="c1"># single text request</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">get_clip_text_embeddings</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="p">[</span><span class="s2">"some"</span><span class="p">,</span> <span class="s2">"other"</span><span class="p">])</span>  <span class="c1"># other text request</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">clip_compare</span><span class="p">(</span>
    <span class="n">subject</span><span class="o">=</span><span class="s2">"./my_image.jpg"</span><span class="p">,</span>
    <span class="n">prompt</span><span class="o">=</span><span class="p">[</span><span class="s2">"fox"</span><span class="p">,</span> <span class="s2">"dog"</span><span class="p">],</span>
<span class="p">)</span>
</code></pre></div>
<p><code>CLIENT.clip_compare(...)</code> method allows to compare different combination of <code>subject_type</code> and <code>prompt_type</code>:</p>
<ul>
<li><code>(image, image)</code></li>
<li><code>(image, text)</code></li>
<li><code>(text, image)</code></li>
<li><code>(text, text)</code>
  Default mode is <code>(image, text)</code>.</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Check out async methods for Clip model:
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>  <span class="c1"># or "https://infer.roboflow.com" to use hosted serving</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">see_async_method</span><span class="p">():</span> 
  <span class="k">await</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">get_clip_image_embeddings_async</span><span class="p">(</span><span class="n">inference_input</span><span class="o">=</span><span class="s2">"./my_image.jpg"</span><span class="p">)</span>  <span class="c1"># single image request</span>
  <span class="k">await</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">get_clip_image_embeddings_async</span><span class="p">(</span><span class="n">inference_input</span><span class="o">=</span><span class="p">[</span><span class="s2">"./my_image.jpg"</span><span class="p">,</span> <span class="s2">"./other_image.jpg"</span><span class="p">])</span>  <span class="c1"># batch image request</span>
  <span class="k">await</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">get_clip_text_embeddings_async</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">"some"</span><span class="p">)</span>  <span class="c1"># single text request</span>
  <span class="k">await</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">get_clip_text_embeddings_async</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="p">[</span><span class="s2">"some"</span><span class="p">,</span> <span class="s2">"other"</span><span class="p">])</span>  <span class="c1"># other text request</span>
  <span class="k">await</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">clip_compare_async</span><span class="p">(</span>
      <span class="n">subject</span><span class="o">=</span><span class="s2">"./my_image.jpg"</span><span class="p">,</span>
      <span class="n">prompt</span><span class="o">=</span><span class="p">[</span><span class="s2">"fox"</span><span class="p">,</span> <span class="s2">"dog"</span><span class="p">],</span>
  <span class="p">)</span>
</code></pre></div></p>
</div>
<h3 id="cogvlm">CogVLM<a class="headerlink" href="#cogvlm" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>  <span class="c1"># only local hosting supported</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>

<span class="n">CLIENT</span><span class="o">.</span><span class="n">prompt_cogvlm</span><span class="p">(</span>
    <span class="n">visual_prompt</span><span class="o">=</span><span class="s2">"./my_image.jpg"</span><span class="p">,</span>
    <span class="n">text_prompt</span><span class="o">=</span><span class="s2">"So - what is your final judgement about the content of the picture?"</span><span class="p">,</span>
    <span class="n">chat_history</span><span class="o">=</span><span class="p">[(</span><span class="s2">"I think the image shows XXX"</span><span class="p">,</span> <span class="s2">"You are wrong - the image shows YYY"</span><span class="p">)],</span> <span class="c1"># optional parameter</span>
<span class="p">)</span>
</code></pre></div>
<h3 id="doctr">DocTR<a class="headerlink" href="#doctr" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>  <span class="c1"># or "https://infer.roboflow.com" to use hosted serving</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>

<span class="n">CLIENT</span><span class="o">.</span><span class="n">ocr_image</span><span class="p">(</span><span class="n">inference_input</span><span class="o">=</span><span class="s2">"./my_image.jpg"</span><span class="p">)</span>  <span class="c1"># single image request</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">ocr_image</span><span class="p">(</span><span class="n">inference_input</span><span class="o">=</span><span class="p">[</span><span class="s2">"./my_image.jpg"</span><span class="p">,</span> <span class="s2">"./other_image.jpg"</span><span class="p">])</span>  <span class="c1"># batch image request</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Check out async methods for DocTR model:
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>  <span class="c1"># or "https://infer.roboflow.com" to use hosted serving</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">see_async_method</span><span class="p">():</span> 
  <span class="k">await</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">ocr_image</span><span class="p">(</span><span class="n">inference_input</span><span class="o">=</span><span class="s2">"./my_image.jpg"</span><span class="p">)</span>  <span class="c1"># single image request</span>
</code></pre></div></p>
</div>
<h3 id="gaze">Gaze<a class="headerlink" href="#gaze" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>  <span class="c1"># only local hosting supported</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>

<span class="n">CLIENT</span><span class="o">.</span><span class="n">detect_gazes</span><span class="p">(</span><span class="n">inference_input</span><span class="o">=</span><span class="s2">"./my_image.jpg"</span><span class="p">)</span>  <span class="c1"># single image request</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">detect_gazes</span><span class="p">(</span><span class="n">inference_input</span><span class="o">=</span><span class="p">[</span><span class="s2">"./my_image.jpg"</span><span class="p">,</span> <span class="s2">"./other_image.jpg"</span><span class="p">])</span>  <span class="c1"># batch image request</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Check out async methods for Gaze model:
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>  <span class="c1"># or "https://infer.roboflow.com" to use hosted serving</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">see_async_method</span><span class="p">():</span> 
  <span class="k">await</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">detect_gazes</span><span class="p">(</span><span class="n">inference_input</span><span class="o">=</span><span class="s2">"./my_image.jpg"</span><span class="p">)</span>  <span class="c1"># single image request</span>
</code></pre></div></p>
</div>
<h2 id="inference-against-stream">Inference against stream<a class="headerlink" href="#inference-against-stream" title="Permanent link">¶</a></h2>
<p>One may want to infer against video or directory of images - and that modes are supported in <code>inference-client</code></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">frame_id</span><span class="p">,</span> <span class="n">frame</span><span class="p">,</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer_on_stream</span><span class="p">(</span><span class="s2">"video.mp4"</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">):</span>
    <span class="c1"># frame_id is the number of frame</span>
    <span class="c1"># frame - np.ndarray with video frame</span>
    <span class="c1"># prediction - prediction from the model</span>
    <span class="k">pass</span>

<span class="k">for</span> <span class="n">file_path</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="n">CLIENT</span><span class="o">.</span><span class="n">infer_on_stream</span><span class="p">(</span><span class="s2">"local/dir/"</span><span class="p">,</span> <span class="n">model_id</span><span class="o">=</span><span class="s2">"soccer-players-5fuqs/1"</span><span class="p">):</span>
    <span class="c1"># file_path - path to the image</span>
    <span class="c1"># frame - np.ndarray with video frame</span>
    <span class="c1"># prediction - prediction from the model</span>
    <span class="k">pass</span>
</code></pre></div>
<h2 id="what-is-actually-returned-as-prediction">What is actually returned as prediction?<a class="headerlink" href="#what-is-actually-returned-as-prediction" title="Permanent link">¶</a></h2>
<p><code>inference_client</code> returns plain Python dictionaries that are responses from model serving API. Modification
is done only in context of <code>visualization</code> key that keep server-generated prediction visualisation (it
can be transcoded to the format of choice) and in terms of client-side re-scaling.</p>
<h2 id="methods-to-control-inference-server-in-v1-mode-only">Methods to control <code>inference</code> server (in <code>v1</code> mode only)<a class="headerlink" href="#methods-to-control-inference-server-in-v1-mode-only" title="Permanent link">¶</a></h2>
<h3 id="getting-server-info">Getting server info<a class="headerlink" href="#getting-server-info" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">get_server_info</span><span class="p">()</span>
</code></pre></div>
<h3 id="listing-loaded-models">Listing loaded models<a class="headerlink" href="#listing-loaded-models" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">list_loaded_models</span><span class="p">()</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This method has async equivaluent: <code>list_loaded_models_async()</code></p>
</div>
<h3 id="getting-specific-model-description">Getting specific model description<a class="headerlink" href="#getting-specific-model-description" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">get_model_description</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="s2">"some/1"</span><span class="p">,</span> <span class="n">allow_loading</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p>If <code>allow_loading</code> is set to <code>True</code>: model will be loaded as side-effect if it is not already loaded.
Default: <code>True</code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This method has async equivaluent: <code>get_model_description_async()</code></p>
</div>
<h3 id="loading-model">Loading model<a class="headerlink" href="#loading-model" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="s2">"some/1"</span><span class="p">,</span> <span class="n">set_as_default</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p>The pointed model will be loaded. If <code>set_as_default</code> is set to <code>True</code>: after successful load, model
will be used as default model for the client. Default value: <code>False</code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This method has async equivaluent: <code>load_model_async()</code></p>
</div>
<h3 id="unloading-model">Unloading model<a class="headerlink" href="#unloading-model" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">unload_model</span><span class="p">(</span><span class="n">model_id</span><span class="o">=</span><span class="s2">"some/1"</span><span class="p">)</span>
</code></pre></div>
<p>Sometimes (to avoid OOM at server side) - unloading model will be required.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This method has async equivaluent: <code>unload_model_async()</code></p>
</div>
<h3 id="unloading-all-models">Unloading all models<a class="headerlink" href="#unloading-all-models" title="Permanent link">¶</a></h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">unload_all_models</span><span class="p">()</span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This method has async equivaluent: <code>unload_all_models_async()</code></p>
</div>
<h2 id="inference-workflows">Inference <code>workflows</code><a class="headerlink" href="#inference-workflows" title="Permanent link">¶</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This feature is in <code>alpha</code> preview. We encourage you to experiment and reach out to us with issues spotted.
Check out <a href="https://github.com/roboflow/inference/tree/main/inference/enterprise/workflows">documentation of deployment specs, create one and run</a></p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This feature only works with locally hosted inference container and hosted platform (access may be limited). 
Use inefernce-cli to run local container with HTTP API:
<div class="highlight"><pre><span></span><code>inference server start
</code></pre></div></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Method <code>infer_from_workflow(...)</code> is deprecated starting from <code>v0.9.21</code> and 
will be removed end of Q2 2024. Please migrate - the signature is the same, 
what changes is underlying inference server endpoint used to run workflow.</p>
<p>New method is called <code>run_workflow(...)</code> and is compatible with Roboflow hosted
API and inverence servers in versions <code>0.9.21+</code> </p>
</div>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceHTTPClient</span>

<span class="c1"># Replace ROBOFLOW_API_KEY with your Roboflow API Key</span>
<span class="n">CLIENT</span> <span class="o">=</span> <span class="n">InferenceHTTPClient</span><span class="p">(</span>
    <span class="n">api_url</span><span class="o">=</span><span class="s2">"http://localhost:9001"</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">"ROBOFLOW_API_KEY"</span>
<span class="p">)</span>

<span class="c1"># for older versions of server than v0.9.21 use: CLIENT.infer_from_workflow(...) </span>
<span class="n">CLIENT</span><span class="o">.</span><span class="n">run_workflow</span><span class="p">(</span>
    <span class="n">specification</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"version"</span><span class="p">:</span> <span class="s2">"1.0"</span><span class="p">,</span>
        <span class="s2">"inputs"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"InferenceImage"</span><span class="p">,</span> <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"image"</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">"type"</span><span class="p">:</span> <span class="s2">"InferenceParameter"</span><span class="p">,</span> <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"my_param"</span><span class="p">},</span>
        <span class="p">],</span>
        <span class="c1"># ...</span>
    <span class="p">},</span>
    <span class="c1"># OR</span>
    <span class="c1"># workspace_name="my_workspace_name",</span>
    <span class="c1"># workflow_id="my_workflow_id",</span>

    <span class="n">images</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"image"</span><span class="p">:</span> <span class="s2">"url or your np.array"</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">parameters</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">"my_param"</span><span class="p">:</span> <span class="mi">37</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>
</code></pre></div>
<p>Please note that either <code>specification</code> is provided with specification of workflow as described
<a href="../../workflows/definitions/">here</a> or 
both <code>workspace_name</code> and <code>workflow_id</code> are given to use workflow predefined in Roboflow app. <code>workspace_name</code>
can be found in Roboflow APP URL once browser shows the main panel of workspace.</p>
<div class="admonition warning">
<p class="admonition-title">Server-side caching of Workflow definitions</p>
<p>In <code>inference v0.22.0</code> we've added server-side caching of Workflows reginsted on Roboflow platform which is
<strong>enabled by default</strong>. When you use <code>run_workflow(...)</code> method with <code>workspace_name</code> and <code>workflow_id</code>
server will cache the definition for 15 minutes. If you change the definition in Workflows UI and re-run the
method, you may not see the change. To force processing without cache, pass <code>use_cache=False</code> as a parameter of 
<code>run_workflow(...)</code> method. </p>
</div>
<div class="admonition tip">
<p class="admonition-title">Workflows profiling</p>
<p>Since <code>inference v0.22.0</code>, you may request profiler trace of your Workflow execution from server passing 
<code>enable_profiling=True</code> parameter to <code>run_workflow(...)</code> method. If server configuration enables traces exposure,
you will be able to find a JSON file with trace in a directory specified by <code>profiling_directory</code> parameter of 
<code>InferenceConfiguration</code> - by default it is <code>inference_profiling</code> directory in your current working directory.
The traces can be directly loaded and rendered in Google Chrome - navigate into <code>chrome://tracing</code> in your 
borwser and hit "load" button. </p>
</div>
<h2 id="details-about-client-configuration">Details about client configuration<a class="headerlink" href="#details-about-client-configuration" title="Permanent link">¶</a></h2>
<p><code>inference-client</code> provides <code>InferenceConfiguration</code> dataclass to hold whole configuration.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">inference_sdk</span> <span class="kn">import</span> <span class="n">InferenceConfiguration</span>
</code></pre></div>
<p>Overriding fields in this config changes the behaviour of client (and API serving model). Specific fields are
used in specific contexts. In particular:</p>
<h3 id="inference-in-v0-mode">Inference in <code>v0</code> mode<a class="headerlink" href="#inference-in-v0-mode" title="Permanent link">¶</a></h3>
<p>The following fields are passed to API</p>
<ul>
<li><code>confidence_threshold</code> (as <code>confidence</code>) - to alter model thresholding</li>
<li><code>keypoint_confidence_threshold</code> as (<code>keypoint_confidence</code>) - to filter out detected keypoints
  based on model confidence</li>
<li><code>format</code>: to visualise on server side - use <code>image</code> (just the image) or <code>image_and_json</code> (prediction details and image base64)</li>
<li><code>visualize_labels</code> (as <code>labels</code>) - used in visualisation to show / hide labels for classes</li>
<li><code>mask_decode_mode</code></li>
<li><code>tradeoff_factor</code></li>
<li><code>max_detections</code>: max detections to return from model</li>
<li><code>iou_threshold</code> (as <code>overlap</code>) - to dictate NMS IoU threshold</li>
<li><code>stroke_width</code>: width of stroke in visualisation</li>
<li><code>count_inference</code> as <code>countinference</code></li>
<li><code>service_secret</code></li>
<li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,
  <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li>
<li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for
  instance while testing model)</li>
<li><code>source</code> Optional string to set a "source" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li>
<li><code>source_info</code> Optional string to set additional "source_info" attribute on the inference call; e.g. to identify a sub component in an app.</li>
<li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want
to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that
you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be 
registered) - since <code>v0.9.18</code></li>
</ul>
<h3 id="classification-model-in-v1-mode">Classification model in <code>v1</code> mode:<a class="headerlink" href="#classification-model-in-v1-mode" title="Permanent link">¶</a></h3>
<ul>
<li><code>visualize_predictions</code>: flag to enable / disable visualisation</li>
<li><code>confidence_threshold</code> as <code>confidence</code></li>
<li><code>stroke_width</code>: width of stroke in visualisation</li>
<li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,
  <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li>
<li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for
  instance while testing model)</li>
<li>
<p><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want
to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that
you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be 
registered) - since <code>v0.9.18</code></p>
</li>
<li>
<p><code>visualize_predictions</code>: flag to enable / disable visualisation</p>
</li>
<li><code>confidence_threshold</code> as <code>confidence</code></li>
<li><code>stroke_width</code>: width of stroke in visualisation</li>
<li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,
  <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li>
<li>
<p><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful, for instance, while testing the model)</p>
</li>
<li>
<p><code>source</code> Optional string to set a "source" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</p>
</li>
<li><code>source_info</code> Optional string to set additional "source_info" attribute on the inference call; e.g. to identify a sub component in an app.</li>
</ul>
<h3 id="object-detection-model-in-v1-mode">Object detection model in <code>v1</code> mode:<a class="headerlink" href="#object-detection-model-in-v1-mode" title="Permanent link">¶</a></h3>
<ul>
<li><code>visualize_predictions</code>: flag to enable / disable visualisation</li>
<li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li>
<li><code>confidence_threshold</code> as <code>confidence</code></li>
<li><code>class_filter</code> to filter out list of classes</li>
<li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li>
<li><code>fix_batch_size</code></li>
<li><code>iou_threshold</code>: to dictate NMS IoU threshold</li>
<li><code>stroke_width</code>: width of stroke in visualisation</li>
<li><code>max_detections</code>: max detections to return from model</li>
<li><code>max_candidates</code>: max candidates to post-processing from model</li>
<li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,
  <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li>
<li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for
  instance while testing model)</li>
<li><code>source</code> Optional string to set a "source" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li>
<li><code>source_info</code> Optional string to set additional "source_info" attribute on the inference call; e.g. to identify a sub component in an app.</li>
<li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want
to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that
you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be 
registered) - since <code>v0.9.18</code></li>
</ul>
<h3 id="keypoints-detection-model-in-v1-mode">Keypoints detection model in <code>v1</code> mode:<a class="headerlink" href="#keypoints-detection-model-in-v1-mode" title="Permanent link">¶</a></h3>
<ul>
<li><code>visualize_predictions</code>: flag to enable / disable visualisation</li>
<li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li>
<li><code>confidence_threshold</code> as <code>confidence</code></li>
<li><code>keypoint_confidence_threshold</code> as (<code>keypoint_confidence</code>) - to filter out detected keypoints
  based on model confidence</li>
<li><code>class_filter</code> to filter out list of object classes</li>
<li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li>
<li><code>fix_batch_size</code></li>
<li><code>iou_threshold</code>: to dictate NMS IoU threshold</li>
<li><code>stroke_width</code>: width of stroke in visualisation</li>
<li><code>max_detections</code>: max detections to return from model</li>
<li><code>max_candidates</code>: max candidates to post-processing from model</li>
<li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,
  <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li>
<li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for
  instance while testing model)</li>
<li><code>source</code> Optional string to set a "source" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li>
<li><code>source_info</code> Optional string to set additional "source_info" attribute on the inference call; e.g. to identify a sub component in an app.</li>
<li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want
to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that
you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be 
registered) - since <code>v0.9.18</code></li>
</ul>
<h3 id="instance-segmentation-model-in-v1-mode">Instance segmentation model in <code>v1</code> mode:<a class="headerlink" href="#instance-segmentation-model-in-v1-mode" title="Permanent link">¶</a></h3>
<ul>
<li><code>visualize_predictions</code>: flag to enable / disable visualisation</li>
<li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li>
<li><code>confidence_threshold</code> as <code>confidence</code></li>
<li><code>class_filter</code> to filter out list of classes</li>
<li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li>
<li><code>fix_batch_size</code></li>
<li><code>iou_threshold</code>: to dictate NMS IoU threshold</li>
<li><code>stroke_width</code>: width of stroke in visualisation</li>
<li><code>max_detections</code>: max detections to return from model</li>
<li><code>max_candidates</code>: max candidates to post-processing from model</li>
<li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,
  <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li>
<li><code>mask_decode_mode</code></li>
<li><code>tradeoff_factor</code></li>
<li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for
  instance while testing model)</li>
<li><code>source</code> Optional string to set a "source" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li>
<li><code>source_info</code> Optional string to set additional "source_info" attribute on the inference call; e.g. to identify a sub component in an app.</li>
<li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want
to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that
you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be 
registered) - since <code>v0.9.18</code></li>
</ul>
<h3 id="configuration-of-client">Configuration of client<a class="headerlink" href="#configuration-of-client" title="Permanent link">¶</a></h3>
<ul>
<li><code>output_visualisation_format</code>: one of (<code>VisualisationResponseFormat.BASE64</code>, <code>VisualisationResponseFormat.NUMPY</code>,
  <code>VisualisationResponseFormat.PILLOW</code>) - given that server-side visualisation is enabled - one may choose what
  format should be used in output</li>
<li><code>image_extensions_for_directory_scan</code>: while using <code>CLIENT.infer_on_stream(...)</code> with local directory
  this parameter controls type of files (extensions) allowed to be processed -
  default: <code>["jpg", "jpeg", "JPG", "JPEG", "png", "PNG"]</code></li>
<li><code>client_downsizing_disabled</code>: set to <code>False</code> if you want to perform client-side downsizing - default <code>True</code> (
  changed in version <code>0.16.0</code> - previously was <code>False</code>).
  Client-side scaling is only supposed to down-scale (keeping aspect-ratio) the input for inference -
  to utilise internet connection more efficiently (but for the price of images manipulation / transcoding).
  If model registry endpoint is available (mode <code>v1</code>) - model input size information will be used, if not:
  <code>default_max_input_size</code> will be in use.</li>
<li><code>max_concurrent_requests</code> - max number of concurrent requests that can be started </li>
<li><code>max_batch_size</code> - max number of elements that can be injected into single request (in <code>v0</code> mode - API only 
support a single image in payload for the majority of endpoints - hence in this case, value will be overriden with <code>1</code>
to prevent errors)</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The default value for flag <code>client_downsizing_disabled</code> was changed from <code>False</code> to <code>True</code> in release <code>0.16.0</code>!
For clients using models with input size above <code>1024x1024</code> running models on hosted 
platform it should improve predictions quality (as previous default behaviour was causing that input was downsized 
and then artificially upsized on the server side with worse image quality). 
There may be some clients that would like to remain previous settings to potentially improve speed (
when internet connection is a bottleneck and large images are submitted despite small 
model input size). </p>
</div>
<h3 id="configuration-of-workflows-execution">Configuration of Workflows execution<a class="headerlink" href="#configuration-of-workflows-execution" title="Permanent link">¶</a></h3>
<ul>
<li><code>profiling_directory</code>: parameter specify the location where Workflows profiler traces are saved. By default, it is
<code>./inference_profiling</code> directory.</li>
</ul>
<h2 id="faqs">FAQs<a class="headerlink" href="#faqs" title="Permanent link">¶</a></h2>
<h2 id="why-does-the-inference-client-have-two-modes-v0-and-v1">Why does the Inference client have two modes (<code>v0</code> and <code>v1</code>)?<a class="headerlink" href="#why-does-the-inference-client-have-two-modes-v0-and-v1" title="Permanent link">¶</a></h2>
<p>We are constantly improving our <code>infrence</code> package - initial version (<code>v0</code>) is compatible with
models deployed on the Roboflow platform (task types: <code>classification</code>, <code>object-detection</code>, <code>instance-segmentation</code> and
<code>keypoints-detection</code>)
are supported. Version <code>v1</code> is available in locally hosted Docker images with HTTP API.</p>
<p>Locally hosted <code>inference</code> server exposes endpoints for model manipulations, but those endpoints are not available
at the moment for models deployed on the Roboflow platform.</p>
<p><code>api_url</code> parameter passed to <code>InferenceHTTPClient</code> will decide on default client mode - URLs with <code>*.roboflow.com</code>
will be defaulted to version <code>v0</code>.</p>
<p>Usage of model registry control methods with <code>v0</code> clients will raise <code>WrongClientModeError</code>.</p>
</article>
</div>
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
</div>
<button class="md-top md-icon" data-md-component="top" hidden="" type="button">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg>
  Back to top
</button>
</main>
<footer class="md-footer">
<nav aria-label="Footer" class="md-footer__inner md-grid">
<a aria-label="Previous: Inference CLI" class="md-footer__link md-footer__link--prev" href="../inference_cli/">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</div>
<div class="md-footer__title">
<span class="md-footer__direction">
                Previous
              </span>
<div class="md-ellipsis">
                Inference CLI
              </div>
</div>
</a>
<a aria-label="Next: Environmental variables" class="md-footer__link md-footer__link--next" href="../../server_configuration/environmental_variables/">
<div class="md-footer__title">
<span class="md-footer__direction">
                Next
              </span>
<div class="md-ellipsis">
                Environmental variables
              </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
<div class="md-copyright__highlight">
      Roboflow 2024. All rights reserved.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
<div class="md-social">
<a class="md-social__link" href="https://github.com/roboflow" rel="noopener" target="_blank" title="github.com">
<svg viewbox="0 0 496 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
</a>
<a class="md-social__link" href="https://www.youtube.com/roboflow" rel="noopener" target="_blank" title="www.youtube.com">
<svg viewbox="0 0 576 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg>
</a>
<a class="md-social__link" href="https://www.linkedin.com/company/roboflow-ai/mycompany/" rel="noopener" target="_blank" title="www.linkedin.com">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg>
</a>
<a class="md-social__link" href="https://twitter.com/roboflow" rel="noopener" target="_blank" title="twitter.com">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../..", "features": ["navigation.top", "navigation.tabs", "navigation.tabs.sticky", "navigation.prune", "navigation.footer", "navigation.tracking", "navigation.indexes", "navigation.sections", "content.code.copy"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"default": 1.0}}</script>
<script src="../../assets/javascripts/bundle.3220b9d7.min.js"></script>
<script src="https://widget.kapa.ai/kapa-widget.bundle.js"></script>
<script src="../../javascript/init_kapa_widget.js"></script>
<script src="../../javascript/cookbooks.js"></script>
<script>document$.subscribe(() => {
            window.update_swagger_ui_iframe_height = function (id) {
                var iFrameID = document.getElementById(id);
                if (iFrameID) {
                    full_height = (iFrameID.contentWindow.document.body.scrollHeight + 80) + "px";
                    iFrameID.height = full_height;
                    iFrameID.style.height = full_height;
                }
            }
        
            let iframe_id_list = []
            var iframes = document.getElementsByClassName("swagger-ui-iframe");
            for (var i = 0; i < iframes.length; i++) { 
                iframe_id_list.push(iframes[i].getAttribute("id"))
            }
        
            let ticking = true;
            
            document.addEventListener('scroll', function(e) {
                if (!ticking) {
                    window.requestAnimationFrame(()=> {
                        let half_vh = window.innerHeight/2;
                        for(var i = 0; i < iframe_id_list.length; i++) {
                            let element = document.getElementById(iframe_id_list[i])
                            if(element==null){
                                return
                            }
                            let diff = element.getBoundingClientRect().top
                            if(element.contentWindow.update_top_val){
                                element.contentWindow.update_top_val(half_vh - diff)
                            }
                        }
                        ticking = false;
                    });
                    ticking = true;
                }
            });
        
            const dark_scheme_name = "slate"
            
            window.scheme = document.body.getAttribute("data-md-color-scheme")
            const options = {
                attributeFilter: ['data-md-color-scheme'],
            };
            function color_scheme_callback(mutations) {
                for (let mutation of mutations) {
                    if (mutation.attributeName === "data-md-color-scheme") {
                        scheme = document.body.getAttribute("data-md-color-scheme")
                        var iframe_list = document.getElementsByClassName("swagger-ui-iframe")
                        for(var i = 0; i < iframe_list.length; i++) {
                            var ele = iframe_list.item(i);
                            if (ele) {
                                if (scheme === dark_scheme_name) {
                                    ele.contentWindow.enable_dark_mode();
                                } else {
                                    ele.contentWindow.disable_dark_mode();
                                }
                            }
                        }
                    }
                }
            }
            observer = new MutationObserver(color_scheme_callback);
            observer.observe(document.body, options);
            })</script></body>
</html>