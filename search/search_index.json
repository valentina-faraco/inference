{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is Inference?","text":"<p>Roboflow Inference is an open-source platform designed to simplify the deployment of computer vision models. It enables developers to perform object detection, classification, instance segmentation and keypoint detection, and utilize foundation models like CLIP, Segment Anything, and YOLO-World through a Python-native package, a self-hosted inference server, or a fully managed API.</p> <p>Explore our enterprise options for advanced features like server deployment, active learning, and commercial licenses for YOLOv5 and YOLOv8.</p> <p>Get started with our \"Run your first model\" guide</p> Learn about the various ways you can use Inference See all of the models you can run with Inference <p>Here is an example of a model running on a video using Inference:</p>"},{"location":"#install","title":"\ud83d\udcbb install","text":"<p>Inference package requires Python&gt;=3.8,&lt;=3.11. Click here to learn more about running Inference inside Docker.</p> <pre><code>pip install inference\n</code></pre> \ud83d\udc49 running on a GPU    To enhance model performance in GPU-accelerated environments, install CUDA-compatible dependencies instead:    <pre><code>pip install inference-gpu\n</code></pre> \ud83d\udc49 advanced models    Inference supports multiple model types for specialized tasks. From Grounding DINO for identifying objects with a text prompt, to DocTR for OCR, to CogVLM for asking questions about images - you can find out more in the Foundation Models page.        Note that <code>inference</code> and <code>inference-gpu</code> packages install only the minimal shared dependencies. Instead, install model-specific dependencies to ensure code compatibility and license compliance.        The <code>inference</code> and <code>inference-gpu</code> packages install only the minimal shared dependencies. Install model-specific dependencies to ensure code compatibility and license compliance. Learn more about the models supported by Inference.    <pre><code>pip install inference[yolo-world]\n</code></pre>"},{"location":"#quickstart","title":"\ud83d\udd25 quickstart","text":"<p>Use Inference SDK to run models locally with just a few lines of code. The image input can be a URL, a numpy array, or a PIL image.</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\n\nresults = model.infer(\"https://media.roboflow.com/inference/people-walking.jpg\")\n</code></pre> \ud83d\udc49 roboflow models   Set up your <code>ROBOFLOW_API_KEY</code> to access thousands of fine-tuned models shared by the Roboflow Universe community and your custom model. Navigate to \ud83d\udd11 keys section to learn more.  <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"soccer-players-5fuqs/1\")\n\nresults = model.infer(\n    image=\"https://media.roboflow.com/inference/soccer.jpg\",\n    confidence=0.5,\n    iou_threshold=0.5\n)\n</code></pre> \ud83d\udc49 foundational models  - CLIP Embeddings - generate text and image embeddings that you can use for zero-shot classification or assessing image similarity.    <pre><code>from inference.models import Clip\n\nmodel = Clip()\n\nembeddings_text = clip.embed_text(\"a football match\")\nembeddings_image = model.embed_image(\"https://media.roboflow.com/inference/soccer.jpg\")\n</code></pre>  - Segment Anything - segment all objects visible in the image or only those associated with selected points or boxes.    <pre><code>from inference.models import SegmentAnything\n\nmodel = SegmentAnything()\n\nresult = model.segment_image(\"https://media.roboflow.com/inference/soccer.jpg\")\n</code></pre>  - YOLO-World - an almost real-time zero-shot detector that enables the detection of any objects without any training.    <pre><code>from inference.models import YOLOWorld\n\nmodel = YOLOWorld(model_id=\"yolo_world/l\")\n\nresult = model.infer(\n    image=\"https://media.roboflow.com/inference/dog.jpeg\",\n    text=[\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"],\n    confidence=0.03\n)\n</code></pre>"},{"location":"#inference-server","title":"\ud83d\udcdf inference server","text":"<p>You can also run Inference as a microservice with Docker.</p>"},{"location":"#deploy-server","title":"deploy server","text":"<p>The inference server is distributed via Docker. Behind the scenes, inference will download and run the image that is appropriate for your hardware. Here, you can learn more about the supported images.</p> <pre><code>inference server start\n</code></pre>"},{"location":"#run-client","title":"run client","text":"<p>Consume inference server predictions using the HTTP client available in the Inference SDK.</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=&lt;ROBOFLOW_API_KEY&gt;\n)\nwith client.use_model(model_id=\"soccer-players-5fuqs/1\"):\n    predictions = client.infer(\"https://media.roboflow.com/inference/soccer.jpg\")\n</code></pre> <p>If you're using the hosted API, change the local API URL to <code>https://detect.roboflow.com</code>. Accessing the hosted inference server and/or using any of the fine-tuned models require a <code>ROBOFLOW_API_KEY</code>. For further information, visit the \ud83d\udd11 keys section.</p>"},{"location":"#inference-pipeline","title":"\ud83c\udfa5 inference pipeline","text":"<p>The inference pipeline is an efficient method for processing static video files and streams. Select a model, define the video source, and set a callback action. You can choose from predefined callbacks that allow you to display results on the screen or save them to a file.</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\",\n    video_reference=\"https://media.roboflow.com/inference/people-walking.mp4\",\n    on_prediction=render_boxes\n)\n\npipeline.start()\npipeline.join()\n</code></pre>"},{"location":"#keys","title":"\ud83d\udd11 keys","text":"<p>Inference enables the deployment of a wide range of pre-trained and foundational models without an API key. To access thousands of fine-tuned models shared by the Roboflow Universe community, configure your API key.</p> <pre><code>export ROBOFLOW_API_KEY=&lt;YOUR_API_KEY&gt;\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udcda documentation","text":"<p>Visit our documentation to explore comprehensive guides, detailed API references, and a wide array of tutorials designed to help you harness the full potential of the Inference package.</p>"},{"location":"#license","title":"\u00a9 license","text":"<p>The Roboflow Inference code is distributed under the Apache 2.0 license. However, each supported model is subject to its licensing. Detailed information on each model's license can be found here.</p>"},{"location":"#extras","title":"\u26a1\ufe0f extras","text":"<p>Below you can find list of extras available for <code>inference</code> and <code>inference-gpu</code></p> Name Description Notes <code>clip</code> CLIP model <code>N/A</code> <code>gaze</code> L2CS-Net model <code>N/A</code> <code>grounding-dino</code> Grounding Dino model <code>N/A</code> <code>sam</code> SAM and SAM2 models The extras depend on <code>rasterio</code> which require <code>GDAL</code> library to work. If the installation fails with <code>gdal-config</code> command error - run <code>sudo apt-get install libgdal-dev</code> for Linux or follow official installation guide <code>yolo-world</code> Yolo-World model <code>N/A</code> <code>transformers</code> <code>transformers</code> based models, like Florence-2 <code>N/A</code> Installing extras <p>To install specific extras you need to run</p> <p><pre><code>pip install inferenence[extras-name]\n</code></pre> or </p> <pre><code>pip install inferenence-gpu[extras-name]\n</code></pre>"},{"location":"api/","title":"Api","text":"<p>The Roboflow Inference Server provides OpenAPI documentation at the <code>/docs</code> endpoint for use in development.</p> <p>Below is the OpenAPI specification for the Inference Server, rendered with Swagger.</p> <p></p>"},{"location":"contributing/","title":"Contributing to the Roboflow Inference Server \ud83d\udee0\ufe0f","text":"<p>Thank you for your interest in contributing to Inference.</p> <p>We welcome any contributions to help us improve the quality of Inference.</p> <p>Note</p> <p>Interested in seeing a new model in Inference? File a Feature Request on GitHub.</p>"},{"location":"contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>We welcome contributions to:</p> <ol> <li>Add support for running inference on a new model.</li> <li>Report bugs and issues in the project.</li> <li>Submit a request for a new task or feature.</li> <li>Improve our test coverage.</li> </ol>"},{"location":"contributing/#contributing-features","title":"Contributing Features","text":"<p>The Inference Server provides a standard interface through which you can work with computer vision models. With Inference Server, you can use state-of-the-art models with your own weights without having to spend time installing dependencies, configuring environments, and writing inference code.</p> <p>We welcome contributions that add support for new models to the project. Before you begin, please make sure that another contributor has not already begun work on the model you want to add. You can check the project README for our roadmap on adding more models.</p> <p>You will need to add documentation for your model and link to it from the <code>inference-server</code> README. You can add a new page to the <code>docs/models</code> directory that describes your model and how to use it. You can use the existing model documentation as a guide for how to structure your documentation.</p>"},{"location":"contributing/#how-to-contribute-changes","title":"How to Contribute Changes","text":"<p>First, fork this repository to your own GitHub account. Create a new branch that describes your changes (i.e. <code>line-counter-docs</code>). Push your changes to the branch on your fork and then submit a pull request to this repository.</p> <p>When creating new functions, please ensure you have the following:</p> <ol> <li>Docstrings for the function and all parameters.</li> <li>Examples in the documentation for the function.</li> <li>Created an entry in our docs to autogenerate the documentation for the function.</li> </ol> <p>All pull requests will be reviewed by the maintainers of the project. We will provide feedback and ask for changes if necessary.</p> <p>PRs must pass all tests and linting requirements before they can be merged.</p>"},{"location":"contributing/#code-quality","title":"\ud83e\uddf9 Code quality","text":"<p>We provide two handy commands inside the <code>Makefile</code>, namely:</p> <ul> <li><code>make style</code> to format the code</li> <li><code>make check_code_quality</code> to check code quality (PEP8 basically)</li> </ul>"},{"location":"contributing/#tests","title":"\ud83e\uddea Tests","text":"<p><code>pytests</code> is used to run our tests.</p>"},{"location":"cookbooks/","title":"Cookbooks","text":"Inference Cookbooks <p>Read our getting started guides that show how to solve specific problems using Inference.</p> <p></p> <p></p> <p></p>"},{"location":"models/","title":"Models","text":"<p>Roboflow Inference enables you to deploy computer vision models faster than ever.</p> <p>With a <code>pip install inference</code> and <code>inference server start</code>, you can start a server to run a fine-tuned model on images, videos, and streams.</p> <p>Inference supports running object detection, classification, instance segmentation, and foundation models (i.e. SAM, CLIP).</p> <p>You can train and deploy your own custom model or use one of the 50,000+ fine-tuned models shared by the Roboflow Universe community.</p> <p>You can run Inference on an edge device like an NVIDIA Jetson, or on cloud computing platforms like AWS, GCP, and Azure.</p> <p>Get started with our \"Run your first model\" guide</p> <p>Here is an example of a model running on a video using Inference:</p>"},{"location":"models/#features","title":"\ud83d\udcbb Features","text":"<p>Inference provides a scalable method through which you can use computer vision models.</p> <p>Inference is backed by:</p> <ul> <li> <p>A server, so you don\u2019t have to reinvent the wheel when it comes to serving your model to disperate parts of your application.</p> </li> <li> <p>Standard APIs for computer vision tasks, so switching out the model weights and architecture can be done independently of your application code.</p> </li> <li> <p>Model architecture implementations, which implement the tensor parsing glue between images and predictions for supervised models that you've fine-tuned to perform custom tasks.</p> </li> <li> <p>A model registry, so your code can be independent from your model weights &amp; you don't have to re-build and re-deploy every time you want to iterate on your model weights.</p> </li> <li> <p>Data management integrations, so you can collect more images of edge cases to improve your dataset &amp; model the more it sees in the wild.</p> </li> </ul> <p>And more!</p>"},{"location":"models/#install-pip-vs-docker","title":"\ud83d\udccc Install pip vs Docker:","text":"<ul> <li>pip: Installs <code>inference</code> into your Python environment. Lightweight, good for Python-centric projects.</li> <li>Docker: Packages <code>inference</code> with its environment. Ensures consistency across setups; ideal for scalable deployments.</li> </ul>"},{"location":"models/#install","title":"\ud83d\udcbb install","text":""},{"location":"models/#with-onnx-cpu-runtime","title":"With ONNX CPU Runtime:","text":"<p>For CPU powered inference:</p> <pre><code>pip install inference\n</code></pre> <p>or</p> <pre><code>pip install inference-cpu\n</code></pre>"},{"location":"models/#with-onnx-gpu-runtime","title":"With ONNX GPU Runtime:","text":"<p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"models/#without-onnx-runtime","title":"Without ONNX Runtime:","text":"<p>Roboflow Inference uses Onnxruntime as its core inference engine. Onnxruntime provides an array of different execution providers that can optimize inference on differnt target devices. If you decide to install onnxruntime on your own, install inference with:</p> <pre><code>pip install inference-core\n</code></pre> <p>Alternatively, you can take advantage of some advanced execution providers using one of our published docker images.</p>"},{"location":"models/#extras","title":"Extras:","text":"<p>Some functionality requires extra dependencies. These can be installed by specifying the desired extras during installation of Roboflow Inference. e.x. <code>pip install inference[extra]</code></p> extra description <code>clip</code> Ability to use the core <code>CLIP</code> model (by OpenAI) <code>gaze</code> Ability to use the core <code>Gaze</code> model <code>http</code> Ability to run the http interface <code>sam</code> Ability to run the core <code>Segment Anything</code> model (by Meta AI) <code>doctr</code> Ability to use the core <code>doctr</code> model (by Mindee) <code>transformers</code> Ability to use transformers based multi-modal models such as <code>Florence2</code> and <code>PaliGemma</code>. To use Florence2 you will need to manually install flash_attn <p>Note: Both CLIP and Segment Anything require PyTorch to run. These are included in their respective dependencies however PyTorch installs can be highly environment dependent. See the official PyTorch install page for instructions specific to your enviornment.</p> <p>Example install with CLIP dependencies:</p> <pre><code>pip install inference[clip]\n</code></pre>"},{"location":"models/#docker","title":"\ud83d\udc0b docker","text":"<p>You can learn more about Roboflow Inference Docker Image build, pull and run in our documentation.</p> <ul> <li>Run on x86 CPU:</li> </ul> <pre><code>docker run -it --net=host roboflow/roboflow-inference-server-cpu:latest\n</code></pre> <ul> <li>Run on NVIDIA GPU:</li> </ul> <pre><code>docker run -it --network=host --gpus=all roboflow/roboflow-inference-server-gpu:latest\n</code></pre> \ud83d\udc49 more docker run options  - Run on arm64 CPU:  <pre><code>docker run -p 9001:9001 roboflow/roboflow-inference-server-arm-cpu:latest\n</code></pre>  - Run on NVIDIA Jetson with JetPack `4.x` (Deprecated):  <pre><code>docker run --privileged --net=host --runtime=nvidia roboflow/roboflow-inference-server-jetson:latest\n</code></pre>  - Run on NVIDIA Jetson with JetPack `5.x`:  <pre><code>docker run --privileged --net=host --runtime=nvidia roboflow/roboflow-inference-server-jetson-5.1.1:latest\n</code></pre>  - Run on NVIDIA Jetson with JetPack `6.x`:  <pre><code>docker run --privileged --net=host --runtime=nvidia roboflow/roboflow-inference-server-jetson-6.0.0:latest\n</code></pre> <p></p>"},{"location":"models/#cli","title":"\ud83d\udcdf CLI","text":"<p>To use the CLI you will need python 3.7 or higher. To ensure you have the correct version of python, run <code>python --version</code> in your terminal. To install python, follow the instructions here.</p> <p>After you have python installed, install the pypi package <code>inference-cli</code> or <code>inference</code>:</p> <pre><code>pip install inference-cli\n</code></pre> <p>From there you can run the inference server. See Docker quickstart via CLI for more information.</p> <pre><code>inference server start\n</code></pre> <p>CLI supports also stopping the server via: <pre><code>inference server stop\n</code></pre></p> <p>To use the CLI to make inferences, first find your project ID and model version number in Roboflow.</p> <p>See more detailed documentation on HTTP Inference quickstart via CLI.</p> <pre><code>inference infer {image_path} \\\n    --project-id {project_id} \\\n    --model-version {model_version} \\\n    --api-key {api_key}\n</code></pre>"},{"location":"models/#enterprise-license","title":"Enterprise License","text":"<p>With a Roboflow Inference Enterprise License, you can access additional Inference features, including:</p> <ul> <li>Server cluster deployment</li> <li>Active learning</li> <li>YOLOv5 and YOLOv8 model sub-license</li> </ul> <p>To learn more, contact the Roboflow team.</p>"},{"location":"models/#more-roboflow-open-source-projects","title":"More Roboflow Open Source Projects","text":"Project Description supervision General-purpose utilities for use in computer vision projects, from predictions filtering and display to object tracking to model evaluation. Autodistill Automatically label images for use in training computer vision models. Inference (this project) An easy-to-use, production-ready inference server for computer vision supporting deployment of many popular model architectures and fine-tuned models. Notebooks Tutorials for computer vision tasks, from training state-of-the-art models to tracking objects to counting objects in a zone. Collect Automated, intelligent data collection powered by CLIP."},{"location":"docs/reference/nav/","title":"Nav","text":"<ul> <li>inference<ul> <li>core<ul> <li>active_learning<ul> <li>accounting</li> <li>batching</li> <li>cache_operations</li> <li>configuration</li> <li>core</li> <li>entities</li> <li>middlewares</li> <li>post_processing</li> <li>samplers<ul> <li>close_to_threshold</li> <li>contains_classes</li> <li>number_of_detections</li> <li>random</li> </ul> </li> <li>utils</li> </ul> </li> <li>cache<ul> <li>base</li> <li>memory</li> <li>model_artifacts</li> <li>redis</li> <li>serializers</li> </ul> </li> <li>constants</li> <li>devices<ul> <li>utils</li> </ul> </li> <li>entities<ul> <li>common</li> <li>requests<ul> <li>clip</li> <li>cogvlm</li> <li>doctr</li> <li>dynamic_class_base</li> <li>gaze</li> <li>groundingdino</li> <li>inference</li> <li>owlv2</li> <li>sam</li> <li>sam2</li> <li>server_state</li> <li>trocr</li> <li>workflows</li> <li>yolo_world</li> </ul> </li> <li>responses<ul> <li>clip</li> <li>cogvlm</li> <li>gaze</li> <li>groundingdino</li> <li>inference</li> <li>notebooks</li> <li>ocr</li> <li>sam</li> <li>sam2</li> <li>server_state</li> <li>workflows</li> </ul> </li> <li>types</li> </ul> </li> <li>env</li> <li>exceptions</li> <li>interfaces<ul> <li>base</li> <li>camera<ul> <li>camera</li> <li>entities</li> <li>exceptions</li> <li>utils</li> <li>video_source</li> </ul> </li> <li>http<ul> <li>handlers<ul> <li>workflows</li> </ul> </li> <li>http_api</li> <li>middlewares<ul> <li>gzip</li> </ul> </li> <li>orjson_utils</li> </ul> </li> <li>stream<ul> <li>entities</li> <li>inference_pipeline</li> <li>model_handlers<ul> <li>roboflow_models</li> <li>workflows</li> <li>yolo_world</li> </ul> </li> <li>sinks</li> <li>stream</li> <li>utils</li> <li>watchdog</li> </ul> </li> <li>stream_manager<ul> <li>api<ul> <li>entities</li> <li>errors</li> <li>stream_manager_client</li> </ul> </li> <li>manager_app<ul> <li>app</li> <li>communication</li> <li>entities</li> <li>errors</li> <li>inference_pipeline_manager</li> <li>serialisation</li> <li>tcp_server</li> <li>webrtc</li> </ul> </li> </ul> </li> <li>udp<ul> <li>udp_stream</li> </ul> </li> </ul> </li> <li>logger</li> <li>managers<ul> <li>active_learning</li> <li>base</li> <li>decorators<ul> <li>base</li> <li>fixed_size_cache</li> <li>locked_load</li> <li>logger</li> </ul> </li> <li>entities</li> <li>metrics</li> <li>pingback</li> <li>prometheus</li> <li>stub_loader</li> </ul> </li> <li>models<ul> <li>base</li> <li>classification_base</li> <li>defaults</li> <li>instance_segmentation_base</li> <li>keypoints_detection_base</li> <li>object_detection_base</li> <li>roboflow</li> <li>stubs</li> <li>types</li> <li>utils<ul> <li>batching</li> <li>keypoints</li> <li>onnx</li> <li>validate</li> </ul> </li> </ul> </li> <li>nms</li> <li>registries<ul> <li>base</li> <li>roboflow</li> </ul> </li> <li>roboflow_api</li> <li>usage</li> <li>utils<ul> <li>async_utils</li> <li>container</li> <li>drawing</li> <li>environment</li> <li>file_system</li> <li>function</li> <li>hash</li> <li>image_utils</li> <li>notebooks</li> <li>onnx</li> <li>postprocess</li> <li>preprocess</li> <li>requests</li> <li>roboflow</li> <li>s3</li> <li>sqlite_wrapper</li> <li>url_utils</li> <li>visualisation</li> </ul> </li> <li>version</li> <li>warnings</li> <li>workflows<ul> <li>core_steps<ul> <li>analytics<ul> <li>data_aggregator<ul> <li>v1</li> </ul> </li> <li>line_counter<ul> <li>v1</li> <li>v2</li> </ul> </li> <li>path_deviation<ul> <li>v1</li> <li>v2</li> </ul> </li> <li>time_in_zone<ul> <li>v1</li> <li>v2</li> </ul> </li> </ul> </li> <li>classical_cv<ul> <li>camera_focus<ul> <li>v1</li> </ul> </li> <li>contours<ul> <li>v1</li> </ul> </li> <li>convert_grayscale<ul> <li>v1</li> </ul> </li> <li>distance_measurement<ul> <li>v1</li> </ul> </li> <li>dominant_color<ul> <li>v1</li> </ul> </li> <li>image_blur<ul> <li>v1</li> </ul> </li> <li>image_preprocessing<ul> <li>v1</li> </ul> </li> <li>pixel_color_count<ul> <li>v1</li> </ul> </li> <li>sift<ul> <li>v1</li> </ul> </li> <li>sift_comparison<ul> <li>v1</li> <li>v2</li> </ul> </li> <li>size_measurement<ul> <li>v1</li> </ul> </li> <li>template_matching<ul> <li>v1</li> </ul> </li> <li>threshold<ul> <li>v1</li> </ul> </li> </ul> </li> <li>common<ul> <li>entities</li> <li>operators</li> <li>query_language<ul> <li>entities<ul> <li>enums</li> <li>introspection</li> <li>operations</li> <li>types</li> </ul> </li> <li>errors</li> <li>evaluation_engine<ul> <li>core</li> <li>detection<ul> <li>geometry</li> </ul> </li> </ul> </li> <li>introspection<ul> <li>core</li> </ul> </li> <li>operations<ul> <li>booleans<ul> <li>base</li> </ul> </li> <li>classification_results<ul> <li>base</li> </ul> </li> <li>core</li> <li>detection<ul> <li>base</li> </ul> </li> <li>detections<ul> <li>base</li> </ul> </li> <li>dictionaries<ul> <li>base</li> </ul> </li> <li>generic<ul> <li>base</li> </ul> </li> <li>images<ul> <li>base</li> </ul> </li> <li>numbers<ul> <li>base</li> </ul> </li> <li>sequences<ul> <li>base</li> </ul> </li> <li>strings<ul> <li>base</li> </ul> </li> <li>utils</li> </ul> </li> </ul> </li> <li>serializers</li> <li>utils</li> <li>vlms</li> </ul> </li> <li>flow_control<ul> <li>continue_if<ul> <li>v1</li> </ul> </li> <li>rate_limiter<ul> <li>v1</li> </ul> </li> </ul> </li> <li>formatters<ul> <li>csv<ul> <li>v1</li> </ul> </li> <li>expression<ul> <li>v1</li> </ul> </li> <li>first_non_empty_or_default<ul> <li>v1</li> </ul> </li> <li>json_parser<ul> <li>v1</li> </ul> </li> <li>property_definition<ul> <li>v1</li> </ul> </li> <li>vlm_as_classifier<ul> <li>v1</li> </ul> </li> <li>vlm_as_detector<ul> <li>v1</li> </ul> </li> </ul> </li> <li>fusion<ul> <li>detections_classes_replacement<ul> <li>v1</li> </ul> </li> <li>detections_consensus<ul> <li>v1</li> </ul> </li> <li>detections_stitch<ul> <li>v1</li> </ul> </li> <li>dimension_collapse<ul> <li>v1</li> </ul> </li> </ul> </li> <li>loader</li> <li>models<ul> <li>foundation<ul> <li>anthropic_claude<ul> <li>v1</li> </ul> </li> <li>clip_comparison<ul> <li>v1</li> <li>v2</li> </ul> </li> <li>cog_vlm<ul> <li>v1</li> </ul> </li> <li>florence2<ul> <li>v1</li> </ul> </li> <li>google_gemini<ul> <li>v1</li> </ul> </li> <li>google_vision_ocr<ul> <li>v1</li> </ul> </li> <li>lmm<ul> <li>v1</li> </ul> </li> <li>lmm_classifier<ul> <li>v1</li> </ul> </li> <li>ocr<ul> <li>v1</li> </ul> </li> <li>openai<ul> <li>v1</li> <li>v2</li> </ul> </li> <li>segment_anything2<ul> <li>v1</li> </ul> </li> <li>stability_ai<ul> <li>inpainting<ul> <li>v1</li> </ul> </li> </ul> </li> <li>yolo_world<ul> <li>v1</li> </ul> </li> </ul> </li> <li>roboflow<ul> <li>instance_segmentation<ul> <li>v1</li> </ul> </li> <li>keypoint_detection<ul> <li>v1</li> </ul> </li> <li>multi_class_classification<ul> <li>v1</li> </ul> </li> <li>multi_label_classification<ul> <li>v1</li> </ul> </li> <li>object_detection<ul> <li>v1</li> </ul> </li> </ul> </li> <li>third_party<ul> <li>barcode_detection<ul> <li>v1</li> </ul> </li> <li>qr_code_detection<ul> <li>v1</li> </ul> </li> </ul> </li> </ul> </li> <li>sinks<ul> <li>email_notification<ul> <li>v1</li> </ul> </li> <li>local_file<ul> <li>v1</li> </ul> </li> <li>roboflow<ul> <li>custom_metadata<ul> <li>v1</li> </ul> </li> <li>dataset_upload<ul> <li>v1</li> <li>v2</li> </ul> </li> </ul> </li> <li>webhook<ul> <li>v1</li> </ul> </li> </ul> </li> <li>transformations<ul> <li>absolute_static_crop<ul> <li>v1</li> </ul> </li> <li>bounding_rect<ul> <li>v1</li> </ul> </li> <li>byte_tracker<ul> <li>v1</li> <li>v2</li> <li>v3</li> </ul> </li> <li>detection_offset<ul> <li>v1</li> </ul> </li> <li>detections_filter<ul> <li>v1</li> </ul> </li> <li>detections_transformation<ul> <li>v1</li> </ul> </li> <li>dynamic_crop<ul> <li>v1</li> </ul> </li> <li>dynamic_zones<ul> <li>v1</li> </ul> </li> <li>image_slicer<ul> <li>v1</li> </ul> </li> <li>perspective_correction<ul> <li>v1</li> </ul> </li> <li>relative_static_crop<ul> <li>v1</li> </ul> </li> <li>stabilize_detections<ul> <li>v1</li> </ul> </li> <li>stitch_images<ul> <li>v1</li> </ul> </li> <li>stitch_ocr_detections<ul> <li>v1</li> </ul> </li> </ul> </li> <li>visualizations<ul> <li>background_color<ul> <li>v1</li> </ul> </li> <li>blur<ul> <li>v1</li> </ul> </li> <li>bounding_box<ul> <li>v1</li> </ul> </li> <li>circle<ul> <li>v1</li> </ul> </li> <li>color<ul> <li>v1</li> </ul> </li> <li>common<ul> <li>annotators<ul> <li>background_color</li> <li>halo</li> <li>model_comparison</li> <li>polygon</li> </ul> </li> <li>base</li> <li>base_colorable</li> <li>utils</li> </ul> </li> <li>corner<ul> <li>v1</li> </ul> </li> <li>crop<ul> <li>v1</li> </ul> </li> <li>dot<ul> <li>v1</li> </ul> </li> <li>ellipse<ul> <li>v1</li> </ul> </li> <li>halo<ul> <li>v1</li> </ul> </li> <li>keypoint<ul> <li>v1</li> </ul> </li> <li>label<ul> <li>v1</li> </ul> </li> <li>line_zone<ul> <li>v1</li> </ul> </li> <li>mask<ul> <li>v1</li> </ul> </li> <li>model_comparison<ul> <li>v1</li> </ul> </li> <li>pixelate<ul> <li>v1</li> </ul> </li> <li>polygon<ul> <li>v1</li> </ul> </li> <li>polygon_zone<ul> <li>v1</li> </ul> </li> <li>reference_path<ul> <li>v1</li> </ul> </li> <li>trace<ul> <li>v1</li> </ul> </li> <li>triangle<ul> <li>v1</li> </ul> </li> </ul> </li> </ul> </li> <li>errors</li> <li>execution_engine<ul> <li>constants</li> <li>core</li> <li>entities<ul> <li>base</li> <li>engine</li> <li>types</li> </ul> </li> <li>introspection<ul> <li>blocks_loader</li> <li>connections_discovery</li> <li>entities</li> <li>schema_parser</li> <li>selectors_parser</li> <li>utils</li> </ul> </li> <li>profiling<ul> <li>core</li> </ul> </li> <li>v1<ul> <li>compiler<ul> <li>cache</li> <li>core</li> <li>entities</li> <li>graph_constructor</li> <li>graph_traversal</li> <li>reference_type_checker</li> <li>steps_initialiser</li> <li>syntactic_parser</li> <li>utils</li> <li>validator</li> </ul> </li> <li>core</li> <li>debugger<ul> <li>core</li> </ul> </li> <li>dynamic_blocks<ul> <li>block_assembler</li> <li>block_scaffolding</li> <li>entities</li> </ul> </li> <li>entities</li> <li>executor<ul> <li>core</li> <li>execution_data_manager<ul> <li>branching_manager</li> <li>dynamic_batches_manager</li> <li>execution_cache</li> <li>manager</li> <li>step_input_assembler</li> </ul> </li> <li>flow_coordinator</li> <li>output_constructor</li> <li>runtime_input_assembler</li> <li>runtime_input_validator</li> <li>utils</li> </ul> </li> <li>introspection<ul> <li>inputs_discovery</li> <li>kinds_schemas</li> <li>kinds_schemas_register</li> <li>outputs_discovery</li> <li>types_discovery</li> </ul> </li> </ul> </li> </ul> </li> <li>prototypes<ul> <li>block</li> </ul> </li> </ul> </li> </ul> </li> <li>enterprise<ul> <li>parallel<ul> <li>dispatch_manager</li> <li>entrypoint</li> <li>infer</li> <li>parallel_http_api</li> <li>parallel_http_config</li> <li>tasks</li> <li>utils</li> </ul> </li> <li>stream_management<ul> <li>api<ul> <li>app</li> <li>entities</li> <li>errors</li> <li>stream_manager_client</li> </ul> </li> <li>manager<ul> <li>app</li> <li>communication</li> <li>entities</li> <li>errors</li> <li>inference_pipeline_manager</li> <li>serialisation</li> <li>tcp_server</li> </ul> </li> </ul> </li> </ul> </li> <li>models<ul> <li>aliases</li> <li>clip<ul> <li>clip_model</li> </ul> </li> <li>cogvlm<ul> <li>cogvlm</li> </ul> </li> <li>doctr<ul> <li>doctr_model</li> </ul> </li> <li>florence2<ul> <li>florence2</li> <li>utils</li> </ul> </li> <li>gaze<ul> <li>gaze</li> <li>l2cs</li> </ul> </li> <li>grounding_dino<ul> <li>grounding_dino</li> </ul> </li> <li>owlv2<ul> <li>owlv2</li> </ul> </li> <li>paligemma<ul> <li>paligemma</li> </ul> </li> <li>sam<ul> <li>segment_anything</li> </ul> </li> <li>sam2<ul> <li>segment_anything2</li> </ul> </li> <li>transformers<ul> <li>transformers</li> </ul> </li> <li>trocr<ul> <li>trocr</li> </ul> </li> <li>utils</li> <li>vit<ul> <li>vit_classification</li> </ul> </li> <li>yolact<ul> <li>yolact_instance_segmentation</li> </ul> </li> <li>yolo_world<ul> <li>yolo_world</li> </ul> </li> <li>yolonas<ul> <li>yolonas_object_detection</li> </ul> </li> <li>yolov10<ul> <li>yolov10_object_detection</li> </ul> </li> <li>yolov11<ul> <li>yolov11_instance_segmentation</li> <li>yolov11_keypoints_detection</li> <li>yolov11_object_detection</li> </ul> </li> <li>yolov5<ul> <li>yolov5_instance_segmentation</li> <li>yolov5_object_detection</li> </ul> </li> <li>yolov7<ul> <li>yolov7_instance_segmentation</li> </ul> </li> <li>yolov8<ul> <li>yolov8_classification</li> <li>yolov8_instance_segmentation</li> <li>yolov8_keypoints_detection</li> <li>yolov8_object_detection</li> </ul> </li> <li>yolov9<ul> <li>yolov9_object_detection</li> </ul> </li> </ul> </li> <li>usage_tracking<ul> <li>collector</li> <li>config</li> <li>payload_helpers</li> <li>plan_details</li> <li>redis_queue</li> <li>sqlite_queue</li> <li>utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/reference/inference/core/constants/","title":"constants","text":""},{"location":"docs/reference/inference/core/env/","title":"env","text":""},{"location":"docs/reference/inference/core/exceptions/","title":"exceptions","text":""},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.ContentTypeInvalid","title":"<code>ContentTypeInvalid</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the content type is invalid.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class ContentTypeInvalid(Exception):\n    \"\"\"Raised when the content type is invalid.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.ContentTypeMissing","title":"<code>ContentTypeMissing</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the content type is missing.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class ContentTypeMissing(Exception):\n    \"\"\"Raised when the content type is missing.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.EngineIgnitionFailure","title":"<code>EngineIgnitionFailure</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the engine fails to ignite.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class EngineIgnitionFailure(Exception):\n    \"\"\"Raised when the engine fails to ignite.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.InferenceModelNotFound","title":"<code>InferenceModelNotFound</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the inference model is not found.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InferenceModelNotFound(Exception):\n    \"\"\"Raised when the inference model is not found.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.InvalidEnvironmentVariableError","title":"<code>InvalidEnvironmentVariableError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an environment variable is invalid.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InvalidEnvironmentVariableError(Exception):\n    \"\"\"Raised when an environment variable is invalid.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.InvalidMaskDecodeArgument","title":"<code>InvalidMaskDecodeArgument</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an invalid argument is provided for mask decoding.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InvalidMaskDecodeArgument(Exception):\n    \"\"\"Raised when an invalid argument is provided for mask decoding.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.InvalidNumpyInput","title":"<code>InvalidNumpyInput</code>","text":"<p>               Bases: <code>InputImageLoadError</code></p> <p>Raised when the input is an invalid NumPy array.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class InvalidNumpyInput(InputImageLoadError):\n    \"\"\"Raised when the input is an invalid NumPy array.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.MissingApiKeyError","title":"<code>MissingApiKeyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the API key is missing.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class MissingApiKeyError(Exception):\n    \"\"\"Raised when the API key is missing.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.MissingServiceSecretError","title":"<code>MissingServiceSecretError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the service secret is missing.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class MissingServiceSecretError(Exception):\n    \"\"\"Raised when the service secret is missing.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.OnnxProviderNotAvailable","title":"<code>OnnxProviderNotAvailable</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the ONNX provider is not available.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class OnnxProviderNotAvailable(Exception):\n    \"\"\"Raised when the ONNX provider is not available.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/exceptions/#inference.core.exceptions.WorkspaceLoadError","title":"<code>WorkspaceLoadError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when there is an error loading the workspace.</p> <p>Attributes:</p> Name Type Description <code>message</code> <code>str</code> <p>Optional message describing the error.</p> Source code in <code>inference/core/exceptions.py</code> <pre><code>class WorkspaceLoadError(Exception):\n    \"\"\"Raised when there is an error loading the workspace.\n\n    Attributes:\n        message (str): Optional message describing the error.\n    \"\"\"\n</code></pre>"},{"location":"docs/reference/inference/core/logger/","title":"logger","text":""},{"location":"docs/reference/inference/core/nms/","title":"nms","text":""},{"location":"docs/reference/inference/core/nms/#inference.core.nms.non_max_suppression_fast","title":"<code>non_max_suppression_fast(boxes, overlapThresh)</code>","text":"<p>Applies non-maximum suppression to bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>ndarray</code> <p>Array of bounding boxes with confidence scores.</p> required <code>overlapThresh</code> <code>float</code> <p>Overlap threshold for suppression.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of bounding boxes after non-maximum suppression.</p> Source code in <code>inference/core/nms.py</code> <pre><code>def non_max_suppression_fast(boxes, overlapThresh):\n    \"\"\"Applies non-maximum suppression to bounding boxes.\n\n    Args:\n        boxes (np.ndarray): Array of bounding boxes with confidence scores.\n        overlapThresh (float): Overlap threshold for suppression.\n\n    Returns:\n        list: List of bounding boxes after non-maximum suppression.\n    \"\"\"\n    # if there are no boxes, return an empty list\n    if len(boxes) == 0:\n        return []\n    # if the bounding boxes integers, convert them to floats --\n    # this is important since we'll be doing a bunch of divisions\n    if boxes.dtype.kind == \"i\":\n        boxes = boxes.astype(\"float\")\n    # initialize the list of picked indexes\n    pick = []\n    # grab the coordinates of the bounding boxes\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    conf = boxes[:, 4]\n    # compute the area of the bounding boxes and sort the bounding\n    # boxes by the bottom-right y-coordinate of the bounding box\n    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n    idxs = np.argsort(conf)\n    # keep looping while some indexes still remain in the indexes\n    # list\n    while len(idxs) &gt; 0:\n        # grab the last index in the indexes list and add the\n        # index value to the list of picked indexes\n        last = len(idxs) - 1\n        i = idxs[last]\n        pick.append(i)\n        # find the largest (x, y) coordinates for the start of\n        # the bounding box and the smallest (x, y) coordinates\n        # for the end of the bounding box\n        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n        # compute the width and height of the bounding box\n        w = np.maximum(0, xx2 - xx1 + 1)\n        h = np.maximum(0, yy2 - yy1 + 1)\n        # compute the ratio of overlap\n        overlap = (w * h) / area[idxs[:last]]\n        # delete all indexes from the index list that have\n        idxs = np.delete(\n            idxs, np.concatenate(([last], np.where(overlap &gt; overlapThresh)[0]))\n        )\n    # return only the bounding boxes that were picked using the\n    # integer data type\n    return boxes[pick].astype(\"float\")\n</code></pre>"},{"location":"docs/reference/inference/core/nms/#inference.core.nms.w_np_non_max_suppression","title":"<code>w_np_non_max_suppression(prediction, conf_thresh=0.25, iou_thresh=0.45, class_agnostic=False, max_detections=300, max_candidate_detections=3000, timeout_seconds=None, num_masks=0, box_format='xywh')</code>","text":"<p>Applies non-maximum suppression to predictions.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>ndarray</code> <p>Array of predictions. Format for single prediction is [bbox x 4, max_class_confidence, (confidence) x num_of_classes, additional_element x num_masks]</p> required <code>conf_thresh</code> <code>float</code> <p>Confidence threshold. Defaults to 0.25.</p> <code>0.25</code> <code>iou_thresh</code> <code>float</code> <p>IOU threshold. Defaults to 0.45.</p> <code>0.45</code> <code>class_agnostic</code> <code>bool</code> <p>Whether to ignore class labels. Defaults to False.</p> <code>False</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections. Defaults to 300.</p> <code>300</code> <code>max_candidate_detections</code> <code>int</code> <p>Maximum number of candidate detections. Defaults to 3000.</p> <code>3000</code> <code>timeout_seconds</code> <code>Optional[int]</code> <p>Timeout in seconds. Defaults to None.</p> <code>None</code> <code>num_masks</code> <code>int</code> <p>Number of masks. Defaults to 0.</p> <code>0</code> <code>box_format</code> <code>str</code> <p>Format of bounding boxes. Either 'xywh' or 'xyxy'. Defaults to 'xywh'.</p> <code>'xywh'</code> <p>Returns:</p> Name Type Description <code>list</code> <p>List of filtered predictions after non-maximum suppression. Format of a single result is: [bbox x 4, max_class_confidence, max_class_confidence, id_of_class_with_max_confidence, additional_element x num_masks]</p> Source code in <code>inference/core/nms.py</code> <pre><code>def w_np_non_max_suppression(\n    prediction,\n    conf_thresh: float = 0.25,\n    iou_thresh: float = 0.45,\n    class_agnostic: bool = False,\n    max_detections: int = 300,\n    max_candidate_detections: int = 3000,\n    timeout_seconds: Optional[int] = None,\n    num_masks: int = 0,\n    box_format: str = \"xywh\",\n):\n    \"\"\"Applies non-maximum suppression to predictions.\n\n    Args:\n        prediction (np.ndarray): Array of predictions. Format for single prediction is\n            [bbox x 4, max_class_confidence, (confidence) x num_of_classes, additional_element x num_masks]\n        conf_thresh (float, optional): Confidence threshold. Defaults to 0.25.\n        iou_thresh (float, optional): IOU threshold. Defaults to 0.45.\n        class_agnostic (bool, optional): Whether to ignore class labels. Defaults to False.\n        max_detections (int, optional): Maximum number of detections. Defaults to 300.\n        max_candidate_detections (int, optional): Maximum number of candidate detections. Defaults to 3000.\n        timeout_seconds (Optional[int], optional): Timeout in seconds. Defaults to None.\n        num_masks (int, optional): Number of masks. Defaults to 0.\n        box_format (str, optional): Format of bounding boxes. Either 'xywh' or 'xyxy'. Defaults to 'xywh'.\n\n    Returns:\n        list: List of filtered predictions after non-maximum suppression. Format of a single result is:\n            [bbox x 4, max_class_confidence, max_class_confidence, id_of_class_with_max_confidence,\n            additional_element x num_masks]\n    \"\"\"\n    num_classes = prediction.shape[2] - 5 - num_masks\n\n    np_box_corner = np.zeros(prediction.shape)\n    if box_format == \"xywh\":\n        np_box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n        np_box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n        np_box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n        np_box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n        prediction[:, :, :4] = np_box_corner[:, :, :4]\n    elif box_format == \"xyxy\":\n        pass\n    else:\n        raise ValueError(\n            \"box_format must be either 'xywh' or 'xyxy', got {}\".format(box_format)\n        )\n\n    batch_predictions = []\n    for np_image_i, np_image_pred in enumerate(prediction):\n        filtered_predictions = []\n        np_conf_mask = np_image_pred[:, 4] &gt;= conf_thresh\n\n        np_image_pred = np_image_pred[np_conf_mask]\n        cls_confs = np_image_pred[:, 5 : num_classes + 5]\n        if (\n            np_image_pred.shape[0] == 0\n            or np_image_pred.shape[1] == 0\n            or cls_confs.shape[1] == 0\n        ):\n            batch_predictions.append(filtered_predictions)\n            continue\n\n        np_class_conf = np.max(cls_confs, 1)\n        np_class_pred = np.argmax(np_image_pred[:, 5 : num_classes + 5], 1)\n        np_class_conf = np.expand_dims(np_class_conf, axis=1)\n        np_class_pred = np.expand_dims(np_class_pred, axis=1)\n        np_mask_pred = np_image_pred[:, 5 + num_classes :]\n        np_detections = np.append(\n            np.append(\n                np.append(np_image_pred[:, :5], np_class_conf, axis=1),\n                np_class_pred,\n                axis=1,\n            ),\n            np_mask_pred,\n            axis=1,\n        )\n\n        np_unique_labels = np.unique(np_detections[:, 6])\n\n        if class_agnostic:\n            np_detections_class = sorted(\n                np_detections, key=lambda row: row[4], reverse=True\n            )\n            filtered_predictions.extend(\n                non_max_suppression_fast(np.array(np_detections_class), iou_thresh)\n            )\n        else:\n            for c in np_unique_labels:\n                np_detections_class = np_detections[np_detections[:, 6] == c]\n                np_detections_class = sorted(\n                    np_detections_class, key=lambda row: row[4], reverse=True\n                )\n                filtered_predictions.extend(\n                    non_max_suppression_fast(np.array(np_detections_class), iou_thresh)\n                )\n        filtered_predictions = sorted(\n            filtered_predictions, key=lambda row: row[4], reverse=True\n        )\n        batch_predictions.append(filtered_predictions[:max_detections])\n    return batch_predictions\n</code></pre>"},{"location":"docs/reference/inference/core/roboflow_api/","title":"roboflow_api","text":""},{"location":"docs/reference/inference/core/usage/","title":"usage","text":""},{"location":"docs/reference/inference/core/usage/#inference.core.usage.trackUsage","title":"<code>trackUsage(endpoint, actor, n=1)</code>","text":"<p>Tracks the usage of an endpoint by an actor.</p> <p>This function increments the usage count for a given endpoint by an actor. It also handles initialization if the count does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The endpoint being accessed.</p> required <code>actor</code> <code>str</code> <p>The actor accessing the endpoint.</p> required <code>n</code> <code>int</code> <p>The number of times the endpoint was accessed. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>None</code> <p>This function does not return anything but updates the memcache client.</p> Source code in <code>inference/core/usage.py</code> <pre><code>def trackUsage(endpoint, actor, n=1):\n    \"\"\"Tracks the usage of an endpoint by an actor.\n\n    This function increments the usage count for a given endpoint by an actor.\n    It also handles initialization if the count does not exist.\n\n    Args:\n        endpoint (str): The endpoint being accessed.\n        actor (str): The actor accessing the endpoint.\n        n (int, optional): The number of times the endpoint was accessed. Defaults to 1.\n\n    Returns:\n        None: This function does not return anything but updates the memcache client.\n    \"\"\"\n    # count an inference\n    try:\n        job = endpoint + \"endpoint:::actor\" + actor\n        current_infers = memcache_client.incr(job, n)\n        if current_infers is None:  # not yet set; initialize at 1\n            memcache_client.set(job, n)\n            current_infers = n\n\n            # store key\n            job_keys = memcache_client.get(\"JOB_KEYS\")\n            if job_keys is None:\n                memcache_client.add(\"JOB_KEYS\", json.dumps([job]))\n            else:\n                decoded = json.loads(job_keys)\n                decoded.append(job)\n                decoded = list(set(decoded))\n                memcache_client.set(\"JOB_KEYS\", json.dumps(decoded))\n\n            actor_keys = memcache_client.get(\"ACTOR_KEYS\")\n            if actor_keys is None:\n                ak = {}\n                ak[actor] = n\n                memcache_client.add(\"ACTOR_KEYS\", json.dumps(ak))\n            else:\n                decoded = json.loads(actor_keys)\n                if actor in actor_keys:\n                    actor_keys[actor] += n\n                else:\n                    actor_keys[actor] = n\n                memcache_client.set(\"ACTOR_KEYS\", json.dumps(actor_keys))\n\n    except Exception as e:\n        logger.debug(\"WARNING: there was an error in counting this inference\")\n        logger.debug(e)\n</code></pre>"},{"location":"docs/reference/inference/core/version/","title":"version","text":""},{"location":"docs/reference/inference/core/warnings/","title":"warnings","text":""},{"location":"docs/reference/inference/core/active_learning/accounting/","title":"accounting","text":""},{"location":"docs/reference/inference/core/active_learning/accounting/#inference.core.active_learning.accounting.get_images_in_labeling_jobs_of_specific_batch","title":"<code>get_images_in_labeling_jobs_of_specific_batch(all_labeling_jobs, batch_id)</code>","text":"<p>Get the number of images in labeling jobs of a specific batch.</p> <p>Parameters:</p> Name Type Description Default <code>all_labeling_jobs</code> <code>List[dict]</code> <p>All labeling jobs.</p> required <code>batch_id</code> <code>str</code> <p>ID of the batch.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of images in labeling jobs of the batch.</p> Source code in <code>inference/core/active_learning/accounting.py</code> <pre><code>def get_images_in_labeling_jobs_of_specific_batch(\n    all_labeling_jobs: List[dict],\n    batch_id: str,\n) -&gt; int:\n    \"\"\"Get the number of images in labeling jobs of a specific batch.\n\n    Args:\n        all_labeling_jobs: All labeling jobs.\n        batch_id: ID of the batch.\n\n    Returns:\n        The number of images in labeling jobs of the batch.\n\n    \"\"\"\n\n    matching_jobs = []\n    for labeling_job in all_labeling_jobs:\n        if batch_id in labeling_job[\"sourceBatch\"]:\n            matching_jobs.append(labeling_job)\n    return sum(job[\"numImages\"] for job in matching_jobs)\n</code></pre>"},{"location":"docs/reference/inference/core/active_learning/accounting/#inference.core.active_learning.accounting.get_matching_labeling_batch","title":"<code>get_matching_labeling_batch(all_labeling_batches, batch_name)</code>","text":"<p>Get the matching labeling batch.</p> <p>Parameters:</p> Name Type Description Default <code>all_labeling_batches</code> <code>List[dict]</code> <p>All labeling batches.</p> required <code>batch_name</code> <code>str</code> <p>Name of the batch.</p> required <p>Returns:</p> Type Description <code>Optional[dict]</code> <p>The matching labeling batch if found, None otherwise.</p> Source code in <code>inference/core/active_learning/accounting.py</code> <pre><code>def get_matching_labeling_batch(\n    all_labeling_batches: List[dict],\n    batch_name: str,\n) -&gt; Optional[dict]:\n    \"\"\"Get the matching labeling batch.\n\n    Args:\n        all_labeling_batches: All labeling batches.\n        batch_name: Name of the batch.\n\n    Returns:\n        The matching labeling batch if found, None otherwise.\n\n    \"\"\"\n    matching_batch = None\n    for labeling_batch in all_labeling_batches:\n        if labeling_batch[\"name\"] == batch_name:\n            matching_batch = labeling_batch\n            break\n    return matching_batch\n</code></pre>"},{"location":"docs/reference/inference/core/active_learning/accounting/#inference.core.active_learning.accounting.image_can_be_submitted_to_batch","title":"<code>image_can_be_submitted_to_batch(batch_name, workspace_id, dataset_id, max_batch_images, api_key)</code>","text":"<p>Check if an image can be submitted to a batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch_name</code> <code>str</code> <p>Name of the batch.</p> required <code>workspace_id</code> <code>WorkspaceID</code> <p>ID of the workspace.</p> required <code>dataset_id</code> <code>DatasetID</code> <p>ID of the dataset.</p> required <code>max_batch_images</code> <code>Optional[int]</code> <p>Maximum number of images allowed in the batch.</p> required <code>api_key</code> <code>str</code> <p>API key to use for the request.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the image can be submitted to the batch, False otherwise.</p> Source code in <code>inference/core/active_learning/accounting.py</code> <pre><code>def image_can_be_submitted_to_batch(\n    batch_name: str,\n    workspace_id: WorkspaceID,\n    dataset_id: DatasetID,\n    max_batch_images: Optional[int],\n    api_key: str,\n) -&gt; bool:\n    \"\"\"Check if an image can be submitted to a batch.\n\n    Args:\n        batch_name: Name of the batch.\n        workspace_id: ID of the workspace.\n        dataset_id: ID of the dataset.\n        max_batch_images: Maximum number of images allowed in the batch.\n        api_key: API key to use for the request.\n\n    Returns:\n        True if the image can be submitted to the batch, False otherwise.\n    \"\"\"\n    if max_batch_images is None:\n        return True\n    labeling_batches = get_roboflow_labeling_batches(\n        api_key=api_key,\n        workspace_id=workspace_id,\n        dataset_id=dataset_id,\n    )\n    matching_labeling_batch = get_matching_labeling_batch(\n        all_labeling_batches=labeling_batches[\"batches\"],\n        batch_name=batch_name,\n    )\n    if matching_labeling_batch is None:\n        return max_batch_images &gt; 0\n    batch_images_under_labeling = 0\n    if matching_labeling_batch[\"numJobs\"] &gt; 0:\n        labeling_jobs = get_roboflow_labeling_jobs(\n            api_key=api_key, workspace_id=workspace_id, dataset_id=dataset_id\n        )\n        batch_images_under_labeling = get_images_in_labeling_jobs_of_specific_batch(\n            all_labeling_jobs=labeling_jobs[\"jobs\"],\n            batch_id=matching_labeling_batch[\"id\"],\n        )\n    total_batch_images = matching_labeling_batch[\"images\"] + batch_images_under_labeling\n    return max_batch_images &gt; total_batch_images\n</code></pre>"},{"location":"docs/reference/inference/core/active_learning/batching/","title":"batching","text":""},{"location":"docs/reference/inference/core/active_learning/cache_operations/","title":"cache_operations","text":""},{"location":"docs/reference/inference/core/active_learning/configuration/","title":"configuration","text":""},{"location":"docs/reference/inference/core/active_learning/configuration/#inference.core.active_learning.configuration.predictions_incompatible_with_dataset","title":"<code>predictions_incompatible_with_dataset(model_type, dataset_type)</code>","text":"<p>The incompatibility occurs when we mix classification with detection - as detection-based predictions are partially compatible (for instance - for key-points detection we may register bboxes from object detection and manually provide key-points annotations)</p> Source code in <code>inference/core/active_learning/configuration.py</code> <pre><code>def predictions_incompatible_with_dataset(\n    model_type: str,\n    dataset_type: str,\n) -&gt; bool:\n    \"\"\"\n    The incompatibility occurs when we mix classification with detection - as detection-based\n    predictions are partially compatible (for instance - for key-points detection we may register bboxes\n    from object detection and manually provide key-points annotations)\n    \"\"\"\n    model_is_classifier = CLASSIFICATION_TASK in model_type\n    dataset_is_of_type_classification = CLASSIFICATION_TASK in dataset_type\n    return model_is_classifier != dataset_is_of_type_classification\n</code></pre>"},{"location":"docs/reference/inference/core/active_learning/core/","title":"core","text":""},{"location":"docs/reference/inference/core/active_learning/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/active_learning/middlewares/","title":"middlewares","text":""},{"location":"docs/reference/inference/core/active_learning/post_processing/","title":"post_processing","text":""},{"location":"docs/reference/inference/core/active_learning/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/active_learning/samplers/close_to_threshold/","title":"close_to_threshold","text":""},{"location":"docs/reference/inference/core/active_learning/samplers/contains_classes/","title":"contains_classes","text":""},{"location":"docs/reference/inference/core/active_learning/samplers/number_of_detections/","title":"number_of_detections","text":""},{"location":"docs/reference/inference/core/active_learning/samplers/random/","title":"random","text":""},{"location":"docs/reference/inference/core/cache/base/","title":"base","text":""},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache","title":"<code>BaseCache</code>","text":"<p>BaseCache is an abstract base class that defines the interface for a cache.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>class BaseCache:\n    \"\"\"\n    BaseCache is an abstract base class that defines the interface for a cache.\n    \"\"\"\n\n    def get(self, key: str):\n        \"\"\"\n        Gets the value associated with the given key.\n\n        Args:\n            key (str): The key to retrieve the value.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def set(self, key: str, value: str, expire: float = None):\n        \"\"\"\n        Sets a value for a given key with an optional expire time.\n\n        Args:\n            key (str): The key to store the value.\n            value (str): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def zadd(self, key: str, value: str, score: float, expire: float = None):\n        \"\"\"\n        Adds a member with the specified score to the sorted set stored at key.\n\n        Args:\n            key (str): The key of the sorted set.\n            value (str): The value to add to the sorted set.\n            score (float): The score associated with the value.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def zrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n        withscores: bool = False,\n    ):\n        \"\"\"\n        Retrieves a range of members from a sorted set.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The starting index of the range. Defaults to -1.\n            stop (int, optional): The ending index of the range. Defaults to float(\"inf\").\n            withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def zremrangebyscore(\n        self,\n        key: str,\n        start: Optional[int] = -1,\n        stop: Optional[int] = float(\"inf\"),\n    ):\n        \"\"\"\n        Removes all members in a sorted set within the given scores.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The minimum score of the range. Defaults to -1.\n            stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def acquire_lock(self, key: str, expire: float = None) -&gt; Any:\n        raise NotImplementedError()\n\n    @contextmanager\n    def lock(self, key: str, expire: float = None) -&gt; Any:\n        logger.debug(f\"Acquiring lock at cache key: {key}\")\n        l = self.acquire_lock(key, expire=expire)\n        try:\n            yield l\n        finally:\n            logger.debug(f\"Releasing lock at cache key: {key}\")\n            l.release()\n\n    def set_numpy(self, key: str, value: Any, expire: float = None):\n        \"\"\"\n        Caches a numpy array.\n\n        Args:\n            key (str): The key to store the value.\n            value (Any): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_numpy(self, key: str) -&gt; Any:\n        \"\"\"\n        Retrieves a numpy array from the cache.\n\n        Args:\n            key (str): The key of the value to retrieve.\n\n        Raises:\n            NotImplementedError: This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.get","title":"<code>get(key)</code>","text":"<p>Gets the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def get(self, key: str):\n    \"\"\"\n    Gets the value associated with the given key.\n\n    Args:\n        key (str): The key to retrieve the value.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.get_numpy","title":"<code>get_numpy(key)</code>","text":"<p>Retrieves a numpy array from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the value to retrieve.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def get_numpy(self, key: str) -&gt; Any:\n    \"\"\"\n    Retrieves a numpy array from the cache.\n\n    Args:\n        key (str): The key of the value to retrieve.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.set","title":"<code>set(key, value, expire=None)</code>","text":"<p>Sets a value for a given key with an optional expire time.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>str</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def set(self, key: str, value: str, expire: float = None):\n    \"\"\"\n    Sets a value for a given key with an optional expire time.\n\n    Args:\n        key (str): The key to store the value.\n        value (str): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.set_numpy","title":"<code>set_numpy(key, value, expire=None)</code>","text":"<p>Caches a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def set_numpy(self, key: str, value: Any, expire: float = None):\n    \"\"\"\n    Caches a numpy array.\n\n    Args:\n        key (str): The key to store the value.\n        value (Any): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.zadd","title":"<code>zadd(key, value, score, expire=None)</code>","text":"<p>Adds a member with the specified score to the sorted set stored at key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>value</code> <code>str</code> <p>The value to add to the sorted set.</p> required <code>score</code> <code>float</code> <p>The score associated with the value.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def zadd(self, key: str, value: str, score: float, expire: float = None):\n    \"\"\"\n    Adds a member with the specified score to the sorted set stored at key.\n\n    Args:\n        key (str): The key of the sorted set.\n        value (str): The value to add to the sorted set.\n        score (float): The score associated with the value.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.zrangebyscore","title":"<code>zrangebyscore(key, min=-1, max=float('inf'), withscores=False)</code>","text":"<p>Retrieves a range of members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The starting index of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The ending index of the range. Defaults to float(\"inf\").</p> required <code>withscores</code> <code>bool</code> <p>Whether to return the scores along with the values. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def zrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n    withscores: bool = False,\n):\n    \"\"\"\n    Retrieves a range of members from a sorted set.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The starting index of the range. Defaults to -1.\n        stop (int, optional): The ending index of the range. Defaults to float(\"inf\").\n        withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/base/#inference.core.cache.base.BaseCache.zremrangebyscore","title":"<code>zremrangebyscore(key, start=-1, stop=float('inf'))</code>","text":"<p>Removes all members in a sorted set within the given scores.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The minimum score of the range. Defaults to -1.</p> <code>-1</code> <code>stop</code> <code>int</code> <p>The maximum score of the range. Defaults to float(\"inf\").</p> <code>float('inf')</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by subclasses.</p> Source code in <code>inference/core/cache/base.py</code> <pre><code>def zremrangebyscore(\n    self,\n    key: str,\n    start: Optional[int] = -1,\n    stop: Optional[int] = float(\"inf\"),\n):\n    \"\"\"\n    Removes all members in a sorted set within the given scores.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The minimum score of the range. Defaults to -1.\n        stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n    Raises:\n        NotImplementedError: This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/","title":"memory","text":""},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache","title":"<code>MemoryCache</code>","text":"<p>               Bases: <code>BaseCache</code></p> <p>MemoryCache is an in-memory cache that implements the BaseCache interface.</p> <p>Attributes:</p> Name Type Description <code>cache</code> <code>dict</code> <p>A dictionary to store the cache values.</p> <code>expires</code> <code>dict</code> <p>A dictionary to store the expiration times of the cache values.</p> <code>zexpires</code> <code>dict</code> <p>A dictionary to store the expiration times of the sorted set values.</p> <code>_expire_thread</code> <code>Thread</code> <p>A thread that runs the _expire method.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>class MemoryCache(BaseCache):\n    \"\"\"\n    MemoryCache is an in-memory cache that implements the BaseCache interface.\n\n    Attributes:\n        cache (dict): A dictionary to store the cache values.\n        expires (dict): A dictionary to store the expiration times of the cache values.\n        zexpires (dict): A dictionary to store the expiration times of the sorted set values.\n        _expire_thread (threading.Thread): A thread that runs the _expire method.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initializes a new instance of the MemoryCache class.\n        \"\"\"\n        self.cache = dict()\n        self.expires = dict()\n        self.zexpires = dict()\n\n        self._expire_thread = threading.Thread(target=self._expire)\n        self._expire_thread.daemon = True\n        self._expire_thread.start()\n\n    def _expire(self):\n        \"\"\"\n        Removes the expired keys from the cache and zexpires dictionaries.\n\n        This method runs in an infinite loop and sleeps for MEMORY_CACHE_EXPIRE_INTERVAL seconds between each iteration.\n        \"\"\"\n        while True:\n            now = time.time()\n            keys_to_delete = []\n            for k, v in self.expires.copy().items():\n                if v &lt; now:\n                    keys_to_delete.append(k)\n            for k in keys_to_delete:\n                del self.cache[k]\n                del self.expires[k]\n            keys_to_delete = []\n            for k, v in self.zexpires.copy().items():\n                if v &lt; now:\n                    keys_to_delete.append(k)\n            for k in keys_to_delete:\n                del self.cache[k[0]][k[1]]\n                del self.zexpires[k]\n            while time.time() - now &lt; MEMORY_CACHE_EXPIRE_INTERVAL:\n                time.sleep(0.1)\n\n    def get(self, key: str):\n        \"\"\"\n        Gets the value associated with the given key.\n\n        Args:\n            key (str): The key to retrieve the value.\n\n        Returns:\n            str: The value associated with the key, or None if the key does not exist or is expired.\n        \"\"\"\n        if key in self.expires:\n            if self.expires[key] &lt; time.time():\n                del self.cache[key]\n                del self.expires[key]\n                return None\n        return self.cache.get(key)\n\n    def set(self, key: str, value: str, expire: float = None):\n        \"\"\"\n        Sets a value for a given key with an optional expire time.\n\n        Args:\n            key (str): The key to store the value.\n            value (str): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        self.cache[key] = value\n        if expire:\n            self.expires[key] = expire + time.time()\n\n    def zadd(self, key: str, value: Any, score: float, expire: float = None):\n        \"\"\"\n        Adds a member with the specified score to the sorted set stored at key.\n\n        Args:\n            key (str): The key of the sorted set.\n            value (str): The value to add to the sorted set.\n            score (float): The score associated with the value.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        if not key in self.cache:\n            self.cache[key] = dict()\n        self.cache[key][score] = value\n        if expire:\n            self.zexpires[(key, score)] = expire + time.time()\n\n    def zrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n        withscores: bool = False,\n    ):\n        \"\"\"\n        Retrieves a range of members from a sorted set.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The starting score of the range. Defaults to -1.\n            stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n            withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n        Returns:\n            list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n        \"\"\"\n        if not key in self.cache:\n            return []\n        keys = sorted([k for k in self.cache[key].keys() if min &lt;= k &lt;= max])\n        if withscores:\n            return [(self.cache[key][k], k) for k in keys]\n        else:\n            return [self.cache[key][k] for k in keys]\n\n    def zremrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n    ):\n        \"\"\"\n        Removes all members in a sorted set within the given scores.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The minimum score of the range. Defaults to -1.\n            stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n        Returns:\n            int: The number of members removed from the sorted set.\n        \"\"\"\n        res = self.zrangebyscore(key, min=min, max=max, withscores=True)\n        keys_to_delete = [k[1] for k in res]\n        for k in keys_to_delete:\n            del self.cache[key][k]\n        return len(keys_to_delete)\n\n    def acquire_lock(self, key: str, expire=None) -&gt; Any:\n        lock: Optional[Lock] = self.get(key)\n        if lock is None:\n            lock = Lock()\n            self.set(key, lock, expire=expire)\n        if expire is None:\n            expire = -1\n        acquired = lock.acquire(timeout=expire)\n        if not acquired:\n            raise TimeoutError()\n        # refresh the lock\n        self.set(key, lock, expire=expire)\n        return lock\n\n    def set_numpy(self, key: str, value: Any, expire: float = None):\n        return self.set(key, value, expire=expire)\n\n    def get_numpy(self, key: str):\n        return self.get(key)\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.__init__","title":"<code>__init__()</code>","text":"<p>Initializes a new instance of the MemoryCache class.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initializes a new instance of the MemoryCache class.\n    \"\"\"\n    self.cache = dict()\n    self.expires = dict()\n    self.zexpires = dict()\n\n    self._expire_thread = threading.Thread(target=self._expire)\n    self._expire_thread.daemon = True\n    self._expire_thread.start()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.get","title":"<code>get(key)</code>","text":"<p>Gets the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The value associated with the key, or None if the key does not exist or is expired.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def get(self, key: str):\n    \"\"\"\n    Gets the value associated with the given key.\n\n    Args:\n        key (str): The key to retrieve the value.\n\n    Returns:\n        str: The value associated with the key, or None if the key does not exist or is expired.\n    \"\"\"\n    if key in self.expires:\n        if self.expires[key] &lt; time.time():\n            del self.cache[key]\n            del self.expires[key]\n            return None\n    return self.cache.get(key)\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.set","title":"<code>set(key, value, expire=None)</code>","text":"<p>Sets a value for a given key with an optional expire time.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>str</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def set(self, key: str, value: str, expire: float = None):\n    \"\"\"\n    Sets a value for a given key with an optional expire time.\n\n    Args:\n        key (str): The key to store the value.\n        value (str): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    self.cache[key] = value\n    if expire:\n        self.expires[key] = expire + time.time()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.zadd","title":"<code>zadd(key, value, score, expire=None)</code>","text":"<p>Adds a member with the specified score to the sorted set stored at key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>value</code> <code>str</code> <p>The value to add to the sorted set.</p> required <code>score</code> <code>float</code> <p>The score associated with the value.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def zadd(self, key: str, value: Any, score: float, expire: float = None):\n    \"\"\"\n    Adds a member with the specified score to the sorted set stored at key.\n\n    Args:\n        key (str): The key of the sorted set.\n        value (str): The value to add to the sorted set.\n        score (float): The score associated with the value.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    if not key in self.cache:\n        self.cache[key] = dict()\n    self.cache[key][score] = value\n    if expire:\n        self.zexpires[(key, score)] = expire + time.time()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.zrangebyscore","title":"<code>zrangebyscore(key, min=-1, max=float('inf'), withscores=False)</code>","text":"<p>Retrieves a range of members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The starting score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The ending score of the range. Defaults to float(\"inf\").</p> required <code>withscores</code> <code>bool</code> <p>Whether to return the scores along with the values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of values (or value-score pairs if withscores is True) in the specified score range.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def zrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n    withscores: bool = False,\n):\n    \"\"\"\n    Retrieves a range of members from a sorted set.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The starting score of the range. Defaults to -1.\n        stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n        withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n    Returns:\n        list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n    \"\"\"\n    if not key in self.cache:\n        return []\n    keys = sorted([k for k in self.cache[key].keys() if min &lt;= k &lt;= max])\n    if withscores:\n        return [(self.cache[key][k], k) for k in keys]\n    else:\n        return [self.cache[key][k] for k in keys]\n</code></pre>"},{"location":"docs/reference/inference/core/cache/memory/#inference.core.cache.memory.MemoryCache.zremrangebyscore","title":"<code>zremrangebyscore(key, min=-1, max=float('inf'))</code>","text":"<p>Removes all members in a sorted set within the given scores.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The minimum score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The maximum score of the range. Defaults to float(\"inf\").</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of members removed from the sorted set.</p> Source code in <code>inference/core/cache/memory.py</code> <pre><code>def zremrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n):\n    \"\"\"\n    Removes all members in a sorted set within the given scores.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The minimum score of the range. Defaults to -1.\n        stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n    Returns:\n        int: The number of members removed from the sorted set.\n    \"\"\"\n    res = self.zrangebyscore(key, min=min, max=max, withscores=True)\n    keys_to_delete = [k[1] for k in res]\n    for k in keys_to_delete:\n        del self.cache[key][k]\n    return len(keys_to_delete)\n</code></pre>"},{"location":"docs/reference/inference/core/cache/model_artifacts/","title":"model_artifacts","text":""},{"location":"docs/reference/inference/core/cache/redis/","title":"redis","text":""},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache","title":"<code>RedisCache</code>","text":"<p>               Bases: <code>BaseCache</code></p> <p>MemoryCache is an in-memory cache that implements the BaseCache interface.</p> <p>Attributes:</p> Name Type Description <code>cache</code> <code>dict</code> <p>A dictionary to store the cache values.</p> <code>expires</code> <code>dict</code> <p>A dictionary to store the expiration times of the cache values.</p> <code>zexpires</code> <code>dict</code> <p>A dictionary to store the expiration times of the sorted set values.</p> <code>_expire_thread</code> <code>Thread</code> <p>A thread that runs the _expire method.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>class RedisCache(BaseCache):\n    \"\"\"\n    MemoryCache is an in-memory cache that implements the BaseCache interface.\n\n    Attributes:\n        cache (dict): A dictionary to store the cache values.\n        expires (dict): A dictionary to store the expiration times of the cache values.\n        zexpires (dict): A dictionary to store the expiration times of the sorted set values.\n        _expire_thread (threading.Thread): A thread that runs the _expire method.\n    \"\"\"\n\n    def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = 6379,\n        db: int = 0,\n        ssl: bool = False,\n        timeout: float = 2.0,\n    ) -&gt; None:\n        \"\"\"\n        Initializes a new instance of the MemoryCache class.\n        \"\"\"\n        self.client = redis.Redis(\n            host=host,\n            port=port,\n            db=db,\n            decode_responses=False,\n            ssl=ssl,\n            socket_timeout=timeout,\n            socket_connect_timeout=timeout,\n        )\n        logger.debug(\"Attempting to diagnose Redis connection...\")\n        self.client.ping()\n        logger.debug(\"Redis connection established.\")\n        self.zexpires = dict()\n\n        self._expire_thread = threading.Thread(target=self._expire, daemon=True)\n        self._expire_thread.start()\n\n    def _expire(self):\n        \"\"\"\n        Removes the expired keys from the cache and zexpires dictionaries.\n\n        This method runs in an infinite loop and sleeps for MEMORY_CACHE_EXPIRE_INTERVAL seconds between each iteration.\n        \"\"\"\n        while True:\n            logger.debug(\"Redis cleaner thread starts cleaning...\")\n            now = time.time()\n            for k, v in copy(list(self.zexpires.items())):\n                if v &lt; now:\n                    tolerance_factor = 1e-14  # floating point accuracy\n                    self.zremrangebyscore(\n                        k[0], k[1] - tolerance_factor, k[1] + tolerance_factor\n                    )\n                    del self.zexpires[k]\n            logger.debug(\"Redis cleaner finished task.\")\n            sleep_time = MEMORY_CACHE_EXPIRE_INTERVAL - (time.time() - now)\n            time.sleep(max(sleep_time, 0))\n\n    def get(self, key: str):\n        \"\"\"\n        Gets the value associated with the given key.\n\n        Args:\n            key (str): The key to retrieve the value.\n\n        Returns:\n            str: The value associated with the key, or None if the key does not exist or is expired.\n        \"\"\"\n        item = self.client.get(key)\n        if item is not None:\n            try:\n                return json.loads(item)\n            except (TypeError, ValueError):\n                return item\n\n    def set(self, key: str, value: str, expire: float = None):\n        \"\"\"\n        Sets a value for a given key with an optional expire time.\n\n        Args:\n            key (str): The key to store the value.\n            value (str): The value to store.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        if not isinstance(value, bytes):\n            value = json.dumps(value)\n        self.client.set(key, value, ex=expire)\n\n    def zadd(self, key: str, value: Any, score: float, expire: float = None):\n        \"\"\"\n        Adds a member with the specified score to the sorted set stored at key.\n\n        Args:\n            key (str): The key of the sorted set.\n            value (str): The value to add to the sorted set.\n            score (float): The score associated with the value.\n            expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n        \"\"\"\n        # serializable_value = self.ensure_serializable(value)\n        value = json.dumps(value)\n        self.client.zadd(key, {value: score})\n        if expire:\n            self.zexpires[(key, score)] = expire + time.time()\n\n    def zrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n        withscores: bool = False,\n    ):\n        \"\"\"\n        Retrieves a range of members from a sorted set.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The starting score of the range. Defaults to -1.\n            stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n            withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n        Returns:\n            list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n        \"\"\"\n        res = self.client.zrangebyscore(key, min, max, withscores=withscores)\n        if withscores:\n            return [(json.loads(x), y) for x, y in res]\n        else:\n            return [json.loads(x) for x in res]\n\n    def zremrangebyscore(\n        self,\n        key: str,\n        min: Optional[float] = -1,\n        max: Optional[float] = float(\"inf\"),\n    ):\n        \"\"\"\n        Removes all members in a sorted set within the given scores.\n\n        Args:\n            key (str): The key of the sorted set.\n            start (int, optional): The minimum score of the range. Defaults to -1.\n            stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n        Returns:\n            int: The number of members removed from the sorted set.\n        \"\"\"\n        return self.client.zremrangebyscore(key, min, max)\n\n    def ensure_serializable(self, value: Any):\n        if isinstance(value, dict):\n            for k, v in value.items():\n                if isinstance(v, Exception):\n                    value[k] = str(v)\n                elif inspect.isclass(v) and isinstance(v, InferenceResponseImage):\n                    value[k] = v.dict()\n        return value\n\n    def acquire_lock(self, key: str, expire=None) -&gt; Any:\n        l = self.client.lock(key, blocking=True, timeout=expire)\n        acquired = l.acquire(blocking_timeout=expire)\n        if not acquired:\n            raise TimeoutError(\"Couldn't get lock\")\n        # refresh the lock\n        if expire is not None:\n            l.extend(expire)\n        return l\n\n    def set_numpy(self, key: str, value: Any, expire: float = None):\n        serialized_value = pickle.dumps(value)\n        self.set(key, serialized_value, expire=expire)\n\n    def get_numpy(self, key: str) -&gt; Any:\n        serialized_value = self.get(key)\n        if serialized_value is not None:\n            return pickle.loads(serialized_value)\n        else:\n            return None\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.__init__","title":"<code>__init__(host='localhost', port=6379, db=0, ssl=False, timeout=2.0)</code>","text":"<p>Initializes a new instance of the MemoryCache class.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def __init__(\n    self,\n    host: str = \"localhost\",\n    port: int = 6379,\n    db: int = 0,\n    ssl: bool = False,\n    timeout: float = 2.0,\n) -&gt; None:\n    \"\"\"\n    Initializes a new instance of the MemoryCache class.\n    \"\"\"\n    self.client = redis.Redis(\n        host=host,\n        port=port,\n        db=db,\n        decode_responses=False,\n        ssl=ssl,\n        socket_timeout=timeout,\n        socket_connect_timeout=timeout,\n    )\n    logger.debug(\"Attempting to diagnose Redis connection...\")\n    self.client.ping()\n    logger.debug(\"Redis connection established.\")\n    self.zexpires = dict()\n\n    self._expire_thread = threading.Thread(target=self._expire, daemon=True)\n    self._expire_thread.start()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.get","title":"<code>get(key)</code>","text":"<p>Gets the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The value associated with the key, or None if the key does not exist or is expired.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def get(self, key: str):\n    \"\"\"\n    Gets the value associated with the given key.\n\n    Args:\n        key (str): The key to retrieve the value.\n\n    Returns:\n        str: The value associated with the key, or None if the key does not exist or is expired.\n    \"\"\"\n    item = self.client.get(key)\n    if item is not None:\n        try:\n            return json.loads(item)\n        except (TypeError, ValueError):\n            return item\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.set","title":"<code>set(key, value, expire=None)</code>","text":"<p>Sets a value for a given key with an optional expire time.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to store the value.</p> required <code>value</code> <code>str</code> <p>The value to store.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def set(self, key: str, value: str, expire: float = None):\n    \"\"\"\n    Sets a value for a given key with an optional expire time.\n\n    Args:\n        key (str): The key to store the value.\n        value (str): The value to store.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    if not isinstance(value, bytes):\n        value = json.dumps(value)\n    self.client.set(key, value, ex=expire)\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.zadd","title":"<code>zadd(key, value, score, expire=None)</code>","text":"<p>Adds a member with the specified score to the sorted set stored at key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>value</code> <code>str</code> <p>The value to add to the sorted set.</p> required <code>score</code> <code>float</code> <p>The score associated with the value.</p> required <code>expire</code> <code>float</code> <p>The time, in seconds, after which the key will expire. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def zadd(self, key: str, value: Any, score: float, expire: float = None):\n    \"\"\"\n    Adds a member with the specified score to the sorted set stored at key.\n\n    Args:\n        key (str): The key of the sorted set.\n        value (str): The value to add to the sorted set.\n        score (float): The score associated with the value.\n        expire (float, optional): The time, in seconds, after which the key will expire. Defaults to None.\n    \"\"\"\n    # serializable_value = self.ensure_serializable(value)\n    value = json.dumps(value)\n    self.client.zadd(key, {value: score})\n    if expire:\n        self.zexpires[(key, score)] = expire + time.time()\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.zrangebyscore","title":"<code>zrangebyscore(key, min=-1, max=float('inf'), withscores=False)</code>","text":"<p>Retrieves a range of members from a sorted set.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The starting score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The ending score of the range. Defaults to float(\"inf\").</p> required <code>withscores</code> <code>bool</code> <p>Whether to return the scores along with the values. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of values (or value-score pairs if withscores is True) in the specified score range.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def zrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n    withscores: bool = False,\n):\n    \"\"\"\n    Retrieves a range of members from a sorted set.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The starting score of the range. Defaults to -1.\n        stop (int, optional): The ending score of the range. Defaults to float(\"inf\").\n        withscores (bool, optional): Whether to return the scores along with the values. Defaults to False.\n\n    Returns:\n        list: A list of values (or value-score pairs if withscores is True) in the specified score range.\n    \"\"\"\n    res = self.client.zrangebyscore(key, min, max, withscores=withscores)\n    if withscores:\n        return [(json.loads(x), y) for x, y in res]\n    else:\n        return [json.loads(x) for x in res]\n</code></pre>"},{"location":"docs/reference/inference/core/cache/redis/#inference.core.cache.redis.RedisCache.zremrangebyscore","title":"<code>zremrangebyscore(key, min=-1, max=float('inf'))</code>","text":"<p>Removes all members in a sorted set within the given scores.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key of the sorted set.</p> required <code>start</code> <code>int</code> <p>The minimum score of the range. Defaults to -1.</p> required <code>stop</code> <code>int</code> <p>The maximum score of the range. Defaults to float(\"inf\").</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of members removed from the sorted set.</p> Source code in <code>inference/core/cache/redis.py</code> <pre><code>def zremrangebyscore(\n    self,\n    key: str,\n    min: Optional[float] = -1,\n    max: Optional[float] = float(\"inf\"),\n):\n    \"\"\"\n    Removes all members in a sorted set within the given scores.\n\n    Args:\n        key (str): The key of the sorted set.\n        start (int, optional): The minimum score of the range. Defaults to -1.\n        stop (int, optional): The maximum score of the range. Defaults to float(\"inf\").\n\n    Returns:\n        int: The number of members removed from the sorted set.\n    \"\"\"\n    return self.client.zremrangebyscore(key, min, max)\n</code></pre>"},{"location":"docs/reference/inference/core/cache/serializers/","title":"serializers","text":""},{"location":"docs/reference/inference/core/devices/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.get_cpu_id","title":"<code>get_cpu_id()</code>","text":"<p>Fetches the CPU ID based on the operating system.</p> <p>Attempts to get the CPU ID for Windows, Linux, and MacOS. In case of any error or an unsupported OS, returns None.</p> <p>Returns:</p> Type Description <p>Optional[str]: CPU ID string if available, None otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_cpu_id():\n    \"\"\"Fetches the CPU ID based on the operating system.\n\n    Attempts to get the CPU ID for Windows, Linux, and MacOS.\n    In case of any error or an unsupported OS, returns None.\n\n    Returns:\n        Optional[str]: CPU ID string if available, None otherwise.\n    \"\"\"\n    try:\n        if platform.system() == \"Windows\":\n            return os.popen(\"wmic cpu get ProcessorId\").read().strip()\n        elif platform.system() == \"Linux\":\n            return (\n                open(\"/proc/cpuinfo\").read().split(\"processor\")[0].split(\":\")[1].strip()\n            )\n        elif platform.system() == \"Darwin\":\n            import subprocess\n\n            return (\n                subprocess.check_output([\"sysctl\", \"-n\", \"machdep.cpu.brand_string\"])\n                .strip()\n                .decode()\n            )\n    except Exception as e:\n        return None\n</code></pre>"},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.get_device_hostname","title":"<code>get_device_hostname()</code>","text":"<p>Fetches the device's hostname.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The device's hostname.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_device_hostname():\n    \"\"\"Fetches the device's hostname.\n\n    Returns:\n        str: The device's hostname.\n    \"\"\"\n    return platform.node()\n</code></pre>"},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.get_gpu_id","title":"<code>get_gpu_id()</code>","text":"<p>Fetches the GPU ID if a GPU is present.</p> <p>Tries to import and use the <code>GPUtil</code> module to retrieve the GPU information.</p> <p>Returns:</p> Type Description <p>Optional[int]: GPU ID if available, None otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_gpu_id():\n    \"\"\"Fetches the GPU ID if a GPU is present.\n\n    Tries to import and use the `GPUtil` module to retrieve the GPU information.\n\n    Returns:\n        Optional[int]: GPU ID if available, None otherwise.\n    \"\"\"\n    try:\n        import GPUtil\n\n        GPUs = GPUtil.getGPUs()\n        if GPUs:\n            return GPUs[0].id\n    except ImportError:\n        return None\n    except Exception as e:\n        return None\n</code></pre>"},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.get_inference_server_id","title":"<code>get_inference_server_id()</code>","text":"<p>Fetches a unique device ID.</p> <p>Tries to get the GPU ID first, then falls back to CPU ID. If the application is running inside Docker, the Docker container ID is appended to the hostname.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>A unique string representing the device. If unable to determine, returns \"UNKNOWN\".</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_inference_server_id():\n    \"\"\"Fetches a unique device ID.\n\n    Tries to get the GPU ID first, then falls back to CPU ID.\n    If the application is running inside Docker, the Docker container ID is appended to the hostname.\n\n    Returns:\n        str: A unique string representing the device. If unable to determine, returns \"UNKNOWN\".\n    \"\"\"\n    try:\n        if INFERENCE_SERVER_ID is not None:\n            return INFERENCE_SERVER_ID\n        id = random_string(6)\n        gpu_id = get_gpu_id()\n        if gpu_id is not None:\n            return f\"{id}-GPU-{gpu_id}\"\n        jetson_id = get_jetson_id()\n        if jetson_id is not None:\n            return f\"{id}-JETSON-{jetson_id}\"\n        return id\n    except Exception as e:\n        return \"UNKNOWN\"\n</code></pre>"},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.get_jetson_id","title":"<code>get_jetson_id()</code>","text":"<p>Fetches the Jetson device's serial number.</p> <p>Attempts to read the serial number from the device tree. In case of any error, returns None.</p> <p>Returns:</p> Type Description <p>Optional[str]: Jetson device serial number if available, None otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def get_jetson_id():\n    \"\"\"Fetches the Jetson device's serial number.\n\n    Attempts to read the serial number from the device tree.\n    In case of any error, returns None.\n\n    Returns:\n        Optional[str]: Jetson device serial number if available, None otherwise.\n    \"\"\"\n    try:\n        # Fetch the device's serial number\n        if not os.path.exists(\"/proc/device-tree/serial-number\"):\n            return None\n        serial_number = os.popen(\"cat /proc/device-tree/serial-number\").read().strip()\n        if serial_number == \"\":\n            return None\n        return serial_number\n    except Exception as e:\n        return None\n</code></pre>"},{"location":"docs/reference/inference/core/devices/utils/#inference.core.devices.utils.is_running_in_docker","title":"<code>is_running_in_docker()</code>","text":"<p>Checks if the current process is running inside a Docker container.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if running inside a Docker container, False otherwise.</p> Source code in <code>inference/core/devices/utils.py</code> <pre><code>def is_running_in_docker():\n    \"\"\"Checks if the current process is running inside a Docker container.\n\n    Returns:\n        bool: True if running inside a Docker container, False otherwise.\n    \"\"\"\n    return os.path.exists(\"/.dockerenv\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/common/","title":"common","text":""},{"location":"docs/reference/inference/core/entities/types/","title":"types","text":""},{"location":"docs/reference/inference/core/entities/requests/clip/","title":"clip","text":""},{"location":"docs/reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipCompareRequest","title":"<code>ClipCompareRequest</code>","text":"<p>               Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP comparison.</p> <p>Attributes:</p> Name Type Description <code>subject</code> <code>Union[InferenceRequestImage, str]</code> <p>The type of image data provided, one of 'url' or 'base64'.</p> <code>subject_type</code> <code>str</code> <p>The type of subject, one of 'image' or 'text'.</p> <code>prompt</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage, str, List[str], Dict[str, Union[InferenceRequestImage, str]]]</code> <p>The prompt for comparison.</p> <code>prompt_type</code> <code>str</code> <p>The type of prompt, one of 'image' or 'text'.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipCompareRequest(ClipInferenceRequest):\n    \"\"\"Request for CLIP comparison.\n\n    Attributes:\n        subject (Union[InferenceRequestImage, str]): The type of image data provided, one of 'url' or 'base64'.\n        subject_type (str): The type of subject, one of 'image' or 'text'.\n        prompt (Union[List[InferenceRequestImage], InferenceRequestImage, str, List[str], Dict[str, Union[InferenceRequestImage, str]]]): The prompt for comparison.\n        prompt_type (str): The type of prompt, one of 'image' or 'text'.\n    \"\"\"\n\n    subject: Union[InferenceRequestImage, str] = Field(\n        examples=[\"url\"],\n        description=\"The type of image data provided, one of 'url' or 'base64'\",\n    )\n    subject_type: str = Field(\n        default=\"image\",\n        examples=[\"image\"],\n        description=\"The type of subject, one of 'image' or 'text'\",\n    )\n    prompt: Union[\n        List[InferenceRequestImage],\n        InferenceRequestImage,\n        str,\n        List[str],\n        Dict[str, Union[InferenceRequestImage, str]],\n    ]\n    prompt_type: str = Field(\n        default=\"text\",\n        examples=[\"text\"],\n        description=\"The type of prompt, one of 'image' or 'text'\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipImageEmbeddingRequest","title":"<code>ClipImageEmbeddingRequest</code>","text":"<p>               Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP image embedding.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) to be embedded.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipImageEmbeddingRequest(ClipInferenceRequest):\n    \"\"\"Request for CLIP image embedding.\n\n    Attributes:\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) to be embedded.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipInferenceRequest","title":"<code>ClipInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>Request for CLIP inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>clip_version_id</code> <code>Optional[str]</code> <p>The version ID of CLIP to be used for this request.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipInferenceRequest(BaseRequest):\n    \"\"\"Request for CLIP inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        clip_version_id (Optional[str]): The version ID of CLIP to be used for this request.\n    \"\"\"\n\n    clip_version_id: Optional[str] = Field(\n        default=CLIP_VERSION_ID,\n        examples=[\"ViT-B-16\"],\n        description=\"The version ID of CLIP to be used for this request. Must be one of RN101, RN50, RN50x16, RN50x4, RN50x64, ViT-B-16, ViT-B-32, ViT-L-14-336px, and ViT-L-14.\",\n    )\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"clip_version_id\") is None:\n            return None\n        return f\"clip/{values['clip_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/clip/#inference.core.entities.requests.clip.ClipTextEmbeddingRequest","title":"<code>ClipTextEmbeddingRequest</code>","text":"<p>               Bases: <code>ClipInferenceRequest</code></p> <p>Request for CLIP text embedding.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>Union[List[str], str]</code> <p>A string or list of strings.</p> Source code in <code>inference/core/entities/requests/clip.py</code> <pre><code>class ClipTextEmbeddingRequest(ClipInferenceRequest):\n    \"\"\"Request for CLIP text embedding.\n\n    Attributes:\n        text (Union[List[str], str]): A string or list of strings.\n    \"\"\"\n\n    text: Union[List[str], str] = Field(\n        examples=[\"The quick brown fox jumps over the lazy dog\"],\n        description=\"A string or list of strings\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/cogvlm/","title":"cogvlm","text":""},{"location":"docs/reference/inference/core/entities/requests/cogvlm/#inference.core.entities.requests.cogvlm.CogVLMInferenceRequest","title":"<code>CogVLMInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>Request for CogVLM inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>cog_version_id</code> <code>Optional[str]</code> <p>The version ID of CLIP to be used for this request.</p> Source code in <code>inference/core/entities/requests/cogvlm.py</code> <pre><code>class CogVLMInferenceRequest(BaseRequest):\n    \"\"\"Request for CogVLM inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        cog_version_id (Optional[str]): The version ID of CLIP to be used for this request.\n    \"\"\"\n\n    cogvlm_version_id: Optional[str] = Field(\n        default=COGVLM_VERSION_ID,\n        examples=[\"cogvlm-chat-hf\"],\n        description=\"The version ID of CogVLM to be used for this request. See the huggingface model repo at THUDM.\",\n    )\n    model_id: Optional[str] = Field(None)\n    image: InferenceRequestImage = Field(\n        description=\"Image for CogVLM to look at. Use prompt to specify what you want it to do with the image.\"\n    )\n    prompt: str = Field(\n        description=\"Text to be passed to CogVLM. Use to prompt it to describe an image or provide only text to chat with the model.\",\n        examples=[\"Describe this image.\"],\n    )\n    history: Optional[List[Tuple[str, str]]] = Field(\n        None,\n        description=\"Optional chat history, formatted as a list of 2-tuples where the first entry is the user prompt\"\n        \" and the second entry is the generated model response\",\n    )\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"cogvlm_version_id\") is None:\n            return None\n        return f\"cogvlm/{values['cogvlm_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/doctr/","title":"doctr","text":""},{"location":"docs/reference/inference/core/entities/requests/doctr/#inference.core.entities.requests.doctr.DoctrOCRInferenceRequest","title":"<code>DoctrOCRInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>DocTR inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> Source code in <code>inference/core/entities/requests/doctr.py</code> <pre><code>class DoctrOCRInferenceRequest(BaseRequest):\n    \"\"\"\n    DocTR inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    doctr_version_id: Optional[str] = \"default\"\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"doctr_version_id\") is None:\n            return None\n        return f\"doctr/{values['doctr_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/dynamic_class_base/","title":"dynamic_class_base","text":""},{"location":"docs/reference/inference/core/entities/requests/dynamic_class_base/#inference.core.entities.requests.dynamic_class_base.DynamicClassBaseInferenceRequest","title":"<code>DynamicClassBaseInferenceRequest</code>","text":"<p>               Bases: <code>CVInferenceRequest</code></p> <p>Request for zero-shot object detection models (with dynamic class lists).</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings.</p> Source code in <code>inference/core/entities/requests/dynamic_class_base.py</code> <pre><code>class DynamicClassBaseInferenceRequest(CVInferenceRequest):\n    \"\"\"Request for zero-shot object detection models (with dynamic class lists).\n\n    Attributes:\n        text (List[str]): A list of strings.\n    \"\"\"\n\n    model_id: Optional[str] = Field(None)\n    text: List[str] = Field(\n        examples=[[\"person\", \"dog\", \"cat\"]],\n        description=\"A list of strings\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/gaze/","title":"gaze","text":""},{"location":"docs/reference/inference/core/entities/requests/gaze/#inference.core.entities.requests.gaze.GazeDetectionInferenceRequest","title":"<code>GazeDetectionInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>Request for gaze detection inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>gaze_version_id</code> <code>Optional[str]</code> <p>The version ID of Gaze to be used for this request.</p> <code>do_run_face_detection</code> <code>Optional[bool]</code> <p>If true, face detection will be applied; if false, face detection will be ignored and the whole input image will be used for gaze detection.</p> <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) for inference.</p> Source code in <code>inference/core/entities/requests/gaze.py</code> <pre><code>class GazeDetectionInferenceRequest(BaseRequest):\n    \"\"\"Request for gaze detection inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        gaze_version_id (Optional[str]): The version ID of Gaze to be used for this request.\n        do_run_face_detection (Optional[bool]): If true, face detection will be applied; if false, face detection will be ignored and the whole input image will be used for gaze detection.\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) for inference.\n    \"\"\"\n\n    gaze_version_id: Optional[str] = Field(\n        default=GAZE_VERSION_ID,\n        examples=[\"l2cs\"],\n        description=\"The version ID of Gaze to be used for this request. Must be one of l2cs.\",\n    )\n\n    do_run_face_detection: Optional[bool] = Field(\n        default=True,\n        examples=[False],\n        description=\"If true, face detection will be applied; if false, face detection will be ignored and the whole input image will be used for gaze detection\",\n    )\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"gaze_version_id\") is None:\n            return None\n        return f\"gaze/{values['gaze_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/groundingdino/","title":"groundingdino","text":""},{"location":"docs/reference/inference/core/entities/requests/groundingdino/#inference.core.entities.requests.groundingdino.GroundingDINOInferenceRequest","title":"<code>GroundingDINOInferenceRequest</code>","text":"<p>               Bases: <code>DynamicClassBaseInferenceRequest</code></p> <p>Request for Grounding DINO zero-shot predictions.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings.</p> Source code in <code>inference/core/entities/requests/groundingdino.py</code> <pre><code>class GroundingDINOInferenceRequest(DynamicClassBaseInferenceRequest):\n    \"\"\"Request for Grounding DINO zero-shot predictions.\n\n    Attributes:\n        text (List[str]): A list of strings.\n    \"\"\"\n\n    box_threshold: Optional[float] = 0.5\n    grounding_dino_version_id: Optional[str] = \"default\"\n    text_threshold: Optional[float] = 0.5\n    class_agnostic_nms: Optional[bool] = CLASS_AGNOSTIC_NMS\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/","title":"inference","text":""},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.BaseRequest","title":"<code>BaseRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base request for inference.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str_</code> <p>A unique request identifier.</p> <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key that will be passed to the model during initialization for artifact retrieval.</p> <code>start</code> <code>Optional[float]</code> <p>start time of request</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class BaseRequest(BaseModel):\n    \"\"\"Base request for inference.\n\n    Attributes:\n        id (str_): A unique request identifier.\n        api_key (Optional[str]): Roboflow API Key that will be passed to the model during initialization for artifact retrieval.\n        start (Optional[float]): start time of request\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        kwargs[\"id\"] = kwargs.get(\"id\", str(uuid4()))\n        super().__init__(**kwargs)\n\n    model_config = ConfigDict(protected_namespaces=())\n    id: str\n    api_key: Optional[str] = ApiKey\n    start: Optional[float] = None\n    source: Optional[str] = None\n    source_info: Optional[str] = None\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.CVInferenceRequest","title":"<code>CVInferenceRequest</code>","text":"<p>               Bases: <code>InferenceRequest</code></p> <p>Computer Vision inference request.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) for inference.</p> <code>disable_preproc_auto_orient</code> <code>Optional[bool]</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>disable_preproc_contrast</code> <code>Optional[bool]</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>disable_preproc_grayscale</code> <code>Optional[bool]</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>disable_preproc_static_crop</code> <code>Optional[bool]</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class CVInferenceRequest(InferenceRequest):\n    \"\"\"Computer Vision inference request.\n\n    Attributes:\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) for inference.\n        disable_preproc_auto_orient (Optional[bool]): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (Optional[bool]): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (Optional[bool]): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (Optional[bool]): If true, the static crop preprocessing step is disabled for this call. Default is False.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    disable_preproc_auto_orient: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the auto orient preprocessing step is disabled for this call.\",\n    )\n    disable_preproc_contrast: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the auto contrast preprocessing step is disabled for this call.\",\n    )\n    disable_preproc_grayscale: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the grayscale preprocessing step is disabled for this call.\",\n    )\n    disable_preproc_static_crop: Optional[bool] = Field(\n        default=False,\n        description=\"If true, the static crop preprocessing step is disabled for this call.\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.ClassificationInferenceRequest","title":"<code>ClassificationInferenceRequest</code>","text":"<p>               Bases: <code>CVInferenceRequest</code></p> <p>Classification inference request.</p> <p>Attributes:</p> Name Type Description <code>confidence</code> <code>Optional[float]</code> <p>The confidence threshold used to filter out predictions.</p> <code>visualization_stroke_width</code> <code>Optional[int]</code> <p>The stroke width used when visualizing predictions.</p> <code>visualize_predictions</code> <code>Optional[bool]</code> <p>If true, the predictions will be drawn on the original image and returned as a base64 string.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class ClassificationInferenceRequest(CVInferenceRequest):\n    \"\"\"Classification inference request.\n\n    Attributes:\n        confidence (Optional[float]): The confidence threshold used to filter out predictions.\n        visualization_stroke_width (Optional[int]): The stroke width used when visualizing predictions.\n        visualize_predictions (Optional[bool]): If true, the predictions will be drawn on the original image and returned as a base64 string.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        kwargs[\"model_type\"] = \"classification\"\n        super().__init__(**kwargs)\n\n    confidence: Optional[float] = Field(\n        default=0.4,\n        examples=[0.5],\n        description=\"The confidence threshold used to filter out predictions\",\n    )\n    visualization_stroke_width: Optional[int] = Field(\n        default=1,\n        examples=[1],\n        description=\"The stroke width used when visualizing predictions\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be drawn on the original image and returned as a base64 string\",\n    )\n    disable_active_learning: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n    )\n    active_learning_target_dataset: Optional[str] = Field(\n        default=None,\n        examples=[\"my_dataset\"],\n        description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.InferenceRequest","title":"<code>InferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>Base request for inference.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>A unique model identifier.</p> <code>model_type</code> <code>Optional[str]</code> <p>The type of the model, usually referring to what task the model performs.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class InferenceRequest(BaseRequest):\n    \"\"\"Base request for inference.\n\n    Attributes:\n        model_id (str): A unique model identifier.\n        model_type (Optional[str]): The type of the model, usually referring to what task the model performs.\n    \"\"\"\n\n    model_id: Optional[str] = ModelID\n    model_type: Optional[str] = ModelType\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.InferenceRequestImage","title":"<code>InferenceRequestImage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Image data for inference request.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of image data provided, one of 'url', 'base64', or 'numpy'.</p> <code>value</code> <code>Optional[Any]</code> <p>Image data corresponding to the image type.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class InferenceRequestImage(BaseModel):\n    \"\"\"Image data for inference request.\n\n    Attributes:\n        type (str): The type of image data provided, one of 'url', 'base64', or 'numpy'.\n        value (Optional[Any]): Image data corresponding to the image type.\n    \"\"\"\n\n    type: str = Field(\n        examples=[\"url\"],\n        description=\"The type of image data provided, one of 'url', 'base64', or 'numpy'\",\n    )\n    value: Optional[Any] = Field(\n        None,\n        examples=[\"http://www.example-image-url.com\"],\n        description=\"Image data corresponding to the image type, if type = 'url' then value is a string containing the url of an image, else if type = 'base64' then value is a string containing base64 encoded image data, else if type = 'numpy' then value is binary numpy data serialized using pickle.dumps(); array should 3 dimensions, channels last, with values in the range [0,255].\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.InstanceSegmentationInferenceRequest","title":"<code>InstanceSegmentationInferenceRequest</code>","text":"<p>               Bases: <code>ObjectDetectionInferenceRequest</code></p> <p>Instance Segmentation inference request.</p> <p>Attributes:</p> Name Type Description <code>mask_decode_mode</code> <code>Optional[str]</code> <p>The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'.</p> <code>tradeoff_factor</code> <code>Optional[float]</code> <p>The amount to tradeoff between 0='fast' and 1='accurate'.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class InstanceSegmentationInferenceRequest(ObjectDetectionInferenceRequest):\n    \"\"\"Instance Segmentation inference request.\n\n    Attributes:\n        mask_decode_mode (Optional[str]): The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'.\n        tradeoff_factor (Optional[float]): The amount to tradeoff between 0='fast' and 1='accurate'.\n    \"\"\"\n\n    mask_decode_mode: Optional[str] = Field(\n        default=\"accurate\",\n        examples=[\"accurate\"],\n        description=\"The mode used to decode instance segmentation masks, one of 'accurate', 'fast', 'tradeoff'\",\n    )\n    tradeoff_factor: Optional[float] = Field(\n        default=0.0,\n        examples=[0.5],\n        description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.ObjectDetectionInferenceRequest","title":"<code>ObjectDetectionInferenceRequest</code>","text":"<p>               Bases: <code>CVInferenceRequest</code></p> <p>Object Detection inference request.</p> <p>Attributes:</p> Name Type Description <code>class_agnostic_nms</code> <code>Optional[bool]</code> <p>If true, NMS is applied to all detections at once, if false, NMS is applied per class.</p> <code>class_filter</code> <code>Optional[List[str]]</code> <p>If provided, only predictions for the listed classes will be returned.</p> <code>confidence</code> <code>Optional[float]</code> <p>The confidence threshold used to filter out predictions.</p> <code>fix_batch_size</code> <code>Optional[bool]</code> <p>If true, the batch size will be fixed to the maximum batch size configured for this server.</p> <code>iou_threshold</code> <code>Optional[float]</code> <p>The IoU threshold that must be met for a box pair to be considered duplicate during NMS.</p> <code>max_detections</code> <code>Optional[int]</code> <p>The maximum number of detections that will be returned.</p> <code>max_candidates</code> <code>Optional[int]</code> <p>The maximum number of candidate detections passed to NMS.</p> <code>visualization_labels</code> <code>Optional[bool]</code> <p>If true, labels will be rendered on prediction visualizations.</p> <code>visualization_stroke_width</code> <code>Optional[int]</code> <p>The stroke width used when visualizing predictions.</p> <code>visualize_predictions</code> <code>Optional[bool]</code> <p>If true, the predictions will be drawn on the original image and returned as a base64 string.</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>class ObjectDetectionInferenceRequest(CVInferenceRequest):\n    \"\"\"Object Detection inference request.\n\n    Attributes:\n        class_agnostic_nms (Optional[bool]): If true, NMS is applied to all detections at once, if false, NMS is applied per class.\n        class_filter (Optional[List[str]]): If provided, only predictions for the listed classes will be returned.\n        confidence (Optional[float]): The confidence threshold used to filter out predictions.\n        fix_batch_size (Optional[bool]): If true, the batch size will be fixed to the maximum batch size configured for this server.\n        iou_threshold (Optional[float]): The IoU threshold that must be met for a box pair to be considered duplicate during NMS.\n        max_detections (Optional[int]): The maximum number of detections that will be returned.\n        max_candidates (Optional[int]): The maximum number of candidate detections passed to NMS.\n        visualization_labels (Optional[bool]): If true, labels will be rendered on prediction visualizations.\n        visualization_stroke_width (Optional[int]): The stroke width used when visualizing predictions.\n        visualize_predictions (Optional[bool]): If true, the predictions will be drawn on the original image and returned as a base64 string.\n    \"\"\"\n\n    class_agnostic_nms: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, NMS is applied to all detections at once, if false, NMS is applied per class\",\n    )\n    class_filter: Optional[List[str]] = Field(\n        default=None,\n        examples=[[\"class-1\", \"class-2\", \"class-n\"]],\n        description=\"If provided, only predictions for the listed classes will be returned\",\n    )\n    confidence: Optional[float] = Field(\n        default=0.4,\n        examples=[0.5],\n        description=\"The confidence threshold used to filter out predictions\",\n    )\n    fix_batch_size: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the batch size will be fixed to the maximum batch size configured for this server\",\n    )\n    iou_threshold: Optional[float] = Field(\n        default=0.3,\n        examples=[0.5],\n        description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n    )\n    max_detections: Optional[int] = Field(\n        default=300,\n        examples=[300],\n        description=\"The maximum number of detections that will be returned\",\n    )\n    max_candidates: Optional[int] = Field(\n        default=3000,\n        description=\"The maximum number of candidate detections passed to NMS\",\n    )\n    visualization_labels: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, labels will be rendered on prediction visualizations\",\n    )\n    visualization_stroke_width: Optional[int] = Field(\n        default=1,\n        examples=[1],\n        description=\"The stroke width used when visualizing predictions\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be drawn on the original image and returned as a base64 string\",\n    )\n    disable_active_learning: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n    )\n    active_learning_target_dataset: Optional[str] = Field(\n        default=None,\n        examples=[\"my_dataset\"],\n        description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/inference/#inference.core.entities.requests.inference.request_from_type","title":"<code>request_from_type(model_type, request_dict)</code>","text":"<p>Uses original request id</p> Source code in <code>inference/core/entities/requests/inference.py</code> <pre><code>def request_from_type(model_type, request_dict):\n    \"\"\"Uses original request id\"\"\"\n    if model_type == \"classification\":\n        request = ClassificationInferenceRequest(**request_dict)\n    elif model_type == \"instance-segmentation\":\n        request = InstanceSegmentationInferenceRequest(**request_dict)\n    elif model_type == \"object-detection\":\n        request = ObjectDetectionInferenceRequest(**request_dict)\n    else:\n        raise ValueError(f\"Uknown task type {model_type}\")\n    request.id = request_dict.get(\"id\", request.id)\n    return request\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/owlv2/","title":"owlv2","text":""},{"location":"docs/reference/inference/core/entities/requests/owlv2/#inference.core.entities.requests.owlv2.OwlV2InferenceRequest","title":"<code>OwlV2InferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>Request for gaze detection inference.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>owlv2_version_id</code> <code>Optional[str]</code> <p>The version ID of Gaze to be used for this request.</p> <code>image</code> <code>Union[List[InferenceRequestImage], InferenceRequestImage]</code> <p>Image(s) for inference.</p> <code>training_data</code> <code>List[TrainingImage]</code> <p>Training data to ground the model on</p> <code>confidence</code> <code>float</code> <p>Confidence threshold to filter predictions by</p> Source code in <code>inference/core/entities/requests/owlv2.py</code> <pre><code>class OwlV2InferenceRequest(BaseRequest):\n    \"\"\"Request for gaze detection inference.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        owlv2_version_id (Optional[str]): The version ID of Gaze to be used for this request.\n        image (Union[List[InferenceRequestImage], InferenceRequestImage]): Image(s) for inference.\n        training_data (List[TrainingImage]): Training data to ground the model on\n        confidence (float): Confidence threshold to filter predictions by\n    \"\"\"\n\n    owlv2_version_id: Optional[str] = Field(\n        default=OWLV2_VERSION_ID,\n        examples=[\"owlv2-base-patch16-ensemble\"],\n        description=\"The version ID of owlv2 to be used for this request.\",\n    )\n    model_id: Optional[str] = Field(\n        default=None, description=\"Model id to be used in the request.\"\n    )\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage] = Field(\n        description=\"Images to run the model on\"\n    )\n    training_data: List[TrainingImage] = Field(\n        description=\"Training images for the owlvit model to learn form\"\n    )\n    confidence: Optional[float] = Field(\n        default=0.99,\n        examples=[0.99],\n        description=\"Default confidence threshold for owlvit predictions. \"\n        \"Needs to be much higher than you're used to, probably 0.99 - 0.9999\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, return visualized predictions as a base64 string\",\n    )\n    visualization_labels: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, labels will be rendered on prediction visualizations\",\n    )\n    visualization_stroke_width: Optional[int] = Field(\n        default=1,\n        examples=[1],\n        description=\"The stroke width used when visualizing predictions\",\n    )\n    visualize_predictions: Optional[bool] = Field(\n        default=False,\n        examples=[False],\n        description=\"If true, the predictions will be drawn on the original image and returned as a base64 string\",\n    )\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"owl2_version_id\") is None:\n            return None\n        return f\"google/{values['owl2_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/sam/","title":"sam","text":""},{"location":"docs/reference/inference/core/entities/requests/sam/#inference.core.entities.requests.sam.SamEmbeddingRequest","title":"<code>SamEmbeddingRequest</code>","text":"<p>               Bases: <code>SamInferenceRequest</code></p> <p>SAM embedding request.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Optional[InferenceRequestImage]</code> <p>The image to be embedded.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be embedded used to cache the embedding.</p> <code>format</code> <code>Optional[str]</code> <p>The format of the response. Must be one of json or binary.</p> Source code in <code>inference/core/entities/requests/sam.py</code> <pre><code>class SamEmbeddingRequest(SamInferenceRequest):\n    \"\"\"SAM embedding request.\n\n    Attributes:\n        image (Optional[inference.core.entities.requests.inference.InferenceRequestImage]): The image to be embedded.\n        image_id (Optional[str]): The ID of the image to be embedded used to cache the embedding.\n        format (Optional[str]): The format of the response. Must be one of json or binary.\n    \"\"\"\n\n    image: Optional[InferenceRequestImage] = Field(\n        default=None,\n        description=\"The image to be embedded\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        examples=[\"image_id\"],\n        description=\"The ID of the image to be embedded used to cache the embedding.\",\n    )\n    format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the response. Must be one of json or binary. If binary, embedding is returned as a binary numpy array.\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/sam/#inference.core.entities.requests.sam.SamInferenceRequest","title":"<code>SamInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>SAM inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>sam_version_id</code> <code>Optional[str]</code> <p>The version ID of SAM to be used for this request.</p> Source code in <code>inference/core/entities/requests/sam.py</code> <pre><code>class SamInferenceRequest(BaseRequest):\n    \"\"\"SAM inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        sam_version_id (Optional[str]): The version ID of SAM to be used for this request.\n    \"\"\"\n\n    sam_version_id: Optional[str] = Field(\n        default=SAM_VERSION_ID,\n        examples=[\"vit_h\"],\n        description=\"The version ID of SAM to be used for this request. Must be one of vit_h, vit_l, or vit_b.\",\n    )\n\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"sam_version_id\") is None:\n            return None\n        return f\"sam/{values['sam_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/sam/#inference.core.entities.requests.sam.SamSegmentationRequest","title":"<code>SamSegmentationRequest</code>","text":"<p>               Bases: <code>SamInferenceRequest</code></p> <p>SAM segmentation request.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>Optional[Union[List[List[List[List[float]]]], Any]]</code> <p>The embeddings to be decoded.</p> <code>embeddings_format</code> <code>Optional[str]</code> <p>The format of the embeddings.</p> <code>format</code> <code>Optional[str]</code> <p>The format of the response.</p> <code>image</code> <code>Optional[InferenceRequestImage]</code> <p>The image to be segmented.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be segmented used to retrieve cached embeddings.</p> <code>has_mask_input</code> <code>Optional[bool]</code> <p>Whether or not the request includes a mask input.</p> <code>mask_input</code> <code>Optional[Union[List[List[List[float]]], Any]]</code> <p>The set of output masks.</p> <code>mask_input_format</code> <code>Optional[str]</code> <p>The format of the mask input.</p> <code>orig_im_size</code> <code>Optional[List[int]]</code> <p>The original size of the image used to generate the embeddings.</p> <code>point_coords</code> <code>Optional[List[List[float]]]</code> <p>The coordinates of the interactive points used during decoding.</p> <code>point_labels</code> <code>Optional[List[float]]</code> <p>The labels of the interactive points used during decoding.</p> <code>use_mask_input_cache</code> <code>Optional[bool]</code> <p>Whether or not to use the mask input cache.</p> Source code in <code>inference/core/entities/requests/sam.py</code> <pre><code>class SamSegmentationRequest(SamInferenceRequest):\n    \"\"\"SAM segmentation request.\n\n    Attributes:\n        embeddings (Optional[Union[List[List[List[List[float]]]], Any]]): The embeddings to be decoded.\n        embeddings_format (Optional[str]): The format of the embeddings.\n        format (Optional[str]): The format of the response.\n        image (Optional[InferenceRequestImage]): The image to be segmented.\n        image_id (Optional[str]): The ID of the image to be segmented used to retrieve cached embeddings.\n        has_mask_input (Optional[bool]): Whether or not the request includes a mask input.\n        mask_input (Optional[Union[List[List[List[float]]], Any]]): The set of output masks.\n        mask_input_format (Optional[str]): The format of the mask input.\n        orig_im_size (Optional[List[int]]): The original size of the image used to generate the embeddings.\n        point_coords (Optional[List[List[float]]]): The coordinates of the interactive points used during decoding.\n        point_labels (Optional[List[float]]): The labels of the interactive points used during decoding.\n        use_mask_input_cache (Optional[bool]): Whether or not to use the mask input cache.\n    \"\"\"\n\n    embeddings: Optional[Union[List[List[List[List[float]]]], Any]] = Field(\n        None,\n        examples=[\"[[[[0.1, 0.2, 0.3, ...] ...] ...]]\"],\n        description=\"The embeddings to be decoded. The dimensions of the embeddings are 1 x 256 x 64 x 64. If embeddings is not provided, image must be provided.\",\n    )\n    embeddings_format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the embeddings. Must be one of json or binary. If binary, embeddings are expected to be a binary numpy array.\",\n    )\n    format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the response. Must be one of json or binary. If binary, masks are returned as binary numpy arrays. If json, masks are converted to polygons, then returned as json.\",\n    )\n    image: Optional[InferenceRequestImage] = Field(\n        default=None,\n        description=\"The image to be segmented. Only required if embeddings are not provided.\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        examples=[\"image_id\"],\n        description=\"The ID of the image to be segmented used to retrieve cached embeddings. If an embedding is cached, it will be used instead of generating a new embedding. If no embedding is cached, a new embedding will be generated and cached.\",\n    )\n    has_mask_input: Optional[bool] = Field(\n        default=False,\n        examples=[True],\n        description=\"Whether or not the request includes a mask input. If true, the mask input must be provided.\",\n    )\n    mask_input: Optional[Union[List[List[List[float]]], Any]] = Field(\n        default=None,\n        description=\"The set of output masks. If request format is json, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If request format is binary, masks is a list of binary numpy arrays. The dimensions of each mask are 256 x 256. This is the same as the output, low resolution mask from the previous inference.\",\n    )\n    mask_input_format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the mask input. Must be one of json or binary. If binary, mask input is expected to be a binary numpy array.\",\n    )\n    orig_im_size: Optional[List[int]] = Field(\n        default=None,\n        examples=[[640, 320]],\n        description=\"The original size of the image used to generate the embeddings. This is only required if the image is not provided.\",\n    )\n    point_coords: Optional[List[List[float]]] = Field(\n        default=[[0.0, 0.0]],\n        examples=[[[10.0, 10.0]]],\n        description=\"The coordinates of the interactive points used during decoding. Each point (x,y pair) corresponds to a label in point_labels.\",\n    )\n    point_labels: Optional[List[float]] = Field(\n        default=[-1],\n        examples=[[1]],\n        description=\"The labels of the interactive points used during decoding. A 1 represents a positive point (part of the object to be segmented). A -1 represents a negative point (not part of the object to be segmented). Each label corresponds to a point in point_coords.\",\n    )\n    use_mask_input_cache: Optional[bool] = Field(\n        default=True,\n        examples=[True],\n        description=\"Whether or not to use the mask input cache. If true, the mask input cache will be used if it exists. If false, the mask input cache will not be used.\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/sam2/","title":"sam2","text":""},{"location":"docs/reference/inference/core/entities/requests/sam2/#inference.core.entities.requests.sam2.Sam2EmbeddingRequest","title":"<code>Sam2EmbeddingRequest</code>","text":"<p>               Bases: <code>Sam2InferenceRequest</code></p> <p>SAM embedding request.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Optional[InferenceRequestImage]</code> <p>The image to be embedded.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be embedded used to cache the embedding.</p> <code>format</code> <code>Optional[str]</code> <p>The format of the response. Must be one of json or binary.</p> Source code in <code>inference/core/entities/requests/sam2.py</code> <pre><code>class Sam2EmbeddingRequest(Sam2InferenceRequest):\n    \"\"\"SAM embedding request.\n\n    Attributes:\n        image (Optional[inference.core.entities.requests.inference.InferenceRequestImage]): The image to be embedded.\n        image_id (Optional[str]): The ID of the image to be embedded used to cache the embedding.\n        format (Optional[str]): The format of the response. Must be one of json or binary.\n    \"\"\"\n\n    image: Optional[InferenceRequestImage] = Field(\n        default=None,\n        description=\"The image to be embedded\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        examples=[\"image_id\"],\n        description=\"The ID of the image to be embedded used to cache the embedding.\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/sam2/#inference.core.entities.requests.sam2.Sam2InferenceRequest","title":"<code>Sam2InferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>SAM2 inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> <code>sam2_version_id</code> <code>Optional[str]</code> <p>The version ID of SAM2 to be used for this request.</p> Source code in <code>inference/core/entities/requests/sam2.py</code> <pre><code>class Sam2InferenceRequest(BaseRequest):\n    \"\"\"SAM2 inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n        sam2_version_id (Optional[str]): The version ID of SAM2 to be used for this request.\n    \"\"\"\n\n    sam2_version_id: Optional[str] = Field(\n        default=SAM2_VERSION_ID,\n        examples=[\"hiera_large\"],\n        description=\"The version ID of SAM to be used for this request. Must be one of hiera_tiny, hiera_small, hiera_large, hiera_b_plus\",\n    )\n\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"sam_version_id\") is None:\n            return None\n        return f\"sam2/{values['sam_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/sam2/#inference.core.entities.requests.sam2.Sam2SegmentationRequest","title":"<code>Sam2SegmentationRequest</code>","text":"<p>               Bases: <code>Sam2InferenceRequest</code></p> <p>SAM segmentation request.</p> <p>Attributes:</p> Name Type Description <code>format</code> <code>Optional[str]</code> <p>The format of the response.</p> <code>image</code> <code>InferenceRequestImage</code> <p>The image to be segmented.</p> <code>image_id</code> <code>Optional[str]</code> <p>The ID of the image to be segmented used to retrieve cached embeddings.</p> <code>point_coords</code> <code>Optional[List[List[float]]]</code> <p>The coordinates of the interactive points used during decoding.</p> <code>point_labels</code> <code>Optional[List[float]]</code> <p>The labels of the interactive points used during decoding.</p> Source code in <code>inference/core/entities/requests/sam2.py</code> <pre><code>class Sam2SegmentationRequest(Sam2InferenceRequest):\n    \"\"\"SAM segmentation request.\n\n    Attributes:\n        format (Optional[str]): The format of the response.\n        image (InferenceRequestImage): The image to be segmented.\n        image_id (Optional[str]): The ID of the image to be segmented used to retrieve cached embeddings.\n        point_coords (Optional[List[List[float]]]): The coordinates of the interactive points used during decoding.\n        point_labels (Optional[List[float]]): The labels of the interactive points used during decoding.\n    \"\"\"\n\n    format: Optional[str] = Field(\n        default=\"json\",\n        examples=[\"json\"],\n        description=\"The format of the response. Must be one of json or binary. If binary, masks are returned as binary numpy arrays. If json, masks are converted to polygons, then returned as json.\",\n    )\n    image: InferenceRequestImage = Field(\n        description=\"The image to be segmented.\",\n    )\n    image_id: Optional[str] = Field(\n        default=None,\n        examples=[\"image_id\"],\n        description=\"The ID of the image to be segmented used to retrieve cached embeddings. If an embedding is cached, it will be used instead of generating a new embedding. If no embedding is cached, a new embedding will be generated and cached.\",\n    )\n    prompts: Sam2PromptSet = Field(\n        default=Sam2PromptSet(prompts=None),\n        example=[{\"prompts\": [{\"points\": [{\"x\": 100, \"y\": 100, \"positive\": True}]}]}],\n        description=\"A list of prompts for masks to predict. Each prompt can include a bounding box and / or a set of postive or negative points\",\n    )\n    multimask_output: bool = Field(\n        default=True,\n        examples=[True],\n        description=\"If true, the model will return three masks. \"\n        \"For ambiguous input prompts (such as a single click), this will often \"\n        \"produce better masks than a single prediction. If only a single \"\n        \"mask is needed, the model's predicted quality score can be used \"\n        \"to select the best mask. For non-ambiguous prompts, such as multiple \"\n        \"input prompts, multimask_output=False can give better results.\",\n    )\n\n    save_logits_to_cache: bool = Field(\n        default=False,\n        description=\"If True, saves the low-resolution logits to the cache for potential future use. \"\n        \"This can speed up subsequent requests with similar prompts on the same image. \"\n        \"This feature is ignored if DISABLE_SAM2_LOGITS_CACHE env variable is set True\",\n    )\n    load_logits_from_cache: bool = Field(\n        default=False,\n        description=\"If True, attempts to load previously cached low-resolution logits for the given image and prompt set. \"\n        \"This can significantly speed up inference when making multiple similar requests on the same image. \"\n        \"This feature is ignored if DISABLE_SAM2_LOGITS_CACHE env variable is set True\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/server_state/","title":"server_state","text":""},{"location":"docs/reference/inference/core/entities/requests/server_state/#inference.core.entities.requests.server_state.AddModelRequest","title":"<code>AddModelRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request to add a model to the inference server.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>A unique model identifier.</p> <code>model_type</code> <code>Optional[str]</code> <p>The type of the model, usually referring to what task the model performs.</p> <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key that will be passed to the model during initialization for artifact retrieval.</p> Source code in <code>inference/core/entities/requests/server_state.py</code> <pre><code>class AddModelRequest(BaseModel):\n    \"\"\"Request to add a model to the inference server.\n\n    Attributes:\n        model_id (str): A unique model identifier.\n        model_type (Optional[str]): The type of the model, usually referring to what task the model performs.\n        api_key (Optional[str]): Roboflow API Key that will be passed to the model during initialization for artifact retrieval.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n    model_id: str = ModelID\n    model_type: Optional[str] = ModelType\n    api_key: Optional[str] = ApiKey\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/server_state/#inference.core.entities.requests.server_state.ClearModelRequest","title":"<code>ClearModelRequest</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request to clear a model from the inference server.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>A unique model identifier.</p> Source code in <code>inference/core/entities/requests/server_state.py</code> <pre><code>class ClearModelRequest(BaseModel):\n    \"\"\"Request to clear a model from the inference server.\n\n    Attributes:\n        model_id (str): A unique model identifier.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n    model_id: str = ModelID\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/trocr/","title":"trocr","text":""},{"location":"docs/reference/inference/core/entities/requests/trocr/#inference.core.entities.requests.trocr.TrOCRInferenceRequest","title":"<code>TrOCRInferenceRequest</code>","text":"<p>               Bases: <code>BaseRequest</code></p> <p>TrOCR inference request.</p> <p>Attributes:</p> Name Type Description <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API Key.</p> Source code in <code>inference/core/entities/requests/trocr.py</code> <pre><code>class TrOCRInferenceRequest(BaseRequest):\n    \"\"\"\n    TrOCR inference request.\n\n    Attributes:\n        api_key (Optional[str]): Roboflow API Key.\n    \"\"\"\n\n    image: Union[List[InferenceRequestImage], InferenceRequestImage]\n    trocr_version_id: Optional[str] = \"trocr-base-printed\"\n    model_id: Optional[str] = Field(None)\n\n    # TODO[pydantic]: We couldn't refactor the `validator`, please replace it by `field_validator` manually.\n    # Check https://docs.pydantic.dev/dev-v2/migration/#changes-to-validators for more information.\n    @validator(\"model_id\", always=True, allow_reuse=True)\n    def validate_model_id(cls, value, values):\n        if value is not None:\n            return value\n        if values.get(\"trocr_version_id\") is None:\n            return None\n        return f\"trocr/{values['trocr_version_id']}\"\n</code></pre>"},{"location":"docs/reference/inference/core/entities/requests/workflows/","title":"workflows","text":""},{"location":"docs/reference/inference/core/entities/requests/yolo_world/","title":"yolo_world","text":""},{"location":"docs/reference/inference/core/entities/requests/yolo_world/#inference.core.entities.requests.yolo_world.YOLOWorldInferenceRequest","title":"<code>YOLOWorldInferenceRequest</code>","text":"<p>               Bases: <code>DynamicClassBaseInferenceRequest</code></p> <p>Request for Grounding DINO zero-shot predictions.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>List[str]</code> <p>A list of strings.</p> Source code in <code>inference/core/entities/requests/yolo_world.py</code> <pre><code>class YOLOWorldInferenceRequest(DynamicClassBaseInferenceRequest):\n    \"\"\"Request for Grounding DINO zero-shot predictions.\n\n    Attributes:\n        text (List[str]): A list of strings.\n    \"\"\"\n\n    yolo_world_version_id: Optional[str] = \"l\"\n    confidence: Optional[float] = DEFAULT_CONFIDENCE\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/clip/","title":"clip","text":""},{"location":"docs/reference/inference/core/entities/responses/clip/#inference.core.entities.responses.clip.ClipCompareResponse","title":"<code>ClipCompareResponse</code>","text":"<p>               Bases: <code>InferenceResponse</code></p> <p>Response for CLIP comparison.</p> <p>Attributes:</p> Name Type Description <code>similarity</code> <code>Union[List[float], Dict[str, float]]</code> <p>Similarity scores.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the similarity scores including preprocessing.</p> Source code in <code>inference/core/entities/responses/clip.py</code> <pre><code>class ClipCompareResponse(InferenceResponse):\n    \"\"\"Response for CLIP comparison.\n\n    Attributes:\n        similarity (Union[List[float], Dict[str, float]]): Similarity scores.\n        time (float): The time in seconds it took to produce the similarity scores including preprocessing.\n    \"\"\"\n\n    similarity: Union[List[float], Dict[str, float]]\n    time: Optional[float] = Field(\n        None,\n        description=\"The time in seconds it took to produce the similarity scores including preprocessing\",\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/clip/#inference.core.entities.responses.clip.ClipEmbeddingResponse","title":"<code>ClipEmbeddingResponse</code>","text":"<p>               Bases: <code>InferenceResponse</code></p> <p>Response for CLIP embedding.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>List[List[float]]</code> <p>A list of embeddings, each embedding is a list of floats.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the embeddings including preprocessing.</p> Source code in <code>inference/core/entities/responses/clip.py</code> <pre><code>class ClipEmbeddingResponse(InferenceResponse):\n    \"\"\"Response for CLIP embedding.\n\n    Attributes:\n        embeddings (List[List[float]]): A list of embeddings, each embedding is a list of floats.\n        time (float): The time in seconds it took to produce the embeddings including preprocessing.\n    \"\"\"\n\n    embeddings: List[List[float]] = Field(\n        examples=[\"[[0.12, 0.23, 0.34, ..., 0.43]]\"],\n        description=\"A list of embeddings, each embedding is a list of floats\",\n    )\n    time: Optional[float] = Field(\n        None,\n        description=\"The time in seconds it took to produce the embeddings including preprocessing\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/cogvlm/","title":"cogvlm","text":""},{"location":"docs/reference/inference/core/entities/responses/gaze/","title":"gaze","text":""},{"location":"docs/reference/inference/core/entities/responses/gaze/#inference.core.entities.responses.gaze.GazeDetectionInferenceResponse","title":"<code>GazeDetectionInferenceResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response for gaze detection inference.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[GazeDetectionPrediction]</code> <p>List of gaze detection predictions.</p> <code>time</code> <code>float</code> <p>The processing time (second).</p> Source code in <code>inference/core/entities/responses/gaze.py</code> <pre><code>class GazeDetectionInferenceResponse(BaseModel):\n    \"\"\"Response for gaze detection inference.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.gaze.GazeDetectionPrediction]): List of gaze detection predictions.\n        time (float): The processing time (second).\n    \"\"\"\n\n    predictions: List[GazeDetectionPrediction]\n\n    time: float = Field(description=\"The processing time (second)\")\n    time_face_det: Optional[float] = Field(\n        None, description=\"The face detection time (second)\"\n    )\n    time_gaze_det: Optional[float] = Field(\n        None, description=\"The gaze detection time (second)\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/gaze/#inference.core.entities.responses.gaze.GazeDetectionPrediction","title":"<code>GazeDetectionPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Gaze Detection prediction.</p> <p>Attributes:</p> Name Type Description <code>face</code> <code>FaceDetectionPrediction</code> <p>The face prediction.</p> <code>yaw</code> <code>float</code> <p>Yaw (radian) of the detected face.</p> <code>pitch</code> <code>float</code> <p>Pitch (radian) of the detected face.</p> Source code in <code>inference/core/entities/responses/gaze.py</code> <pre><code>class GazeDetectionPrediction(BaseModel):\n    \"\"\"Gaze Detection prediction.\n\n    Attributes:\n        face (inference.core.entities.responses.inference.FaceDetectionPrediction): The face prediction.\n        yaw (float): Yaw (radian) of the detected face.\n        pitch (float): Pitch (radian) of the detected face.\n    \"\"\"\n\n    face: FaceDetectionPrediction\n\n    yaw: float = Field(description=\"Yaw (radian) of the detected face\")\n    pitch: float = Field(description=\"Pitch (radian) of the detected face\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/groundingdino/","title":"groundingdino","text":""},{"location":"docs/reference/inference/core/entities/responses/inference/","title":"inference","text":""},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ClassificationInferenceResponse","title":"<code>ClassificationInferenceResponse</code>","text":"<p>               Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Classification inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[ClassificationPrediction]</code> <p>List of classification predictions.</p> <code>top</code> <code>str</code> <p>The top predicted class label.</p> <code>confidence</code> <code>float</code> <p>The confidence of the top predicted class label.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ClassificationInferenceResponse(CvInferenceResponse, WithVisualizationResponse):\n    \"\"\"Classification inference response.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.inference.ClassificationPrediction]): List of classification predictions.\n        top (str): The top predicted class label.\n        confidence (float): The confidence of the top predicted class label.\n    \"\"\"\n\n    predictions: List[ClassificationPrediction]\n    top: str = Field(description=\"The top predicted class label\")\n    confidence: float = Field(\n        description=\"The confidence of the top predicted class label\"\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ClassificationPrediction","title":"<code>ClassificationPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Classification prediction.</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>The predicted class label.</p> <code>class_id</code> <code>int</code> <p>Numeric ID associated with the class label.</p> <code>confidence</code> <code>float</code> <p>The class label confidence as a fraction between 0 and 1.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ClassificationPrediction(BaseModel):\n    \"\"\"Classification prediction.\n\n    Attributes:\n        class_name (str): The predicted class label.\n        class_id (int): Numeric ID associated with the class label.\n        confidence (float): The class label confidence as a fraction between 0 and 1.\n    \"\"\"\n\n    class_name: str = Field(alias=\"class\", description=\"The predicted class label\")\n    class_id: int = Field(description=\"Numeric ID associated with the class label\")\n    confidence: float = Field(\n        description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.CvInferenceResponse","title":"<code>CvInferenceResponse</code>","text":"<p>               Bases: <code>InferenceResponse</code></p> <p>Computer Vision inference response.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>Union[List[InferenceResponseImage], InferenceResponseImage]</code> <p>Image(s) used in inference.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class CvInferenceResponse(InferenceResponse):\n    \"\"\"Computer Vision inference response.\n\n    Attributes:\n        image (Union[List[inference.core.entities.responses.inference.InferenceResponseImage], inference.core.entities.responses.inference.InferenceResponseImage]): Image(s) used in inference.\n    \"\"\"\n\n    image: Union[List[InferenceResponseImage], InferenceResponseImage]\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.FaceDetectionPrediction","title":"<code>FaceDetectionPrediction</code>","text":"<p>               Bases: <code>ObjectDetectionPrediction</code></p> <p>Face Detection prediction.</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>fixed value \"face\".</p> <code>landmarks</code> <code>Union[List[Point], List[Point3D]]</code> <p>The detected face landmarks.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class FaceDetectionPrediction(ObjectDetectionPrediction):\n    \"\"\"Face Detection prediction.\n\n    Attributes:\n        class_name (str): fixed value \"face\".\n        landmarks (Union[List[inference.core.entities.responses.inference.Point], List[inference.core.entities.responses.inference.Point3D]]): The detected face landmarks.\n    \"\"\"\n\n    class_id: Optional[int] = Field(\n        description=\"The class id of the prediction\", default=0\n    )\n    class_name: str = Field(\n        alias=\"class\", default=\"face\", description=\"The predicted class label\"\n    )\n    landmarks: Union[List[Point], List[Point3D]]\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InferenceResponse","title":"<code>InferenceResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base inference response.</p> <p>Attributes:</p> Name Type Description <code>inference_id</code> <code>Optional[str]</code> <p>Unique identifier of inference</p> <code>frame_id</code> <code>Optional[int]</code> <p>The frame id of the image used in inference if the input was a video.</p> <code>time</code> <code>Optional[float]</code> <p>The time in seconds it took to produce the predictions including image preprocessing.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InferenceResponse(BaseModel):\n    \"\"\"Base inference response.\n\n    Attributes:\n        inference_id (Optional[str]): Unique identifier of inference\n        frame_id (Optional[int]): The frame id of the image used in inference if the input was a video.\n        time (Optional[float]): The time in seconds it took to produce the predictions including image preprocessing.\n    \"\"\"\n\n    model_config = ConfigDict(protected_namespaces=())\n    inference_id: Optional[str] = Field(\n        description=\"Unique identifier of inference\", default=None\n    )\n    frame_id: Optional[int] = Field(\n        default=None,\n        description=\"The frame id of the image used in inference if the input was a video\",\n    )\n    time: Optional[float] = Field(\n        default=None,\n        description=\"The time in seconds it took to produce the predictions including image preprocessing\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InferenceResponseImage","title":"<code>InferenceResponseImage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Inference response image information.</p> <p>Attributes:</p> Name Type Description <code>width</code> <code>int</code> <p>The original width of the image used in inference.</p> <code>height</code> <code>int</code> <p>The original height of the image used in inference.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InferenceResponseImage(BaseModel):\n    \"\"\"Inference response image information.\n\n    Attributes:\n        width (int): The original width of the image used in inference.\n        height (int): The original height of the image used in inference.\n    \"\"\"\n\n    width: int = Field(description=\"The original width of the image used in inference\")\n    height: int = Field(\n        description=\"The original height of the image used in inference\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InstanceSegmentationInferenceResponse","title":"<code>InstanceSegmentationInferenceResponse</code>","text":"<p>               Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Instance Segmentation inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[InstanceSegmentationPrediction]</code> <p>List of instance segmentation predictions.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InstanceSegmentationInferenceResponse(\n    CvInferenceResponse, WithVisualizationResponse\n):\n    \"\"\"Instance Segmentation inference response.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.inference.InstanceSegmentationPrediction]): List of instance segmentation predictions.\n    \"\"\"\n\n    predictions: List[InstanceSegmentationPrediction]\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.InstanceSegmentationPrediction","title":"<code>InstanceSegmentationPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Instance Segmentation prediction.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>float</code> <p>The center x-axis pixel coordinate of the prediction.</p> <code>y</code> <code>float</code> <p>The center y-axis pixel coordinate of the prediction.</p> <code>width</code> <code>float</code> <p>The width of the prediction bounding box in number of pixels.</p> <code>height</code> <code>float</code> <p>The height of the prediction bounding box in number of pixels.</p> <code>confidence</code> <code>float</code> <p>The detection confidence as a fraction between 0 and 1.</p> <code>class_name</code> <code>str</code> <p>The predicted class label.</p> <code>class_confidence</code> <code>Union[float, None]</code> <p>The class label confidence as a fraction between 0 and 1.</p> <code>points</code> <code>List[Point]</code> <p>The list of points that make up the instance polygon.</p> <code>class_id</code> <code>int</code> <p>int = Field(description=\"The class id of the prediction\")</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class InstanceSegmentationPrediction(BaseModel):\n    \"\"\"Instance Segmentation prediction.\n\n    Attributes:\n        x (float): The center x-axis pixel coordinate of the prediction.\n        y (float): The center y-axis pixel coordinate of the prediction.\n        width (float): The width of the prediction bounding box in number of pixels.\n        height (float): The height of the prediction bounding box in number of pixels.\n        confidence (float): The detection confidence as a fraction between 0 and 1.\n        class_name (str): The predicted class label.\n        class_confidence (Union[float, None]): The class label confidence as a fraction between 0 and 1.\n        points (List[Point]): The list of points that make up the instance polygon.\n        class_id: int = Field(description=\"The class id of the prediction\")\n    \"\"\"\n\n    x: float = Field(description=\"The center x-axis pixel coordinate of the prediction\")\n    y: float = Field(description=\"The center y-axis pixel coordinate of the prediction\")\n    width: float = Field(\n        description=\"The width of the prediction bounding box in number of pixels\"\n    )\n    height: float = Field(\n        description=\"The height of the prediction bounding box in number of pixels\"\n    )\n    confidence: float = Field(\n        description=\"The detection confidence as a fraction between 0 and 1\"\n    )\n    class_name: str = Field(alias=\"class\", description=\"The predicted class label\")\n\n    class_confidence: Union[float, None] = Field(\n        None, description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n    points: List[Point] = Field(\n        description=\"The list of points that make up the instance polygon\"\n    )\n    class_id: int = Field(description=\"The class id of the prediction\")\n    detection_id: str = Field(\n        description=\"Unique identifier of detection\",\n        default_factory=lambda: str(uuid4()),\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.MultiLabelClassificationInferenceResponse","title":"<code>MultiLabelClassificationInferenceResponse</code>","text":"<p>               Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Multi-label Classification inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>Dict[str, MultiLabelClassificationPrediction]</code> <p>Dictionary of multi-label classification predictions.</p> <code>predicted_classes</code> <code>List[str]</code> <p>The list of predicted classes.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class MultiLabelClassificationInferenceResponse(\n    CvInferenceResponse, WithVisualizationResponse\n):\n    \"\"\"Multi-label Classification inference response.\n\n    Attributes:\n        predictions (Dict[str, inference.core.entities.responses.inference.MultiLabelClassificationPrediction]): Dictionary of multi-label classification predictions.\n        predicted_classes (List[str]): The list of predicted classes.\n    \"\"\"\n\n    predictions: Dict[str, MultiLabelClassificationPrediction]\n    predicted_classes: List[str] = Field(description=\"The list of predicted classes\")\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.MultiLabelClassificationPrediction","title":"<code>MultiLabelClassificationPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Multi-label Classification prediction.</p> <p>Attributes:</p> Name Type Description <code>confidence</code> <code>float</code> <p>The class label confidence as a fraction between 0 and 1.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class MultiLabelClassificationPrediction(BaseModel):\n    \"\"\"Multi-label Classification prediction.\n\n    Attributes:\n        confidence (float): The class label confidence as a fraction between 0 and 1.\n    \"\"\"\n\n    confidence: float = Field(\n        description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n    class_id: int = Field(description=\"Numeric ID associated with the class label\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ObjectDetectionInferenceResponse","title":"<code>ObjectDetectionInferenceResponse</code>","text":"<p>               Bases: <code>CvInferenceResponse</code>, <code>WithVisualizationResponse</code></p> <p>Object Detection inference response.</p> <p>Attributes:</p> Name Type Description <code>predictions</code> <code>List[ObjectDetectionPrediction]</code> <p>List of object detection predictions.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ObjectDetectionInferenceResponse(CvInferenceResponse, WithVisualizationResponse):\n    \"\"\"Object Detection inference response.\n\n    Attributes:\n        predictions (List[inference.core.entities.responses.inference.ObjectDetectionPrediction]): List of object detection predictions.\n    \"\"\"\n\n    predictions: List[ObjectDetectionPrediction]\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.ObjectDetectionPrediction","title":"<code>ObjectDetectionPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Object Detection prediction.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>float</code> <p>The center x-axis pixel coordinate of the prediction.</p> <code>y</code> <code>float</code> <p>The center y-axis pixel coordinate of the prediction.</p> <code>width</code> <code>float</code> <p>The width of the prediction bounding box in number of pixels.</p> <code>height</code> <code>float</code> <p>The height of the prediction bounding box in number of pixels.</p> <code>confidence</code> <code>float</code> <p>The detection confidence as a fraction between 0 and 1.</p> <code>class_name</code> <code>str</code> <p>The predicted class label.</p> <code>class_confidence</code> <code>Union[float, None]</code> <p>The class label confidence as a fraction between 0 and 1.</p> <code>class_id</code> <code>int</code> <p>The class id of the prediction</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class ObjectDetectionPrediction(BaseModel):\n    \"\"\"Object Detection prediction.\n\n    Attributes:\n        x (float): The center x-axis pixel coordinate of the prediction.\n        y (float): The center y-axis pixel coordinate of the prediction.\n        width (float): The width of the prediction bounding box in number of pixels.\n        height (float): The height of the prediction bounding box in number of pixels.\n        confidence (float): The detection confidence as a fraction between 0 and 1.\n        class_name (str): The predicted class label.\n        class_confidence (Union[float, None]): The class label confidence as a fraction between 0 and 1.\n        class_id (int): The class id of the prediction\n    \"\"\"\n\n    x: float = Field(description=\"The center x-axis pixel coordinate of the prediction\")\n    y: float = Field(description=\"The center y-axis pixel coordinate of the prediction\")\n    width: float = Field(\n        description=\"The width of the prediction bounding box in number of pixels\"\n    )\n    height: float = Field(\n        description=\"The height of the prediction bounding box in number of pixels\"\n    )\n    confidence: float = Field(\n        description=\"The detection confidence as a fraction between 0 and 1\"\n    )\n    class_name: str = Field(alias=\"class\", description=\"The predicted class label\")\n\n    class_confidence: Union[float, None] = Field(\n        None, description=\"The class label confidence as a fraction between 0 and 1\"\n    )\n    class_id: int = Field(description=\"The class id of the prediction\")\n    tracker_id: Optional[int] = Field(\n        description=\"The tracker id of the prediction if tracking is enabled\",\n        default=None,\n    )\n    detection_id: str = Field(\n        description=\"Unique identifier of detection\",\n        default_factory=lambda: str(uuid4()),\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.Point","title":"<code>Point</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Point coordinates.</p> <p>Attributes:</p> Name Type Description <code>x</code> <code>float</code> <p>The x-axis pixel coordinate of the point.</p> <code>y</code> <code>float</code> <p>The y-axis pixel coordinate of the point.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class Point(BaseModel):\n    \"\"\"Point coordinates.\n\n    Attributes:\n        x (float): The x-axis pixel coordinate of the point.\n        y (float): The y-axis pixel coordinate of the point.\n    \"\"\"\n\n    x: float = Field(description=\"The x-axis pixel coordinate of the point\")\n    y: float = Field(description=\"The y-axis pixel coordinate of the point\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.Point3D","title":"<code>Point3D</code>","text":"<p>               Bases: <code>Point</code></p> <p>3D Point coordinates.</p> <p>Attributes:</p> Name Type Description <code>z</code> <code>float</code> <p>The z-axis pixel coordinate of the point.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class Point3D(Point):\n    \"\"\"3D Point coordinates.\n\n    Attributes:\n        z (float): The z-axis pixel coordinate of the point.\n    \"\"\"\n\n    z: float = Field(description=\"The z-axis pixel coordinate of the point\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/inference/#inference.core.entities.responses.inference.WithVisualizationResponse","title":"<code>WithVisualizationResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response with visualization.</p> <p>Attributes:</p> Name Type Description <code>visualization</code> <code>Optional[Any]</code> <p>Base64 encoded string containing prediction visualization image data.</p> Source code in <code>inference/core/entities/responses/inference.py</code> <pre><code>class WithVisualizationResponse(BaseModel):\n    \"\"\"Response with visualization.\n\n    Attributes:\n        visualization (Optional[Any]): Base64 encoded string containing prediction visualization image data.\n    \"\"\"\n\n    visualization: Optional[Any] = Field(\n        default=None,\n        description=\"Base64 encoded string containing prediction visualization image data\",\n    )\n\n    @field_serializer(\"visualization\", when_used=\"json\")\n    def serialize_visualisation(self, visualization: Optional[Any]) -&gt; Optional[str]:\n        if visualization is None:\n            return None\n        return base64.b64encode(visualization).decode(\"utf-8\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/notebooks/","title":"notebooks","text":""},{"location":"docs/reference/inference/core/entities/responses/notebooks/#inference.core.entities.responses.notebooks.NotebookStartResponse","title":"<code>NotebookStartResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response model for notebook start request</p> Source code in <code>inference/core/entities/responses/notebooks.py</code> <pre><code>class NotebookStartResponse(BaseModel):\n    \"\"\"Response model for notebook start request\"\"\"\n\n    success: str = Field(..., description=\"Status of the request\")\n    message: str = Field(..., description=\"Message of the request\", optional=True)\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/ocr/","title":"ocr","text":""},{"location":"docs/reference/inference/core/entities/responses/ocr/#inference.core.entities.responses.ocr.OCRInferenceResponse","title":"<code>OCRInferenceResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>OCR Inference response.</p> <p>Attributes:</p> Name Type Description <code>result</code> <code>str</code> <p>The OCR recognition result.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the inference including preprocessing.</p> Source code in <code>inference/core/entities/responses/ocr.py</code> <pre><code>class OCRInferenceResponse(BaseModel):\n    \"\"\"\n    OCR Inference response.\n\n    Attributes:\n        result (str): The OCR recognition result.\n        time: The time in seconds it took to produce the inference including preprocessing.\n    \"\"\"\n\n    result: str = Field(description=\"The OCR recognition result.\")\n    time: float = Field(\n        description=\"The time in seconds it took to produce the inference including preprocessing.\"\n    )\n    parent_id: Optional[str] = Field(\n        description=\"Identifier of parent image region. Useful when stack of detection-models is in use to refer the RoI being the input to inference\",\n        default=None,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/sam/","title":"sam","text":""},{"location":"docs/reference/inference/core/entities/responses/sam/#inference.core.entities.responses.sam.SamEmbeddingResponse","title":"<code>SamEmbeddingResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>SAM embedding response.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>Union[List[List[List[List[float]]]], Any]</code> <p>The SAM embedding.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the embeddings including preprocessing.</p> Source code in <code>inference/core/entities/responses/sam.py</code> <pre><code>class SamEmbeddingResponse(BaseModel):\n    \"\"\"SAM embedding response.\n\n    Attributes:\n        embeddings (Union[List[List[List[List[float]]]], Any]): The SAM embedding.\n        time (float): The time in seconds it took to produce the embeddings including preprocessing.\n    \"\"\"\n\n    embeddings: Union[List[List[List[List[float]]]], Any] = Field(\n        examples=[\"[[[[0.1, 0.2, 0.3, ...] ...] ...]]\"],\n        description=\"If request format is json, embeddings is a series of nested lists representing the SAM embedding. If request format is binary, embeddings is a binary numpy array. The dimensions of the embedding are 1 x 256 x 64 x 64.\",\n    )\n    time: float = Field(\n        description=\"The time in seconds it took to produce the embeddings including preprocessing\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/sam/#inference.core.entities.responses.sam.SamSegmentationResponse","title":"<code>SamSegmentationResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>SAM segmentation response.</p> <p>Attributes:</p> Name Type Description <code>masks</code> <code>Union[List[List[List[int]]], Any]</code> <p>The set of output masks.</p> <code>low_res_masks</code> <code>Union[List[List[List[int]]], Any]</code> <p>The set of output low-resolution masks.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the segmentation including preprocessing.</p> Source code in <code>inference/core/entities/responses/sam.py</code> <pre><code>class SamSegmentationResponse(BaseModel):\n    \"\"\"SAM segmentation response.\n\n    Attributes:\n        masks (Union[List[List[List[int]]], Any]): The set of output masks.\n        low_res_masks (Union[List[List[List[int]]], Any]): The set of output low-resolution masks.\n        time (float): The time in seconds it took to produce the segmentation including preprocessing.\n    \"\"\"\n\n    masks: Union[List[List[List[int]]], Any] = Field(\n        description=\"The set of output masks. If request format is json, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If request format is binary, masks is a list of binary numpy arrays. The dimensions of each mask are the same as the dimensions of the input image.\",\n    )\n    low_res_masks: Union[List[List[List[int]]], Any] = Field(\n        description=\"The set of output masks. If request format is json, masks is a list of polygons, where each polygon is a list of points, where each point is a tuple containing the x,y pixel coordinates of the point. If request format is binary, masks is a list of binary numpy arrays. The dimensions of each mask are 256 x 256\",\n    )\n    time: float = Field(\n        description=\"The time in seconds it took to produce the segmentation including preprocessing\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/sam2/","title":"sam2","text":""},{"location":"docs/reference/inference/core/entities/responses/sam2/#inference.core.entities.responses.sam2.Sam2EmbeddingResponse","title":"<code>Sam2EmbeddingResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>SAM embedding response.</p> <p>Attributes:</p> Name Type Description <code>embeddings</code> <code>Union[List[List[List[List[float]]]], Any]</code> <p>The SAM embedding.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the embeddings including preprocessing.</p> Source code in <code>inference/core/entities/responses/sam2.py</code> <pre><code>class Sam2EmbeddingResponse(BaseModel):\n    \"\"\"SAM embedding response.\n\n    Attributes:\n        embeddings (Union[List[List[List[List[float]]]], Any]): The SAM embedding.\n        time (float): The time in seconds it took to produce the embeddings including preprocessing.\n    \"\"\"\n\n    image_id: str = Field(description=\"Image id embeddings are cached to\")\n    time: float = Field(\n        description=\"The time in seconds it took to produce the embeddings including preprocessing\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/sam2/#inference.core.entities.responses.sam2.Sam2SegmentationPrediction","title":"<code>Sam2SegmentationPrediction</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>SAM segmentation prediction.</p> <p>Attributes:</p> Name Type Description <code>masks</code> <code>Union[List[List[List[int]]], Any]</code> <p>The set of output masks.</p> <code>low_res_masks</code> <code>Union[List[List[List[int]]], Any]</code> <p>The set of output low-resolution masks.</p> <code>time</code> <code>float</code> <p>The time in seconds it took to produce the segmentation including preprocessing.</p> Source code in <code>inference/core/entities/responses/sam2.py</code> <pre><code>class Sam2SegmentationPrediction(BaseModel):\n    \"\"\"SAM segmentation prediction.\n\n    Attributes:\n        masks (Union[List[List[List[int]]], Any]): The set of output masks.\n        low_res_masks (Union[List[List[List[int]]], Any]): The set of output low-resolution masks.\n        time (float): The time in seconds it took to produce the segmentation including preprocessing.\n    \"\"\"\n\n    masks: List[List[List[int]]] = Field(\n        description=\"The set of points for output mask as polygon. Each element of list represents single point.\",\n    )\n    confidence: float = Field(description=\"Masks confidences\")\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/server_state/","title":"server_state","text":""},{"location":"docs/reference/inference/core/entities/responses/server_state/#inference.core.entities.responses.server_state.ServerVersionInfo","title":"<code>ServerVersionInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Server version information.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Server name.</p> <code>version</code> <code>str</code> <p>Server version.</p> <code>uuid</code> <code>str</code> <p>Server UUID.</p> Source code in <code>inference/core/entities/responses/server_state.py</code> <pre><code>class ServerVersionInfo(BaseModel):\n    \"\"\"Server version information.\n\n    Attributes:\n        name (str): Server name.\n        version (str): Server version.\n        uuid (str): Server UUID.\n    \"\"\"\n\n    name: str = Field(examples=[\"Roboflow Inference Server\"])\n    version: str = Field(examples=[\"0.0.1\"])\n    uuid: str = Field(examples=[\"9c18c6f4-2266-41fb-8a0f-c12ae28f6fbe\"])\n</code></pre>"},{"location":"docs/reference/inference/core/entities/responses/workflows/","title":"workflows","text":""},{"location":"docs/reference/inference/core/interfaces/base/","title":"base","text":""},{"location":"docs/reference/inference/core/interfaces/base/#inference.core.interfaces.base.BaseInterface","title":"<code>BaseInterface</code>","text":"<p>Base interface class which accepts a model manager on initialization</p> Source code in <code>inference/core/interfaces/base.py</code> <pre><code>class BaseInterface:\n    \"\"\"Base interface class which accepts a model manager on initialization\"\"\"\n\n    def __init__(self, model_manager: ModelManager) -&gt; None:\n        self.model_manager = model_manager\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/","title":"camera","text":""},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream","title":"<code>WebcamStream</code>","text":"<p>Class to handle webcam streaming using a separate thread.</p> <p>Attributes:</p> Name Type Description <code>stream_id</code> <code>int</code> <p>The ID of the webcam stream.</p> <code>frame_id</code> <code>int</code> <p>A counter for the current frame.</p> <code>vcap</code> <code>VideoCapture</code> <p>OpenCV video capture object.</p> <code>width</code> <code>int</code> <p>The width of the video frame.</p> <code>height</code> <code>int</code> <p>The height of the video frame.</p> <code>fps_input_stream</code> <code>int</code> <p>Frames per second of the input stream.</p> <code>grabbed</code> <code>bool</code> <p>A flag indicating if a frame was successfully grabbed.</p> <code>frame</code> <code>array</code> <p>The current frame as a NumPy array.</p> <code>pil_image</code> <code>Image</code> <p>The current frame as a PIL image.</p> <code>stopped</code> <code>bool</code> <p>A flag indicating if the stream is stopped.</p> <code>t</code> <code>Thread</code> <p>The thread used to update the stream.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>class WebcamStream:\n    \"\"\"Class to handle webcam streaming using a separate thread.\n\n    Attributes:\n        stream_id (int): The ID of the webcam stream.\n        frame_id (int): A counter for the current frame.\n        vcap (VideoCapture): OpenCV video capture object.\n        width (int): The width of the video frame.\n        height (int): The height of the video frame.\n        fps_input_stream (int): Frames per second of the input stream.\n        grabbed (bool): A flag indicating if a frame was successfully grabbed.\n        frame (array): The current frame as a NumPy array.\n        pil_image (Image): The current frame as a PIL image.\n        stopped (bool): A flag indicating if the stream is stopped.\n        t (Thread): The thread used to update the stream.\n    \"\"\"\n\n    def __init__(self, stream_id=0, enforce_fps=False):\n        \"\"\"Initialize the webcam stream.\n\n        Args:\n            stream_id (int, optional): The ID of the webcam stream. Defaults to 0.\n        \"\"\"\n        self.stream_id = stream_id\n        self.enforce_fps = enforce_fps\n        self.frame_id = 0\n        self.vcap = cv2.VideoCapture(self.stream_id)\n\n        for key in os.environ:\n            if key.startswith(\"CV2_CAP_PROP\"):\n                opencv_prop = key[4:]\n                opencv_constant = getattr(cv2, opencv_prop, None)\n                if opencv_constant is not None:\n                    value = int(os.getenv(key))\n                    self.vcap.set(opencv_constant, value)\n                    logger.info(f\"set {opencv_prop} to {value}\")\n                else:\n                    logger.warn(f\"Property {opencv_prop} not found in cv2\")\n\n        self.width = int(self.vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        self.height = int(self.vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        self.file_mode = self.vcap.get(cv2.CAP_PROP_FRAME_COUNT) &gt; 0\n        if self.enforce_fps and not self.file_mode:\n            logger.warn(\n                \"Ignoring enforce_fps flag for this stream. It is not compatible with streams and will cause the process to crash\"\n            )\n            self.enforce_fps = False\n        self.max_fps = None\n        if self.vcap.isOpened() is False:\n            logger.debug(\"[Exiting]: Error accessing webcam stream.\")\n            exit(0)\n        self.fps_input_stream = int(self.vcap.get(cv2.CAP_PROP_FPS))\n        logger.debug(\n            \"FPS of webcam hardware/input stream: {}\".format(self.fps_input_stream)\n        )\n        self.grabbed, self.frame = self.vcap.read()\n        self.pil_image = Image.fromarray(cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB))\n        if self.grabbed is False:\n            logger.debug(\"[Exiting] No more frames to read\")\n            exit(0)\n        self.stopped = True\n        self.t = Thread(target=self.update, args=())\n        self.t.daemon = True\n\n    def start(self):\n        \"\"\"Start the thread for reading frames.\"\"\"\n        self.stopped = False\n        self.t.start()\n\n    def update(self):\n        \"\"\"Update the frame by reading from the webcam.\"\"\"\n        frame_id = 0\n        next_frame_time = 0\n        t0 = time.perf_counter()\n        while True:\n            t1 = time.perf_counter()\n            if self.stopped is True:\n                break\n\n            self.grabbed = self.vcap.grab()\n            if self.grabbed is False:\n                logger.debug(\"[Exiting] No more frames to read\")\n                self.stopped = True\n                break\n            frame_id += 1\n            # We can't retrieve each frame on nano and other lower powered devices quickly enough to keep up with the stream.\n            # By default, we will only retrieve frames when we'll be ready process them (determined by self.max_fps).\n            if t1 &gt; next_frame_time:\n                ret, frame = self.vcap.retrieve()\n                if frame is None:\n                    logger.debug(\"[Exiting] Frame not available for read\")\n                    self.stopped = True\n                    break\n                logger.debug(\n                    f\"retrieved frame {frame_id}, effective FPS: {frame_id / (t1 - t0):.2f}\"\n                )\n                self.frame_id = frame_id\n                self.frame = frame\n                while self.file_mode and self.enforce_fps and self.max_fps is None:\n                    # sleep until we have processed the first frame and we know what our FPS should be\n                    time.sleep(0.01)\n                if self.max_fps is None:\n                    self.max_fps = 30\n                next_frame_time = t1 + (1 / self.max_fps) + 0.02\n            if self.file_mode:\n                t2 = time.perf_counter()\n                if self.enforce_fps:\n                    # when enforce_fps is true, grab video frames 1:1 with inference speed\n                    time_to_sleep = next_frame_time - t2\n                else:\n                    # otherwise, grab at native FPS of the video file\n                    time_to_sleep = (1 / self.fps_input_stream) - (t2 - t1)\n                if time_to_sleep &gt; 0:\n                    time.sleep(time_to_sleep)\n        self.vcap.release()\n\n    def read_opencv(self):\n        \"\"\"Read the current frame using OpenCV.\n\n        Returns:\n            array, int: The current frame as a NumPy array, and the frame ID.\n        \"\"\"\n        return self.frame, self.frame_id\n\n    def stop(self):\n        \"\"\"Stop the webcam stream.\"\"\"\n        self.stopped = True\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.__init__","title":"<code>__init__(stream_id=0, enforce_fps=False)</code>","text":"<p>Initialize the webcam stream.</p> <p>Parameters:</p> Name Type Description Default <code>stream_id</code> <code>int</code> <p>The ID of the webcam stream. Defaults to 0.</p> <code>0</code> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def __init__(self, stream_id=0, enforce_fps=False):\n    \"\"\"Initialize the webcam stream.\n\n    Args:\n        stream_id (int, optional): The ID of the webcam stream. Defaults to 0.\n    \"\"\"\n    self.stream_id = stream_id\n    self.enforce_fps = enforce_fps\n    self.frame_id = 0\n    self.vcap = cv2.VideoCapture(self.stream_id)\n\n    for key in os.environ:\n        if key.startswith(\"CV2_CAP_PROP\"):\n            opencv_prop = key[4:]\n            opencv_constant = getattr(cv2, opencv_prop, None)\n            if opencv_constant is not None:\n                value = int(os.getenv(key))\n                self.vcap.set(opencv_constant, value)\n                logger.info(f\"set {opencv_prop} to {value}\")\n            else:\n                logger.warn(f\"Property {opencv_prop} not found in cv2\")\n\n    self.width = int(self.vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    self.height = int(self.vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    self.file_mode = self.vcap.get(cv2.CAP_PROP_FRAME_COUNT) &gt; 0\n    if self.enforce_fps and not self.file_mode:\n        logger.warn(\n            \"Ignoring enforce_fps flag for this stream. It is not compatible with streams and will cause the process to crash\"\n        )\n        self.enforce_fps = False\n    self.max_fps = None\n    if self.vcap.isOpened() is False:\n        logger.debug(\"[Exiting]: Error accessing webcam stream.\")\n        exit(0)\n    self.fps_input_stream = int(self.vcap.get(cv2.CAP_PROP_FPS))\n    logger.debug(\n        \"FPS of webcam hardware/input stream: {}\".format(self.fps_input_stream)\n    )\n    self.grabbed, self.frame = self.vcap.read()\n    self.pil_image = Image.fromarray(cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB))\n    if self.grabbed is False:\n        logger.debug(\"[Exiting] No more frames to read\")\n        exit(0)\n    self.stopped = True\n    self.t = Thread(target=self.update, args=())\n    self.t.daemon = True\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.read_opencv","title":"<code>read_opencv()</code>","text":"<p>Read the current frame using OpenCV.</p> <p>Returns:</p> Type Description <p>array, int: The current frame as a NumPy array, and the frame ID.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def read_opencv(self):\n    \"\"\"Read the current frame using OpenCV.\n\n    Returns:\n        array, int: The current frame as a NumPy array, and the frame ID.\n    \"\"\"\n    return self.frame, self.frame_id\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.start","title":"<code>start()</code>","text":"<p>Start the thread for reading frames.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def start(self):\n    \"\"\"Start the thread for reading frames.\"\"\"\n    self.stopped = False\n    self.t.start()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.stop","title":"<code>stop()</code>","text":"<p>Stop the webcam stream.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def stop(self):\n    \"\"\"Stop the webcam stream.\"\"\"\n    self.stopped = True\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/camera/#inference.core.interfaces.camera.camera.WebcamStream.update","title":"<code>update()</code>","text":"<p>Update the frame by reading from the webcam.</p> Source code in <code>inference/core/interfaces/camera/camera.py</code> <pre><code>def update(self):\n    \"\"\"Update the frame by reading from the webcam.\"\"\"\n    frame_id = 0\n    next_frame_time = 0\n    t0 = time.perf_counter()\n    while True:\n        t1 = time.perf_counter()\n        if self.stopped is True:\n            break\n\n        self.grabbed = self.vcap.grab()\n        if self.grabbed is False:\n            logger.debug(\"[Exiting] No more frames to read\")\n            self.stopped = True\n            break\n        frame_id += 1\n        # We can't retrieve each frame on nano and other lower powered devices quickly enough to keep up with the stream.\n        # By default, we will only retrieve frames when we'll be ready process them (determined by self.max_fps).\n        if t1 &gt; next_frame_time:\n            ret, frame = self.vcap.retrieve()\n            if frame is None:\n                logger.debug(\"[Exiting] Frame not available for read\")\n                self.stopped = True\n                break\n            logger.debug(\n                f\"retrieved frame {frame_id}, effective FPS: {frame_id / (t1 - t0):.2f}\"\n            )\n            self.frame_id = frame_id\n            self.frame = frame\n            while self.file_mode and self.enforce_fps and self.max_fps is None:\n                # sleep until we have processed the first frame and we know what our FPS should be\n                time.sleep(0.01)\n            if self.max_fps is None:\n                self.max_fps = 30\n            next_frame_time = t1 + (1 / self.max_fps) + 0.02\n        if self.file_mode:\n            t2 = time.perf_counter()\n            if self.enforce_fps:\n                # when enforce_fps is true, grab video frames 1:1 with inference speed\n                time_to_sleep = next_frame_time - t2\n            else:\n                # otherwise, grab at native FPS of the video file\n                time_to_sleep = (1 / self.fps_input_stream) - (t2 - t1)\n            if time_to_sleep &gt; 0:\n                time.sleep(time_to_sleep)\n    self.vcap.release()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/interfaces/camera/entities/#inference.core.interfaces.camera.entities.StatusUpdate","title":"<code>StatusUpdate</code>  <code>dataclass</code>","text":"<p>Represents a status update event in the system.</p> <p>Attributes:</p> Name Type Description <code>timestamp</code> <code>datetime</code> <p>The timestamp when the status update was created.</p> <code>severity</code> <code>UpdateSeverity</code> <p>The severity level of the update.</p> <code>event_type</code> <code>str</code> <p>A string representing the type of the event.</p> <code>payload</code> <code>dict</code> <p>A dictionary containing data relevant to the update.</p> <code>context</code> <code>str</code> <p>A string providing additional context about the update.</p> Source code in <code>inference/core/interfaces/camera/entities.py</code> <pre><code>@dataclass(frozen=True)\nclass StatusUpdate:\n    \"\"\"Represents a status update event in the system.\n\n    Attributes:\n        timestamp (datetime): The timestamp when the status update was created.\n        severity (UpdateSeverity): The severity level of the update.\n        event_type (str): A string representing the type of the event.\n        payload (dict): A dictionary containing data relevant to the update.\n        context (str): A string providing additional context about the update.\n    \"\"\"\n\n    timestamp: datetime\n    severity: UpdateSeverity\n    event_type: str\n    payload: dict\n    context: str\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/entities/#inference.core.interfaces.camera.entities.UpdateSeverity","title":"<code>UpdateSeverity</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for defining different levels of update severity.</p> <p>Attributes:</p> Name Type Description <code>DEBUG</code> <code>int</code> <p>A debugging severity level.</p> <code>INFO</code> <code>int</code> <p>An informational severity level.</p> <code>WARNING</code> <code>int</code> <p>A warning severity level.</p> <code>ERROR</code> <code>int</code> <p>An error severity level.</p> Source code in <code>inference/core/interfaces/camera/entities.py</code> <pre><code>class UpdateSeverity(Enum):\n    \"\"\"Enumeration for defining different levels of update severity.\n\n    Attributes:\n        DEBUG (int): A debugging severity level.\n        INFO (int): An informational severity level.\n        WARNING (int): A warning severity level.\n        ERROR (int): An error severity level.\n    \"\"\"\n\n    DEBUG = logging.DEBUG\n    INFO = logging.INFO\n    WARNING = logging.WARNING\n    ERROR = logging.ERROR\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/entities/#inference.core.interfaces.camera.entities.VideoFrame","title":"<code>VideoFrame</code>  <code>dataclass</code>","text":"<p>Represents a single frame of video data.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>ndarray</code> <p>The image data of the frame as a NumPy array.</p> <code>frame_id</code> <code>FrameID</code> <p>A unique identifier for the frame.</p> <code>frame_timestamp</code> <code>FrameTimestamp</code> <p>The timestamp when the frame was captured.</p> <code>source_id</code> <code>int</code> <p>The index of the video_reference element which was passed to InferencePipeline for this frame (useful when multiple streams are passed to InferencePipeline).</p> <code>fps</code> <code>Optional[float]</code> <p>FPS of source (if possible to be acquired)</p> <code>comes_from_video_file</code> <code>Optional[bool]</code> <p>flag to determine if frame comes from video file</p> Source code in <code>inference/core/interfaces/camera/entities.py</code> <pre><code>@dataclass(frozen=True)\nclass VideoFrame:\n    \"\"\"Represents a single frame of video data.\n\n    Attributes:\n        image (np.ndarray): The image data of the frame as a NumPy array.\n        frame_id (FrameID): A unique identifier for the frame.\n        frame_timestamp (FrameTimestamp): The timestamp when the frame was captured.\n        source_id (int): The index of the video_reference element which was passed to InferencePipeline for this frame\n            (useful when multiple streams are passed to InferencePipeline).\n        fps (Optional[float]): FPS of source (if possible to be acquired)\n        comes_from_video_file (Optional[bool]): flag to determine if frame comes from video file\n    \"\"\"\n\n    image: np.ndarray\n    frame_id: FrameID\n    frame_timestamp: FrameTimestamp\n    fps: Optional[float] = None\n    source_id: Optional[int] = None\n    comes_from_video_file: Optional[bool] = None\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/exceptions/","title":"exceptions","text":""},{"location":"docs/reference/inference/core/interfaces/camera/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.RateLimiter","title":"<code>RateLimiter</code>","text":"<p>Implements rate upper-bound rate limiting by ensuring estimate_next_tick_delay() to be at min 1 / desired_fps, not letting the client obeying outcomes to exceed assumed rate.</p> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>class RateLimiter:\n    \"\"\"\n    Implements rate upper-bound rate limiting by ensuring estimate_next_tick_delay()\n    to be at min 1 / desired_fps, not letting the client obeying outcomes to exceed\n    assumed rate.\n    \"\"\"\n\n    def __init__(self, desired_fps: Union[float, int]):\n        self._desired_fps = max(desired_fps, MINIMAL_FPS)\n        self._last_tick: Optional[float] = None\n\n    def tick(self) -&gt; None:\n        self._last_tick = time.monotonic()\n\n    def estimate_next_action_delay(self) -&gt; float:\n        if self._last_tick is None:\n            return 0.0\n        desired_delay = 1 / self._desired_fps\n        time_since_last_tick = time.monotonic() - self._last_tick\n        return max(desired_delay - time_since_last_tick, 0.0)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.VideoSourcesManager","title":"<code>VideoSourcesManager</code>","text":"<p>This class should be treated as internal building block of stream multiplexer - not for external use.</p> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>class VideoSourcesManager:\n    \"\"\"\n    This class should be treated as internal building block of stream multiplexer - not for external use.\n    \"\"\"\n\n    @classmethod\n    def init(\n        cls,\n        video_sources: VideoSources,\n        should_stop: Callable[[], bool],\n        on_reconnection_error: Callable[[Optional[int], SourceConnectionError], None],\n    ) -&gt; \"VideoSourcesManager\":\n        return cls(\n            video_sources=video_sources,\n            should_stop=should_stop,\n            on_reconnection_error=on_reconnection_error,\n        )\n\n    def __init__(\n        self,\n        video_sources: VideoSources,\n        should_stop: Callable[[], bool],\n        on_reconnection_error: Callable[[Optional[int], SourceConnectionError], None],\n    ):\n        self._video_sources = video_sources\n        self._reconnection_threads: Dict[int, Thread] = {}\n        self._external_should_stop = should_stop\n        self._on_reconnection_error = on_reconnection_error\n        self._enforce_stop: Dict[int, bool] = {}\n        self._ended_sources: Set[int] = set()\n        self._threads_to_join: Set[int] = set()\n        self._last_batch_yielded_time = datetime.now()\n\n    def retrieve_frames_from_sources(\n        self,\n        batch_collection_timeout: Optional[float],\n    ) -&gt; Optional[List[VideoFrame]]:\n        batch_frames = []\n        if batch_collection_timeout is not None:\n            batch_timeout_moment = self._last_batch_yielded_time + timedelta(\n                seconds=batch_collection_timeout\n            )\n        else:\n            batch_timeout_moment = None\n        for source_ord, (source, source_should_reconnect) in enumerate(\n            zip(self._video_sources.all_sources, self._video_sources.allow_reconnection)\n        ):\n            if self._external_should_stop():\n                self.join_all_reconnection_threads(include_not_finished=True)\n                return None\n            if self._is_source_inactive(source_ord=source_ord):\n                continue\n            batch_time_left = (\n                None\n                if batch_timeout_moment is None\n                else max((batch_timeout_moment - datetime.now()).total_seconds(), 0.0)\n            )\n            try:\n                frame = source.read_frame(timeout=batch_time_left)\n                if frame is not None:\n                    batch_frames.append(frame)\n            except EndOfStreamError:\n                self._register_end_of_stream(source_ord=source_ord)\n        self.join_all_reconnection_threads()\n        self._last_batch_yielded_time = datetime.now()\n        return batch_frames\n\n    def all_sources_ended(self) -&gt; bool:\n        return len(self._ended_sources) &gt;= len(self._video_sources.all_sources)\n\n    def join_all_reconnection_threads(self, include_not_finished: bool = False) -&gt; None:\n        for source_ord in copy(self._threads_to_join):\n            self._purge_reconnection_thread(source_ord=source_ord)\n        if not include_not_finished:\n            return None\n        for source_ord in list(self._reconnection_threads.keys()):\n            self._purge_reconnection_thread(source_ord=source_ord)\n\n    def _is_source_inactive(self, source_ord: int) -&gt; bool:\n        return (\n            source_ord in self._ended_sources\n            or source_ord in self._reconnection_threads\n        )\n\n    def _register_end_of_stream(self, source_ord: int) -&gt; None:\n        source_should_reconnect = self._video_sources.allow_reconnection[source_ord]\n        if source_should_reconnect:\n            self._reconnect_source(source_ord=source_ord)\n        else:\n            self._ended_sources.add(source_ord)\n\n    def _reconnect_source(self, source_ord: int) -&gt; None:\n        if source_ord in self._reconnection_threads:\n            return None\n        self._reconnection_threads[source_ord] = Thread(\n            target=_attempt_reconnect,\n            args=(\n                self._video_sources.all_sources[source_ord],\n                partial(self._should_stop, source_ord=source_ord),\n                self._on_reconnection_error,\n                partial(self._register_thread_to_join, source_ord=source_ord),\n                partial(self._register_reconnection_fatal_error, source_ord=source_ord),\n            ),\n        )\n        self._reconnection_threads[source_ord].start()\n\n    def _register_reconnection_fatal_error(self, source_ord: int) -&gt; None:\n        self._register_thread_to_join(source_ord=source_ord)\n        self._ended_sources.add(source_ord)\n\n    def _register_thread_to_join(self, source_ord: int) -&gt; None:\n        self._threads_to_join.add(source_ord)\n\n    def _purge_reconnection_thread(self, source_ord: int) -&gt; None:\n        if source_ord not in self._reconnection_threads:\n            return None\n        self._enforce_stop[source_ord] = True\n        self._reconnection_threads[source_ord].join()\n        del self._reconnection_threads[source_ord]\n        self._enforce_stop[source_ord] = False\n        if source_ord in self._threads_to_join:\n            self._threads_to_join.remove(source_ord)\n\n    def _should_stop(self, source_ord: int) -&gt; bool:\n        if self._external_should_stop():\n            return True\n        return self._enforce_stop.get(source_ord, False)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.get_video_frames_generator","title":"<code>get_video_frames_generator(video, max_fps=None, limiter_strategy=None)</code>","text":"<p>Util function to create a frames generator from <code>VideoSource</code> with possibility to limit FPS of consumed frames and dictate what to do if frames are produced to fast.</p> <p>Parameters:</p> Name Type Description Default <code>video</code> <code>Union[VideoSource, str, int]</code> <p>Either instance of VideoSource or video reference accepted by VideoSource.init(...)</p> required <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>value of maximum FPS rate of generated frames - can be used to limit generation frequency</p> <code>None</code> <code>limiter_strategy</code> <code>Optional[FPSLimiterStrategy]</code> <p>strategy used to deal with frames decoding exceeding limit of <code>max_fps</code>. By default - for files, in the interest of processing all frames - generation will be awaited, for streams - frames will be dropped on the floor.</p> <code>None</code> <p>Returns: generator of <code>VideoFrame</code></p> Example <pre><code>from inference.core.interfaces.camera.utils import get_video_frames_generator\n\nfor frame in get_video_frames_generator(\n    video=\"./some.mp4\",\n    max_fps=50,\n):\n     pass\n</code></pre> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>def get_video_frames_generator(\n    video: Union[VideoSource, str, int],\n    max_fps: Optional[Union[float, int]] = None,\n    limiter_strategy: Optional[FPSLimiterStrategy] = None,\n) -&gt; Generator[VideoFrame, None, None]:\n    \"\"\"\n    Util function to create a frames generator from `VideoSource` with possibility to\n    limit FPS of consumed frames and dictate what to do if frames are produced to fast.\n\n    Args:\n        video (Union[VideoSource, str, int]): Either instance of VideoSource or video reference accepted\n            by VideoSource.init(...)\n        max_fps (Optional[Union[float, int]]): value of maximum FPS rate of generated frames - can be used to limit\n            generation frequency\n        limiter_strategy (Optional[FPSLimiterStrategy]): strategy used to deal with frames decoding exceeding\n            limit of `max_fps`. By default - for files, in the interest of processing all frames -\n            generation will be awaited, for streams - frames will be dropped on the floor.\n    Returns: generator of `VideoFrame`\n\n    Example:\n        ```python\n        from inference.core.interfaces.camera.utils import get_video_frames_generator\n\n        for frame in get_video_frames_generator(\n            video=\"./some.mp4\",\n            max_fps=50,\n        ):\n             pass\n        ```\n    \"\"\"\n    is_managed_source = False\n    if issubclass(type(video), str) or issubclass(type(video), int):\n        video = VideoSource.init(\n            video_reference=video,\n        )\n        video.start()\n        is_managed_source = True\n    if max_fps is None:\n        yield from video\n        if is_managed_source:\n            video.terminate(purge_frames_buffer=True)\n        return None\n    limiter_strategy = resolve_limiter_strategy(\n        explicitly_defined_strategy=limiter_strategy,\n        source_properties=video.describe_source().source_properties,\n    )\n    yield from limit_frame_rate(\n        frames_generator=video, max_fps=max_fps, strategy=limiter_strategy\n    )\n    if is_managed_source:\n        video.terminate(purge_frames_buffer=True)\n    return None\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/utils/#inference.core.interfaces.camera.utils.multiplex_videos","title":"<code>multiplex_videos(videos, max_fps=None, limiter_strategy=None, batch_collection_timeout=None, force_stream_reconnection=True, should_stop=never_stop, on_reconnection_error=log_error)</code>","text":"<p>Function that is supposed to provide a generator over frames from multiple video sources. It is capable to initialise <code>VideoSource</code> from references to video files or streams and grab frames from all the sources - each running individual decoding on separate thread. In each cycle it attempts to grab frames from all sources (and wait at max <code>batch_collection_timeout</code> for whole batch to be collected). If frame from specific source cannot be collected in that time - it is simply not included in returned list. If after batch collection list of frames is empty - new collection start immediately. Collection does not account for sources that lost connectivity (example: streams that went offline). If that does not happen and stream has large latency - without reasonable <code>batch_collection_timeout</code> it will slow down processing - so please set it up in PROD solutions. In case of video streams (not video files) - given that <code>force_stream_reconnection=True</code> function will attempt to re-connect to disconnected source using background thread, not impairing batch frames collection and that source is not going to block frames retrieval even if infinite <code>batch_collection_timeout=None</code> is set. Similarly, when processing files - video file that is shorter than other passed into processing will not block the whole flow after End Of Stream (EOS).</p> <p>All sources must be accessible on start - if that's not the case - logic function raises <code>SourceConnectionError</code> and closes all video sources it opened on it own. Disconnections at later stages are handled by re-connection mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>videos</code> <code>List[Union[VideoSource, str, int]]</code> <p>List with references to video sources. Elements can be pre-initialised <code>VideoSource</code> instances, str with stream URI or file location or int representing camera device attached to the PC/server running the code.</p> required <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Upper-bound of processing speed - to be used when one wants at max <code>max_fps</code> video frames per second to be yielded from all sources by the generator.</p> <code>None</code> <code>limiter_strategy</code> <code>Optional[FPSLimiterStrategy]</code> <p>strategy used to deal with frames decoding exceeding limit of <code>max_fps</code>. For video files, in the interest of processing all frames - we recommend WAIT mode,  for streams - frames should be dropped on the floor with DROP strategy. Not setting the strategy equals  using automatic mode - WAIT if all sources are files and DROP otherwise</p> <code>None</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>maximum await time to get batch of predictions from all sources. <code>None</code> means infinite timeout.</p> <code>None</code> <code>force_stream_reconnection</code> <code>bool</code> <p>Flag to decide on reconnection to streams (files are never re-connected)</p> <code>True</code> <code>should_stop</code> <code>Callable[[], bool]</code> <p>external stop signal that is periodically checked - to denote that video consumption stopped - make the function to return True</p> <code>never_stop</code> <code>on_reconnection_error</code> <code>Callable[[Optional[int], SourceConnectionError], None]</code> <p>Function that will be called whenever source cannot re-connect after disconnection. First parameter is source_id, second is connection error instance.</p> <code>log_error</code> <p>Returns Generator[List[VideoFrame], None, None]: allowing to iterate through frames from multiple video sources.</p> <p>Raises:</p> Type Description <code>SourceConnectionError</code> <p>when one or more source is not reachable at start of generation</p> Example <pre><code>from inference.core.interfaces.camera.utils import multiplex_videos\n\nfor frames in multiplex_videos(videos=[\"./some.mp4\", \"./other.mp4\"]):\n     for frame in frames:\n        pass  # do something with frame\n</code></pre> Source code in <code>inference/core/interfaces/camera/utils.py</code> <pre><code>def multiplex_videos(\n    videos: List[Union[VideoSource, str, int]],\n    max_fps: Optional[Union[float, int]] = None,\n    limiter_strategy: Optional[FPSLimiterStrategy] = None,\n    batch_collection_timeout: Optional[float] = None,\n    force_stream_reconnection: bool = True,\n    should_stop: Callable[[], bool] = never_stop,\n    on_reconnection_error: Callable[\n        [Optional[int], SourceConnectionError], None\n    ] = log_error,\n) -&gt; Generator[List[VideoFrame], None, None]:\n    \"\"\"\n    Function that is supposed to provide a generator over frames from multiple video sources. It is capable to\n    initialise `VideoSource` from references to video files or streams and grab frames from all the sources -\n    each running individual decoding on separate thread. In each cycle it attempts to grab frames from all sources\n    (and wait at max `batch_collection_timeout` for whole batch to be collected). If frame from specific source\n    cannot be collected in that time - it is simply not included in returned list. If after batch collection list of\n    frames is empty - new collection start immediately. Collection does not account for\n    sources that lost connectivity (example: streams that went offline). If that does not happen and stream has\n    large latency - without reasonable `batch_collection_timeout` it will slow down processing - so please\n    set it up in PROD solutions. In case of video streams (not video files) - given that\n    `force_stream_reconnection=True` function will attempt to re-connect to disconnected source using background thread,\n    not impairing batch frames collection and that source is not going to block frames retrieval even if infinite\n    `batch_collection_timeout=None` is set. Similarly, when processing files - video file that is shorter than other\n    passed into processing will not block the whole flow after End Of Stream (EOS).\n\n    All sources must be accessible on start - if that's not the case - logic function raises `SourceConnectionError`\n    and closes all video sources it opened on it own. Disconnections at later stages are handled by re-connection\n    mechanism.\n\n    Args:\n        videos (List[Union[VideoSource, str, int]]): List with references to video sources. Elements can be\n            pre-initialised `VideoSource` instances, str with stream URI or file location or int representing\n            camera device attached to the PC/server running the code.\n        max_fps (Optional[Union[float, int]]): Upper-bound of processing speed - to be used when one wants at max\n            `max_fps` video frames per second to be yielded from all sources by the generator.\n        limiter_strategy (Optional[FPSLimiterStrategy]): strategy used to deal with frames decoding exceeding\n            limit of `max_fps`. For video files, in the interest of processing all frames - we recommend WAIT mode,\n             for streams - frames should be dropped on the floor with DROP strategy. Not setting the strategy equals\n             using automatic mode - WAIT if all sources are files and DROP otherwise\n        batch_collection_timeout (Optional[float]): maximum await time to get batch of predictions from all sources.\n            `None` means infinite timeout.\n        force_stream_reconnection (bool): Flag to decide on reconnection to streams (files are never re-connected)\n        should_stop (Callable[[], bool]): external stop signal that is periodically checked - to denote that\n            video consumption stopped - make the function to return True\n        on_reconnection_error (Callable[[Optional[int], SourceConnectionError], None]): Function that will be\n            called whenever source cannot re-connect after disconnection. First parameter is source_id, second\n            is connection error instance.\n\n    Returns Generator[List[VideoFrame], None, None]: allowing to iterate through frames from multiple video sources.\n\n    Raises:\n        SourceConnectionError: when one or more source is not reachable at start of generation\n\n    Example:\n        ```python\n        from inference.core.interfaces.camera.utils import multiplex_videos\n\n        for frames in multiplex_videos(videos=[\"./some.mp4\", \"./other.mp4\"]):\n             for frame in frames:\n                pass  # do something with frame\n        ```\n    \"\"\"\n    video_sources = _prepare_video_sources(\n        videos=videos, force_stream_reconnection=force_stream_reconnection\n    )\n    if any(rule is None for rule in video_sources.allow_reconnection):\n        logger.warning(\"Could not connect to all sources.\")\n        return None\n    generator = _multiplex_videos(\n        video_sources=video_sources,\n        batch_collection_timeout=batch_collection_timeout,\n        should_stop=should_stop,\n        on_reconnection_error=on_reconnection_error,\n    )\n    if max_fps is None:\n        yield from generator\n        return None\n    max_fps = max_fps / len(videos)\n    if limiter_strategy is None:\n        limiter_strategy = negotiate_rate_limiter_strategy_for_multiple_sources(\n            video_sources=video_sources.all_sources,\n        )\n    yield from limit_frame_rate(\n        frames_generator=generator, max_fps=max_fps, strategy=limiter_strategy\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/","title":"video_source","text":""},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoConsumer","title":"<code>VideoConsumer</code>","text":"<p>This class should be consumed as part of internal implementation. It provides abstraction around stream consumption strategies.</p> <p>It must always be given the same video source for consecutive invocations, otherwise the internal state does not make sense.</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>class VideoConsumer:\n    \"\"\"\n    This class should be consumed as part of internal implementation.\n    It provides abstraction around stream consumption strategies.\n\n    It must always be given the same video source for consecutive invocations,\n    otherwise the internal state does not make sense.\n    \"\"\"\n\n    @classmethod\n    def init(\n        cls,\n        buffer_filling_strategy: Optional[BufferFillingStrategy],\n        adaptive_mode_stream_pace_tolerance: float,\n        adaptive_mode_reader_pace_tolerance: float,\n        minimum_adaptive_mode_samples: int,\n        maximum_adaptive_frames_dropped_in_row: int,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n        desired_fps: Optional[Union[float, int]] = None,\n    ) -&gt; \"VideoConsumer\":\n        minimum_adaptive_mode_samples = max(minimum_adaptive_mode_samples, 2)\n        reader_pace_monitor = sv.FPSMonitor(\n            sample_size=10 * minimum_adaptive_mode_samples\n        )\n        stream_consumption_pace_monitor = sv.FPSMonitor(\n            sample_size=10 * minimum_adaptive_mode_samples\n        )\n        decoding_pace_monitor = sv.FPSMonitor(\n            sample_size=10 * minimum_adaptive_mode_samples\n        )\n        return cls(\n            buffer_filling_strategy=buffer_filling_strategy,\n            adaptive_mode_stream_pace_tolerance=adaptive_mode_stream_pace_tolerance,\n            adaptive_mode_reader_pace_tolerance=adaptive_mode_reader_pace_tolerance,\n            minimum_adaptive_mode_samples=minimum_adaptive_mode_samples,\n            maximum_adaptive_frames_dropped_in_row=maximum_adaptive_frames_dropped_in_row,\n            status_update_handlers=status_update_handlers,\n            reader_pace_monitor=reader_pace_monitor,\n            stream_consumption_pace_monitor=stream_consumption_pace_monitor,\n            decoding_pace_monitor=decoding_pace_monitor,\n            desired_fps=desired_fps,\n        )\n\n    def __init__(\n        self,\n        buffer_filling_strategy: Optional[BufferFillingStrategy],\n        adaptive_mode_stream_pace_tolerance: float,\n        adaptive_mode_reader_pace_tolerance: float,\n        minimum_adaptive_mode_samples: int,\n        maximum_adaptive_frames_dropped_in_row: int,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n        reader_pace_monitor: sv.FPSMonitor,\n        stream_consumption_pace_monitor: sv.FPSMonitor,\n        decoding_pace_monitor: sv.FPSMonitor,\n        desired_fps: Optional[Union[float, int]],\n    ):\n        self._buffer_filling_strategy = buffer_filling_strategy\n        self._frame_counter = 0\n        self._adaptive_mode_stream_pace_tolerance = adaptive_mode_stream_pace_tolerance\n        self._adaptive_mode_reader_pace_tolerance = adaptive_mode_reader_pace_tolerance\n        self._minimum_adaptive_mode_samples = minimum_adaptive_mode_samples\n        self._maximum_adaptive_frames_dropped_in_row = (\n            maximum_adaptive_frames_dropped_in_row\n        )\n        self._adaptive_frames_dropped_in_row = 0\n        self._reader_pace_monitor = reader_pace_monitor\n        self._stream_consumption_pace_monitor = stream_consumption_pace_monitor\n        self._decoding_pace_monitor = decoding_pace_monitor\n        self._desired_fps = desired_fps\n        self._declared_source_fps = None\n        self._is_source_video_file = None\n        self._status_update_handlers = status_update_handlers\n        self._next_frame_from_video_to_accept = 1\n\n    @property\n    def buffer_filling_strategy(self) -&gt; Optional[BufferFillingStrategy]:\n        return self._buffer_filling_strategy\n\n    def reset(self, source_properties: SourceProperties) -&gt; None:\n        if source_properties.is_file:\n            self._set_file_mode_buffering_strategies()\n        else:\n            self._set_stream_mode_buffering_strategies()\n        self._reader_pace_monitor.reset()\n        self.reset_stream_consumption_pace()\n        self._decoding_pace_monitor.reset()\n        self._adaptive_frames_dropped_in_row = 0\n        self._next_frame_from_video_to_accept = self._frame_counter + 1\n\n    def reset_stream_consumption_pace(self) -&gt; None:\n        self._stream_consumption_pace_monitor.reset()\n\n    def notify_frame_consumed(self) -&gt; None:\n        self._reader_pace_monitor.tick()\n\n    def consume_frame(\n        self,\n        video: VideoFrameProducer,\n        declared_source_fps: Optional[float],\n        is_source_video_file: Optional[bool],\n        buffer: Queue,\n        frames_buffering_allowed: bool,\n        source_id: Optional[int] = None,\n    ) -&gt; bool:\n        if self._is_source_video_file is None:\n            source_properties = video.discover_source_properties()\n            self._is_source_video_file = source_properties.is_file\n            self._declared_source_fps = source_properties.fps\n        frame_timestamp = datetime.now()\n        success = video.grab()\n        self._stream_consumption_pace_monitor.tick()\n        if not success:\n            return False\n        self._frame_counter += 1\n        send_video_source_status_update(\n            severity=UpdateSeverity.DEBUG,\n            event_type=FRAME_CAPTURED_EVENT,\n            payload={\n                \"frame_timestamp\": frame_timestamp,\n                \"frame_id\": self._frame_counter,\n                \"source_id\": source_id,\n            },\n            status_update_handlers=self._status_update_handlers,\n        )\n        if self._video_fps_should_be_sub_sampled():\n            return True\n        return self._consume_stream_frame(\n            video=video,\n            declared_source_fps=declared_source_fps,\n            is_source_video_file=is_source_video_file,\n            frame_timestamp=frame_timestamp,\n            buffer=buffer,\n            frames_buffering_allowed=frames_buffering_allowed,\n            source_id=source_id,\n        )\n\n    def _set_file_mode_buffering_strategies(self) -&gt; None:\n        if self._buffer_filling_strategy is None:\n            self._buffer_filling_strategy = BufferFillingStrategy.WAIT\n\n    def _set_stream_mode_buffering_strategies(self) -&gt; None:\n        if self._buffer_filling_strategy is None:\n            self._buffer_filling_strategy = BufferFillingStrategy.ADAPTIVE_DROP_OLDEST\n\n    def _video_fps_should_be_sub_sampled(self) -&gt; bool:\n        if self._desired_fps is None:\n            return False\n        if self._is_source_video_file:\n            actual_fps = self._declared_source_fps\n        else:\n            fraction_of_pace_monitor_samples = (\n                len(self._stream_consumption_pace_monitor.all_timestamps)\n                / self._stream_consumption_pace_monitor.all_timestamps.maxlen\n            )\n            if fraction_of_pace_monitor_samples &lt; 0.9:\n                actual_fps = self._declared_source_fps\n            elif hasattr(self._stream_consumption_pace_monitor, \"fps\"):\n                actual_fps = self._stream_consumption_pace_monitor.fps\n            else:\n                actual_fps = self._stream_consumption_pace_monitor()\n        if self._frame_counter == self._next_frame_from_video_to_accept:\n            stride = calculate_video_file_stride(\n                actual_fps=actual_fps,\n                desired_fps=self._desired_fps,\n            )\n            self._next_frame_from_video_to_accept += stride\n            return False\n        # skipping frame\n        return True\n\n    def _consume_stream_frame(\n        self,\n        video: VideoFrameProducer,\n        declared_source_fps: Optional[float],\n        is_source_video_file: Optional[bool],\n        frame_timestamp: datetime,\n        buffer: Queue,\n        frames_buffering_allowed: bool,\n        source_id: Optional[int],\n    ) -&gt; bool:\n        \"\"\"\n        Returns: boolean flag with success status\n        \"\"\"\n        if not frames_buffering_allowed:\n            send_frame_drop_update(\n                frame_timestamp=frame_timestamp,\n                frame_id=self._frame_counter,\n                cause=\"Buffering not allowed at the moment\",\n                status_update_handlers=self._status_update_handlers,\n                source_id=source_id,\n            )\n            return True\n        if self._frame_should_be_adaptively_dropped(\n            declared_source_fps=declared_source_fps\n        ):\n            self._adaptive_frames_dropped_in_row += 1\n            send_frame_drop_update(\n                frame_timestamp=frame_timestamp,\n                frame_id=self._frame_counter,\n                cause=\"ADAPTIVE strategy\",\n                status_update_handlers=self._status_update_handlers,\n                source_id=source_id,\n            )\n            return True\n        self._adaptive_frames_dropped_in_row = 0\n        if (\n            not buffer.full()\n            or self._buffer_filling_strategy is BufferFillingStrategy.WAIT\n        ):\n            return decode_video_frame_to_buffer(\n                frame_timestamp=frame_timestamp,\n                frame_id=self._frame_counter,\n                video=video,\n                buffer=buffer,\n                decoding_pace_monitor=self._decoding_pace_monitor,\n                source_id=source_id,\n                fps=declared_source_fps,\n                comes_from_video_file=is_source_video_file,\n            )\n        if self._buffer_filling_strategy in DROP_OLDEST_STRATEGIES:\n            return self._process_stream_frame_dropping_oldest(\n                frame_timestamp=frame_timestamp,\n                video=video,\n                buffer=buffer,\n                source_id=source_id,\n                is_video_file=is_source_video_file,\n            )\n        send_frame_drop_update(\n            frame_timestamp=frame_timestamp,\n            frame_id=self._frame_counter,\n            cause=\"DROP_LATEST strategy\",\n            status_update_handlers=self._status_update_handlers,\n            source_id=source_id,\n        )\n        return True\n\n    def _frame_should_be_adaptively_dropped(\n        self, declared_source_fps: Optional[float]\n    ) -&gt; bool:\n        if self._buffer_filling_strategy not in ADAPTIVE_STRATEGIES:\n            return False\n        if (\n            self._adaptive_frames_dropped_in_row\n            &gt;= self._maximum_adaptive_frames_dropped_in_row\n        ):\n            return False\n        if (\n            len(self._stream_consumption_pace_monitor.all_timestamps)\n            &lt;= self._minimum_adaptive_mode_samples\n        ):\n            # not enough observations\n            return False\n        if hasattr(self._stream_consumption_pace_monitor, \"fps\"):\n            stream_consumption_pace = self._stream_consumption_pace_monitor.fps\n        else:\n            stream_consumption_pace = self._stream_consumption_pace_monitor()\n        announced_stream_fps = stream_consumption_pace\n        if declared_source_fps is not None and declared_source_fps &gt; 0:\n            announced_stream_fps = declared_source_fps\n        if (\n            announced_stream_fps - stream_consumption_pace\n            &gt; self._adaptive_mode_stream_pace_tolerance\n        ):\n            # cannot keep up with stream emission\n            return True\n        if (\n            len(self._reader_pace_monitor.all_timestamps)\n            &lt;= self._minimum_adaptive_mode_samples\n        ) or (\n            len(self._decoding_pace_monitor.all_timestamps)\n            &lt;= self._minimum_adaptive_mode_samples\n        ):\n            # not enough observations\n            return False\n        actual_reader_pace = get_fps_if_tick_happens_now(\n            fps_monitor=self._reader_pace_monitor\n        )\n        if hasattr(self._decoding_pace_monitor, \"fps\"):\n            decoding_pace = self._decoding_pace_monitor.fps\n        else:\n            decoding_pace = self._decoding_pace_monitor()\n        if (\n            decoding_pace - actual_reader_pace\n            &gt; self._adaptive_mode_reader_pace_tolerance\n        ):\n            # we are too fast for the reader - time to save compute on decoding\n            return True\n        return False\n\n    def _process_stream_frame_dropping_oldest(\n        self,\n        frame_timestamp: datetime,\n        video: VideoFrameProducer,\n        buffer: Queue,\n        source_id: Optional[int],\n        is_video_file: bool,\n    ) -&gt; bool:\n        drop_single_frame_from_buffer(\n            buffer=buffer,\n            cause=\"DROP_OLDEST strategy\",\n            status_update_handlers=self._status_update_handlers,\n        )\n        return decode_video_frame_to_buffer(\n            frame_timestamp=frame_timestamp,\n            frame_id=self._frame_counter,\n            video=video,\n            buffer=buffer,\n            decoding_pace_monitor=self._decoding_pace_monitor,\n            source_id=source_id,\n            comes_from_video_file=is_video_file,\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource","title":"<code>VideoSource</code>","text":"Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>class VideoSource:\n    @classmethod\n    def init(\n        cls,\n        video_reference: VideoSourceIdentifier,\n        buffer_size: int = DEFAULT_BUFFER_SIZE,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        adaptive_mode_stream_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE,\n        adaptive_mode_reader_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE,\n        minimum_adaptive_mode_samples: int = DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES,\n        maximum_adaptive_frames_dropped_in_row: int = DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW,\n        video_source_properties: Optional[Dict[str, float]] = None,\n        source_id: Optional[int] = None,\n        desired_fps: Optional[Union[float, int]] = None,\n    ):\n        \"\"\"\n        This class is meant to represent abstraction over video sources - both video files and\n        on-line streams that are possible to be consumed and used by other components of `inference`\n        library.\n\n        Before digging into details of the class behaviour, it is advised to familiarise with the following\n        concepts and implementation assumptions:\n\n        1. Video file can be accessed from local (or remote) storage by the consumer in a pace dictated by\n            its processing capabilities. If processing is faster than the frame rate of video, operations\n            may be executed in a time shorter than the time of video playback. In the opposite case - consumer\n            may freely decode and process frames in its own pace, without risk for failures due to temporal\n            dependencies of processing - this is classical offline processing example.\n        2. Video streams, on the other hand, usually need to be consumed in a pace near to their frame-rate -\n            in other words - this is on-line processing example. Consumer being faster than incoming stream\n            frames cannot utilise its resources to the full extent as not-yet-delivered data would be needed.\n            Slow consumer, however, may not be able to process everything on time and to keep up with the pace\n            of stream - some frames would need to be dropped. Otherwise - over time, consumer could go out of\n            sync with the stream causing decoding failures or unpredictable behavior.\n\n        To fit those two types of video sources, `VideoSource` introduces the concept of buffered decoding of\n        video stream (like at the YouTube - player buffers some frames that are soon to be displayed).\n        The way on how buffer is filled and consumed dictates the behavior of `VideoSource`.\n\n        Starting from `BufferFillingStrategy` - we have 3 basic options:\n        * WAIT: in case of slow video consumption, when buffer is full - `VideoSource` will wait for\n        the empty spot in buffer before next frame will be processed - this is suitable in cases when\n        we want to ensure EACH FRAME of the video to be processed\n        * DROP_OLDEST: when buffer is full, the frame that sits there for the longest time will be dropped -\n        this is suitable for cases when we want to process the most recent frames possible\n        * DROP_LATEST: when buffer is full, the newly decoded frame is dropped - useful in cases when\n        it is expected to have processing performance drops, but we would like to consume portions of\n        video that are locally smooth - but this is probably the least common use-case.\n\n        On top of that - there are two ADAPTIVE strategies: ADAPTIVE_DROP_OLDEST and ADAPTIVE_DROP_LATEST,\n        which are equivalent to DROP_OLDEST and DROP_LATEST with adaptive decoding feature enabled. The notion\n        of that mode will be described later.\n\n        Naturally, decoded frames must also be consumed. `VideoSource` provides a handy interface for reading\n        a video source frames by a SINGLE consumer. Consumption strategy can also be dictated via\n        `BufferConsumptionStrategy`:\n        * LAZY - consume all the frames from decoding buffer one-by-one\n        * EAGER - at each readout - take all frames already buffered, drop all of them apart from the most recent\n\n        In consequence - there are various combinations of `BufferFillingStrategy` and `BufferConsumptionStrategy`.\n        The most popular would be:\n        * `BufferFillingStrategy.WAIT` and `BufferConsumptionStrategy.LAZY` - to always decode and process each and\n            every frame of the source (useful while processing video files - and default behaviour enforced by\n            `inference` if there is no explicit configuration)\n        * `BufferFillingStrategy.DROP_OLDEST` and `BufferConsumptionStrategy.EAGER` - to always process the most\n            recent frames of source (useful while processing video streams when low latency [real-time experience]\n            is required - ADAPTIVE version of this is default for streams)\n\n        ADAPTIVE strategies were introduced to handle corner-cases, when consumer hardware is not capable to consume\n        video stream and process frames at the same time (for instance - Nvidia Jetson devices running processing\n        against hi-res streams with high FPS ratio). It acts with buffer in nearly the same way as `DROP_OLDEST`\n        and `DROP_LATEST` strategies, but there are two more conditions that may influence frame drop:\n        * announced rate of source - which in fact dictate the pace of frames grabbing from incoming stream that\n        MUST be met by consumer to avoid strange decoding issues causing decoder to fail - if the pace of frame grabbing\n        deviates too much - decoding will be postponed, and frames dropped to grab next ones sooner\n        * consumption rate - in resource constraints environment, not only decoding is problematic from the performance\n        perspective - but also heavy processing. If consumer is not quick enough - allocating more useful resources\n        for decoding frames that may never be processed is a waste. That's why - if decoding happens more frequently\n        than consumption of frame - ADAPTIVE mode causes decoding to be done in a slower pace and more frames are just\n        grabbed and dropped on the floor.\n        ADAPTIVE mode increases latency slightly, but may be the only way to operate in some cases.\n        Behaviour of adaptive mode, including the maximum acceptable deviations of frames grabbing pace from source,\n        reader pace and maximum number of consecutive frames dropped in ADAPTIVE mode are configurable by clients,\n        with reasonable defaults being set.\n\n        `VideoSource` emits events regarding its activity - which can be intercepted by custom handlers. Take\n        into account that they are always executed in context of thread invoking them (and should be fast to complete,\n        otherwise may block the flow of stream consumption). All errors raised will be emitted as logger warnings only.\n\n        `VideoSource` implementation is naturally multithreading, with different thread decoding video and different\n        one consuming it and manipulating source state. Implementation of user interface is thread-safe, although\n        stream it is meant to be consumed by a single thread only.\n\n        ENV variables involved:\n        * VIDEO_SOURCE_BUFFER_SIZE - default: 64\n        * VIDEO_SOURCE_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE - default: 0.1\n        * VIDEO_SOURCE_ADAPTIVE_MODE_READER_PACE_TOLERANCE - default: 5.0\n        * VIDEO_SOURCE_MINIMUM_ADAPTIVE_MODE_SAMPLES - default: 10\n        * VIDEO_SOURCE_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW - default: 16\n\n        As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n\n        Args:\n            video_reference (Union[str, int]): Either str with file or stream reference, or int representing device ID\n            buffer_size (int): size of decoding buffer\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers for status updates\n            buffer_filling_strategy (Optional[BufferFillingStrategy]): Settings for buffer filling strategy - if not\n                given - automatic choice regarding source type will be applied\n            buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Settings for buffer consumption strategy,\n                if not given - automatic choice regarding source type will be applied\n            adaptive_mode_stream_pace_tolerance (float): Maximum deviation between frames grabbing pace and stream pace\n                that will not trigger adaptive mode frame drop\n            adaptive_mode_reader_pace_tolerance (float): Maximum deviation between decoding pace and stream consumption\n                pace that will not trigger adaptive mode frame drop\n            minimum_adaptive_mode_samples (int): Minimal number of frames to be used to establish actual pace of\n                processing, before adaptive mode can drop any frame\n            maximum_adaptive_frames_dropped_in_row (int): Maximum number of frames dropped in row due to application of\n                adaptive strategy\n            video_source_properties (Optional[dict[str, float]]): Optional dictionary with video source properties\n                corresponding to OpenCV VideoCapture properties cv2.CAP_PROP_* to set values for the video source.\n            source_id (Optional[int]): Optional identifier of video source - mainly useful to recognise specific source\n                when multiple ones are in use. Identifier will be added to emitted frames and updates. It is advised\n                to keep it unique within all sources in use.\n\n        Returns: Instance of `VideoSource` class\n        \"\"\"\n        frames_buffer = Queue(maxsize=buffer_size)\n        if status_update_handlers is None:\n            status_update_handlers = []\n        video_consumer = VideoConsumer.init(\n            buffer_filling_strategy=buffer_filling_strategy,\n            adaptive_mode_stream_pace_tolerance=adaptive_mode_stream_pace_tolerance,\n            adaptive_mode_reader_pace_tolerance=adaptive_mode_reader_pace_tolerance,\n            minimum_adaptive_mode_samples=minimum_adaptive_mode_samples,\n            maximum_adaptive_frames_dropped_in_row=maximum_adaptive_frames_dropped_in_row,\n            status_update_handlers=status_update_handlers,\n            desired_fps=desired_fps,\n        )\n        return cls(\n            stream_reference=video_reference,\n            frames_buffer=frames_buffer,\n            status_update_handlers=status_update_handlers,\n            buffer_consumption_strategy=buffer_consumption_strategy,\n            video_consumer=video_consumer,\n            video_source_properties=video_source_properties,\n            source_id=source_id,\n        )\n\n    def __init__(\n        self,\n        stream_reference: VideoSourceIdentifier,\n        frames_buffer: Queue,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n        buffer_consumption_strategy: Optional[BufferConsumptionStrategy],\n        video_consumer: \"VideoConsumer\",\n        video_source_properties: Optional[Dict[str, float]],\n        source_id: Optional[int],\n    ):\n        self._stream_reference = stream_reference\n        self._video: Optional[VideoFrameProducer] = None\n        self._source_properties: Optional[SourceProperties] = None\n        self._frames_buffer = frames_buffer\n        self._status_update_handlers = status_update_handlers\n        self._buffer_consumption_strategy = buffer_consumption_strategy\n        self._video_consumer = video_consumer\n        self._state = StreamState.NOT_STARTED\n        self._playback_allowed = Event()\n        self._frames_buffering_allowed = True\n        self._stream_consumption_thread: Optional[Thread] = None\n        self._state_change_lock = Lock()\n        self._video_source_properties = video_source_properties or {}\n        self._source_id = source_id\n\n    @property\n    def source_id(self) -&gt; Optional[int]:\n        return self._source_id\n\n    @lock_state_transition\n    def restart(\n        self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n    ) -&gt; None:\n        \"\"\"\n        Method to restart source consumption. Eligible to be used in states:\n        [MUTED, RUNNING, PAUSED, ENDED, ERROR].\n        End state:\n        * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n        * ERROR - if it was not possible to connect with source\n\n        Thread safe - only one transition of states possible at the time.\n\n        Args:\n            wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n                completion of this operation.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n            * SourceConnectionError: if source cannot be connected\n        \"\"\"\n        if self._state not in RESTART_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not RESTART stream in state: {self._state}\"\n            )\n        self._restart(\n            wait_on_frames_consumption=wait_on_frames_consumption,\n            purge_frames_buffer=purge_frames_buffer,\n        )\n\n    @lock_state_transition\n    def start(self) -&gt; None:\n        \"\"\"\n        Method to be used to start source consumption. Eligible to be used in states:\n        [NOT_STARTED, ENDED, (RESTARTING - which is internal state only)]\n        End state:\n        * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n        * ERROR - if it was not possible to connect with source\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n            * SourceConnectionError: if source cannot be connected\n        \"\"\"\n        if self._state not in START_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not START stream in state: {self._state}\"\n            )\n        self._start()\n\n    @lock_state_transition\n    def terminate(\n        self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n    ) -&gt; None:\n        \"\"\"\n        Method to be used to terminate source consumption. Eligible to be used in states:\n        [MUTED, RUNNING, PAUSED, ENDED, ERROR, (RESTARTING - which is internal state only)]\n        End state:\n        * ENDED - indicating success of the process\n        * ERROR - if error with processing occurred\n\n        Must be used to properly dispose resources at the end.\n\n        Thread safe - only one transition of states possible at the time.\n\n        Args:\n            wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n                completion of this operation.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in TERMINATE_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not TERMINATE stream in state: {self._state}\"\n            )\n        self._terminate(\n            wait_on_frames_consumption=wait_on_frames_consumption,\n            purge_frames_buffer=purge_frames_buffer,\n        )\n\n    @lock_state_transition\n    def pause(self) -&gt; None:\n        \"\"\"\n        Method to be used to pause source consumption. During pause - no new frames are consumed.\n        Used on on-line streams for too long may cause stream disconnection.\n        Eligible to be used in states:\n        [RUNNING]\n        End state:\n        * PAUSED\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in PAUSE_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not PAUSE stream in state: {self._state}\"\n            )\n        self._pause()\n\n    @lock_state_transition\n    def mute(self) -&gt; None:\n        \"\"\"\n        Method to be used to mute source consumption. Muting is an equivalent of pause for stream - where\n        frames grabbing is not put on hold, just new frames decoding and buffering is not allowed - causing\n        intermediate frames to be dropped. May be also used against files, although arguably less useful.\n        Eligible to be used in states:\n        [RUNNING]\n        End state:\n        * MUTED\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in MUTE_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not MUTE stream in state: {self._state}\"\n            )\n        self._mute()\n\n    @lock_state_transition\n    def resume(self) -&gt; None:\n        \"\"\"\n        Method to recover from pause or mute into running state.\n        [PAUSED, MUTED]\n        End state:\n        * RUNNING\n\n        Thread safe - only one transition of states possible at the time.\n\n        Returns: None\n        Throws:\n            * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        \"\"\"\n        if self._state not in RESUME_ELIGIBLE_STATES:\n            raise StreamOperationNotAllowedError(\n                f\"Could not RESUME stream in state: {self._state}\"\n            )\n        self._resume()\n\n    def get_state(self) -&gt; StreamState:\n        \"\"\"\n        Method to get current state of the `VideoSource`\n\n        Returns: StreamState\n        \"\"\"\n        return self._state\n\n    def frame_ready(self) -&gt; bool:\n        \"\"\"\n        Method to check if decoded frame is ready for consumer\n\n        Returns: boolean flag indicating frame readiness\n        \"\"\"\n        return not self._frames_buffer.empty()\n\n    def read_frame(self, timeout: Optional[float] = None) -&gt; Optional[VideoFrame]:\n        \"\"\"\n        Method to be used by the consumer to get decoded source frame.\n\n        Returns: VideoFrame object with decoded frame and its metadata.\n        Throws:\n            * EndOfStreamError: when trying to get the frame from closed source.\n        \"\"\"\n        video_frame: Optional[Union[VideoFrame, str]] = get_from_queue(\n            queue=self._frames_buffer,\n            on_successful_read=self._video_consumer.notify_frame_consumed,\n            timeout=timeout,\n            purge=self._buffer_consumption_strategy is BufferConsumptionStrategy.EAGER,\n        )\n        if video_frame == POISON_PILL:\n            raise EndOfStreamError(\n                \"Attempted to retrieve frame from stream that already ended.\"\n            )\n        if video_frame is not None:\n            send_video_source_status_update(\n                severity=UpdateSeverity.DEBUG,\n                event_type=FRAME_CONSUMED_EVENT,\n                payload={\n                    \"frame_timestamp\": video_frame.frame_timestamp,\n                    \"frame_id\": video_frame.frame_id,\n                    \"source_id\": video_frame.source_id,\n                },\n                status_update_handlers=self._status_update_handlers,\n            )\n        return video_frame\n\n    def describe_source(self) -&gt; SourceMetadata:\n        serialized_source_reference = self._stream_reference\n        if callable(serialized_source_reference):\n            serialized_source_reference = str(self._stream_reference)\n        return SourceMetadata(\n            source_properties=self._source_properties,\n            source_reference=serialized_source_reference,\n            buffer_size=self._frames_buffer.maxsize,\n            state=self._state,\n            buffer_filling_strategy=self._video_consumer.buffer_filling_strategy,\n            buffer_consumption_strategy=self._buffer_consumption_strategy,\n            source_id=self._source_id,\n        )\n\n    def _restart(\n        self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n    ) -&gt; None:\n        self._terminate(\n            wait_on_frames_consumption=wait_on_frames_consumption,\n            purge_frames_buffer=purge_frames_buffer,\n        )\n        self._change_state(target_state=StreamState.RESTARTING)\n        self._playback_allowed = Event()\n        self._frames_buffering_allowed = True\n        self._video: Optional[VideoFrameProducer] = None\n        self._source_properties: Optional[SourceProperties] = None\n        self._start()\n\n    def _start(self) -&gt; None:\n        self._change_state(target_state=StreamState.INITIALISING)\n        if callable(self._stream_reference):\n            self._video = self._stream_reference()\n        else:\n            self._video = CV2VideoFrameProducer(self._stream_reference)\n        if not self._video.isOpened():\n            self._change_state(target_state=StreamState.ERROR)\n            raise SourceConnectionError(\n                f\"Cannot connect to video source under reference: {self._stream_reference}\"\n            )\n        self._video.initialize_source_properties(self._video_source_properties)\n        self._source_properties = self._video.discover_source_properties()\n        self._video_consumer.reset(source_properties=self._source_properties)\n        if self._source_properties.is_file:\n            self._set_file_mode_consumption_strategies()\n        else:\n            self._set_stream_mode_consumption_strategies()\n        self._playback_allowed.set()\n        self._stream_consumption_thread = Thread(target=self._consume_video)\n        self._stream_consumption_thread.start()\n\n    def _terminate(\n        self, wait_on_frames_consumption: bool, purge_frames_buffer: bool\n    ) -&gt; None:\n        if self._state in RESUME_ELIGIBLE_STATES:\n            self._resume()\n        previous_state = self._state\n        self._change_state(target_state=StreamState.TERMINATING)\n        if purge_frames_buffer:\n            _ = get_from_queue(queue=self._frames_buffer, timeout=0.0, purge=True)\n        if self._stream_consumption_thread is not None:\n            self._stream_consumption_thread.join()\n        if wait_on_frames_consumption:\n            self._frames_buffer.join()\n        if previous_state is not StreamState.ERROR:\n            self._change_state(target_state=StreamState.ENDED)\n\n    def _pause(self) -&gt; None:\n        self._playback_allowed.clear()\n        self._change_state(target_state=StreamState.PAUSED)\n\n    def _mute(self) -&gt; None:\n        self._frames_buffering_allowed = False\n        self._change_state(target_state=StreamState.MUTED)\n\n    def _resume(self) -&gt; None:\n        previous_state = self._state\n        self._change_state(target_state=StreamState.RUNNING)\n        if previous_state is StreamState.PAUSED:\n            self._video_consumer.reset_stream_consumption_pace()\n            self._playback_allowed.set()\n        if previous_state is StreamState.MUTED:\n            self._frames_buffering_allowed = True\n\n    def _set_file_mode_consumption_strategies(self) -&gt; None:\n        if self._buffer_consumption_strategy is None:\n            self._buffer_consumption_strategy = BufferConsumptionStrategy.LAZY\n\n    def _set_stream_mode_consumption_strategies(self) -&gt; None:\n        if self._buffer_consumption_strategy is None:\n            self._buffer_consumption_strategy = BufferConsumptionStrategy.EAGER\n\n    def _consume_video(self) -&gt; None:\n        send_video_source_status_update(\n            severity=UpdateSeverity.INFO,\n            event_type=VIDEO_CONSUMPTION_STARTED_EVENT,\n            status_update_handlers=self._status_update_handlers,\n            payload={\"source_id\": self._source_id},\n        )\n        logger.info(f\"Video consumption started\")\n        try:\n            if self._state is not StreamState.TERMINATING:\n                self._change_state(target_state=StreamState.RUNNING)\n            declared_source_fps, is_video_file = None, None\n            if self._source_properties is not None:\n                declared_source_fps = self._source_properties.fps\n                is_video_file = self._source_properties.is_file\n            while self._video.isOpened():\n                if self._state is StreamState.TERMINATING:\n                    break\n                self._playback_allowed.wait()\n                success = self._video_consumer.consume_frame(\n                    video=self._video,\n                    declared_source_fps=declared_source_fps,\n                    is_source_video_file=is_video_file,\n                    buffer=self._frames_buffer,\n                    frames_buffering_allowed=self._frames_buffering_allowed,\n                    source_id=self._source_id,\n                )\n                if not success:\n                    break\n            self._frames_buffer.put(POISON_PILL)\n            self._video.release()\n            self._change_state(target_state=StreamState.ENDED)\n            send_video_source_status_update(\n                severity=UpdateSeverity.INFO,\n                event_type=VIDEO_CONSUMPTION_FINISHED_EVENT,\n                status_update_handlers=self._status_update_handlers,\n                payload={\"source_id\": self._source_id},\n            )\n            logger.info(f\"Video consumption finished\")\n        except Exception as error:\n            self._change_state(target_state=StreamState.ERROR)\n            payload = {\n                \"source_id\": self._source_id,\n                \"error_type\": error.__class__.__name__,\n                \"error_message\": str(error),\n                \"error_context\": \"stream_consumer_thread\",\n            }\n            send_video_source_status_update(\n                severity=UpdateSeverity.ERROR,\n                event_type=SOURCE_ERROR_EVENT,\n                payload=payload,\n                status_update_handlers=self._status_update_handlers,\n            )\n            logger.exception(\"Encountered error in video consumption thread\")\n\n    def _change_state(self, target_state: StreamState) -&gt; None:\n        payload = {\n            \"previous_state\": self._state,\n            \"new_state\": target_state,\n            \"source_id\": self._source_id,\n        }\n        self._state = target_state\n        send_video_source_status_update(\n            severity=UpdateSeverity.INFO,\n            event_type=SOURCE_STATE_UPDATE_EVENT,\n            payload=payload,\n            status_update_handlers=self._status_update_handlers,\n        )\n\n    def __iter__(self) -&gt; \"VideoSource\":\n        return self\n\n    def __next__(self) -&gt; VideoFrame:\n        \"\"\"\n        Method allowing to use `VideoSource` convenient to read frames\n\n        Returns: VideoFrame\n\n        Example:\n            ```python\n            source = VideoSource.init(video_reference=\"./some.mp4\")\n            source.start()\n\n            for frame in source:\n                 pass\n            ```\n        \"\"\"\n        try:\n            return self.read_frame()\n        except EndOfStreamError:\n            raise StopIteration()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.__next__","title":"<code>__next__()</code>","text":"<p>Method allowing to use <code>VideoSource</code> convenient to read frames</p> <p>Returns: VideoFrame</p> Example <pre><code>source = VideoSource.init(video_reference=\"./some.mp4\")\nsource.start()\n\nfor frame in source:\n     pass\n</code></pre> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def __next__(self) -&gt; VideoFrame:\n    \"\"\"\n    Method allowing to use `VideoSource` convenient to read frames\n\n    Returns: VideoFrame\n\n    Example:\n        ```python\n        source = VideoSource.init(video_reference=\"./some.mp4\")\n        source.start()\n\n        for frame in source:\n             pass\n        ```\n    \"\"\"\n    try:\n        return self.read_frame()\n    except EndOfStreamError:\n        raise StopIteration()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.frame_ready","title":"<code>frame_ready()</code>","text":"<p>Method to check if decoded frame is ready for consumer</p> <p>Returns: boolean flag indicating frame readiness</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def frame_ready(self) -&gt; bool:\n    \"\"\"\n    Method to check if decoded frame is ready for consumer\n\n    Returns: boolean flag indicating frame readiness\n    \"\"\"\n    return not self._frames_buffer.empty()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.get_state","title":"<code>get_state()</code>","text":"<p>Method to get current state of the <code>VideoSource</code></p> <p>Returns: StreamState</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def get_state(self) -&gt; StreamState:\n    \"\"\"\n    Method to get current state of the `VideoSource`\n\n    Returns: StreamState\n    \"\"\"\n    return self._state\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.init","title":"<code>init(video_reference, buffer_size=DEFAULT_BUFFER_SIZE, status_update_handlers=None, buffer_filling_strategy=None, buffer_consumption_strategy=None, adaptive_mode_stream_pace_tolerance=DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE, adaptive_mode_reader_pace_tolerance=DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE, minimum_adaptive_mode_samples=DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES, maximum_adaptive_frames_dropped_in_row=DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW, video_source_properties=None, source_id=None, desired_fps=None)</code>  <code>classmethod</code>","text":"<p>This class is meant to represent abstraction over video sources - both video files and on-line streams that are possible to be consumed and used by other components of <code>inference</code> library.</p> <p>Before digging into details of the class behaviour, it is advised to familiarise with the following concepts and implementation assumptions:</p> <ol> <li>Video file can be accessed from local (or remote) storage by the consumer in a pace dictated by     its processing capabilities. If processing is faster than the frame rate of video, operations     may be executed in a time shorter than the time of video playback. In the opposite case - consumer     may freely decode and process frames in its own pace, without risk for failures due to temporal     dependencies of processing - this is classical offline processing example.</li> <li>Video streams, on the other hand, usually need to be consumed in a pace near to their frame-rate -     in other words - this is on-line processing example. Consumer being faster than incoming stream     frames cannot utilise its resources to the full extent as not-yet-delivered data would be needed.     Slow consumer, however, may not be able to process everything on time and to keep up with the pace     of stream - some frames would need to be dropped. Otherwise - over time, consumer could go out of     sync with the stream causing decoding failures or unpredictable behavior.</li> </ol> <p>To fit those two types of video sources, <code>VideoSource</code> introduces the concept of buffered decoding of video stream (like at the YouTube - player buffers some frames that are soon to be displayed). The way on how buffer is filled and consumed dictates the behavior of <code>VideoSource</code>.</p> <p>Starting from <code>BufferFillingStrategy</code> - we have 3 basic options: * WAIT: in case of slow video consumption, when buffer is full - <code>VideoSource</code> will wait for the empty spot in buffer before next frame will be processed - this is suitable in cases when we want to ensure EACH FRAME of the video to be processed * DROP_OLDEST: when buffer is full, the frame that sits there for the longest time will be dropped - this is suitable for cases when we want to process the most recent frames possible * DROP_LATEST: when buffer is full, the newly decoded frame is dropped - useful in cases when it is expected to have processing performance drops, but we would like to consume portions of video that are locally smooth - but this is probably the least common use-case.</p> <p>On top of that - there are two ADAPTIVE strategies: ADAPTIVE_DROP_OLDEST and ADAPTIVE_DROP_LATEST, which are equivalent to DROP_OLDEST and DROP_LATEST with adaptive decoding feature enabled. The notion of that mode will be described later.</p> <p>Naturally, decoded frames must also be consumed. <code>VideoSource</code> provides a handy interface for reading a video source frames by a SINGLE consumer. Consumption strategy can also be dictated via <code>BufferConsumptionStrategy</code>: * LAZY - consume all the frames from decoding buffer one-by-one * EAGER - at each readout - take all frames already buffered, drop all of them apart from the most recent</p> <p>In consequence - there are various combinations of <code>BufferFillingStrategy</code> and <code>BufferConsumptionStrategy</code>. The most popular would be: * <code>BufferFillingStrategy.WAIT</code> and <code>BufferConsumptionStrategy.LAZY</code> - to always decode and process each and     every frame of the source (useful while processing video files - and default behaviour enforced by     <code>inference</code> if there is no explicit configuration) * <code>BufferFillingStrategy.DROP_OLDEST</code> and <code>BufferConsumptionStrategy.EAGER</code> - to always process the most     recent frames of source (useful while processing video streams when low latency [real-time experience]     is required - ADAPTIVE version of this is default for streams)</p> <p>ADAPTIVE strategies were introduced to handle corner-cases, when consumer hardware is not capable to consume video stream and process frames at the same time (for instance - Nvidia Jetson devices running processing against hi-res streams with high FPS ratio). It acts with buffer in nearly the same way as <code>DROP_OLDEST</code> and <code>DROP_LATEST</code> strategies, but there are two more conditions that may influence frame drop: * announced rate of source - which in fact dictate the pace of frames grabbing from incoming stream that MUST be met by consumer to avoid strange decoding issues causing decoder to fail - if the pace of frame grabbing deviates too much - decoding will be postponed, and frames dropped to grab next ones sooner * consumption rate - in resource constraints environment, not only decoding is problematic from the performance perspective - but also heavy processing. If consumer is not quick enough - allocating more useful resources for decoding frames that may never be processed is a waste. That's why - if decoding happens more frequently than consumption of frame - ADAPTIVE mode causes decoding to be done in a slower pace and more frames are just grabbed and dropped on the floor. ADAPTIVE mode increases latency slightly, but may be the only way to operate in some cases. Behaviour of adaptive mode, including the maximum acceptable deviations of frames grabbing pace from source, reader pace and maximum number of consecutive frames dropped in ADAPTIVE mode are configurable by clients, with reasonable defaults being set.</p> <p><code>VideoSource</code> emits events regarding its activity - which can be intercepted by custom handlers. Take into account that they are always executed in context of thread invoking them (and should be fast to complete, otherwise may block the flow of stream consumption). All errors raised will be emitted as logger warnings only.</p> <p><code>VideoSource</code> implementation is naturally multithreading, with different thread decoding video and different one consuming it and manipulating source state. Implementation of user interface is thread-safe, although stream it is meant to be consumed by a single thread only.</p> <p>ENV variables involved: * VIDEO_SOURCE_BUFFER_SIZE - default: 64 * VIDEO_SOURCE_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE - default: 0.1 * VIDEO_SOURCE_ADAPTIVE_MODE_READER_PACE_TOLERANCE - default: 5.0 * VIDEO_SOURCE_MINIMUM_ADAPTIVE_MODE_SAMPLES - default: 10 * VIDEO_SOURCE_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW - default: 16</p> <p>As an <code>inference</code> user, please use .init() method instead of constructor to instantiate objects.</p> <p>Parameters:</p> Name Type Description Default <code>video_reference</code> <code>Union[str, int]</code> <p>Either str with file or stream reference, or int representing device ID</p> required <code>buffer_size</code> <code>int</code> <p>size of decoding buffer</p> <code>DEFAULT_BUFFER_SIZE</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers for status updates</p> <code>None</code> <code>buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Settings for buffer filling strategy - if not given - automatic choice regarding source type will be applied</p> <code>None</code> <code>buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Settings for buffer consumption strategy, if not given - automatic choice regarding source type will be applied</p> <code>None</code> <code>adaptive_mode_stream_pace_tolerance</code> <code>float</code> <p>Maximum deviation between frames grabbing pace and stream pace that will not trigger adaptive mode frame drop</p> <code>DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE</code> <code>adaptive_mode_reader_pace_tolerance</code> <code>float</code> <p>Maximum deviation between decoding pace and stream consumption pace that will not trigger adaptive mode frame drop</p> <code>DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE</code> <code>minimum_adaptive_mode_samples</code> <code>int</code> <p>Minimal number of frames to be used to establish actual pace of processing, before adaptive mode can drop any frame</p> <code>DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES</code> <code>maximum_adaptive_frames_dropped_in_row</code> <code>int</code> <p>Maximum number of frames dropped in row due to application of adaptive strategy</p> <code>DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW</code> <code>video_source_properties</code> <code>Optional[dict[str, float]]</code> <p>Optional dictionary with video source properties corresponding to OpenCV VideoCapture properties cv2.CAP_PROP_* to set values for the video source.</p> <code>None</code> <code>source_id</code> <code>Optional[int]</code> <p>Optional identifier of video source - mainly useful to recognise specific source when multiple ones are in use. Identifier will be added to emitted frames and updates. It is advised to keep it unique within all sources in use.</p> <code>None</code> <p>Returns: Instance of <code>VideoSource</code> class</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@classmethod\ndef init(\n    cls,\n    video_reference: VideoSourceIdentifier,\n    buffer_size: int = DEFAULT_BUFFER_SIZE,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    adaptive_mode_stream_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE,\n    adaptive_mode_reader_pace_tolerance: float = DEFAULT_ADAPTIVE_MODE_READER_PACE_TOLERANCE,\n    minimum_adaptive_mode_samples: int = DEFAULT_MINIMUM_ADAPTIVE_MODE_SAMPLES,\n    maximum_adaptive_frames_dropped_in_row: int = DEFAULT_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW,\n    video_source_properties: Optional[Dict[str, float]] = None,\n    source_id: Optional[int] = None,\n    desired_fps: Optional[Union[float, int]] = None,\n):\n    \"\"\"\n    This class is meant to represent abstraction over video sources - both video files and\n    on-line streams that are possible to be consumed and used by other components of `inference`\n    library.\n\n    Before digging into details of the class behaviour, it is advised to familiarise with the following\n    concepts and implementation assumptions:\n\n    1. Video file can be accessed from local (or remote) storage by the consumer in a pace dictated by\n        its processing capabilities. If processing is faster than the frame rate of video, operations\n        may be executed in a time shorter than the time of video playback. In the opposite case - consumer\n        may freely decode and process frames in its own pace, without risk for failures due to temporal\n        dependencies of processing - this is classical offline processing example.\n    2. Video streams, on the other hand, usually need to be consumed in a pace near to their frame-rate -\n        in other words - this is on-line processing example. Consumer being faster than incoming stream\n        frames cannot utilise its resources to the full extent as not-yet-delivered data would be needed.\n        Slow consumer, however, may not be able to process everything on time and to keep up with the pace\n        of stream - some frames would need to be dropped. Otherwise - over time, consumer could go out of\n        sync with the stream causing decoding failures or unpredictable behavior.\n\n    To fit those two types of video sources, `VideoSource` introduces the concept of buffered decoding of\n    video stream (like at the YouTube - player buffers some frames that are soon to be displayed).\n    The way on how buffer is filled and consumed dictates the behavior of `VideoSource`.\n\n    Starting from `BufferFillingStrategy` - we have 3 basic options:\n    * WAIT: in case of slow video consumption, when buffer is full - `VideoSource` will wait for\n    the empty spot in buffer before next frame will be processed - this is suitable in cases when\n    we want to ensure EACH FRAME of the video to be processed\n    * DROP_OLDEST: when buffer is full, the frame that sits there for the longest time will be dropped -\n    this is suitable for cases when we want to process the most recent frames possible\n    * DROP_LATEST: when buffer is full, the newly decoded frame is dropped - useful in cases when\n    it is expected to have processing performance drops, but we would like to consume portions of\n    video that are locally smooth - but this is probably the least common use-case.\n\n    On top of that - there are two ADAPTIVE strategies: ADAPTIVE_DROP_OLDEST and ADAPTIVE_DROP_LATEST,\n    which are equivalent to DROP_OLDEST and DROP_LATEST with adaptive decoding feature enabled. The notion\n    of that mode will be described later.\n\n    Naturally, decoded frames must also be consumed. `VideoSource` provides a handy interface for reading\n    a video source frames by a SINGLE consumer. Consumption strategy can also be dictated via\n    `BufferConsumptionStrategy`:\n    * LAZY - consume all the frames from decoding buffer one-by-one\n    * EAGER - at each readout - take all frames already buffered, drop all of them apart from the most recent\n\n    In consequence - there are various combinations of `BufferFillingStrategy` and `BufferConsumptionStrategy`.\n    The most popular would be:\n    * `BufferFillingStrategy.WAIT` and `BufferConsumptionStrategy.LAZY` - to always decode and process each and\n        every frame of the source (useful while processing video files - and default behaviour enforced by\n        `inference` if there is no explicit configuration)\n    * `BufferFillingStrategy.DROP_OLDEST` and `BufferConsumptionStrategy.EAGER` - to always process the most\n        recent frames of source (useful while processing video streams when low latency [real-time experience]\n        is required - ADAPTIVE version of this is default for streams)\n\n    ADAPTIVE strategies were introduced to handle corner-cases, when consumer hardware is not capable to consume\n    video stream and process frames at the same time (for instance - Nvidia Jetson devices running processing\n    against hi-res streams with high FPS ratio). It acts with buffer in nearly the same way as `DROP_OLDEST`\n    and `DROP_LATEST` strategies, but there are two more conditions that may influence frame drop:\n    * announced rate of source - which in fact dictate the pace of frames grabbing from incoming stream that\n    MUST be met by consumer to avoid strange decoding issues causing decoder to fail - if the pace of frame grabbing\n    deviates too much - decoding will be postponed, and frames dropped to grab next ones sooner\n    * consumption rate - in resource constraints environment, not only decoding is problematic from the performance\n    perspective - but also heavy processing. If consumer is not quick enough - allocating more useful resources\n    for decoding frames that may never be processed is a waste. That's why - if decoding happens more frequently\n    than consumption of frame - ADAPTIVE mode causes decoding to be done in a slower pace and more frames are just\n    grabbed and dropped on the floor.\n    ADAPTIVE mode increases latency slightly, but may be the only way to operate in some cases.\n    Behaviour of adaptive mode, including the maximum acceptable deviations of frames grabbing pace from source,\n    reader pace and maximum number of consecutive frames dropped in ADAPTIVE mode are configurable by clients,\n    with reasonable defaults being set.\n\n    `VideoSource` emits events regarding its activity - which can be intercepted by custom handlers. Take\n    into account that they are always executed in context of thread invoking them (and should be fast to complete,\n    otherwise may block the flow of stream consumption). All errors raised will be emitted as logger warnings only.\n\n    `VideoSource` implementation is naturally multithreading, with different thread decoding video and different\n    one consuming it and manipulating source state. Implementation of user interface is thread-safe, although\n    stream it is meant to be consumed by a single thread only.\n\n    ENV variables involved:\n    * VIDEO_SOURCE_BUFFER_SIZE - default: 64\n    * VIDEO_SOURCE_ADAPTIVE_MODE_STREAM_PACE_TOLERANCE - default: 0.1\n    * VIDEO_SOURCE_ADAPTIVE_MODE_READER_PACE_TOLERANCE - default: 5.0\n    * VIDEO_SOURCE_MINIMUM_ADAPTIVE_MODE_SAMPLES - default: 10\n    * VIDEO_SOURCE_MAXIMUM_ADAPTIVE_FRAMES_DROPPED_IN_ROW - default: 16\n\n    As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n\n    Args:\n        video_reference (Union[str, int]): Either str with file or stream reference, or int representing device ID\n        buffer_size (int): size of decoding buffer\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers for status updates\n        buffer_filling_strategy (Optional[BufferFillingStrategy]): Settings for buffer filling strategy - if not\n            given - automatic choice regarding source type will be applied\n        buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Settings for buffer consumption strategy,\n            if not given - automatic choice regarding source type will be applied\n        adaptive_mode_stream_pace_tolerance (float): Maximum deviation between frames grabbing pace and stream pace\n            that will not trigger adaptive mode frame drop\n        adaptive_mode_reader_pace_tolerance (float): Maximum deviation between decoding pace and stream consumption\n            pace that will not trigger adaptive mode frame drop\n        minimum_adaptive_mode_samples (int): Minimal number of frames to be used to establish actual pace of\n            processing, before adaptive mode can drop any frame\n        maximum_adaptive_frames_dropped_in_row (int): Maximum number of frames dropped in row due to application of\n            adaptive strategy\n        video_source_properties (Optional[dict[str, float]]): Optional dictionary with video source properties\n            corresponding to OpenCV VideoCapture properties cv2.CAP_PROP_* to set values for the video source.\n        source_id (Optional[int]): Optional identifier of video source - mainly useful to recognise specific source\n            when multiple ones are in use. Identifier will be added to emitted frames and updates. It is advised\n            to keep it unique within all sources in use.\n\n    Returns: Instance of `VideoSource` class\n    \"\"\"\n    frames_buffer = Queue(maxsize=buffer_size)\n    if status_update_handlers is None:\n        status_update_handlers = []\n    video_consumer = VideoConsumer.init(\n        buffer_filling_strategy=buffer_filling_strategy,\n        adaptive_mode_stream_pace_tolerance=adaptive_mode_stream_pace_tolerance,\n        adaptive_mode_reader_pace_tolerance=adaptive_mode_reader_pace_tolerance,\n        minimum_adaptive_mode_samples=minimum_adaptive_mode_samples,\n        maximum_adaptive_frames_dropped_in_row=maximum_adaptive_frames_dropped_in_row,\n        status_update_handlers=status_update_handlers,\n        desired_fps=desired_fps,\n    )\n    return cls(\n        stream_reference=video_reference,\n        frames_buffer=frames_buffer,\n        status_update_handlers=status_update_handlers,\n        buffer_consumption_strategy=buffer_consumption_strategy,\n        video_consumer=video_consumer,\n        video_source_properties=video_source_properties,\n        source_id=source_id,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.mute","title":"<code>mute()</code>","text":"<p>Method to be used to mute source consumption. Muting is an equivalent of pause for stream - where frames grabbing is not put on hold, just new frames decoding and buffering is not allowed - causing intermediate frames to be dropped. May be also used against files, although arguably less useful. Eligible to be used in states: [RUNNING] End state: * MUTED</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef mute(self) -&gt; None:\n    \"\"\"\n    Method to be used to mute source consumption. Muting is an equivalent of pause for stream - where\n    frames grabbing is not put on hold, just new frames decoding and buffering is not allowed - causing\n    intermediate frames to be dropped. May be also used against files, although arguably less useful.\n    Eligible to be used in states:\n    [RUNNING]\n    End state:\n    * MUTED\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in MUTE_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not MUTE stream in state: {self._state}\"\n        )\n    self._mute()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.pause","title":"<code>pause()</code>","text":"<p>Method to be used to pause source consumption. During pause - no new frames are consumed. Used on on-line streams for too long may cause stream disconnection. Eligible to be used in states: [RUNNING] End state: * PAUSED</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef pause(self) -&gt; None:\n    \"\"\"\n    Method to be used to pause source consumption. During pause - no new frames are consumed.\n    Used on on-line streams for too long may cause stream disconnection.\n    Eligible to be used in states:\n    [RUNNING]\n    End state:\n    * PAUSED\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in PAUSE_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not PAUSE stream in state: {self._state}\"\n        )\n    self._pause()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.read_frame","title":"<code>read_frame(timeout=None)</code>","text":"<p>Method to be used by the consumer to get decoded source frame.</p> <p>Throws:     * EndOfStreamError: when trying to get the frame from closed source.</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def read_frame(self, timeout: Optional[float] = None) -&gt; Optional[VideoFrame]:\n    \"\"\"\n    Method to be used by the consumer to get decoded source frame.\n\n    Returns: VideoFrame object with decoded frame and its metadata.\n    Throws:\n        * EndOfStreamError: when trying to get the frame from closed source.\n    \"\"\"\n    video_frame: Optional[Union[VideoFrame, str]] = get_from_queue(\n        queue=self._frames_buffer,\n        on_successful_read=self._video_consumer.notify_frame_consumed,\n        timeout=timeout,\n        purge=self._buffer_consumption_strategy is BufferConsumptionStrategy.EAGER,\n    )\n    if video_frame == POISON_PILL:\n        raise EndOfStreamError(\n            \"Attempted to retrieve frame from stream that already ended.\"\n        )\n    if video_frame is not None:\n        send_video_source_status_update(\n            severity=UpdateSeverity.DEBUG,\n            event_type=FRAME_CONSUMED_EVENT,\n            payload={\n                \"frame_timestamp\": video_frame.frame_timestamp,\n                \"frame_id\": video_frame.frame_id,\n                \"source_id\": video_frame.source_id,\n            },\n            status_update_handlers=self._status_update_handlers,\n        )\n    return video_frame\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.restart","title":"<code>restart(wait_on_frames_consumption=True, purge_frames_buffer=False)</code>","text":"<p>Method to restart source consumption. Eligible to be used in states: [MUTED, RUNNING, PAUSED, ENDED, ERROR]. End state: * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed * ERROR - if it was not possible to connect with source</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Parameters:</p> Name Type Description Default <code>wait_on_frames_consumption</code> <code>bool</code> <p>Flag telling if all frames from buffer must be consumed before completion of this operation.</p> <code>True</code> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source     * SourceConnectionError: if source cannot be connected</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef restart(\n    self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n) -&gt; None:\n    \"\"\"\n    Method to restart source consumption. Eligible to be used in states:\n    [MUTED, RUNNING, PAUSED, ENDED, ERROR].\n    End state:\n    * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n    * ERROR - if it was not possible to connect with source\n\n    Thread safe - only one transition of states possible at the time.\n\n    Args:\n        wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n            completion of this operation.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        * SourceConnectionError: if source cannot be connected\n    \"\"\"\n    if self._state not in RESTART_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not RESTART stream in state: {self._state}\"\n        )\n    self._restart(\n        wait_on_frames_consumption=wait_on_frames_consumption,\n        purge_frames_buffer=purge_frames_buffer,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.resume","title":"<code>resume()</code>","text":"<p>Method to recover from pause or mute into running state. [PAUSED, MUTED] End state: * RUNNING</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef resume(self) -&gt; None:\n    \"\"\"\n    Method to recover from pause or mute into running state.\n    [PAUSED, MUTED]\n    End state:\n    * RUNNING\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in RESUME_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not RESUME stream in state: {self._state}\"\n        )\n    self._resume()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.start","title":"<code>start()</code>","text":"<p>Method to be used to start source consumption. Eligible to be used in states: [NOT_STARTED, ENDED, (RESTARTING - which is internal state only)] End state: * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed * ERROR - if it was not possible to connect with source</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source     * SourceConnectionError: if source cannot be connected</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef start(self) -&gt; None:\n    \"\"\"\n    Method to be used to start source consumption. Eligible to be used in states:\n    [NOT_STARTED, ENDED, (RESTARTING - which is internal state only)]\n    End state:\n    * INITIALISING - that should change into RUNNING once first frame is ready to be grabbed\n    * ERROR - if it was not possible to connect with source\n\n    Thread safe - only one transition of states possible at the time.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n        * SourceConnectionError: if source cannot be connected\n    \"\"\"\n    if self._state not in START_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not START stream in state: {self._state}\"\n        )\n    self._start()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource.terminate","title":"<code>terminate(wait_on_frames_consumption=True, purge_frames_buffer=False)</code>","text":"<p>Method to be used to terminate source consumption. Eligible to be used in states: [MUTED, RUNNING, PAUSED, ENDED, ERROR, (RESTARTING - which is internal state only)] End state: * ENDED - indicating success of the process * ERROR - if error with processing occurred</p> <p>Must be used to properly dispose resources at the end.</p> <p>Thread safe - only one transition of states possible at the time.</p> <p>Parameters:</p> Name Type Description Default <code>wait_on_frames_consumption</code> <code>bool</code> <p>Flag telling if all frames from buffer must be consumed before completion of this operation.</p> <code>True</code> <p>Throws:     * StreamOperationNotAllowedError: if executed in context of incorrect state of the source</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>@lock_state_transition\ndef terminate(\n    self, wait_on_frames_consumption: bool = True, purge_frames_buffer: bool = False\n) -&gt; None:\n    \"\"\"\n    Method to be used to terminate source consumption. Eligible to be used in states:\n    [MUTED, RUNNING, PAUSED, ENDED, ERROR, (RESTARTING - which is internal state only)]\n    End state:\n    * ENDED - indicating success of the process\n    * ERROR - if error with processing occurred\n\n    Must be used to properly dispose resources at the end.\n\n    Thread safe - only one transition of states possible at the time.\n\n    Args:\n        wait_on_frames_consumption (bool): Flag telling if all frames from buffer must be consumed before\n            completion of this operation.\n\n    Returns: None\n    Throws:\n        * StreamOperationNotAllowedError: if executed in context of incorrect state of the source\n    \"\"\"\n    if self._state not in TERMINATE_ELIGIBLE_STATES:\n        raise StreamOperationNotAllowedError(\n            f\"Could not TERMINATE stream in state: {self._state}\"\n        )\n    self._terminate(\n        wait_on_frames_consumption=wait_on_frames_consumption,\n        purge_frames_buffer=purge_frames_buffer,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.get_from_queue","title":"<code>get_from_queue(queue, timeout=None, on_successful_read=lambda: None, purge=False)</code>","text":"<p>Function is supposed to take element from the queue waiting on the first element to appear using <code>timeout</code> parameter. One may ask to go to the very last element of the queue and return it - then <code>purge</code> should be set to True. No additional wait on new elements to appear happen and the purge stops once queue is free returning last element consumed. queue.task_done() and on_successful_read(...) will be called on each received element.</p> Source code in <code>inference/core/interfaces/camera/video_source.py</code> <pre><code>def get_from_queue(\n    queue: Queue,\n    timeout: Optional[float] = None,\n    on_successful_read: Callable[[], None] = lambda: None,\n    purge: bool = False,\n) -&gt; Optional[Any]:\n    \"\"\"\n    Function is supposed to take element from the queue waiting on the first element to appear using `timeout`\n    parameter. One may ask to go to the very last element of the queue and return it - then `purge` should be set\n    to True. No additional wait on new elements to appear happen and the purge stops once queue is free returning last\n    element consumed.\n    queue.task_done() and on_successful_read(...) will be called on each received element.\n    \"\"\"\n    result = None\n    if queue.empty() or not purge:\n        try:\n            result = queue.get(timeout=timeout)\n            queue.task_done()\n            on_successful_read()\n        except Empty:\n            pass\n    while not queue.empty() and purge:\n        result = queue.get()\n        queue.task_done()\n        on_successful_read()\n    return result\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/http/http_api/","title":"http_api","text":""},{"location":"docs/reference/inference/core/interfaces/http/http_api/#inference.core.interfaces.http.http_api.HttpInterface","title":"<code>HttpInterface</code>","text":"<p>               Bases: <code>BaseInterface</code></p> <p>Roboflow defined HTTP interface for a general-purpose inference server.</p> <p>This class sets up the FastAPI application and adds necessary middleware, as well as initializes the model manager and model registry for the inference server.</p> <p>Attributes:</p> Name Type Description <code>app</code> <code>FastAPI</code> <p>The FastAPI application instance.</p> <code>model_manager</code> <code>ModelManager</code> <p>The manager for handling different models.</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>class HttpInterface(BaseInterface):\n    \"\"\"Roboflow defined HTTP interface for a general-purpose inference server.\n\n    This class sets up the FastAPI application and adds necessary middleware,\n    as well as initializes the model manager and model registry for the inference server.\n\n    Attributes:\n        app (FastAPI): The FastAPI application instance.\n        model_manager (ModelManager): The manager for handling different models.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_manager: ModelManager,\n        root_path: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the HttpInterface with given model manager and model registry.\n\n        Args:\n            model_manager (ModelManager): The manager for handling different models.\n            root_path (Optional[str]): The root path for the FastAPI application.\n\n        Description:\n            Deploy Roboflow trained models to nearly any compute environment!\n        \"\"\"\n        description = \"Roboflow inference server\"\n        app = FastAPI(\n            title=\"Roboflow Inference Server\",\n            description=description,\n            version=__version__,\n            terms_of_service=\"https://roboflow.com/terms\",\n            contact={\n                \"name\": \"Roboflow Inc.\",\n                \"url\": \"https://roboflow.com/contact\",\n                \"email\": \"help@roboflow.com\",\n            },\n            license_info={\n                \"name\": \"Apache 2.0\",\n                \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n            },\n            root_path=root_path,\n        )\n\n        if ENABLE_PROMETHEUS:\n            InferenceInstrumentator(\n                app, model_manager=model_manager, endpoint=\"/metrics\"\n            )\n\n        if METLO_KEY:\n            app.add_middleware(\n                ASGIMiddleware, host=\"https://app.metlo.com\", api_key=METLO_KEY\n            )\n        if LAMBDA:\n            app.add_middleware(LambdaMiddleware)\n\n        if len(ALLOW_ORIGINS) &gt; 0:\n            app.add_middleware(\n                CORSMiddleware,\n                allow_origins=ALLOW_ORIGINS,\n                allow_credentials=True,\n                allow_methods=[\"*\"],\n                allow_headers=[\"*\"],\n            )\n\n        # Optionally add middleware for profiling the FastAPI server and underlying inference API code\n        if PROFILE:\n            app.add_middleware(\n                CProfileMiddleware,\n                enable=True,\n                server_app=app,\n                filename=\"/profile/output.pstats\",\n                strip_dirs=False,\n                sort_by=\"cumulative\",\n            )\n        app.add_middleware(asgi_correlation_id.CorrelationIdMiddleware)\n\n        if METRICS_ENABLED:\n\n            @app.middleware(\"http\")\n            async def count_errors(request: Request, call_next):\n                \"\"\"Middleware to count errors.\n\n                Args:\n                    request (Request): The incoming request.\n                    call_next (Callable): The next middleware or endpoint to call.\n\n                Returns:\n                    Response: The response from the next middleware or endpoint.\n                \"\"\"\n                response = await call_next(request)\n                if self.model_manager.pingback and response.status_code &gt;= 400:\n                    self.model_manager.num_errors += 1\n                return response\n\n        if not LAMBDA:\n\n            @app.get(\"/device/stats\")\n            async def device_stats():\n                not_configured_error_message = {\n                    \"error\": \"Device statistics endpoint is not enabled.\",\n                    \"hint\": \"Mount the Docker socket and point its location when running the docker \"\n                    \"container to collect device stats \"\n                    \"(i.e. `docker run ... -v /var/run/docker.sock:/var/run/docker.sock \"\n                    \"-e DOCKER_SOCKET_PATH=/var/run/docker.sock ...`).\",\n                }\n                if not DOCKER_SOCKET_PATH:\n                    return JSONResponse(\n                        status_code=404,\n                        content=not_configured_error_message,\n                    )\n                if not is_docker_socket_mounted(docker_socket_path=DOCKER_SOCKET_PATH):\n                    return JSONResponse(\n                        status_code=500,\n                        content=not_configured_error_message,\n                    )\n                container_stats = get_container_stats(\n                    docker_socket_path=DOCKER_SOCKET_PATH\n                )\n                return JSONResponse(status_code=200, content=container_stats)\n\n        if DEDICATED_DEPLOYMENT_WORKSPACE_URL:\n            cached_api_keys = dict()\n            cached_projects = dict()\n\n            @app.middleware(\"http\")\n            async def check_authorization(request: Request, call_next):\n                # exclusions\n                skip_check = (\n                    request.method not in [\"GET\", \"POST\"]\n                    or request.url.path\n                    in [\n                        \"/\",\n                        \"/info\",\n                        \"/workflows/blocks/describe\",\n                        \"/workflows/definition/schema\",\n                    ]\n                    or request.url.path.startswith(\"/static/\")\n                    or request.url.path.startswith(\"/_next/\")\n                )\n                if skip_check:\n                    return await call_next(request)\n\n                def _unauthorized_response(msg):\n                    return JSONResponse(\n                        status_code=401,\n                        content={\n                            \"status\": 401,\n                            \"message\": msg,\n                        },\n                    )\n\n                # check api_key\n                req_params = request.query_params\n                json_params = dict()\n                if (\n                    request.headers.get(\"content-type\", None) == \"application/json\"\n                    and int(request.headers.get(\"content-length\", 0)) &gt; 0\n                ):\n                    json_params = await request.json()\n                api_key = req_params.get(\"api_key\", None) or json_params.get(\n                    \"api_key\", None\n                )\n\n                if cached_api_keys.get(api_key, 0) &lt; time.time():\n                    try:\n                        workspace_url = (\n                            get_roboflow_workspace(api_key)\n                            if api_key is not None\n                            else None\n                        )\n\n                        if workspace_url != DEDICATED_DEPLOYMENT_WORKSPACE_URL:\n                            return _unauthorized_response(\"Unauthorized api_key\")\n\n                        cached_api_keys[api_key] = (\n                            time.time() + 3600\n                        )  # expired after 1 hour\n                    except RoboflowAPINotAuthorizedError as e:\n                        return _unauthorized_response(\"Unauthorized api_key\")\n\n                # check project_url\n                model_id = json_params.get(\"model_id\", \"\")\n                project_url = (\n                    req_params.get(\"project\", None)\n                    or json_params.get(\"project\", None)\n                    or model_id.split(\"/\")[0]\n                )\n                # only check when project_url is not None\n                if (\n                    project_url is not None\n                    and cached_projects.get(project_url, 0) &lt; time.time()\n                ):\n                    try:\n                        _ = get_roboflow_dataset_type(\n                            api_key, DEDICATED_DEPLOYMENT_WORKSPACE_URL, project_url\n                        )\n\n                        cached_projects[project_url] = (\n                            time.time() + 3600\n                        )  # expired after 1 hour\n                    except RoboflowAPINotNotFoundError as e:\n                        return _unauthorized_response(\"Unauthorized project\")\n\n                return await call_next(request)\n\n        self.app = app\n        self.model_manager = model_manager\n        self.stream_manager_client: Optional[StreamManagerClient] = None\n\n        if ENABLE_STREAM_API:\n            operations_timeout = os.getenv(\"STREAM_MANAGER_OPERATIONS_TIMEOUT\")\n            if operations_timeout is not None:\n                operations_timeout = float(operations_timeout)\n            self.stream_manager_client = StreamManagerClient.init(\n                host=os.getenv(\"STREAM_MANAGER_HOST\", \"127.0.0.1\"),\n                port=int(os.getenv(\"STREAM_MANAGER_PORT\", \"7070\")),\n                operations_timeout=operations_timeout,\n            )\n\n        async def process_inference_request(\n            inference_request: InferenceRequest, **kwargs\n        ) -&gt; InferenceResponse:\n            \"\"\"Processes an inference request by calling the appropriate model.\n\n            Args:\n                inference_request (InferenceRequest): The request containing model ID and other inference details.\n\n            Returns:\n                InferenceResponse: The response containing the inference results.\n            \"\"\"\n            de_aliased_model_id = resolve_roboflow_model_alias(\n                model_id=inference_request.model_id\n            )\n            self.model_manager.add_model(de_aliased_model_id, inference_request.api_key)\n            resp = await self.model_manager.infer_from_request(\n                de_aliased_model_id, inference_request, **kwargs\n            )\n            return orjson_response(resp)\n\n        def process_workflow_inference_request(\n            workflow_request: WorkflowInferenceRequest,\n            workflow_specification: dict,\n            background_tasks: Optional[BackgroundTasks],\n            profiler: WorkflowsProfiler,\n        ) -&gt; WorkflowInferenceResponse:\n            workflow_init_parameters = {\n                \"workflows_core.model_manager\": model_manager,\n                \"workflows_core.api_key\": workflow_request.api_key,\n                \"workflows_core.background_tasks\": background_tasks,\n            }\n            execution_engine = ExecutionEngine.init(\n                workflow_definition=workflow_specification,\n                init_parameters=workflow_init_parameters,\n                max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n                prevent_local_images_loading=True,\n                profiler=profiler,\n            )\n            result = execution_engine.run(runtime_parameters=workflow_request.inputs)\n            with profiler.profile_execution_phase(\n                name=\"workflow_results_serialisation\",\n                categories=[\"inference_package_operation\"],\n            ):\n                outputs = serialise_workflow_result(\n                    result=result,\n                    excluded_fields=workflow_request.excluded_fields,\n                )\n            profiler_trace = profiler.export_trace()\n            response = WorkflowInferenceResponse(\n                outputs=outputs,\n                profiler_trace=profiler_trace,\n            )\n            return orjson_response(response=response)\n\n        def load_core_model(\n            inference_request: InferenceRequest,\n            api_key: Optional[str] = None,\n            core_model: str = None,\n        ) -&gt; None:\n            \"\"\"Loads a core model (e.g., \"clip\" or \"sam\") into the model manager.\n\n            Args:\n                inference_request (InferenceRequest): The request containing version and other details.\n                api_key (Optional[str]): The API key for the request.\n                core_model (str): The core model type, e.g., \"clip\" or \"sam\".\n\n            Returns:\n                str: The core model ID.\n            \"\"\"\n            if api_key:\n                inference_request.api_key = api_key\n            version_id_field = f\"{core_model}_version_id\"\n            core_model_id = (\n                f\"{core_model}/{inference_request.__getattribute__(version_id_field)}\"\n            )\n            self.model_manager.add_model(core_model_id, inference_request.api_key)\n            return core_model_id\n\n        load_clip_model = partial(load_core_model, core_model=\"clip\")\n        \"\"\"Loads the CLIP model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The CLIP model ID.\n        \"\"\"\n\n        load_sam_model = partial(load_core_model, core_model=\"sam\")\n        \"\"\"Loads the SAM model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The SAM model ID.\n        \"\"\"\n        load_sam2_model = partial(load_core_model, core_model=\"sam2\")\n        \"\"\"Loads the SAM2 model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The SAM2 model ID.\n        \"\"\"\n\n        load_gaze_model = partial(load_core_model, core_model=\"gaze\")\n        \"\"\"Loads the GAZE model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The GAZE model ID.\n        \"\"\"\n\n        load_doctr_model = partial(load_core_model, core_model=\"doctr\")\n        \"\"\"Loads the DocTR model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The DocTR model ID.\n        \"\"\"\n        load_cogvlm_model = partial(load_core_model, core_model=\"cogvlm\")\n        load_paligemma_model = partial(load_core_model, core_model=\"paligemma\")\n\n        load_grounding_dino_model = partial(\n            load_core_model, core_model=\"grounding_dino\"\n        )\n        \"\"\"Loads the Grounding DINO model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The Grounding DINO model ID.\n        \"\"\"\n\n        load_yolo_world_model = partial(load_core_model, core_model=\"yolo_world\")\n        load_owlv2_model = partial(load_core_model, core_model=\"owlv2\")\n        \"\"\"Loads the YOLO World model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The YOLO World model ID.\n        \"\"\"\n\n        load_trocr_model = partial(load_core_model, core_model=\"trocr\")\n        \"\"\"Loads the TrOCR model into the model manager.\n\n        Args:\n        inference_request: The request containing version and other details.\n        api_key: The API key for the request.\n\n        Returns:\n        The TrOCR model ID.\n        \"\"\"\n\n        @app.get(\n            \"/info\",\n            response_model=ServerVersionInfo,\n            summary=\"Info\",\n            description=\"Get the server name and version number\",\n        )\n        async def root():\n            \"\"\"Endpoint to get the server name and version number.\n\n            Returns:\n                ServerVersionInfo: The server version information.\n            \"\"\"\n            return ServerVersionInfo(\n                name=\"Roboflow Inference Server\",\n                version=__version__,\n                uuid=GLOBAL_INFERENCE_SERVER_ID,\n            )\n\n        # The current AWS Lambda authorizer only supports path parameters, therefore we can only use the legacy infer route. This case statement excludes routes which won't work for the current Lambda authorizer.\n        if not LAMBDA:\n\n            @app.get(\n                \"/model/registry\",\n                response_model=ModelsDescriptions,\n                summary=\"Get model keys\",\n                description=\"Get the ID of each loaded model\",\n            )\n            async def registry():\n                \"\"\"Get the ID of each loaded model in the registry.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/registry\")\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n            @app.post(\n                \"/model/add\",\n                response_model=ModelsDescriptions,\n                summary=\"Load a model\",\n                description=\"Load the model with the given model ID\",\n            )\n            @with_route_exceptions\n            async def model_add(request: AddModelRequest):\n                \"\"\"Load the model with the given model ID into the model manager.\n\n                Args:\n                    request (AddModelRequest): The request containing the model ID and optional API key.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/add\")\n                de_aliased_model_id = resolve_roboflow_model_alias(\n                    model_id=request.model_id\n                )\n                self.model_manager.add_model(de_aliased_model_id, request.api_key)\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n            @app.post(\n                \"/model/remove\",\n                response_model=ModelsDescriptions,\n                summary=\"Remove a model\",\n                description=\"Remove the model with the given model ID\",\n            )\n            @with_route_exceptions\n            async def model_remove(request: ClearModelRequest):\n                \"\"\"Remove the model with the given model ID from the model manager.\n\n                Args:\n                    request (ClearModelRequest): The request containing the model ID to be removed.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/remove\")\n                de_aliased_model_id = resolve_roboflow_model_alias(\n                    model_id=request.model_id\n                )\n                self.model_manager.remove(de_aliased_model_id)\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n            @app.post(\n                \"/model/clear\",\n                response_model=ModelsDescriptions,\n                summary=\"Remove all models\",\n                description=\"Remove all loaded models\",\n            )\n            @with_route_exceptions\n            async def model_clear():\n                \"\"\"Remove all loaded models from the model manager.\n\n                Returns:\n                    ModelsDescriptions: The object containing models descriptions\n                \"\"\"\n                logger.debug(f\"Reached /model/clear\")\n                self.model_manager.clear()\n                models_descriptions = self.model_manager.describe_models()\n                return ModelsDescriptions.from_models_descriptions(\n                    models_descriptions=models_descriptions\n                )\n\n            @app.post(\n                \"/infer/object_detection\",\n                response_model=Union[\n                    ObjectDetectionInferenceResponse,\n                    List[ObjectDetectionInferenceResponse],\n                    StubResponse,\n                ],\n                summary=\"Object detection infer\",\n                description=\"Run inference with the specified object detection model\",\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            async def infer_object_detection(\n                inference_request: ObjectDetectionInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ):\n                \"\"\"Run inference with the specified object detection model.\n\n                Args:\n                    inference_request (ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    Union[ObjectDetectionInferenceResponse, List[ObjectDetectionInferenceResponse]]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/object_detection\")\n                return await process_inference_request(\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                )\n\n            @app.post(\n                \"/infer/instance_segmentation\",\n                response_model=Union[\n                    InstanceSegmentationInferenceResponse, StubResponse\n                ],\n                summary=\"Instance segmentation infer\",\n                description=\"Run inference with the specified instance segmentation model\",\n            )\n            @with_route_exceptions\n            async def infer_instance_segmentation(\n                inference_request: InstanceSegmentationInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ):\n                \"\"\"Run inference with the specified instance segmentation model.\n\n                Args:\n                    inference_request (InstanceSegmentationInferenceRequest): The request containing the necessary details for instance segmentation.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    InstanceSegmentationInferenceResponse: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/instance_segmentation\")\n                return await process_inference_request(\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                )\n\n            @app.post(\n                \"/infer/classification\",\n                response_model=Union[\n                    ClassificationInferenceResponse,\n                    MultiLabelClassificationInferenceResponse,\n                    StubResponse,\n                ],\n                summary=\"Classification infer\",\n                description=\"Run inference with the specified classification model\",\n            )\n            @with_route_exceptions\n            async def infer_classification(\n                inference_request: ClassificationInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ):\n                \"\"\"Run inference with the specified classification model.\n\n                Args:\n                    inference_request (ClassificationInferenceRequest): The request containing the necessary details for classification.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/classification\")\n                return await process_inference_request(\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                )\n\n            @app.post(\n                \"/infer/keypoints_detection\",\n                response_model=Union[KeypointsDetectionInferenceResponse, StubResponse],\n                summary=\"Keypoints detection infer\",\n                description=\"Run inference with the specified keypoints detection model\",\n            )\n            @with_route_exceptions\n            async def infer_keypoints(\n                inference_request: KeypointsDetectionInferenceRequest,\n            ):\n                \"\"\"Run inference with the specified keypoints detection model.\n\n                Args:\n                    inference_request (KeypointsDetectionInferenceRequest): The request containing the necessary details for keypoints detection.\n\n                Returns:\n                    Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/keypoints_detection\")\n                return await process_inference_request(inference_request)\n\n            if LMM_ENABLED:\n\n                @app.post(\n                    \"/infer/lmm\",\n                    response_model=Union[\n                        LMMInferenceResponse,\n                        List[LMMInferenceResponse],\n                        StubResponse,\n                    ],\n                    summary=\"Large multi-modal model infer\",\n                    description=\"Run inference with the specified large multi-modal model\",\n                    response_model_exclude_none=True,\n                )\n                @with_route_exceptions\n                async def infer_lmm(\n                    inference_request: LMMInferenceRequest,\n                ):\n                    \"\"\"Run inference with the specified object detection model.\n\n                    Args:\n                        inference_request (ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n                        background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                    Returns:\n                        Union[ObjectDetectionInferenceResponse, List[ObjectDetectionInferenceResponse]]: The response containing the inference results.\n                    \"\"\"\n                    logger.debug(f\"Reached /infer/lmm\")\n                    return await process_inference_request(inference_request)\n\n        if not DISABLE_WORKFLOW_ENDPOINTS:\n\n            @app.post(\n                \"/{workspace_name}/workflows/{workflow_id}/describe_interface\",\n                response_model=DescribeInterfaceResponse,\n                summary=\"Endpoint to describe interface of predefined workflow\",\n                description=\"Checks Roboflow API for workflow definition, once acquired - describes workflow inputs and outputs\",\n            )\n            @with_route_exceptions\n            async def describe_predefined_workflow_interface(\n                workspace_name: str,\n                workflow_id: str,\n                workflow_request: PredefinedWorkflowDescribeInterfaceRequest,\n            ) -&gt; DescribeInterfaceResponse:\n                workflow_specification = get_workflow_specification(\n                    api_key=workflow_request.api_key,\n                    workspace_id=workspace_name,\n                    workflow_id=workflow_id,\n                    use_cache=workflow_request.use_cache,\n                )\n                return handle_describe_workflows_interface(\n                    definition=workflow_specification,\n                )\n\n            @app.post(\n                \"/workflows/describe_interface\",\n                response_model=DescribeInterfaceResponse,\n                summary=\"Endpoint to describe interface of workflow given in request\",\n                description=\"Parses workflow definition and retrieves describes inputs and outputs\",\n            )\n            @with_route_exceptions\n            async def describe_workflow_interface(\n                workflow_request: WorkflowSpecificationDescribeInterfaceRequest,\n            ) -&gt; DescribeInterfaceResponse:\n                return handle_describe_workflows_interface(\n                    definition=workflow_request.specification,\n                )\n\n            @app.post(\n                \"/{workspace_name}/workflows/{workflow_id}\",\n                response_model=WorkflowInferenceResponse,\n                summary=\"Endpoint to run predefined workflow\",\n                description=\"Checks Roboflow API for workflow definition, once acquired - parses and executes injecting runtime parameters from request body\",\n            )\n            @app.post(\n                \"/infer/workflows/{workspace_name}/{workflow_id}\",\n                response_model=WorkflowInferenceResponse,\n                summary=\"[LEGACY] Endpoint to run predefined workflow\",\n                description=\"Checks Roboflow API for workflow definition, once acquired - parses and executes injecting runtime parameters from request body. This endpoint is deprecated and will be removed end of Q2 2024\",\n                deprecated=True,\n            )\n            @with_route_exceptions\n            async def infer_from_predefined_workflow(\n                workspace_name: str,\n                workflow_id: str,\n                workflow_request: PredefinedWorkflowInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ) -&gt; WorkflowInferenceResponse:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                if ENABLE_WORKFLOWS_PROFILING and workflow_request.enable_profiling:\n                    profiler = BaseWorkflowsProfiler.init(\n                        max_runs_in_buffer=WORKFLOWS_PROFILER_BUFFER_SIZE,\n                    )\n                else:\n                    profiler = NullWorkflowsProfiler.init()\n                with profiler.profile_execution_phase(\n                    name=\"workflow_definition_fetching\",\n                    categories=[\"inference_package_operation\"],\n                ):\n                    workflow_specification = get_workflow_specification(\n                        api_key=workflow_request.api_key,\n                        workspace_id=workspace_name,\n                        workflow_id=workflow_id,\n                        use_cache=workflow_request.use_cache,\n                    )\n                return process_workflow_inference_request(\n                    workflow_request=workflow_request,\n                    workflow_specification=workflow_specification,\n                    background_tasks=background_tasks if not LAMBDA else None,\n                    profiler=profiler,\n                )\n\n            @app.post(\n                \"/workflows/run\",\n                response_model=WorkflowInferenceResponse,\n                summary=\"Endpoint to run workflow specification provided in payload\",\n                description=\"Parses and executes workflow specification, injecting runtime parameters from request body.\",\n            )\n            @app.post(\n                \"/infer/workflows\",\n                response_model=WorkflowInferenceResponse,\n                summary=\"[LEGACY] Endpoint to run workflow specification provided in payload\",\n                description=\"Parses and executes workflow specification, injecting runtime parameters from request body. This endpoint is deprecated and will be removed end of Q2 2024.\",\n                deprecated=True,\n            )\n            @with_route_exceptions\n            async def infer_from_workflow(\n                workflow_request: WorkflowSpecificationInferenceRequest,\n                background_tasks: BackgroundTasks,\n            ) -&gt; WorkflowInferenceResponse:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                if ENABLE_WORKFLOWS_PROFILING and workflow_request.enable_profiling:\n                    profiler = BaseWorkflowsProfiler.init(\n                        max_runs_in_buffer=WORKFLOWS_PROFILER_BUFFER_SIZE,\n                    )\n                else:\n                    profiler = NullWorkflowsProfiler.init()\n                return process_workflow_inference_request(\n                    workflow_request=workflow_request,\n                    workflow_specification=workflow_request.specification,\n                    background_tasks=background_tasks if not LAMBDA else None,\n                    profiler=profiler,\n                )\n\n            @app.get(\n                \"/workflows/execution_engine/versions\",\n                response_model=ExecutionEngineVersions,\n                summary=\"Returns available Execution Engine versions sorted from oldest to newest\",\n                description=\"Returns available Execution Engine versions sorted from oldest to newest\",\n            )\n            @with_route_exceptions\n            async def get_execution_engine_versions() -&gt; ExecutionEngineVersions:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                versions = get_available_versions()\n                return ExecutionEngineVersions(versions=versions)\n\n            @app.get(\n                \"/workflows/blocks/describe\",\n                response_model=WorkflowsBlocksDescription,\n                summary=\"[LEGACY] Endpoint to get definition of workflows blocks that are accessible\",\n                description=\"Endpoint provides detailed information about workflows building blocks that are \"\n                \"accessible in the inference server. This information could be used to programmatically \"\n                \"build / display workflows.\",\n                deprecated=True,\n            )\n            @with_route_exceptions\n            async def describe_workflows_blocks(\n                request: Request,\n            ) -&gt; Union[WorkflowsBlocksDescription, Response]:\n                result = handle_describe_workflows_blocks_request()\n                return gzip_response_if_requested(request=request, response=result)\n\n            @app.post(\n                \"/workflows/blocks/describe\",\n                response_model=WorkflowsBlocksDescription,\n                summary=\"[EXPERIMENTAL] Endpoint to get definition of workflows blocks that are accessible\",\n                description=\"Endpoint provides detailed information about workflows building blocks that are \"\n                \"accessible in the inference server. This information could be used to programmatically \"\n                \"build / display workflows. Additionally - in request body one can specify list of \"\n                \"dynamic blocks definitions which will be transformed into blocks and used to generate \"\n                \"schemas and definitions of connections\",\n            )\n            @with_route_exceptions\n            async def describe_workflows_blocks(\n                request: Request,\n                request_payload: Optional[DescribeBlocksRequest] = None,\n            ) -&gt; Union[WorkflowsBlocksDescription, Response]:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                dynamic_blocks_definitions = None\n                requested_execution_engine_version = None\n                if request_payload is not None:\n                    dynamic_blocks_definitions = (\n                        request_payload.dynamic_blocks_definitions\n                    )\n                    requested_execution_engine_version = (\n                        request_payload.execution_engine_version\n                    )\n                result = handle_describe_workflows_blocks_request(\n                    dynamic_blocks_definitions=dynamic_blocks_definitions,\n                    requested_execution_engine_version=requested_execution_engine_version,\n                )\n                return gzip_response_if_requested(request=request, response=result)\n\n            @app.get(\n                \"/workflows/definition/schema\",\n                response_model=WorkflowsBlocksSchemaDescription,\n                summary=\"Endpoint to fetch the workflows block schema\",\n                description=\"Endpoint to fetch the schema of all available blocks. This information can be \"\n                \"used to validate workflow definitions and suggest syntax in the JSON editor.\",\n            )\n            @with_route_exceptions\n            async def get_workflow_schema() -&gt; WorkflowsBlocksSchemaDescription:\n                return get_workflow_schema_description()\n\n            @app.post(\n                \"/workflows/blocks/dynamic_outputs\",\n                response_model=List[OutputDefinition],\n                summary=\"[EXPERIMENTAL] Endpoint to get definition of dynamic output for workflow step\",\n                description=\"Endpoint to be used when step outputs can be discovered only after \"\n                \"filling manifest with data.\",\n            )\n            @with_route_exceptions\n            async def get_dynamic_block_outputs(\n                step_manifest: Dict[str, Any]\n            ) -&gt; List[OutputDefinition]:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                # Potentially TODO: dynamic blocks do not support dynamic outputs, but if it changes\n                # we need to provide dynamic blocks manifests here\n                dummy_workflow_definition = {\n                    \"version\": \"1.0\",\n                    \"inputs\": [],\n                    \"steps\": [step_manifest],\n                    \"outputs\": [],\n                }\n                available_blocks = load_workflow_blocks()\n                parsed_definition = parse_workflow_definition(\n                    raw_workflow_definition=dummy_workflow_definition,\n                    available_blocks=available_blocks,\n                )\n                parsed_manifest = parsed_definition.steps[0]\n                return parsed_manifest.get_actual_outputs()\n\n            @app.post(\n                \"/workflows/validate\",\n                response_model=WorkflowValidationStatus,\n                summary=\"[EXPERIMENTAL] Endpoint to validate\",\n                description=\"Endpoint provides a way to check validity of JSON workflow definition.\",\n            )\n            @with_route_exceptions\n            async def validate_workflow(\n                specification: dict,\n            ) -&gt; WorkflowValidationStatus:\n                # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n                step_execution_mode = StepExecutionMode(WORKFLOWS_STEP_EXECUTION_MODE)\n                workflow_init_parameters = {\n                    \"workflows_core.model_manager\": model_manager,\n                    \"workflows_core.api_key\": None,\n                    \"workflows_core.background_tasks\": None,\n                    \"workflows_core.step_execution_mode\": step_execution_mode,\n                }\n                _ = ExecutionEngine.init(\n                    workflow_definition=specification,\n                    init_parameters=workflow_init_parameters,\n                    max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n                    prevent_local_images_loading=True,\n                )\n                return WorkflowValidationStatus(status=\"ok\")\n\n        if ENABLE_STREAM_API:\n\n            @app.get(\n                \"/inference_pipelines/list\",\n                response_model=ListPipelinesResponse,\n                summary=\"[EXPERIMENTAL] List active InferencePipelines\",\n                description=\"[EXPERIMENTAL] Listing all active InferencePipelines processing videos\",\n            )\n            @with_route_exceptions\n            async def list_pipelines(_: Request) -&gt; ListPipelinesResponse:\n                return await self.stream_manager_client.list_pipelines()\n\n            @app.get(\n                \"/inference_pipelines/{pipeline_id}/status\",\n                response_model=InferencePipelineStatusResponse,\n                summary=\"[EXPERIMENTAL] Get status of InferencePipeline\",\n                description=\"[EXPERIMENTAL] Get status of InferencePipeline\",\n            )\n            @with_route_exceptions\n            async def get_status(pipeline_id: str) -&gt; InferencePipelineStatusResponse:\n                return await self.stream_manager_client.get_status(\n                    pipeline_id=pipeline_id\n                )\n\n            @app.post(\n                \"/inference_pipelines/initialise\",\n                response_model=CommandResponse,\n                summary=\"[EXPERIMENTAL] Starts new InferencePipeline\",\n                description=\"[EXPERIMENTAL] Starts new InferencePipeline\",\n            )\n            @with_route_exceptions\n            async def initialise(request: InitialisePipelinePayload) -&gt; CommandResponse:\n                return await self.stream_manager_client.initialise_pipeline(\n                    initialisation_request=request\n                )\n\n            @app.post(\n                \"/inference_pipelines/initialise_webrtc\",\n                response_model=InitializeWebRTCPipelineResponse,\n                summary=\"[EXPERIMENTAL] Establishes WebRTC peer connection and starts new InferencePipeline consuming video track\",\n                description=\"[EXPERIMENTAL] Establishes WebRTC peer connection and starts new InferencePipeline consuming video track\",\n            )\n            @with_route_exceptions\n            async def initialise_webrtc_inference_pipeline(\n                request: InitialiseWebRTCPipelinePayload,\n            ) -&gt; CommandResponse:\n                resp = await self.stream_manager_client.initialise_webrtc_pipeline(\n                    initialisation_request=request\n                )\n                return resp\n\n            @app.post(\n                \"/inference_pipelines/{pipeline_id}/pause\",\n                response_model=CommandResponse,\n                summary=\"[EXPERIMENTAL] Pauses the InferencePipeline\",\n                description=\"[EXPERIMENTAL] Pauses the InferencePipeline\",\n            )\n            @with_route_exceptions\n            async def pause(pipeline_id: str) -&gt; CommandResponse:\n                return await self.stream_manager_client.pause_pipeline(\n                    pipeline_id=pipeline_id\n                )\n\n            @app.post(\n                \"/inference_pipelines/{pipeline_id}/resume\",\n                response_model=CommandResponse,\n                summary=\"[EXPERIMENTAL] Resumes the InferencePipeline\",\n                description=\"[EXPERIMENTAL] Resumes the InferencePipeline\",\n            )\n            @with_route_exceptions\n            async def resume(pipeline_id: str) -&gt; CommandResponse:\n                return await self.stream_manager_client.resume_pipeline(\n                    pipeline_id=pipeline_id\n                )\n\n            @app.post(\n                \"/inference_pipelines/{pipeline_id}/terminate\",\n                response_model=CommandResponse,\n                summary=\"[EXPERIMENTAL] Terminates the InferencePipeline\",\n                description=\"[EXPERIMENTAL] Terminates the InferencePipeline\",\n            )\n            @with_route_exceptions\n            async def terminate(pipeline_id: str) -&gt; CommandResponse:\n                return await self.stream_manager_client.terminate_pipeline(\n                    pipeline_id=pipeline_id\n                )\n\n            @app.get(\n                \"/inference_pipelines/{pipeline_id}/consume\",\n                response_model=ConsumePipelineResponse,\n                summary=\"[EXPERIMENTAL] Consumes InferencePipeline result\",\n                description=\"[EXPERIMENTAL] Consumes InferencePipeline result\",\n            )\n            @with_route_exceptions\n            async def consume(\n                pipeline_id: str,\n                request: Optional[ConsumeResultsPayload] = None,\n            ) -&gt; ConsumePipelineResponse:\n                if request is None:\n                    request = ConsumeResultsPayload()\n                return await self.stream_manager_client.consume_pipeline_result(\n                    pipeline_id=pipeline_id,\n                    excluded_fields=request.excluded_fields,\n                )\n\n        if CORE_MODELS_ENABLED:\n            if CORE_MODEL_CLIP_ENABLED:\n\n                @app.post(\n                    \"/clip/embed_image\",\n                    response_model=ClipEmbeddingResponse,\n                    summary=\"CLIP Image Embeddings\",\n                    description=\"Run the Open AI CLIP model to embed image data.\",\n                )\n                @with_route_exceptions\n                async def clip_embed_image(\n                    inference_request: ClipImageEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Embeds image data using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (ClipImageEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ClipEmbeddingResponse: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /clip/embed_image\")\n                    clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                    response = await self.model_manager.infer_from_request(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor)\n                    return response\n\n                @app.post(\n                    \"/clip/embed_text\",\n                    response_model=ClipEmbeddingResponse,\n                    summary=\"CLIP Text Embeddings\",\n                    description=\"Run the Open AI CLIP model to embed text data.\",\n                )\n                @with_route_exceptions\n                async def clip_embed_text(\n                    inference_request: ClipTextEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Embeds text data using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (ClipTextEmbeddingRequest): The request containing the text to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ClipEmbeddingResponse: The response containing the embedded text.\n                    \"\"\"\n                    logger.debug(f\"Reached /clip/embed_text\")\n                    clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                    response = await self.model_manager.infer_from_request(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor)\n                    return response\n\n                @app.post(\n                    \"/clip/compare\",\n                    response_model=ClipCompareResponse,\n                    summary=\"CLIP Compare\",\n                    description=\"Run the Open AI CLIP model to compute similarity scores.\",\n                )\n                @with_route_exceptions\n                async def clip_compare(\n                    inference_request: ClipCompareRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Computes similarity scores using the OpenAI CLIP model.\n\n                    Args:\n                        inference_request (ClipCompareRequest): The request containing the data to be compared.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ClipCompareResponse: The response containing the similarity scores.\n                    \"\"\"\n                    logger.debug(f\"Reached /clip/compare\")\n                    clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                    response = await self.model_manager.infer_from_request(\n                        clip_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(clip_model_id, actor, n=2)\n                    return response\n\n            if CORE_MODEL_GROUNDINGDINO_ENABLED:\n\n                @app.post(\n                    \"/grounding_dino/infer\",\n                    response_model=ObjectDetectionInferenceResponse,\n                    summary=\"Grounding DINO inference.\",\n                    description=\"Run the Grounding DINO zero-shot object detection model.\",\n                )\n                @with_route_exceptions\n                async def grounding_dino_infer(\n                    inference_request: GroundingDINOInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Embeds image data using the Grounding DINO model.\n\n                    Args:\n                        inference_request GroundingDINOInferenceRequest): The request containing the image on which to run object detection.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ObjectDetectionInferenceResponse: The object detection response.\n                    \"\"\"\n                    logger.debug(f\"Reached /grounding_dino/infer\")\n                    grounding_dino_model_id = load_grounding_dino_model(\n                        inference_request, api_key=api_key\n                    )\n                    response = await self.model_manager.infer_from_request(\n                        grounding_dino_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(grounding_dino_model_id, actor)\n                    return response\n\n            if CORE_MODEL_YOLO_WORLD_ENABLED:\n\n                @app.post(\n                    \"/yolo_world/infer\",\n                    response_model=ObjectDetectionInferenceResponse,\n                    summary=\"YOLO-World inference.\",\n                    description=\"Run the YOLO-World zero-shot object detection model.\",\n                    response_model_exclude_none=True,\n                )\n                @with_route_exceptions\n                async def yolo_world_infer(\n                    inference_request: YOLOWorldInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Runs the YOLO-World zero-shot object detection model.\n\n                    Args:\n                        inference_request (YOLOWorldInferenceRequest): The request containing the image on which to run object detection.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        ObjectDetectionInferenceResponse: The object detection response.\n                    \"\"\"\n                    logger.debug(f\"Reached /yolo_world/infer. Loading model\")\n                    yolo_world_model_id = load_yolo_world_model(\n                        inference_request, api_key=api_key\n                    )\n                    logger.debug(\"YOLOWorld model loaded. Staring the inference.\")\n                    response = await self.model_manager.infer_from_request(\n                        yolo_world_model_id, inference_request\n                    )\n                    logger.debug(\"YOLOWorld prediction available.\")\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(yolo_world_model_id, actor)\n                        logger.debug(\"Usage of YOLOWorld denoted.\")\n                    return response\n\n            if CORE_MODEL_DOCTR_ENABLED:\n\n                @app.post(\n                    \"/doctr/ocr\",\n                    response_model=OCRInferenceResponse,\n                    summary=\"DocTR OCR response\",\n                    description=\"Run the DocTR OCR model to retrieve text in an image.\",\n                )\n                @with_route_exceptions\n                async def doctr_retrieve_text(\n                    inference_request: DoctrOCRInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Embeds image data using the DocTR model.\n\n                    Args:\n                        inference_request (M.DoctrOCRInferenceRequest): The request containing the image from which to retrieve text.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.OCRInferenceResponse: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /doctr/ocr\")\n                    doctr_model_id = load_doctr_model(\n                        inference_request, api_key=api_key\n                    )\n                    response = await self.model_manager.infer_from_request(\n                        doctr_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(doctr_model_id, actor)\n                    return response\n\n            if CORE_MODEL_SAM_ENABLED:\n\n                @app.post(\n                    \"/sam/embed_image\",\n                    response_model=SamEmbeddingResponse,\n                    summary=\"SAM Image Embeddings\",\n                    description=\"Run the Meta AI Segmant Anything Model to embed image data.\",\n                )\n                @with_route_exceptions\n                async def sam_embed_image(\n                    inference_request: SamEmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                    Args:\n                        inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.SamEmbeddingResponse or Response: The response containing the embedded image.\n                    \"\"\"\n                    logger.debug(f\"Reached /sam/embed_image\")\n                    sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                    model_response = await self.model_manager.infer_from_request(\n                        sam_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(sam_model_id, actor)\n                    if inference_request.format == \"binary\":\n                        return Response(\n                            content=model_response.embeddings,\n                            headers={\"Content-Type\": \"application/octet-stream\"},\n                        )\n                    return model_response\n\n                @app.post(\n                    \"/sam/segment_image\",\n                    response_model=SamSegmentationResponse,\n                    summary=\"SAM Image Segmentation\",\n                    description=\"Run the Meta AI Segmant Anything Model to generate segmenations for image data.\",\n                )\n                @with_route_exceptions\n                async def sam_segment_image(\n                    inference_request: SamSegmentationRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Generates segmentations for image data using the Meta AI Segmant Anything Model (SAM).\n\n                    Args:\n                        inference_request (SamSegmentationRequest): The request containing the image to be segmented.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.SamSegmentationResponse or Response: The response containing the segmented image.\n                    \"\"\"\n                    logger.debug(f\"Reached /sam/segment_image\")\n                    sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                    model_response = await self.model_manager.infer_from_request(\n                        sam_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(sam_model_id, actor)\n                    if inference_request.format == \"binary\":\n                        return Response(\n                            content=model_response,\n                            headers={\"Content-Type\": \"application/octet-stream\"},\n                        )\n                    return model_response\n\n            if CORE_MODEL_SAM2_ENABLED:\n\n                @app.post(\n                    \"/sam2/embed_image\",\n                    response_model=Sam2EmbeddingResponse,\n                    summary=\"SAM2 Image Embeddings\",\n                    description=\"Run the Meta AI Segment Anything 2 Model to embed image data.\",\n                )\n                @with_route_exceptions\n                async def sam2_embed_image(\n                    inference_request: Sam2EmbeddingRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Embeds image data using the Meta AI Segment Anything Model (SAM).\n\n                    Args:\n                        inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.Sam2EmbeddingResponse or Response: The response affirming the image has been embedded\n                    \"\"\"\n                    logger.debug(f\"Reached /sam2/embed_image\")\n                    sam2_model_id = load_sam2_model(inference_request, api_key=api_key)\n                    model_response = await self.model_manager.infer_from_request(\n                        sam2_model_id, inference_request\n                    )\n                    return model_response\n\n                @app.post(\n                    \"/sam2/segment_image\",\n                    response_model=Sam2SegmentationResponse,\n                    summary=\"SAM2 Image Segmentation\",\n                    description=\"Run the Meta AI Segment Anything 2 Model to generate segmenations for image data.\",\n                )\n                @with_route_exceptions\n                async def sam2_segment_image(\n                    inference_request: Sam2SegmentationRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Generates segmentations for image data using the Meta AI Segment Anything Model (SAM).\n\n                    Args:\n                        inference_request (Sam2SegmentationRequest): The request containing the image to be segmented.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.SamSegmentationResponse or Response: The response containing the segmented image.\n                    \"\"\"\n                    logger.debug(f\"Reached /sam2/segment_image\")\n                    sam2_model_id = load_sam2_model(inference_request, api_key=api_key)\n                    model_response = await self.model_manager.infer_from_request(\n                        sam2_model_id, inference_request\n                    )\n                    if inference_request.format == \"binary\":\n                        return Response(\n                            content=model_response,\n                            headers={\"Content-Type\": \"application/octet-stream\"},\n                        )\n                    return model_response\n\n            if CORE_MODEL_OWLV2_ENABLED:\n\n                @app.post(\n                    \"/owlv2/infer\",\n                    response_model=ObjectDetectionInferenceResponse,\n                    summary=\"Owlv2 image prompting\",\n                    description=\"Run the google owlv2 model to few-shot object detect\",\n                )\n                @with_route_exceptions\n                async def owlv2_infer(\n                    inference_request: OwlV2InferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                    Args:\n                        inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.Sam2EmbeddingResponse or Response: The response affirming the image has been embedded\n                    \"\"\"\n                    logger.debug(f\"Reached /owlv2/infer\")\n                    owl2_model_id = load_owlv2_model(inference_request, api_key=api_key)\n                    model_response = await self.model_manager.infer_from_request(\n                        owl2_model_id, inference_request\n                    )\n                    return model_response\n\n            if CORE_MODEL_GAZE_ENABLED:\n\n                @app.post(\n                    \"/gaze/gaze_detection\",\n                    response_model=List[GazeDetectionInferenceResponse],\n                    summary=\"Gaze Detection\",\n                    description=\"Run the gaze detection model to detect gaze.\",\n                )\n                @with_route_exceptions\n                async def gaze_detection(\n                    inference_request: GazeDetectionInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Detect gaze using the gaze detection model.\n\n                    Args:\n                        inference_request (M.GazeDetectionRequest): The request containing the image to be detected.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.GazeDetectionResponse: The response containing all the detected faces and the corresponding gazes.\n                    \"\"\"\n                    logger.debug(f\"Reached /gaze/gaze_detection\")\n                    gaze_model_id = load_gaze_model(inference_request, api_key=api_key)\n                    response = await self.model_manager.infer_from_request(\n                        gaze_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(gaze_model_id, actor)\n                    return response\n\n            if CORE_MODEL_COGVLM_ENABLED:\n\n                @app.post(\n                    \"/llm/cogvlm\",\n                    response_model=CogVLMResponse,\n                    summary=\"CogVLM\",\n                    description=\"Run the CogVLM model to chat or describe an image.\",\n                )\n                @with_route_exceptions\n                async def cog_vlm(\n                    inference_request: CogVLMInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Chat with CogVLM or ask it about an image. Multi-image requests not currently supported.\n\n                    Args:\n                        inference_request (M.CogVLMInferenceRequest): The request containing the prompt and image to be described.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        M.CogVLMResponse: The model's text response\n                    \"\"\"\n                    logger.debug(f\"Reached /llm/cogvlm\")\n                    cog_model_id = load_cogvlm_model(inference_request, api_key=api_key)\n                    response = await self.model_manager.infer_from_request(\n                        cog_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(cog_model_id, actor)\n                    return response\n\n            if CORE_MODEL_TROCR_ENABLED:\n\n                @app.post(\n                    \"/ocr/trocr\",\n                    response_model=OCRInferenceResponse,\n                    summary=\"TrOCR OCR response\",\n                    description=\"Run the TrOCR model to retrieve text in an image.\",\n                )\n                @with_route_exceptions\n                async def trocr_retrieve_text(\n                    inference_request: TrOCRInferenceRequest,\n                    request: Request,\n                    api_key: Optional[str] = Query(\n                        None,\n                        description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                    ),\n                ):\n                    \"\"\"\n                    Retrieves text from image data using the TrOCR model.\n\n                    Args:\n                        inference_request (TrOCRInferenceRequest): The request containing the image from which to retrieve text.\n                        api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                        request (Request, default Body()): The HTTP request.\n\n                    Returns:\n                        OCRInferenceResponse: The response containing the retrieved text.\n                    \"\"\"\n                    logger.debug(f\"Reached /trocr/ocr\")\n                    trocr_model_id = load_trocr_model(\n                        inference_request, api_key=api_key\n                    )\n                    response = await self.model_manager.infer_from_request(\n                        trocr_model_id, inference_request\n                    )\n                    if LAMBDA:\n                        actor = request.scope[\"aws.event\"][\"requestContext\"][\n                            \"authorizer\"\n                        ][\"lambda\"][\"actor\"]\n                        trackUsage(trocr_model_id, actor)\n                    return response\n\n        if not LAMBDA:\n\n            @app.get(\n                \"/notebook/start\",\n                summary=\"Jupyter Lab Server Start\",\n                description=\"Starts a jupyter lab server for running development code\",\n            )\n            @with_route_exceptions\n            async def notebook_start(browserless: bool = False):\n                \"\"\"Starts a jupyter lab server for running development code.\n\n                Args:\n                    inference_request (NotebookStartRequest): The request containing the necessary details for starting a jupyter lab server.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    NotebookStartResponse: The response containing the URL of the jupyter lab server.\n                \"\"\"\n                logger.debug(f\"Reached /notebook/start\")\n                if NOTEBOOK_ENABLED:\n                    start_notebook()\n                    if browserless:\n                        return {\n                            \"success\": True,\n                            \"message\": f\"Jupyter Lab server started at http://localhost:{NOTEBOOK_PORT}?token={NOTEBOOK_PASSWORD}\",\n                        }\n                    else:\n                        sleep(2)\n                        return RedirectResponse(\n                            f\"http://localhost:{NOTEBOOK_PORT}/lab/tree/quickstart.ipynb?token={NOTEBOOK_PASSWORD}\"\n                        )\n                else:\n                    if browserless:\n                        return {\n                            \"success\": False,\n                            \"message\": \"Notebook server is not enabled. Enable notebooks via the NOTEBOOK_ENABLED environment variable.\",\n                        }\n                    else:\n                        return RedirectResponse(f\"/notebook-instructions.html\")\n\n        if LEGACY_ROUTE_ENABLED:\n\n            class IntStringConvertor(StringConvertor):\n                \"\"\"\n                Match digits but keep them as string.\n                \"\"\"\n\n                regex = \"\\d+\"\n\n            register_url_convertor(\"int_string\", IntStringConvertor())\n\n            # Legacy object detection inference path for backwards compatability\n            @app.get(\n                \"/{dataset_id}/{version_id:int_string}\",\n                # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n                response_model=Union[\n                    InstanceSegmentationInferenceResponse,\n                    KeypointsDetectionInferenceResponse,\n                    ObjectDetectionInferenceResponse,\n                    ClassificationInferenceResponse,\n                    MultiLabelClassificationInferenceResponse,\n                    StubResponse,\n                    Any,\n                ],\n                response_model_exclude_none=True,\n            )\n            @app.post(\n                \"/{dataset_id}/{version_id:int_string}\",\n                # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n                response_model=Union[\n                    InstanceSegmentationInferenceResponse,\n                    KeypointsDetectionInferenceResponse,\n                    ObjectDetectionInferenceResponse,\n                    ClassificationInferenceResponse,\n                    MultiLabelClassificationInferenceResponse,\n                    StubResponse,\n                    Any,\n                ],\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            async def legacy_infer_from_request(\n                background_tasks: BackgroundTasks,\n                request: Request,\n                dataset_id: str = Path(\n                    description=\"ID of a Roboflow dataset corresponding to the model to use for inference\"\n                ),\n                version_id: str = Path(\n                    description=\"ID of a Roboflow dataset version corresponding to the model to use for inference\"\n                ),\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n                confidence: float = Query(\n                    0.4,\n                    description=\"The confidence threshold used to filter out predictions\",\n                ),\n                keypoint_confidence: float = Query(\n                    0.0,\n                    description=\"The confidence threshold used to filter out keypoints that are not visible based on model confidence\",\n                ),\n                format: str = Query(\n                    \"json\",\n                    description=\"One of 'json' or 'image'. If 'json' prediction data is return as a JSON string. If 'image' prediction data is visualized and overlayed on the original input image.\",\n                ),\n                image: Optional[str] = Query(\n                    None,\n                    description=\"The publically accessible URL of an image to use for inference.\",\n                ),\n                image_type: Optional[str] = Query(\n                    \"base64\",\n                    description=\"One of base64 or numpy. Note, numpy input is not supported for Roboflow Hosted Inference.\",\n                ),\n                labels: Optional[bool] = Query(\n                    False,\n                    description=\"If true, labels will be include in any inference visualization.\",\n                ),\n                mask_decode_mode: Optional[str] = Query(\n                    \"accurate\",\n                    description=\"One of 'accurate' or 'fast'. If 'accurate' the mask will be decoded using the original image size. If 'fast' the mask will be decoded using the original mask size. 'accurate' is slower but more accurate.\",\n                ),\n                tradeoff_factor: Optional[float] = Query(\n                    0.0,\n                    description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n                ),\n                max_detections: int = Query(\n                    300,\n                    description=\"The maximum number of detections to return. This is used to limit the number of predictions returned by the model. The model may return more predictions than this number, but only the top `max_detections` predictions will be returned.\",\n                ),\n                overlap: float = Query(\n                    0.3,\n                    description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n                ),\n                stroke: int = Query(\n                    1, description=\"The stroke width used when visualizing predictions\"\n                ),\n                countinference: Optional[bool] = Query(\n                    True,\n                    description=\"If false, does not track inference against usage.\",\n                    include_in_schema=False,\n                ),\n                service_secret: Optional[str] = Query(\n                    None,\n                    description=\"Shared secret used to authenticate requests to the inference server from internal services (e.g. to allow disabling inference usage tracking via the `countinference` query parameter)\",\n                    include_in_schema=False,\n                ),\n                disable_preproc_auto_orient: Optional[bool] = Query(\n                    False, description=\"If true, disables automatic image orientation\"\n                ),\n                disable_preproc_contrast: Optional[bool] = Query(\n                    False, description=\"If true, disables automatic contrast adjustment\"\n                ),\n                disable_preproc_grayscale: Optional[bool] = Query(\n                    False,\n                    description=\"If true, disables automatic grayscale conversion\",\n                ),\n                disable_preproc_static_crop: Optional[bool] = Query(\n                    False, description=\"If true, disables automatic static crop\"\n                ),\n                disable_active_learning: Optional[bool] = Query(\n                    default=False,\n                    description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n                ),\n                active_learning_target_dataset: Optional[str] = Query(\n                    default=None,\n                    description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n                ),\n                source: Optional[str] = Query(\n                    \"external\",\n                    description=\"The source of the inference request\",\n                ),\n                source_info: Optional[str] = Query(\n                    \"external\",\n                    description=\"The detailed source information of the inference request\",\n                ),\n            ):\n                \"\"\"\n                Legacy inference endpoint for object detection, instance segmentation, and classification.\n\n                Args:\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n                    dataset_id (str): ID of a Roboflow dataset corresponding to the model to use for inference.\n                    version_id (str): ID of a Roboflow dataset version corresponding to the model to use for inference.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    # Other parameters described in the function signature...\n\n                Returns:\n                    Union[InstanceSegmentationInferenceResponse, KeypointsDetectionInferenceRequest, ObjectDetectionInferenceResponse, ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse, Any]: The response containing the inference results.\n                \"\"\"\n                logger.debug(\n                    f\"Reached legacy route /:dataset_id/:version_id with {dataset_id}/{version_id}\"\n                )\n                model_id = f\"{dataset_id}/{version_id}\"\n\n                if confidence &gt;= 1:\n                    confidence /= 100\n                elif confidence &lt; 0.01:\n                    confidence = 0.01\n\n                if overlap &gt;= 1:\n                    overlap /= 100\n\n                if image is not None:\n                    request_image = InferenceRequestImage(type=\"url\", value=image)\n                else:\n                    if \"Content-Type\" not in request.headers:\n                        raise ContentTypeMissing(\n                            f\"Request must include a Content-Type header\"\n                        )\n                    if \"multipart/form-data\" in request.headers[\"Content-Type\"]:\n                        form_data = await request.form()\n                        base64_image_str = await form_data[\"file\"].read()\n                        base64_image_str = base64.b64encode(base64_image_str)\n                        request_image = InferenceRequestImage(\n                            type=\"base64\", value=base64_image_str.decode(\"ascii\")\n                        )\n                    elif (\n                        \"application/x-www-form-urlencoded\"\n                        in request.headers[\"Content-Type\"]\n                        or \"application/json\" in request.headers[\"Content-Type\"]\n                    ):\n                        data = await request.body()\n                        request_image = InferenceRequestImage(\n                            type=image_type, value=data\n                        )\n                    else:\n                        raise ContentTypeInvalid(\n                            f\"Invalid Content-Type: {request.headers['Content-Type']}\"\n                        )\n\n                if not countinference and service_secret != ROBOFLOW_SERVICE_SECRET:\n                    raise MissingServiceSecretError(\n                        \"Service secret is required to disable inference usage tracking\"\n                    )\n                if LAMBDA:\n                    request_model_id = (\n                        request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                            \"lambda\"\n                        ][\"model\"][\"endpoint\"]\n                        .replace(\"--\", \"/\")\n                        .replace(\"rf-\", \"\")\n                        .replace(\"nu-\", \"\")\n                    )\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                        \"lambda\"\n                    ][\"actor\"]\n                    if countinference:\n                        trackUsage(request_model_id, actor)\n                    else:\n                        if service_secret != ROBOFLOW_SERVICE_SECRET:\n                            raise MissingServiceSecretError(\n                                \"Service secret is required to disable inference usage tracking\"\n                            )\n                else:\n                    request_model_id = model_id\n                logger.debug(\n                    f\"State of model registry: {self.model_manager.describe_models()}\"\n                )\n                self.model_manager.add_model(\n                    request_model_id, api_key, model_id_alias=model_id\n                )\n\n                task_type = self.model_manager.get_task_type(model_id, api_key=api_key)\n                inference_request_type = ObjectDetectionInferenceRequest\n                args = dict()\n                if task_type == \"instance-segmentation\":\n                    inference_request_type = InstanceSegmentationInferenceRequest\n                    args = {\n                        \"mask_decode_mode\": mask_decode_mode,\n                        \"tradeoff_factor\": tradeoff_factor,\n                    }\n                elif task_type == \"classification\":\n                    inference_request_type = ClassificationInferenceRequest\n                elif task_type == \"keypoint-detection\":\n                    inference_request_type = KeypointsDetectionInferenceRequest\n                    args = {\"keypoint_confidence\": keypoint_confidence}\n                inference_request = inference_request_type(\n                    api_key=api_key,\n                    model_id=model_id,\n                    image=request_image,\n                    confidence=confidence,\n                    iou_threshold=overlap,\n                    max_detections=max_detections,\n                    visualization_labels=labels,\n                    visualization_stroke_width=stroke,\n                    visualize_predictions=(\n                        format == \"image\" or format == \"image_and_json\"\n                    ),\n                    disable_preproc_auto_orient=disable_preproc_auto_orient,\n                    disable_preproc_contrast=disable_preproc_contrast,\n                    disable_preproc_grayscale=disable_preproc_grayscale,\n                    disable_preproc_static_crop=disable_preproc_static_crop,\n                    disable_active_learning=disable_active_learning,\n                    active_learning_target_dataset=active_learning_target_dataset,\n                    source=source,\n                    source_info=source_info,\n                    usage_billable=countinference,\n                    **args,\n                )\n                inference_response = await self.model_manager.infer_from_request(\n                    inference_request.model_id,\n                    inference_request,\n                    active_learning_eligible=True,\n                    background_tasks=background_tasks,\n                )\n                logger.debug(\"Response ready.\")\n                if format == \"image\":\n                    return Response(\n                        content=inference_response.visualization,\n                        media_type=\"image/jpeg\",\n                    )\n                else:\n                    return orjson_response(inference_response)\n\n        if not LAMBDA:\n            # Legacy clear cache endpoint for backwards compatability\n            @app.get(\"/clear_cache\", response_model=str)\n            async def legacy_clear_cache():\n                \"\"\"\n                Clears the model cache.\n\n                This endpoint provides a way to clear the cache of loaded models.\n\n                Returns:\n                    str: A string indicating that the cache has been cleared.\n                \"\"\"\n                logger.debug(f\"Reached /clear_cache\")\n                await model_clear()\n                return \"Cache Cleared\"\n\n            # Legacy add model endpoint for backwards compatability\n            @app.get(\"/start/{dataset_id}/{version_id}\")\n            async def model_add(dataset_id: str, version_id: str, api_key: str = None):\n                \"\"\"\n                Starts a model inference session.\n\n                This endpoint initializes and starts an inference session for the specified model version.\n\n                Args:\n                    dataset_id (str): ID of a Roboflow dataset corresponding to the model.\n                    version_id (str): ID of a Roboflow dataset version corresponding to the model.\n                    api_key (str, optional): Roboflow API Key for artifact retrieval.\n\n                Returns:\n                    JSONResponse: A response object containing the status and a success message.\n                \"\"\"\n                logger.debug(\n                    f\"Reached /start/{dataset_id}/{version_id} with {dataset_id}/{version_id}\"\n                )\n                model_id = f\"{dataset_id}/{version_id}\"\n                self.model_manager.add_model(model_id, api_key)\n\n                return JSONResponse(\n                    {\n                        \"status\": 200,\n                        \"message\": \"inference session started from local memory.\",\n                    }\n                )\n\n        app.mount(\n            \"/\",\n            StaticFiles(directory=\"./inference/landing/out\", html=True),\n            name=\"static\",\n        )\n\n    def run(self):\n        uvicorn.run(self.app, host=\"127.0.0.1\", port=8080)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/http/http_api/#inference.core.interfaces.http.http_api.HttpInterface.__init__","title":"<code>__init__(model_manager, root_path=None)</code>","text":"<p>Initializes the HttpInterface with given model manager and model registry.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>The manager for handling different models.</p> required <code>root_path</code> <code>Optional[str]</code> <p>The root path for the FastAPI application.</p> <code>None</code> Description <p>Deploy Roboflow trained models to nearly any compute environment!</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>def __init__(\n    self,\n    model_manager: ModelManager,\n    root_path: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the HttpInterface with given model manager and model registry.\n\n    Args:\n        model_manager (ModelManager): The manager for handling different models.\n        root_path (Optional[str]): The root path for the FastAPI application.\n\n    Description:\n        Deploy Roboflow trained models to nearly any compute environment!\n    \"\"\"\n    description = \"Roboflow inference server\"\n    app = FastAPI(\n        title=\"Roboflow Inference Server\",\n        description=description,\n        version=__version__,\n        terms_of_service=\"https://roboflow.com/terms\",\n        contact={\n            \"name\": \"Roboflow Inc.\",\n            \"url\": \"https://roboflow.com/contact\",\n            \"email\": \"help@roboflow.com\",\n        },\n        license_info={\n            \"name\": \"Apache 2.0\",\n            \"url\": \"https://www.apache.org/licenses/LICENSE-2.0.html\",\n        },\n        root_path=root_path,\n    )\n\n    if ENABLE_PROMETHEUS:\n        InferenceInstrumentator(\n            app, model_manager=model_manager, endpoint=\"/metrics\"\n        )\n\n    if METLO_KEY:\n        app.add_middleware(\n            ASGIMiddleware, host=\"https://app.metlo.com\", api_key=METLO_KEY\n        )\n    if LAMBDA:\n        app.add_middleware(LambdaMiddleware)\n\n    if len(ALLOW_ORIGINS) &gt; 0:\n        app.add_middleware(\n            CORSMiddleware,\n            allow_origins=ALLOW_ORIGINS,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n\n    # Optionally add middleware for profiling the FastAPI server and underlying inference API code\n    if PROFILE:\n        app.add_middleware(\n            CProfileMiddleware,\n            enable=True,\n            server_app=app,\n            filename=\"/profile/output.pstats\",\n            strip_dirs=False,\n            sort_by=\"cumulative\",\n        )\n    app.add_middleware(asgi_correlation_id.CorrelationIdMiddleware)\n\n    if METRICS_ENABLED:\n\n        @app.middleware(\"http\")\n        async def count_errors(request: Request, call_next):\n            \"\"\"Middleware to count errors.\n\n            Args:\n                request (Request): The incoming request.\n                call_next (Callable): The next middleware or endpoint to call.\n\n            Returns:\n                Response: The response from the next middleware or endpoint.\n            \"\"\"\n            response = await call_next(request)\n            if self.model_manager.pingback and response.status_code &gt;= 400:\n                self.model_manager.num_errors += 1\n            return response\n\n    if not LAMBDA:\n\n        @app.get(\"/device/stats\")\n        async def device_stats():\n            not_configured_error_message = {\n                \"error\": \"Device statistics endpoint is not enabled.\",\n                \"hint\": \"Mount the Docker socket and point its location when running the docker \"\n                \"container to collect device stats \"\n                \"(i.e. `docker run ... -v /var/run/docker.sock:/var/run/docker.sock \"\n                \"-e DOCKER_SOCKET_PATH=/var/run/docker.sock ...`).\",\n            }\n            if not DOCKER_SOCKET_PATH:\n                return JSONResponse(\n                    status_code=404,\n                    content=not_configured_error_message,\n                )\n            if not is_docker_socket_mounted(docker_socket_path=DOCKER_SOCKET_PATH):\n                return JSONResponse(\n                    status_code=500,\n                    content=not_configured_error_message,\n                )\n            container_stats = get_container_stats(\n                docker_socket_path=DOCKER_SOCKET_PATH\n            )\n            return JSONResponse(status_code=200, content=container_stats)\n\n    if DEDICATED_DEPLOYMENT_WORKSPACE_URL:\n        cached_api_keys = dict()\n        cached_projects = dict()\n\n        @app.middleware(\"http\")\n        async def check_authorization(request: Request, call_next):\n            # exclusions\n            skip_check = (\n                request.method not in [\"GET\", \"POST\"]\n                or request.url.path\n                in [\n                    \"/\",\n                    \"/info\",\n                    \"/workflows/blocks/describe\",\n                    \"/workflows/definition/schema\",\n                ]\n                or request.url.path.startswith(\"/static/\")\n                or request.url.path.startswith(\"/_next/\")\n            )\n            if skip_check:\n                return await call_next(request)\n\n            def _unauthorized_response(msg):\n                return JSONResponse(\n                    status_code=401,\n                    content={\n                        \"status\": 401,\n                        \"message\": msg,\n                    },\n                )\n\n            # check api_key\n            req_params = request.query_params\n            json_params = dict()\n            if (\n                request.headers.get(\"content-type\", None) == \"application/json\"\n                and int(request.headers.get(\"content-length\", 0)) &gt; 0\n            ):\n                json_params = await request.json()\n            api_key = req_params.get(\"api_key\", None) or json_params.get(\n                \"api_key\", None\n            )\n\n            if cached_api_keys.get(api_key, 0) &lt; time.time():\n                try:\n                    workspace_url = (\n                        get_roboflow_workspace(api_key)\n                        if api_key is not None\n                        else None\n                    )\n\n                    if workspace_url != DEDICATED_DEPLOYMENT_WORKSPACE_URL:\n                        return _unauthorized_response(\"Unauthorized api_key\")\n\n                    cached_api_keys[api_key] = (\n                        time.time() + 3600\n                    )  # expired after 1 hour\n                except RoboflowAPINotAuthorizedError as e:\n                    return _unauthorized_response(\"Unauthorized api_key\")\n\n            # check project_url\n            model_id = json_params.get(\"model_id\", \"\")\n            project_url = (\n                req_params.get(\"project\", None)\n                or json_params.get(\"project\", None)\n                or model_id.split(\"/\")[0]\n            )\n            # only check when project_url is not None\n            if (\n                project_url is not None\n                and cached_projects.get(project_url, 0) &lt; time.time()\n            ):\n                try:\n                    _ = get_roboflow_dataset_type(\n                        api_key, DEDICATED_DEPLOYMENT_WORKSPACE_URL, project_url\n                    )\n\n                    cached_projects[project_url] = (\n                        time.time() + 3600\n                    )  # expired after 1 hour\n                except RoboflowAPINotNotFoundError as e:\n                    return _unauthorized_response(\"Unauthorized project\")\n\n            return await call_next(request)\n\n    self.app = app\n    self.model_manager = model_manager\n    self.stream_manager_client: Optional[StreamManagerClient] = None\n\n    if ENABLE_STREAM_API:\n        operations_timeout = os.getenv(\"STREAM_MANAGER_OPERATIONS_TIMEOUT\")\n        if operations_timeout is not None:\n            operations_timeout = float(operations_timeout)\n        self.stream_manager_client = StreamManagerClient.init(\n            host=os.getenv(\"STREAM_MANAGER_HOST\", \"127.0.0.1\"),\n            port=int(os.getenv(\"STREAM_MANAGER_PORT\", \"7070\")),\n            operations_timeout=operations_timeout,\n        )\n\n    async def process_inference_request(\n        inference_request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes an inference request by calling the appropriate model.\n\n        Args:\n            inference_request (InferenceRequest): The request containing model ID and other inference details.\n\n        Returns:\n            InferenceResponse: The response containing the inference results.\n        \"\"\"\n        de_aliased_model_id = resolve_roboflow_model_alias(\n            model_id=inference_request.model_id\n        )\n        self.model_manager.add_model(de_aliased_model_id, inference_request.api_key)\n        resp = await self.model_manager.infer_from_request(\n            de_aliased_model_id, inference_request, **kwargs\n        )\n        return orjson_response(resp)\n\n    def process_workflow_inference_request(\n        workflow_request: WorkflowInferenceRequest,\n        workflow_specification: dict,\n        background_tasks: Optional[BackgroundTasks],\n        profiler: WorkflowsProfiler,\n    ) -&gt; WorkflowInferenceResponse:\n        workflow_init_parameters = {\n            \"workflows_core.model_manager\": model_manager,\n            \"workflows_core.api_key\": workflow_request.api_key,\n            \"workflows_core.background_tasks\": background_tasks,\n        }\n        execution_engine = ExecutionEngine.init(\n            workflow_definition=workflow_specification,\n            init_parameters=workflow_init_parameters,\n            max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n            prevent_local_images_loading=True,\n            profiler=profiler,\n        )\n        result = execution_engine.run(runtime_parameters=workflow_request.inputs)\n        with profiler.profile_execution_phase(\n            name=\"workflow_results_serialisation\",\n            categories=[\"inference_package_operation\"],\n        ):\n            outputs = serialise_workflow_result(\n                result=result,\n                excluded_fields=workflow_request.excluded_fields,\n            )\n        profiler_trace = profiler.export_trace()\n        response = WorkflowInferenceResponse(\n            outputs=outputs,\n            profiler_trace=profiler_trace,\n        )\n        return orjson_response(response=response)\n\n    def load_core_model(\n        inference_request: InferenceRequest,\n        api_key: Optional[str] = None,\n        core_model: str = None,\n    ) -&gt; None:\n        \"\"\"Loads a core model (e.g., \"clip\" or \"sam\") into the model manager.\n\n        Args:\n            inference_request (InferenceRequest): The request containing version and other details.\n            api_key (Optional[str]): The API key for the request.\n            core_model (str): The core model type, e.g., \"clip\" or \"sam\".\n\n        Returns:\n            str: The core model ID.\n        \"\"\"\n        if api_key:\n            inference_request.api_key = api_key\n        version_id_field = f\"{core_model}_version_id\"\n        core_model_id = (\n            f\"{core_model}/{inference_request.__getattribute__(version_id_field)}\"\n        )\n        self.model_manager.add_model(core_model_id, inference_request.api_key)\n        return core_model_id\n\n    load_clip_model = partial(load_core_model, core_model=\"clip\")\n    \"\"\"Loads the CLIP model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The CLIP model ID.\n    \"\"\"\n\n    load_sam_model = partial(load_core_model, core_model=\"sam\")\n    \"\"\"Loads the SAM model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The SAM model ID.\n    \"\"\"\n    load_sam2_model = partial(load_core_model, core_model=\"sam2\")\n    \"\"\"Loads the SAM2 model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The SAM2 model ID.\n    \"\"\"\n\n    load_gaze_model = partial(load_core_model, core_model=\"gaze\")\n    \"\"\"Loads the GAZE model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The GAZE model ID.\n    \"\"\"\n\n    load_doctr_model = partial(load_core_model, core_model=\"doctr\")\n    \"\"\"Loads the DocTR model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The DocTR model ID.\n    \"\"\"\n    load_cogvlm_model = partial(load_core_model, core_model=\"cogvlm\")\n    load_paligemma_model = partial(load_core_model, core_model=\"paligemma\")\n\n    load_grounding_dino_model = partial(\n        load_core_model, core_model=\"grounding_dino\"\n    )\n    \"\"\"Loads the Grounding DINO model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The Grounding DINO model ID.\n    \"\"\"\n\n    load_yolo_world_model = partial(load_core_model, core_model=\"yolo_world\")\n    load_owlv2_model = partial(load_core_model, core_model=\"owlv2\")\n    \"\"\"Loads the YOLO World model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The YOLO World model ID.\n    \"\"\"\n\n    load_trocr_model = partial(load_core_model, core_model=\"trocr\")\n    \"\"\"Loads the TrOCR model into the model manager.\n\n    Args:\n    inference_request: The request containing version and other details.\n    api_key: The API key for the request.\n\n    Returns:\n    The TrOCR model ID.\n    \"\"\"\n\n    @app.get(\n        \"/info\",\n        response_model=ServerVersionInfo,\n        summary=\"Info\",\n        description=\"Get the server name and version number\",\n    )\n    async def root():\n        \"\"\"Endpoint to get the server name and version number.\n\n        Returns:\n            ServerVersionInfo: The server version information.\n        \"\"\"\n        return ServerVersionInfo(\n            name=\"Roboflow Inference Server\",\n            version=__version__,\n            uuid=GLOBAL_INFERENCE_SERVER_ID,\n        )\n\n    # The current AWS Lambda authorizer only supports path parameters, therefore we can only use the legacy infer route. This case statement excludes routes which won't work for the current Lambda authorizer.\n    if not LAMBDA:\n\n        @app.get(\n            \"/model/registry\",\n            response_model=ModelsDescriptions,\n            summary=\"Get model keys\",\n            description=\"Get the ID of each loaded model\",\n        )\n        async def registry():\n            \"\"\"Get the ID of each loaded model in the registry.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/registry\")\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n        @app.post(\n            \"/model/add\",\n            response_model=ModelsDescriptions,\n            summary=\"Load a model\",\n            description=\"Load the model with the given model ID\",\n        )\n        @with_route_exceptions\n        async def model_add(request: AddModelRequest):\n            \"\"\"Load the model with the given model ID into the model manager.\n\n            Args:\n                request (AddModelRequest): The request containing the model ID and optional API key.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/add\")\n            de_aliased_model_id = resolve_roboflow_model_alias(\n                model_id=request.model_id\n            )\n            self.model_manager.add_model(de_aliased_model_id, request.api_key)\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n        @app.post(\n            \"/model/remove\",\n            response_model=ModelsDescriptions,\n            summary=\"Remove a model\",\n            description=\"Remove the model with the given model ID\",\n        )\n        @with_route_exceptions\n        async def model_remove(request: ClearModelRequest):\n            \"\"\"Remove the model with the given model ID from the model manager.\n\n            Args:\n                request (ClearModelRequest): The request containing the model ID to be removed.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/remove\")\n            de_aliased_model_id = resolve_roboflow_model_alias(\n                model_id=request.model_id\n            )\n            self.model_manager.remove(de_aliased_model_id)\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n        @app.post(\n            \"/model/clear\",\n            response_model=ModelsDescriptions,\n            summary=\"Remove all models\",\n            description=\"Remove all loaded models\",\n        )\n        @with_route_exceptions\n        async def model_clear():\n            \"\"\"Remove all loaded models from the model manager.\n\n            Returns:\n                ModelsDescriptions: The object containing models descriptions\n            \"\"\"\n            logger.debug(f\"Reached /model/clear\")\n            self.model_manager.clear()\n            models_descriptions = self.model_manager.describe_models()\n            return ModelsDescriptions.from_models_descriptions(\n                models_descriptions=models_descriptions\n            )\n\n        @app.post(\n            \"/infer/object_detection\",\n            response_model=Union[\n                ObjectDetectionInferenceResponse,\n                List[ObjectDetectionInferenceResponse],\n                StubResponse,\n            ],\n            summary=\"Object detection infer\",\n            description=\"Run inference with the specified object detection model\",\n            response_model_exclude_none=True,\n        )\n        @with_route_exceptions\n        async def infer_object_detection(\n            inference_request: ObjectDetectionInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ):\n            \"\"\"Run inference with the specified object detection model.\n\n            Args:\n                inference_request (ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                Union[ObjectDetectionInferenceResponse, List[ObjectDetectionInferenceResponse]]: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/object_detection\")\n            return await process_inference_request(\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n            )\n\n        @app.post(\n            \"/infer/instance_segmentation\",\n            response_model=Union[\n                InstanceSegmentationInferenceResponse, StubResponse\n            ],\n            summary=\"Instance segmentation infer\",\n            description=\"Run inference with the specified instance segmentation model\",\n        )\n        @with_route_exceptions\n        async def infer_instance_segmentation(\n            inference_request: InstanceSegmentationInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ):\n            \"\"\"Run inference with the specified instance segmentation model.\n\n            Args:\n                inference_request (InstanceSegmentationInferenceRequest): The request containing the necessary details for instance segmentation.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                InstanceSegmentationInferenceResponse: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/instance_segmentation\")\n            return await process_inference_request(\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n            )\n\n        @app.post(\n            \"/infer/classification\",\n            response_model=Union[\n                ClassificationInferenceResponse,\n                MultiLabelClassificationInferenceResponse,\n                StubResponse,\n            ],\n            summary=\"Classification infer\",\n            description=\"Run inference with the specified classification model\",\n        )\n        @with_route_exceptions\n        async def infer_classification(\n            inference_request: ClassificationInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ):\n            \"\"\"Run inference with the specified classification model.\n\n            Args:\n                inference_request (ClassificationInferenceRequest): The request containing the necessary details for classification.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/classification\")\n            return await process_inference_request(\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n            )\n\n        @app.post(\n            \"/infer/keypoints_detection\",\n            response_model=Union[KeypointsDetectionInferenceResponse, StubResponse],\n            summary=\"Keypoints detection infer\",\n            description=\"Run inference with the specified keypoints detection model\",\n        )\n        @with_route_exceptions\n        async def infer_keypoints(\n            inference_request: KeypointsDetectionInferenceRequest,\n        ):\n            \"\"\"Run inference with the specified keypoints detection model.\n\n            Args:\n                inference_request (KeypointsDetectionInferenceRequest): The request containing the necessary details for keypoints detection.\n\n            Returns:\n                Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]: The response containing the inference results.\n            \"\"\"\n            logger.debug(f\"Reached /infer/keypoints_detection\")\n            return await process_inference_request(inference_request)\n\n        if LMM_ENABLED:\n\n            @app.post(\n                \"/infer/lmm\",\n                response_model=Union[\n                    LMMInferenceResponse,\n                    List[LMMInferenceResponse],\n                    StubResponse,\n                ],\n                summary=\"Large multi-modal model infer\",\n                description=\"Run inference with the specified large multi-modal model\",\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            async def infer_lmm(\n                inference_request: LMMInferenceRequest,\n            ):\n                \"\"\"Run inference with the specified object detection model.\n\n                Args:\n                    inference_request (ObjectDetectionInferenceRequest): The request containing the necessary details for object detection.\n                    background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n                Returns:\n                    Union[ObjectDetectionInferenceResponse, List[ObjectDetectionInferenceResponse]]: The response containing the inference results.\n                \"\"\"\n                logger.debug(f\"Reached /infer/lmm\")\n                return await process_inference_request(inference_request)\n\n    if not DISABLE_WORKFLOW_ENDPOINTS:\n\n        @app.post(\n            \"/{workspace_name}/workflows/{workflow_id}/describe_interface\",\n            response_model=DescribeInterfaceResponse,\n            summary=\"Endpoint to describe interface of predefined workflow\",\n            description=\"Checks Roboflow API for workflow definition, once acquired - describes workflow inputs and outputs\",\n        )\n        @with_route_exceptions\n        async def describe_predefined_workflow_interface(\n            workspace_name: str,\n            workflow_id: str,\n            workflow_request: PredefinedWorkflowDescribeInterfaceRequest,\n        ) -&gt; DescribeInterfaceResponse:\n            workflow_specification = get_workflow_specification(\n                api_key=workflow_request.api_key,\n                workspace_id=workspace_name,\n                workflow_id=workflow_id,\n                use_cache=workflow_request.use_cache,\n            )\n            return handle_describe_workflows_interface(\n                definition=workflow_specification,\n            )\n\n        @app.post(\n            \"/workflows/describe_interface\",\n            response_model=DescribeInterfaceResponse,\n            summary=\"Endpoint to describe interface of workflow given in request\",\n            description=\"Parses workflow definition and retrieves describes inputs and outputs\",\n        )\n        @with_route_exceptions\n        async def describe_workflow_interface(\n            workflow_request: WorkflowSpecificationDescribeInterfaceRequest,\n        ) -&gt; DescribeInterfaceResponse:\n            return handle_describe_workflows_interface(\n                definition=workflow_request.specification,\n            )\n\n        @app.post(\n            \"/{workspace_name}/workflows/{workflow_id}\",\n            response_model=WorkflowInferenceResponse,\n            summary=\"Endpoint to run predefined workflow\",\n            description=\"Checks Roboflow API for workflow definition, once acquired - parses and executes injecting runtime parameters from request body\",\n        )\n        @app.post(\n            \"/infer/workflows/{workspace_name}/{workflow_id}\",\n            response_model=WorkflowInferenceResponse,\n            summary=\"[LEGACY] Endpoint to run predefined workflow\",\n            description=\"Checks Roboflow API for workflow definition, once acquired - parses and executes injecting runtime parameters from request body. This endpoint is deprecated and will be removed end of Q2 2024\",\n            deprecated=True,\n        )\n        @with_route_exceptions\n        async def infer_from_predefined_workflow(\n            workspace_name: str,\n            workflow_id: str,\n            workflow_request: PredefinedWorkflowInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ) -&gt; WorkflowInferenceResponse:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            if ENABLE_WORKFLOWS_PROFILING and workflow_request.enable_profiling:\n                profiler = BaseWorkflowsProfiler.init(\n                    max_runs_in_buffer=WORKFLOWS_PROFILER_BUFFER_SIZE,\n                )\n            else:\n                profiler = NullWorkflowsProfiler.init()\n            with profiler.profile_execution_phase(\n                name=\"workflow_definition_fetching\",\n                categories=[\"inference_package_operation\"],\n            ):\n                workflow_specification = get_workflow_specification(\n                    api_key=workflow_request.api_key,\n                    workspace_id=workspace_name,\n                    workflow_id=workflow_id,\n                    use_cache=workflow_request.use_cache,\n                )\n            return process_workflow_inference_request(\n                workflow_request=workflow_request,\n                workflow_specification=workflow_specification,\n                background_tasks=background_tasks if not LAMBDA else None,\n                profiler=profiler,\n            )\n\n        @app.post(\n            \"/workflows/run\",\n            response_model=WorkflowInferenceResponse,\n            summary=\"Endpoint to run workflow specification provided in payload\",\n            description=\"Parses and executes workflow specification, injecting runtime parameters from request body.\",\n        )\n        @app.post(\n            \"/infer/workflows\",\n            response_model=WorkflowInferenceResponse,\n            summary=\"[LEGACY] Endpoint to run workflow specification provided in payload\",\n            description=\"Parses and executes workflow specification, injecting runtime parameters from request body. This endpoint is deprecated and will be removed end of Q2 2024.\",\n            deprecated=True,\n        )\n        @with_route_exceptions\n        async def infer_from_workflow(\n            workflow_request: WorkflowSpecificationInferenceRequest,\n            background_tasks: BackgroundTasks,\n        ) -&gt; WorkflowInferenceResponse:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            if ENABLE_WORKFLOWS_PROFILING and workflow_request.enable_profiling:\n                profiler = BaseWorkflowsProfiler.init(\n                    max_runs_in_buffer=WORKFLOWS_PROFILER_BUFFER_SIZE,\n                )\n            else:\n                profiler = NullWorkflowsProfiler.init()\n            return process_workflow_inference_request(\n                workflow_request=workflow_request,\n                workflow_specification=workflow_request.specification,\n                background_tasks=background_tasks if not LAMBDA else None,\n                profiler=profiler,\n            )\n\n        @app.get(\n            \"/workflows/execution_engine/versions\",\n            response_model=ExecutionEngineVersions,\n            summary=\"Returns available Execution Engine versions sorted from oldest to newest\",\n            description=\"Returns available Execution Engine versions sorted from oldest to newest\",\n        )\n        @with_route_exceptions\n        async def get_execution_engine_versions() -&gt; ExecutionEngineVersions:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            versions = get_available_versions()\n            return ExecutionEngineVersions(versions=versions)\n\n        @app.get(\n            \"/workflows/blocks/describe\",\n            response_model=WorkflowsBlocksDescription,\n            summary=\"[LEGACY] Endpoint to get definition of workflows blocks that are accessible\",\n            description=\"Endpoint provides detailed information about workflows building blocks that are \"\n            \"accessible in the inference server. This information could be used to programmatically \"\n            \"build / display workflows.\",\n            deprecated=True,\n        )\n        @with_route_exceptions\n        async def describe_workflows_blocks(\n            request: Request,\n        ) -&gt; Union[WorkflowsBlocksDescription, Response]:\n            result = handle_describe_workflows_blocks_request()\n            return gzip_response_if_requested(request=request, response=result)\n\n        @app.post(\n            \"/workflows/blocks/describe\",\n            response_model=WorkflowsBlocksDescription,\n            summary=\"[EXPERIMENTAL] Endpoint to get definition of workflows blocks that are accessible\",\n            description=\"Endpoint provides detailed information about workflows building blocks that are \"\n            \"accessible in the inference server. This information could be used to programmatically \"\n            \"build / display workflows. Additionally - in request body one can specify list of \"\n            \"dynamic blocks definitions which will be transformed into blocks and used to generate \"\n            \"schemas and definitions of connections\",\n        )\n        @with_route_exceptions\n        async def describe_workflows_blocks(\n            request: Request,\n            request_payload: Optional[DescribeBlocksRequest] = None,\n        ) -&gt; Union[WorkflowsBlocksDescription, Response]:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            dynamic_blocks_definitions = None\n            requested_execution_engine_version = None\n            if request_payload is not None:\n                dynamic_blocks_definitions = (\n                    request_payload.dynamic_blocks_definitions\n                )\n                requested_execution_engine_version = (\n                    request_payload.execution_engine_version\n                )\n            result = handle_describe_workflows_blocks_request(\n                dynamic_blocks_definitions=dynamic_blocks_definitions,\n                requested_execution_engine_version=requested_execution_engine_version,\n            )\n            return gzip_response_if_requested(request=request, response=result)\n\n        @app.get(\n            \"/workflows/definition/schema\",\n            response_model=WorkflowsBlocksSchemaDescription,\n            summary=\"Endpoint to fetch the workflows block schema\",\n            description=\"Endpoint to fetch the schema of all available blocks. This information can be \"\n            \"used to validate workflow definitions and suggest syntax in the JSON editor.\",\n        )\n        @with_route_exceptions\n        async def get_workflow_schema() -&gt; WorkflowsBlocksSchemaDescription:\n            return get_workflow_schema_description()\n\n        @app.post(\n            \"/workflows/blocks/dynamic_outputs\",\n            response_model=List[OutputDefinition],\n            summary=\"[EXPERIMENTAL] Endpoint to get definition of dynamic output for workflow step\",\n            description=\"Endpoint to be used when step outputs can be discovered only after \"\n            \"filling manifest with data.\",\n        )\n        @with_route_exceptions\n        async def get_dynamic_block_outputs(\n            step_manifest: Dict[str, Any]\n        ) -&gt; List[OutputDefinition]:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            # Potentially TODO: dynamic blocks do not support dynamic outputs, but if it changes\n            # we need to provide dynamic blocks manifests here\n            dummy_workflow_definition = {\n                \"version\": \"1.0\",\n                \"inputs\": [],\n                \"steps\": [step_manifest],\n                \"outputs\": [],\n            }\n            available_blocks = load_workflow_blocks()\n            parsed_definition = parse_workflow_definition(\n                raw_workflow_definition=dummy_workflow_definition,\n                available_blocks=available_blocks,\n            )\n            parsed_manifest = parsed_definition.steps[0]\n            return parsed_manifest.get_actual_outputs()\n\n        @app.post(\n            \"/workflows/validate\",\n            response_model=WorkflowValidationStatus,\n            summary=\"[EXPERIMENTAL] Endpoint to validate\",\n            description=\"Endpoint provides a way to check validity of JSON workflow definition.\",\n        )\n        @with_route_exceptions\n        async def validate_workflow(\n            specification: dict,\n        ) -&gt; WorkflowValidationStatus:\n            # TODO: get rid of async: https://github.com/roboflow/inference/issues/569\n            step_execution_mode = StepExecutionMode(WORKFLOWS_STEP_EXECUTION_MODE)\n            workflow_init_parameters = {\n                \"workflows_core.model_manager\": model_manager,\n                \"workflows_core.api_key\": None,\n                \"workflows_core.background_tasks\": None,\n                \"workflows_core.step_execution_mode\": step_execution_mode,\n            }\n            _ = ExecutionEngine.init(\n                workflow_definition=specification,\n                init_parameters=workflow_init_parameters,\n                max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n                prevent_local_images_loading=True,\n            )\n            return WorkflowValidationStatus(status=\"ok\")\n\n    if ENABLE_STREAM_API:\n\n        @app.get(\n            \"/inference_pipelines/list\",\n            response_model=ListPipelinesResponse,\n            summary=\"[EXPERIMENTAL] List active InferencePipelines\",\n            description=\"[EXPERIMENTAL] Listing all active InferencePipelines processing videos\",\n        )\n        @with_route_exceptions\n        async def list_pipelines(_: Request) -&gt; ListPipelinesResponse:\n            return await self.stream_manager_client.list_pipelines()\n\n        @app.get(\n            \"/inference_pipelines/{pipeline_id}/status\",\n            response_model=InferencePipelineStatusResponse,\n            summary=\"[EXPERIMENTAL] Get status of InferencePipeline\",\n            description=\"[EXPERIMENTAL] Get status of InferencePipeline\",\n        )\n        @with_route_exceptions\n        async def get_status(pipeline_id: str) -&gt; InferencePipelineStatusResponse:\n            return await self.stream_manager_client.get_status(\n                pipeline_id=pipeline_id\n            )\n\n        @app.post(\n            \"/inference_pipelines/initialise\",\n            response_model=CommandResponse,\n            summary=\"[EXPERIMENTAL] Starts new InferencePipeline\",\n            description=\"[EXPERIMENTAL] Starts new InferencePipeline\",\n        )\n        @with_route_exceptions\n        async def initialise(request: InitialisePipelinePayload) -&gt; CommandResponse:\n            return await self.stream_manager_client.initialise_pipeline(\n                initialisation_request=request\n            )\n\n        @app.post(\n            \"/inference_pipelines/initialise_webrtc\",\n            response_model=InitializeWebRTCPipelineResponse,\n            summary=\"[EXPERIMENTAL] Establishes WebRTC peer connection and starts new InferencePipeline consuming video track\",\n            description=\"[EXPERIMENTAL] Establishes WebRTC peer connection and starts new InferencePipeline consuming video track\",\n        )\n        @with_route_exceptions\n        async def initialise_webrtc_inference_pipeline(\n            request: InitialiseWebRTCPipelinePayload,\n        ) -&gt; CommandResponse:\n            resp = await self.stream_manager_client.initialise_webrtc_pipeline(\n                initialisation_request=request\n            )\n            return resp\n\n        @app.post(\n            \"/inference_pipelines/{pipeline_id}/pause\",\n            response_model=CommandResponse,\n            summary=\"[EXPERIMENTAL] Pauses the InferencePipeline\",\n            description=\"[EXPERIMENTAL] Pauses the InferencePipeline\",\n        )\n        @with_route_exceptions\n        async def pause(pipeline_id: str) -&gt; CommandResponse:\n            return await self.stream_manager_client.pause_pipeline(\n                pipeline_id=pipeline_id\n            )\n\n        @app.post(\n            \"/inference_pipelines/{pipeline_id}/resume\",\n            response_model=CommandResponse,\n            summary=\"[EXPERIMENTAL] Resumes the InferencePipeline\",\n            description=\"[EXPERIMENTAL] Resumes the InferencePipeline\",\n        )\n        @with_route_exceptions\n        async def resume(pipeline_id: str) -&gt; CommandResponse:\n            return await self.stream_manager_client.resume_pipeline(\n                pipeline_id=pipeline_id\n            )\n\n        @app.post(\n            \"/inference_pipelines/{pipeline_id}/terminate\",\n            response_model=CommandResponse,\n            summary=\"[EXPERIMENTAL] Terminates the InferencePipeline\",\n            description=\"[EXPERIMENTAL] Terminates the InferencePipeline\",\n        )\n        @with_route_exceptions\n        async def terminate(pipeline_id: str) -&gt; CommandResponse:\n            return await self.stream_manager_client.terminate_pipeline(\n                pipeline_id=pipeline_id\n            )\n\n        @app.get(\n            \"/inference_pipelines/{pipeline_id}/consume\",\n            response_model=ConsumePipelineResponse,\n            summary=\"[EXPERIMENTAL] Consumes InferencePipeline result\",\n            description=\"[EXPERIMENTAL] Consumes InferencePipeline result\",\n        )\n        @with_route_exceptions\n        async def consume(\n            pipeline_id: str,\n            request: Optional[ConsumeResultsPayload] = None,\n        ) -&gt; ConsumePipelineResponse:\n            if request is None:\n                request = ConsumeResultsPayload()\n            return await self.stream_manager_client.consume_pipeline_result(\n                pipeline_id=pipeline_id,\n                excluded_fields=request.excluded_fields,\n            )\n\n    if CORE_MODELS_ENABLED:\n        if CORE_MODEL_CLIP_ENABLED:\n\n            @app.post(\n                \"/clip/embed_image\",\n                response_model=ClipEmbeddingResponse,\n                summary=\"CLIP Image Embeddings\",\n                description=\"Run the Open AI CLIP model to embed image data.\",\n            )\n            @with_route_exceptions\n            async def clip_embed_image(\n                inference_request: ClipImageEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Embeds image data using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (ClipImageEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ClipEmbeddingResponse: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /clip/embed_image\")\n                clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                response = await self.model_manager.infer_from_request(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor)\n                return response\n\n            @app.post(\n                \"/clip/embed_text\",\n                response_model=ClipEmbeddingResponse,\n                summary=\"CLIP Text Embeddings\",\n                description=\"Run the Open AI CLIP model to embed text data.\",\n            )\n            @with_route_exceptions\n            async def clip_embed_text(\n                inference_request: ClipTextEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Embeds text data using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (ClipTextEmbeddingRequest): The request containing the text to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ClipEmbeddingResponse: The response containing the embedded text.\n                \"\"\"\n                logger.debug(f\"Reached /clip/embed_text\")\n                clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                response = await self.model_manager.infer_from_request(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor)\n                return response\n\n            @app.post(\n                \"/clip/compare\",\n                response_model=ClipCompareResponse,\n                summary=\"CLIP Compare\",\n                description=\"Run the Open AI CLIP model to compute similarity scores.\",\n            )\n            @with_route_exceptions\n            async def clip_compare(\n                inference_request: ClipCompareRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Computes similarity scores using the OpenAI CLIP model.\n\n                Args:\n                    inference_request (ClipCompareRequest): The request containing the data to be compared.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ClipCompareResponse: The response containing the similarity scores.\n                \"\"\"\n                logger.debug(f\"Reached /clip/compare\")\n                clip_model_id = load_clip_model(inference_request, api_key=api_key)\n                response = await self.model_manager.infer_from_request(\n                    clip_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(clip_model_id, actor, n=2)\n                return response\n\n        if CORE_MODEL_GROUNDINGDINO_ENABLED:\n\n            @app.post(\n                \"/grounding_dino/infer\",\n                response_model=ObjectDetectionInferenceResponse,\n                summary=\"Grounding DINO inference.\",\n                description=\"Run the Grounding DINO zero-shot object detection model.\",\n            )\n            @with_route_exceptions\n            async def grounding_dino_infer(\n                inference_request: GroundingDINOInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Embeds image data using the Grounding DINO model.\n\n                Args:\n                    inference_request GroundingDINOInferenceRequest): The request containing the image on which to run object detection.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ObjectDetectionInferenceResponse: The object detection response.\n                \"\"\"\n                logger.debug(f\"Reached /grounding_dino/infer\")\n                grounding_dino_model_id = load_grounding_dino_model(\n                    inference_request, api_key=api_key\n                )\n                response = await self.model_manager.infer_from_request(\n                    grounding_dino_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(grounding_dino_model_id, actor)\n                return response\n\n        if CORE_MODEL_YOLO_WORLD_ENABLED:\n\n            @app.post(\n                \"/yolo_world/infer\",\n                response_model=ObjectDetectionInferenceResponse,\n                summary=\"YOLO-World inference.\",\n                description=\"Run the YOLO-World zero-shot object detection model.\",\n                response_model_exclude_none=True,\n            )\n            @with_route_exceptions\n            async def yolo_world_infer(\n                inference_request: YOLOWorldInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Runs the YOLO-World zero-shot object detection model.\n\n                Args:\n                    inference_request (YOLOWorldInferenceRequest): The request containing the image on which to run object detection.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    ObjectDetectionInferenceResponse: The object detection response.\n                \"\"\"\n                logger.debug(f\"Reached /yolo_world/infer. Loading model\")\n                yolo_world_model_id = load_yolo_world_model(\n                    inference_request, api_key=api_key\n                )\n                logger.debug(\"YOLOWorld model loaded. Staring the inference.\")\n                response = await self.model_manager.infer_from_request(\n                    yolo_world_model_id, inference_request\n                )\n                logger.debug(\"YOLOWorld prediction available.\")\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(yolo_world_model_id, actor)\n                    logger.debug(\"Usage of YOLOWorld denoted.\")\n                return response\n\n        if CORE_MODEL_DOCTR_ENABLED:\n\n            @app.post(\n                \"/doctr/ocr\",\n                response_model=OCRInferenceResponse,\n                summary=\"DocTR OCR response\",\n                description=\"Run the DocTR OCR model to retrieve text in an image.\",\n            )\n            @with_route_exceptions\n            async def doctr_retrieve_text(\n                inference_request: DoctrOCRInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Embeds image data using the DocTR model.\n\n                Args:\n                    inference_request (M.DoctrOCRInferenceRequest): The request containing the image from which to retrieve text.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.OCRInferenceResponse: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /doctr/ocr\")\n                doctr_model_id = load_doctr_model(\n                    inference_request, api_key=api_key\n                )\n                response = await self.model_manager.infer_from_request(\n                    doctr_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(doctr_model_id, actor)\n                return response\n\n        if CORE_MODEL_SAM_ENABLED:\n\n            @app.post(\n                \"/sam/embed_image\",\n                response_model=SamEmbeddingResponse,\n                summary=\"SAM Image Embeddings\",\n                description=\"Run the Meta AI Segmant Anything Model to embed image data.\",\n            )\n            @with_route_exceptions\n            async def sam_embed_image(\n                inference_request: SamEmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                Args:\n                    inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.SamEmbeddingResponse or Response: The response containing the embedded image.\n                \"\"\"\n                logger.debug(f\"Reached /sam/embed_image\")\n                sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                model_response = await self.model_manager.infer_from_request(\n                    sam_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(sam_model_id, actor)\n                if inference_request.format == \"binary\":\n                    return Response(\n                        content=model_response.embeddings,\n                        headers={\"Content-Type\": \"application/octet-stream\"},\n                    )\n                return model_response\n\n            @app.post(\n                \"/sam/segment_image\",\n                response_model=SamSegmentationResponse,\n                summary=\"SAM Image Segmentation\",\n                description=\"Run the Meta AI Segmant Anything Model to generate segmenations for image data.\",\n            )\n            @with_route_exceptions\n            async def sam_segment_image(\n                inference_request: SamSegmentationRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Generates segmentations for image data using the Meta AI Segmant Anything Model (SAM).\n\n                Args:\n                    inference_request (SamSegmentationRequest): The request containing the image to be segmented.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.SamSegmentationResponse or Response: The response containing the segmented image.\n                \"\"\"\n                logger.debug(f\"Reached /sam/segment_image\")\n                sam_model_id = load_sam_model(inference_request, api_key=api_key)\n                model_response = await self.model_manager.infer_from_request(\n                    sam_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(sam_model_id, actor)\n                if inference_request.format == \"binary\":\n                    return Response(\n                        content=model_response,\n                        headers={\"Content-Type\": \"application/octet-stream\"},\n                    )\n                return model_response\n\n        if CORE_MODEL_SAM2_ENABLED:\n\n            @app.post(\n                \"/sam2/embed_image\",\n                response_model=Sam2EmbeddingResponse,\n                summary=\"SAM2 Image Embeddings\",\n                description=\"Run the Meta AI Segment Anything 2 Model to embed image data.\",\n            )\n            @with_route_exceptions\n            async def sam2_embed_image(\n                inference_request: Sam2EmbeddingRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Embeds image data using the Meta AI Segment Anything Model (SAM).\n\n                Args:\n                    inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.Sam2EmbeddingResponse or Response: The response affirming the image has been embedded\n                \"\"\"\n                logger.debug(f\"Reached /sam2/embed_image\")\n                sam2_model_id = load_sam2_model(inference_request, api_key=api_key)\n                model_response = await self.model_manager.infer_from_request(\n                    sam2_model_id, inference_request\n                )\n                return model_response\n\n            @app.post(\n                \"/sam2/segment_image\",\n                response_model=Sam2SegmentationResponse,\n                summary=\"SAM2 Image Segmentation\",\n                description=\"Run the Meta AI Segment Anything 2 Model to generate segmenations for image data.\",\n            )\n            @with_route_exceptions\n            async def sam2_segment_image(\n                inference_request: Sam2SegmentationRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Generates segmentations for image data using the Meta AI Segment Anything Model (SAM).\n\n                Args:\n                    inference_request (Sam2SegmentationRequest): The request containing the image to be segmented.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.SamSegmentationResponse or Response: The response containing the segmented image.\n                \"\"\"\n                logger.debug(f\"Reached /sam2/segment_image\")\n                sam2_model_id = load_sam2_model(inference_request, api_key=api_key)\n                model_response = await self.model_manager.infer_from_request(\n                    sam2_model_id, inference_request\n                )\n                if inference_request.format == \"binary\":\n                    return Response(\n                        content=model_response,\n                        headers={\"Content-Type\": \"application/octet-stream\"},\n                    )\n                return model_response\n\n        if CORE_MODEL_OWLV2_ENABLED:\n\n            @app.post(\n                \"/owlv2/infer\",\n                response_model=ObjectDetectionInferenceResponse,\n                summary=\"Owlv2 image prompting\",\n                description=\"Run the google owlv2 model to few-shot object detect\",\n            )\n            @with_route_exceptions\n            async def owlv2_infer(\n                inference_request: OwlV2InferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Embeds image data using the Meta AI Segmant Anything Model (SAM).\n\n                Args:\n                    inference_request (SamEmbeddingRequest): The request containing the image to be embedded.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.Sam2EmbeddingResponse or Response: The response affirming the image has been embedded\n                \"\"\"\n                logger.debug(f\"Reached /owlv2/infer\")\n                owl2_model_id = load_owlv2_model(inference_request, api_key=api_key)\n                model_response = await self.model_manager.infer_from_request(\n                    owl2_model_id, inference_request\n                )\n                return model_response\n\n        if CORE_MODEL_GAZE_ENABLED:\n\n            @app.post(\n                \"/gaze/gaze_detection\",\n                response_model=List[GazeDetectionInferenceResponse],\n                summary=\"Gaze Detection\",\n                description=\"Run the gaze detection model to detect gaze.\",\n            )\n            @with_route_exceptions\n            async def gaze_detection(\n                inference_request: GazeDetectionInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Detect gaze using the gaze detection model.\n\n                Args:\n                    inference_request (M.GazeDetectionRequest): The request containing the image to be detected.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.GazeDetectionResponse: The response containing all the detected faces and the corresponding gazes.\n                \"\"\"\n                logger.debug(f\"Reached /gaze/gaze_detection\")\n                gaze_model_id = load_gaze_model(inference_request, api_key=api_key)\n                response = await self.model_manager.infer_from_request(\n                    gaze_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(gaze_model_id, actor)\n                return response\n\n        if CORE_MODEL_COGVLM_ENABLED:\n\n            @app.post(\n                \"/llm/cogvlm\",\n                response_model=CogVLMResponse,\n                summary=\"CogVLM\",\n                description=\"Run the CogVLM model to chat or describe an image.\",\n            )\n            @with_route_exceptions\n            async def cog_vlm(\n                inference_request: CogVLMInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Chat with CogVLM or ask it about an image. Multi-image requests not currently supported.\n\n                Args:\n                    inference_request (M.CogVLMInferenceRequest): The request containing the prompt and image to be described.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    M.CogVLMResponse: The model's text response\n                \"\"\"\n                logger.debug(f\"Reached /llm/cogvlm\")\n                cog_model_id = load_cogvlm_model(inference_request, api_key=api_key)\n                response = await self.model_manager.infer_from_request(\n                    cog_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(cog_model_id, actor)\n                return response\n\n        if CORE_MODEL_TROCR_ENABLED:\n\n            @app.post(\n                \"/ocr/trocr\",\n                response_model=OCRInferenceResponse,\n                summary=\"TrOCR OCR response\",\n                description=\"Run the TrOCR model to retrieve text in an image.\",\n            )\n            @with_route_exceptions\n            async def trocr_retrieve_text(\n                inference_request: TrOCRInferenceRequest,\n                request: Request,\n                api_key: Optional[str] = Query(\n                    None,\n                    description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n                ),\n            ):\n                \"\"\"\n                Retrieves text from image data using the TrOCR model.\n\n                Args:\n                    inference_request (TrOCRInferenceRequest): The request containing the image from which to retrieve text.\n                    api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                    request (Request, default Body()): The HTTP request.\n\n                Returns:\n                    OCRInferenceResponse: The response containing the retrieved text.\n                \"\"\"\n                logger.debug(f\"Reached /trocr/ocr\")\n                trocr_model_id = load_trocr_model(\n                    inference_request, api_key=api_key\n                )\n                response = await self.model_manager.infer_from_request(\n                    trocr_model_id, inference_request\n                )\n                if LAMBDA:\n                    actor = request.scope[\"aws.event\"][\"requestContext\"][\n                        \"authorizer\"\n                    ][\"lambda\"][\"actor\"]\n                    trackUsage(trocr_model_id, actor)\n                return response\n\n    if not LAMBDA:\n\n        @app.get(\n            \"/notebook/start\",\n            summary=\"Jupyter Lab Server Start\",\n            description=\"Starts a jupyter lab server for running development code\",\n        )\n        @with_route_exceptions\n        async def notebook_start(browserless: bool = False):\n            \"\"\"Starts a jupyter lab server for running development code.\n\n            Args:\n                inference_request (NotebookStartRequest): The request containing the necessary details for starting a jupyter lab server.\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n\n            Returns:\n                NotebookStartResponse: The response containing the URL of the jupyter lab server.\n            \"\"\"\n            logger.debug(f\"Reached /notebook/start\")\n            if NOTEBOOK_ENABLED:\n                start_notebook()\n                if browserless:\n                    return {\n                        \"success\": True,\n                        \"message\": f\"Jupyter Lab server started at http://localhost:{NOTEBOOK_PORT}?token={NOTEBOOK_PASSWORD}\",\n                    }\n                else:\n                    sleep(2)\n                    return RedirectResponse(\n                        f\"http://localhost:{NOTEBOOK_PORT}/lab/tree/quickstart.ipynb?token={NOTEBOOK_PASSWORD}\"\n                    )\n            else:\n                if browserless:\n                    return {\n                        \"success\": False,\n                        \"message\": \"Notebook server is not enabled. Enable notebooks via the NOTEBOOK_ENABLED environment variable.\",\n                    }\n                else:\n                    return RedirectResponse(f\"/notebook-instructions.html\")\n\n    if LEGACY_ROUTE_ENABLED:\n\n        class IntStringConvertor(StringConvertor):\n            \"\"\"\n            Match digits but keep them as string.\n            \"\"\"\n\n            regex = \"\\d+\"\n\n        register_url_convertor(\"int_string\", IntStringConvertor())\n\n        # Legacy object detection inference path for backwards compatability\n        @app.get(\n            \"/{dataset_id}/{version_id:int_string}\",\n            # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n            response_model=Union[\n                InstanceSegmentationInferenceResponse,\n                KeypointsDetectionInferenceResponse,\n                ObjectDetectionInferenceResponse,\n                ClassificationInferenceResponse,\n                MultiLabelClassificationInferenceResponse,\n                StubResponse,\n                Any,\n            ],\n            response_model_exclude_none=True,\n        )\n        @app.post(\n            \"/{dataset_id}/{version_id:int_string}\",\n            # Order matters in this response model Union. It will use the first matching model. For example, Object Detection Inference Response is a subset of Instance segmentation inference response, so instance segmentation must come first in order for the matching logic to work.\n            response_model=Union[\n                InstanceSegmentationInferenceResponse,\n                KeypointsDetectionInferenceResponse,\n                ObjectDetectionInferenceResponse,\n                ClassificationInferenceResponse,\n                MultiLabelClassificationInferenceResponse,\n                StubResponse,\n                Any,\n            ],\n            response_model_exclude_none=True,\n        )\n        @with_route_exceptions\n        async def legacy_infer_from_request(\n            background_tasks: BackgroundTasks,\n            request: Request,\n            dataset_id: str = Path(\n                description=\"ID of a Roboflow dataset corresponding to the model to use for inference\"\n            ),\n            version_id: str = Path(\n                description=\"ID of a Roboflow dataset version corresponding to the model to use for inference\"\n            ),\n            api_key: Optional[str] = Query(\n                None,\n                description=\"Roboflow API Key that will be passed to the model during initialization for artifact retrieval\",\n            ),\n            confidence: float = Query(\n                0.4,\n                description=\"The confidence threshold used to filter out predictions\",\n            ),\n            keypoint_confidence: float = Query(\n                0.0,\n                description=\"The confidence threshold used to filter out keypoints that are not visible based on model confidence\",\n            ),\n            format: str = Query(\n                \"json\",\n                description=\"One of 'json' or 'image'. If 'json' prediction data is return as a JSON string. If 'image' prediction data is visualized and overlayed on the original input image.\",\n            ),\n            image: Optional[str] = Query(\n                None,\n                description=\"The publically accessible URL of an image to use for inference.\",\n            ),\n            image_type: Optional[str] = Query(\n                \"base64\",\n                description=\"One of base64 or numpy. Note, numpy input is not supported for Roboflow Hosted Inference.\",\n            ),\n            labels: Optional[bool] = Query(\n                False,\n                description=\"If true, labels will be include in any inference visualization.\",\n            ),\n            mask_decode_mode: Optional[str] = Query(\n                \"accurate\",\n                description=\"One of 'accurate' or 'fast'. If 'accurate' the mask will be decoded using the original image size. If 'fast' the mask will be decoded using the original mask size. 'accurate' is slower but more accurate.\",\n            ),\n            tradeoff_factor: Optional[float] = Query(\n                0.0,\n                description=\"The amount to tradeoff between 0='fast' and 1='accurate'\",\n            ),\n            max_detections: int = Query(\n                300,\n                description=\"The maximum number of detections to return. This is used to limit the number of predictions returned by the model. The model may return more predictions than this number, but only the top `max_detections` predictions will be returned.\",\n            ),\n            overlap: float = Query(\n                0.3,\n                description=\"The IoU threhsold that must be met for a box pair to be considered duplicate during NMS\",\n            ),\n            stroke: int = Query(\n                1, description=\"The stroke width used when visualizing predictions\"\n            ),\n            countinference: Optional[bool] = Query(\n                True,\n                description=\"If false, does not track inference against usage.\",\n                include_in_schema=False,\n            ),\n            service_secret: Optional[str] = Query(\n                None,\n                description=\"Shared secret used to authenticate requests to the inference server from internal services (e.g. to allow disabling inference usage tracking via the `countinference` query parameter)\",\n                include_in_schema=False,\n            ),\n            disable_preproc_auto_orient: Optional[bool] = Query(\n                False, description=\"If true, disables automatic image orientation\"\n            ),\n            disable_preproc_contrast: Optional[bool] = Query(\n                False, description=\"If true, disables automatic contrast adjustment\"\n            ),\n            disable_preproc_grayscale: Optional[bool] = Query(\n                False,\n                description=\"If true, disables automatic grayscale conversion\",\n            ),\n            disable_preproc_static_crop: Optional[bool] = Query(\n                False, description=\"If true, disables automatic static crop\"\n            ),\n            disable_active_learning: Optional[bool] = Query(\n                default=False,\n                description=\"If true, the predictions will be prevented from registration by Active Learning (if the functionality is enabled)\",\n            ),\n            active_learning_target_dataset: Optional[str] = Query(\n                default=None,\n                description=\"Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id\",\n            ),\n            source: Optional[str] = Query(\n                \"external\",\n                description=\"The source of the inference request\",\n            ),\n            source_info: Optional[str] = Query(\n                \"external\",\n                description=\"The detailed source information of the inference request\",\n            ),\n        ):\n            \"\"\"\n            Legacy inference endpoint for object detection, instance segmentation, and classification.\n\n            Args:\n                background_tasks: (BackgroundTasks) pool of fastapi background tasks\n                dataset_id (str): ID of a Roboflow dataset corresponding to the model to use for inference.\n                version_id (str): ID of a Roboflow dataset version corresponding to the model to use for inference.\n                api_key (Optional[str], default None): Roboflow API Key passed to the model during initialization for artifact retrieval.\n                # Other parameters described in the function signature...\n\n            Returns:\n                Union[InstanceSegmentationInferenceResponse, KeypointsDetectionInferenceRequest, ObjectDetectionInferenceResponse, ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse, Any]: The response containing the inference results.\n            \"\"\"\n            logger.debug(\n                f\"Reached legacy route /:dataset_id/:version_id with {dataset_id}/{version_id}\"\n            )\n            model_id = f\"{dataset_id}/{version_id}\"\n\n            if confidence &gt;= 1:\n                confidence /= 100\n            elif confidence &lt; 0.01:\n                confidence = 0.01\n\n            if overlap &gt;= 1:\n                overlap /= 100\n\n            if image is not None:\n                request_image = InferenceRequestImage(type=\"url\", value=image)\n            else:\n                if \"Content-Type\" not in request.headers:\n                    raise ContentTypeMissing(\n                        f\"Request must include a Content-Type header\"\n                    )\n                if \"multipart/form-data\" in request.headers[\"Content-Type\"]:\n                    form_data = await request.form()\n                    base64_image_str = await form_data[\"file\"].read()\n                    base64_image_str = base64.b64encode(base64_image_str)\n                    request_image = InferenceRequestImage(\n                        type=\"base64\", value=base64_image_str.decode(\"ascii\")\n                    )\n                elif (\n                    \"application/x-www-form-urlencoded\"\n                    in request.headers[\"Content-Type\"]\n                    or \"application/json\" in request.headers[\"Content-Type\"]\n                ):\n                    data = await request.body()\n                    request_image = InferenceRequestImage(\n                        type=image_type, value=data\n                    )\n                else:\n                    raise ContentTypeInvalid(\n                        f\"Invalid Content-Type: {request.headers['Content-Type']}\"\n                    )\n\n            if not countinference and service_secret != ROBOFLOW_SERVICE_SECRET:\n                raise MissingServiceSecretError(\n                    \"Service secret is required to disable inference usage tracking\"\n                )\n            if LAMBDA:\n                request_model_id = (\n                    request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                        \"lambda\"\n                    ][\"model\"][\"endpoint\"]\n                    .replace(\"--\", \"/\")\n                    .replace(\"rf-\", \"\")\n                    .replace(\"nu-\", \"\")\n                )\n                actor = request.scope[\"aws.event\"][\"requestContext\"][\"authorizer\"][\n                    \"lambda\"\n                ][\"actor\"]\n                if countinference:\n                    trackUsage(request_model_id, actor)\n                else:\n                    if service_secret != ROBOFLOW_SERVICE_SECRET:\n                        raise MissingServiceSecretError(\n                            \"Service secret is required to disable inference usage tracking\"\n                        )\n            else:\n                request_model_id = model_id\n            logger.debug(\n                f\"State of model registry: {self.model_manager.describe_models()}\"\n            )\n            self.model_manager.add_model(\n                request_model_id, api_key, model_id_alias=model_id\n            )\n\n            task_type = self.model_manager.get_task_type(model_id, api_key=api_key)\n            inference_request_type = ObjectDetectionInferenceRequest\n            args = dict()\n            if task_type == \"instance-segmentation\":\n                inference_request_type = InstanceSegmentationInferenceRequest\n                args = {\n                    \"mask_decode_mode\": mask_decode_mode,\n                    \"tradeoff_factor\": tradeoff_factor,\n                }\n            elif task_type == \"classification\":\n                inference_request_type = ClassificationInferenceRequest\n            elif task_type == \"keypoint-detection\":\n                inference_request_type = KeypointsDetectionInferenceRequest\n                args = {\"keypoint_confidence\": keypoint_confidence}\n            inference_request = inference_request_type(\n                api_key=api_key,\n                model_id=model_id,\n                image=request_image,\n                confidence=confidence,\n                iou_threshold=overlap,\n                max_detections=max_detections,\n                visualization_labels=labels,\n                visualization_stroke_width=stroke,\n                visualize_predictions=(\n                    format == \"image\" or format == \"image_and_json\"\n                ),\n                disable_preproc_auto_orient=disable_preproc_auto_orient,\n                disable_preproc_contrast=disable_preproc_contrast,\n                disable_preproc_grayscale=disable_preproc_grayscale,\n                disable_preproc_static_crop=disable_preproc_static_crop,\n                disable_active_learning=disable_active_learning,\n                active_learning_target_dataset=active_learning_target_dataset,\n                source=source,\n                source_info=source_info,\n                usage_billable=countinference,\n                **args,\n            )\n            inference_response = await self.model_manager.infer_from_request(\n                inference_request.model_id,\n                inference_request,\n                active_learning_eligible=True,\n                background_tasks=background_tasks,\n            )\n            logger.debug(\"Response ready.\")\n            if format == \"image\":\n                return Response(\n                    content=inference_response.visualization,\n                    media_type=\"image/jpeg\",\n                )\n            else:\n                return orjson_response(inference_response)\n\n    if not LAMBDA:\n        # Legacy clear cache endpoint for backwards compatability\n        @app.get(\"/clear_cache\", response_model=str)\n        async def legacy_clear_cache():\n            \"\"\"\n            Clears the model cache.\n\n            This endpoint provides a way to clear the cache of loaded models.\n\n            Returns:\n                str: A string indicating that the cache has been cleared.\n            \"\"\"\n            logger.debug(f\"Reached /clear_cache\")\n            await model_clear()\n            return \"Cache Cleared\"\n\n        # Legacy add model endpoint for backwards compatability\n        @app.get(\"/start/{dataset_id}/{version_id}\")\n        async def model_add(dataset_id: str, version_id: str, api_key: str = None):\n            \"\"\"\n            Starts a model inference session.\n\n            This endpoint initializes and starts an inference session for the specified model version.\n\n            Args:\n                dataset_id (str): ID of a Roboflow dataset corresponding to the model.\n                version_id (str): ID of a Roboflow dataset version corresponding to the model.\n                api_key (str, optional): Roboflow API Key for artifact retrieval.\n\n            Returns:\n                JSONResponse: A response object containing the status and a success message.\n            \"\"\"\n            logger.debug(\n                f\"Reached /start/{dataset_id}/{version_id} with {dataset_id}/{version_id}\"\n            )\n            model_id = f\"{dataset_id}/{version_id}\"\n            self.model_manager.add_model(model_id, api_key)\n\n            return JSONResponse(\n                {\n                    \"status\": 200,\n                    \"message\": \"inference session started from local memory.\",\n                }\n            )\n\n    app.mount(\n        \"/\",\n        StaticFiles(directory=\"./inference/landing/out\", html=True),\n        name=\"static\",\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/http/http_api/#inference.core.interfaces.http.http_api.with_route_exceptions","title":"<code>with_route_exceptions(route)</code>","text":"<p>A decorator that wraps a FastAPI route to handle specific exceptions. If an exception is caught, it returns a JSON response with the error message.</p> <p>Parameters:</p> Name Type Description Default <code>route</code> <code>Callable</code> <p>The FastAPI route to be wrapped.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <p>The wrapped route.</p> Source code in <code>inference/core/interfaces/http/http_api.py</code> <pre><code>def with_route_exceptions(route):\n    \"\"\"\n    A decorator that wraps a FastAPI route to handle specific exceptions. If an exception\n    is caught, it returns a JSON response with the error message.\n\n    Args:\n        route (Callable): The FastAPI route to be wrapped.\n\n    Returns:\n        Callable: The wrapped route.\n    \"\"\"\n\n    @wraps(route)\n    async def wrapped_route(*args, **kwargs):\n        try:\n            return await route(*args, **kwargs)\n        except ContentTypeInvalid:\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Invalid Content-Type header provided with request.\"\n                },\n            )\n            traceback.print_exc()\n        except ContentTypeMissing:\n            resp = JSONResponse(\n                status_code=400,\n                content={\"message\": \"Content-Type header not provided with request.\"},\n            )\n            traceback.print_exc()\n        except InputImageLoadError as e:\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": f\"Could not load input image. Cause: {e.get_public_error_details()}\"\n                },\n            )\n            traceback.print_exc()\n        except InvalidModelIDError:\n            resp = JSONResponse(\n                status_code=400,\n                content={\"message\": \"Invalid Model ID sent in request.\"},\n            )\n            traceback.print_exc()\n        except InvalidMaskDecodeArgument:\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Invalid mask decode argument sent. tradeoff_factor must be in [0.0, 1.0], \"\n                    \"mask_decode_mode: must be one of ['accurate', 'fast', 'tradeoff']\"\n                },\n            )\n            traceback.print_exc()\n        except MissingApiKeyError:\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": \"Required Roboflow API key is missing. Visit https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key \"\n                    \"to learn how to retrieve one.\"\n                },\n            )\n            traceback.print_exc()\n        except (\n            WorkflowDefinitionError,\n            ExecutionGraphStructureError,\n            ReferenceTypeError,\n            InvalidReferenceTargetError,\n            RuntimeInputError,\n            InvalidInputTypeError,\n            OperationTypeNotRecognisedError,\n            DynamicBlockError,\n            WorkflowExecutionEngineVersionError,\n            NotSupportedExecutionEngineError,\n        ) as error:\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"context\": error.context,\n                    \"inner_error_type\": error.inner_error_type,\n                    \"inner_error_message\": str(error.inner_error),\n                },\n            )\n        except (\n            ProcessesManagerInvalidPayload,\n            MalformedPayloadError,\n            MessageToBigError,\n        ) as error:\n            resp = JSONResponse(\n                status_code=400,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"inner_error_type\": error.inner_error_type,\n                },\n            )\n        except (RoboflowAPINotAuthorizedError, ProcessesManagerAuthorisationError):\n            resp = JSONResponse(\n                status_code=401,\n                content={\n                    \"message\": \"Unauthorized access to roboflow API - check API key and make sure the key is valid for \"\n                    \"workspace you use. Visit https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key \"\n                    \"to learn how to retrieve one.\"\n                },\n            )\n            traceback.print_exc()\n        except (RoboflowAPINotNotFoundError, InferenceModelNotFound):\n            resp = JSONResponse(\n                status_code=404,\n                content={\n                    \"message\": \"Requested Roboflow resource not found. Make sure that workspace, project or model \"\n                    \"you referred in request exists.\"\n                },\n            )\n            traceback.print_exc()\n        except ProcessesManagerNotFoundError as error:\n            resp = JSONResponse(\n                status_code=404,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"inner_error_type\": error.inner_error_type,\n                },\n            )\n            traceback.print_exc()\n        except (\n            InvalidEnvironmentVariableError,\n            MissingServiceSecretError,\n            ServiceConfigurationError,\n        ):\n            resp = JSONResponse(\n                status_code=500, content={\"message\": \"Service misconfiguration.\"}\n            )\n            traceback.print_exc()\n        except (\n            PreProcessingError,\n            PostProcessingError,\n        ):\n            resp = JSONResponse(\n                status_code=500,\n                content={\n                    \"message\": \"Model configuration related to pre- or post-processing is invalid.\"\n                },\n            )\n            traceback.print_exc()\n        except ModelArtefactError:\n            resp = JSONResponse(\n                status_code=500, content={\"message\": \"Model package is broken.\"}\n            )\n            traceback.print_exc()\n        except OnnxProviderNotAvailable:\n            resp = JSONResponse(\n                status_code=501,\n                content={\n                    \"message\": \"Could not find requested ONNX Runtime Provider. Check that you are using \"\n                    \"the correct docker image on a supported device.\"\n                },\n            )\n            traceback.print_exc()\n        except (\n            MalformedRoboflowAPIResponseError,\n            RoboflowAPIUnsuccessfulRequestError,\n            WorkspaceLoadError,\n            MalformedWorkflowResponseError,\n        ):\n            resp = JSONResponse(\n                status_code=502,\n                content={\"message\": \"Internal error. Request to Roboflow API failed.\"},\n            )\n            traceback.print_exc()\n        except RoboflowAPIConnectionError:\n            resp = JSONResponse(\n                status_code=503,\n                content={\n                    \"message\": \"Internal error. Could not connect to Roboflow API.\"\n                },\n            )\n            traceback.print_exc()\n        except WorkflowError as error:\n            resp = JSONResponse(\n                status_code=500,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"context\": error.context,\n                    \"inner_error_type\": error.inner_error_type,\n                    \"inner_error_message\": str(error.inner_error),\n                },\n            )\n            traceback.print_exc()\n        except (\n            ProcessesManagerClientError,\n            CommunicationProtocolError,\n        ) as error:\n            resp = JSONResponse(\n                status_code=500,\n                content={\n                    \"message\": error.public_message,\n                    \"error_type\": error.__class__.__name__,\n                    \"inner_error_type\": error.inner_error_type,\n                },\n            )\n            traceback.print_exc()\n        except Exception:\n            resp = JSONResponse(status_code=500, content={\"message\": \"Internal error.\"})\n            traceback.print_exc()\n        return resp\n\n    return wrapped_route\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/http/orjson_utils/","title":"orjson_utils","text":""},{"location":"docs/reference/inference/core/interfaces/http/handlers/workflows/","title":"workflows","text":""},{"location":"docs/reference/inference/core/interfaces/http/middlewares/gzip/","title":"gzip","text":""},{"location":"docs/reference/inference/core/interfaces/stream/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/","title":"inference_pipeline","text":""},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/#inference.core.interfaces.stream.inference_pipeline.InferencePipeline","title":"<code>InferencePipeline</code>","text":"Source code in <code>inference/core/interfaces/stream/inference_pipeline.py</code> <pre><code>class InferencePipeline:\n    @classmethod\n    def init(\n        cls,\n        video_reference: Union[VideoSourceIdentifier, List[VideoSourceIdentifier]],\n        model_id: str,\n        on_prediction: SinkHandler = None,\n        api_key: Optional[str] = None,\n        max_fps: Optional[Union[float, int]] = None,\n        watchdog: Optional[PipelineWatchDog] = None,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        class_agnostic_nms: Optional[bool] = None,\n        confidence: Optional[float] = None,\n        iou_threshold: Optional[float] = None,\n        max_candidates: Optional[int] = None,\n        max_detections: Optional[int] = None,\n        mask_decode_mode: Optional[str] = \"accurate\",\n        tradeoff_factor: Optional[float] = 0.0,\n        active_learning_enabled: Optional[bool] = None,\n        video_source_properties: Optional[\n            Union[Dict[str, float], List[Optional[Dict[str, float]]]]\n        ] = None,\n        active_learning_target_dataset: Optional[str] = None,\n        batch_collection_timeout: Optional[float] = None,\n        sink_mode: SinkMode = SinkMode.ADAPTIVE,\n    ) -&gt; \"InferencePipeline\":\n        \"\"\"\n        This class creates the abstraction for making inferences from Roboflow models against video stream.\n        It allows to choose model from Roboflow platform and run predictions against\n        video streams - just by the price of specifying which model to use and what to do with predictions.\n\n        It allows to set the model post-processing parameters (via .init() or env) and intercept updates\n        related to state of pipeline via `PipelineWatchDog` abstraction (although that is something probably\n        useful only for advanced use-cases).\n\n        For maximum efficiency, all separate chunks of processing: video decoding, inference, results dispatching\n        are handled by separate threads.\n\n        Given that reference to stream is passed and connectivity is lost - it attempts to re-connect with delay.\n\n        Since version 0.9.11 it works not only for object detection models but is also compatible with stubs,\n        classification, instance-segmentation and keypoint-detection models.\n\n        Since version 0.9.18, `InferencePipeline` is capable of handling multiple video sources at once. If multiple\n        sources are provided - source multiplexing will happen. One of the change introduced in that release is switch\n        from `get_video_frames_generator(...)` as video frames provider into `multiplex_videos(...)`. For a single\n        video source, the behaviour of `InferencePipeline` is remained unchanged when default parameters are used.\n        For multiple videos - frames are multiplexed, and we can adjust the pipeline behaviour using new configuration\n        options. `batch_collection_timeout` is one of the new option - it is the parameter of `multiplex_videos(...)`\n        that dictates how long the batch frames collection process may wait for all sources to provide video frame.\n        It can be set infinite (None) or with specific value representing fraction of second. We advise that value to\n        be set in production solutions to avoid processing slow-down caused by source with unstable latency spikes.\n        For more information on multiplexing process - please visit `multiplex_videos(...)` function docs.\n        Another change is the way on how sinks work. They can work in `SinkMode.ADAPTIVE` - which means that\n        video frames and predictions will be either provided to sink as list of objects, or specific elements -\n        and the determining factor is number of sources (it will behave SEQUENTIAL for one source and BATCH if multiple\n        ones are provided). All old sinks were adjusted to work in both modes, custom ones should be migrated\n        to reflect changes in sink function signature.\n\n        Args:\n            model_id (str): Name and version of model on the Roboflow platform (example: \"my-model/3\")\n            video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n                predictions against. It can be video file path, stream URL and device (like camera) id\n                (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n                it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n                `sink_mode` parameter comments.\n            on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n                once prediction is ready - passing both decoded frame, their metadata and dict with standard\n                Roboflow model prediction (different for specific types of models).\n            api_key (Optional[str]): Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\"\n                and \"API_KEY\" variables. API key, passed in some form is required.\n            max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n                dictate max FPS of each video source.\n                The implementation details of this option has been changed in release `v0.26.0`. Prior to the release\n                this value, when applied to video files caused the processing to wait `1 / max_fps` seconds before next\n                frame is processed - the new implementation drops the intermediate frames, which seems to be more\n                aligned with peoples expectations.\n                New behaviour is now enabled in experimental mode, by setting environmental variable flag\n                `ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True`. Please note that the new behaviour will\n                be the default one end of Q4 2024!\n            watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n                inference pipeline - if not given null implementation (doing nothing) will be used.\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n                status updates of all elements of the pipeline. Should be used only if detailed inspection of\n                pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n                fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n                without re-raising. Default: None.\n            source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n                video stream decoding behaviour. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n            source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n                video stream frames consumption. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n            class_agnostic_nms (Optional[bool]): Parameter of model post-processing. If not given - value checked in\n                env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"\n            confidence (Optional[float]): Parameter of model post-processing. If not given - value checked in\n                env variable \"CONFIDENCE\" with default \"0.5\"\n            iou_threshold (Optional[float]): Parameter of model post-processing. If not given - value checked in\n                env variable \"IOU_THRESHOLD\" with default \"0.5\"\n            max_candidates (Optional[int]): Parameter of model post-processing. If not given - value checked in\n                env variable \"MAX_CANDIDATES\" with default \"3000\"\n            max_detections (Optional[int]): Parameter of model post-processing. If not given - value checked in\n                env variable \"MAX_DETECTIONS\" with default \"300\"\n            mask_decode_mode: (Optional[str]): Parameter of model post-processing. If not given - model \"accurate\" is\n                used. Applicable for instance segmentation models\n            tradeoff_factor (Optional[float]): Parameter of model post-processing. If not 0.0 - model default is used.\n                Applicable for instance segmentation models\n            active_learning_enabled (Optional[bool]): Flag to enable / disable Active Learning middleware (setting it\n                true does not guarantee any data to be collected, as data collection is controlled by Roboflow backend -\n                it just enables middleware intercepting predictions). If not given, env variable\n                `ACTIVE_LEARNING_ENABLED` will be used. Please point out that Active Learning will be forcefully\n                disabled in a scenario when Roboflow API key is not given, as Roboflow account is required\n                for this feature to be operational.\n            video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n                Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n                cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n                It is optional and if provided can be provided as single dict (applicable for all sources) or\n                as list of configs. Then the list must be of length of `video_reference` and may also contain None\n                values to denote that specific source should remain not configured.\n                Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n            active_learning_target_dataset (Optional[str]): Parameter to be used when Active Learning data registration\n                should happen against different dataset than the one pointed by model_id\n            batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n                to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n                frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n                unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n            sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n                handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n                in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n                in the order of video sources - with None values in the place of vide_frames / predictions that\n                were skipped due to `batch_collection_timeout`.\n                `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n                against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n                To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n                `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n                old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n                prediction element.\n\n        Other ENV variables involved in low-level configuration:\n        * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n        * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n        * ACTIVE_LEARNING_ENABLED - controls Active Learning middleware if explicit parameter not given\n\n        Returns: Instance of InferencePipeline\n\n        Throws:\n            * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n                always if connection to stream is lost.\n        \"\"\"\n        if api_key is None:\n            api_key = API_KEY\n        inference_config = ModelConfig.init(\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            iou_threshold=iou_threshold,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            mask_decode_mode=mask_decode_mode,\n            tradeoff_factor=tradeoff_factor,\n        )\n        model = get_model(model_id=model_id, api_key=api_key)\n        on_video_frame = partial(\n            default_process_frame, model=model, inference_config=inference_config\n        )\n        active_learning_middleware = NullActiveLearningMiddleware()\n        if active_learning_enabled is None:\n            logger.info(\n                f\"`active_learning_enabled` parameter not set - using env `ACTIVE_LEARNING_ENABLED` \"\n                f\"with value: {ACTIVE_LEARNING_ENABLED}\"\n            )\n            active_learning_enabled = ACTIVE_LEARNING_ENABLED\n        if api_key is None:\n            logger.info(\n                f\"Roboflow API key not given - Active Learning is forced to be disabled.\"\n            )\n            active_learning_enabled = False\n        if active_learning_enabled is True:\n            resolved_model_id = resolve_roboflow_model_alias(model_id=model_id)\n            target_dataset = (\n                active_learning_target_dataset or resolved_model_id.split(\"/\")[0]\n            )\n            active_learning_middleware = ThreadingActiveLearningMiddleware.init(\n                api_key=api_key,\n                target_dataset=target_dataset,\n                model_id=resolved_model_id,\n                cache=cache,\n            )\n            al_sink = partial(\n                active_learning_sink,\n                active_learning_middleware=active_learning_middleware,\n                model_type=model.task_type,\n                disable_preproc_auto_orient=DISABLE_PREPROC_AUTO_ORIENT,\n            )\n            logger.info(\n                \"AL enabled - wrapping `on_prediction` with multi_sink() and active_learning_sink()\"\n            )\n            on_prediction = partial(multi_sink, sinks=[on_prediction, al_sink])\n        on_pipeline_start = active_learning_middleware.start_registration_thread\n        on_pipeline_end = active_learning_middleware.stop_registration_thread\n        return cls.init_with_custom_logic(\n            video_reference=video_reference,\n            on_video_frame=on_video_frame,\n            on_prediction=on_prediction,\n            on_pipeline_start=on_pipeline_start,\n            on_pipeline_end=on_pipeline_end,\n            max_fps=max_fps,\n            watchdog=watchdog,\n            status_update_handlers=status_update_handlers,\n            source_buffer_filling_strategy=source_buffer_filling_strategy,\n            source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n            video_source_properties=video_source_properties,\n            batch_collection_timeout=batch_collection_timeout,\n            sink_mode=sink_mode,\n        )\n\n    @classmethod\n    def init_with_yolo_world(\n        cls,\n        video_reference: Union[str, int, List[Union[str, int]]],\n        classes: List[str],\n        model_size: str = \"s\",\n        on_prediction: SinkHandler = None,\n        max_fps: Optional[Union[float, int]] = None,\n        watchdog: Optional[PipelineWatchDog] = None,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        class_agnostic_nms: Optional[bool] = None,\n        confidence: Optional[float] = None,\n        iou_threshold: Optional[float] = None,\n        max_candidates: Optional[int] = None,\n        max_detections: Optional[int] = None,\n        video_source_properties: Optional[Dict[str, float]] = None,\n        batch_collection_timeout: Optional[float] = None,\n        sink_mode: SinkMode = SinkMode.ADAPTIVE,\n    ) -&gt; \"InferencePipeline\":\n        \"\"\"\n        This class creates the abstraction for making inferences from YoloWorld against video stream.\n        The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initializer\n        method.\n\n        Args:\n            video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n                predictions against. It can be video file path, stream URL and device (like camera) id\n                (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n                it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n                `sink_mode` parameter comments.\n            classes (List[str]): List of classes to execute zero-shot detection against\n            model_size (str): version of model - to be chosen from `s`, `m`, `l`\n            on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n                once prediction is ready - passing both decoded frame, their metadata and dict with standard\n                Roboflow Object Detection prediction.\n            max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n                dictate max FPS of each video source.\n                The implementation details of this option has been changed in release `v0.26.0`. Prior to the release\n                this value, when applied to video files caused the processing to wait `1 / max_fps` seconds before next\n                frame is processed - the new implementation drops the intermediate frames, which seems to be more\n                aligned with peoples expectations.\n                New behaviour is now enabled in experimental mode, by setting environmental variable flag\n                `ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True`. Please note that the new behaviour will\n                be the default one end of Q4 2024!\n            watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n                inference pipeline - if not given null implementation (doing nothing) will be used.\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n                status updates of all elements of the pipeline. Should be used only if detailed inspection of\n                pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n                fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n                without re-raising. Default: None.\n            source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n                video stream decoding behaviour. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n            source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n                video stream frames consumption. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n            class_agnostic_nms (Optional[bool]): Parameter of model post-processing. If not given - value checked in\n                env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"\n            confidence (Optional[float]): Parameter of model post-processing. If not given - value checked in\n                env variable \"CONFIDENCE\" with default \"0.5\"\n            iou_threshold (Optional[float]): Parameter of model post-processing. If not given - value checked in\n                env variable \"IOU_THRESHOLD\" with default \"0.5\"\n            max_candidates (Optional[int]): Parameter of model post-processing. If not given - value checked in\n                env variable \"MAX_CANDIDATES\" with default \"3000\"\n            max_detections (Optional[int]): Parameter of model post-processing. If not given - value checked in\n                env variable \"MAX_DETECTIONS\" with default \"300\"\n            video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n                Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n                cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n                It is optional and if provided can be provided as single dict (applicable for all sources) or\n                as list of configs. Then the list must be of length of `video_reference` and may also contain None\n                values to denote that specific source should remain not configured.\n                Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n            batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n                to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n                frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n                unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n            sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n                handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n                in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n                in the order of video sources - with None values in the place of vide_frames / predictions that\n                were skipped due to `batch_collection_timeout`.\n                `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n                against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n                To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n                `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n                old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n                prediction element.\n\n        Other ENV variables involved in low-level configuration:\n        * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n        * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n        Returns: Instance of InferencePipeline\n\n        Throws:\n            * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n                always if connection to stream is lost.\n        \"\"\"\n        inference_config = ModelConfig.init(\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            iou_threshold=iou_threshold,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n        )\n        try:\n            from inference.core.interfaces.stream.model_handlers.yolo_world import (\n                build_yolo_world_inference_function,\n            )\n\n            on_video_frame = build_yolo_world_inference_function(\n                model_id=f\"yolo_world/{model_size}\",\n                classes=classes,\n                inference_config=inference_config,\n            )\n        except ImportError as error:\n            raise CannotInitialiseModelError(\n                f\"Could not initialise yolo_world/{model_size} due to lack of sufficient dependencies. \"\n                f\"Use pip install inference[yolo-world] to install missing dependencies and try again.\"\n            ) from error\n        return cls.init_with_custom_logic(\n            video_reference=video_reference,\n            on_video_frame=on_video_frame,\n            on_prediction=on_prediction,\n            on_pipeline_start=None,\n            on_pipeline_end=None,\n            max_fps=max_fps,\n            watchdog=watchdog,\n            status_update_handlers=status_update_handlers,\n            source_buffer_filling_strategy=source_buffer_filling_strategy,\n            source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n            video_source_properties=video_source_properties,\n            batch_collection_timeout=batch_collection_timeout,\n            sink_mode=sink_mode,\n        )\n\n    @classmethod\n    @experimental(\n        reason=\"Usage of workflows with `InferencePipeline` is an experimental feature. Please report any issues \"\n        \"here: https://github.com/roboflow/inference/issues\"\n    )\n    def init_with_workflow(\n        cls,\n        video_reference: Union[str, int, List[Union[str, int]]],\n        workflow_specification: Optional[dict] = None,\n        workspace_name: Optional[str] = None,\n        workflow_id: Optional[str] = None,\n        api_key: Optional[str] = None,\n        image_input_name: str = \"image\",\n        workflows_parameters: Optional[Dict[str, Any]] = None,\n        on_prediction: SinkHandler = None,\n        max_fps: Optional[Union[float, int]] = None,\n        watchdog: Optional[PipelineWatchDog] = None,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        video_source_properties: Optional[Dict[str, float]] = None,\n        workflow_init_parameters: Optional[Dict[str, Any]] = None,\n        workflows_thread_pool_workers: int = 4,\n        cancel_thread_pool_tasks_on_exit: bool = True,\n        video_metadata_input_name: str = \"video_metadata\",\n        batch_collection_timeout: Optional[float] = None,\n        profiling_directory: str = \"./inference_profiling\",\n        use_workflow_definition_cache: bool = True,\n    ) -&gt; \"InferencePipeline\":\n        \"\"\"\n        This class creates the abstraction for making inferences from given workflow against video stream.\n        The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initializer\n        method.\n\n        Args:\n            video_reference (Union[str, int, List[Union[str, int]]]): Reference of source to be used to make predictions\n                against. It can be video file path, stream URL and device (like camera) id\n                (we handle whatever cv2 handles). It can also be a list of references (since v0.13.0) - and then\n                it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n                `sink_mode` parameter comments.\n            workflow_specification (Optional[dict]): Valid specification of workflow. See [workflow docs](https://github.com/roboflow/inference/tree/main/inference/enterprise/workflows).\n                It can be provided optionally, but if not given, both `workspace_name` and `workflow_id`\n                must be provided.\n            workspace_name (Optional[str]): When using registered workflows - Roboflow workspace name needs to be given.\n            workflow_id (Optional[str]): When using registered workflows - Roboflow workflow id needs to be given.\n            api_key (Optional[str]): Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\"\n                and \"API_KEY\" variables. API key, passed in some form is required.\n            image_input_name (str): Name of input image defined in `workflow_specification` or Workflow definition saved\n                on the Roboflow Platform. `InferencePipeline` will be injecting video frames to workflow through that\n                parameter name.\n            workflows_parameters (Optional[Dict[str, Any]]): Dictionary with additional parameters that can be\n                defined within `workflow_specification`.\n            on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n                once prediction is ready - passing both decoded frame, their metadata and dict with workflow output.\n            max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n                dictate max FPS of each video source.\n                The implementation details of this option has been changed in release `v0.26.0`. Prior to the release\n                this value, when applied to video files caused the processing to wait `1 / max_fps` seconds before next\n                frame is processed - the new implementation drops the intermediate frames, which seems to be more\n                aligned with peoples expectations.\n                New behaviour is now enabled in experimental mode, by setting environmental variable flag\n                `ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True`. Please note that the new behaviour will\n                be the default one end of Q4 2024!\n            watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n                inference pipeline - if not given null implementation (doing nothing) will be used.\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n                status updates of all elements of the pipeline. Should be used only if detailed inspection of\n                pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n                fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n                without re-raising. Default: None.\n            source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n                video stream decoding behaviour. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n            source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n                video stream frames consumption. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n            video_source_properties (Optional[dict[str, float]]): Optional source properties to set up the video source,\n                corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source\n                will be used.\n                Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n            workflow_init_parameters (Optional[Dict[str, Any]]): Additional init parameters to be used by\n                workflows Execution Engine to init steps of your workflow - may be required when running workflows\n                with custom plugins.\n            workflows_thread_pool_workers (int): Number of workers for workflows thread pool which is used\n                by workflows blocks to run background tasks.\n            cancel_thread_pool_tasks_on_exit (bool): Flag to decide if unstated background tasks should be\n                canceled at the end of InferencePipeline processing. By default, when video file ends or\n                pipeline is stopped, tasks that has not started will be cancelled.\n            video_metadata_input_name (str): Name of input for video metadata defined in `workflow_specification` or\n                Workflow definition saved  on the Roboflow Platform. `InferencePipeline` will be injecting video frames\n                metadata to workflows through that parameter name.\n            batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n                to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n                frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n                unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n            profiling_directory (str): Directory where workflows profiler traces will be dumped. To enable profiling\n                export `ENABLE_WORKFLOWS_PROFILING=True` environmental variable. You may specify number of workflow\n                runs in a buffer with environmental variable `WORKFLOWS_PROFILER_BUFFER_SIZE=n` - making last `n`\n                frames to be present in buffer on processing end.\n            use_workflow_definition_cache (bool): Controls usage of cache for workflow definitions. Set this to False\n                when you frequently modify definition saved in Roboflow app and want to fetch the\n                newest version for the request. Only applies for Workflows definitions saved on Roboflow platform.\n\n        Other ENV variables involved in low-level configuration:\n        * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n        * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n        Returns: Instance of InferencePipeline\n\n        Throws:\n            * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n                always if connection to stream is lost.\n            * ValueError if workflow specification not provided and registered workflow not pointed out\n            * NotImplementedError if workflow used against multiple videos which is not supported yet\n            * MissingApiKeyError - if API key is not provided in situation when retrieving workflow definition\n                from Roboflow API is needed\n        \"\"\"\n        if ENABLE_WORKFLOWS_PROFILING:\n            profiler = BaseWorkflowsProfiler.init(\n                max_runs_in_buffer=WORKFLOWS_PROFILER_BUFFER_SIZE\n            )\n        else:\n            profiler = NullWorkflowsProfiler.init()\n        if api_key is None:\n            api_key = API_KEY\n        named_workflow_specified = (workspace_name is not None) and (\n            workflow_id is not None\n        )\n        if not (named_workflow_specified != (workflow_specification is not None)):\n            raise ValueError(\n                \"Parameters (`workspace_name`, `workflow_id`) can be used mutually exclusive with \"\n                \"`workflow_specification`, but at least one must be set.\"\n            )\n        try:\n            from inference.core.interfaces.stream.model_handlers.workflows import (\n                WorkflowRunner,\n            )\n            from inference.core.roboflow_api import get_workflow_specification\n            from inference.core.workflows.execution_engine.core import ExecutionEngine\n\n            if workflow_specification is None:\n                if api_key is None:\n                    raise MissingApiKeyError(\n                        \"Roboflow API key needs to be provided either as parameter or via env variable \"\n                        \"ROBOFLOW_API_KEY. If you do not know how to get API key - visit \"\n                        \"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key to learn how to \"\n                        \"retrieve one.\"\n                    )\n                with profiler.profile_execution_phase(\n                    name=\"workflow_definition_fetching\",\n                    categories=[\"inference_package_operation\"],\n                ):\n                    workflow_specification = get_workflow_specification(\n                        api_key=api_key,\n                        workspace_id=workspace_name,\n                        workflow_id=workflow_id,\n                        use_cache=use_workflow_definition_cache,\n                    )\n            model_registry = RoboflowModelRegistry(ROBOFLOW_MODEL_TYPES)\n            model_manager = BackgroundTaskActiveLearningManager(\n                model_registry=model_registry, cache=cache\n            )\n            model_manager = WithFixedSizeCache(\n                model_manager,\n                max_size=MAX_ACTIVE_MODELS,\n            )\n            if api_key is None:\n                api_key = API_KEY\n            if workflow_init_parameters is None:\n                workflow_init_parameters = {}\n            thread_pool_executor = ThreadPoolExecutor(\n                max_workers=workflows_thread_pool_workers\n            )\n            workflow_init_parameters[\"workflows_core.model_manager\"] = model_manager\n            workflow_init_parameters[\"workflows_core.api_key\"] = api_key\n            workflow_init_parameters[\"workflows_core.thread_pool_executor\"] = (\n                thread_pool_executor\n            )\n            execution_engine = ExecutionEngine.init(\n                workflow_definition=workflow_specification,\n                init_parameters=workflow_init_parameters,\n                workflow_id=workflow_id,\n                profiler=profiler,\n            )\n            workflow_runner = WorkflowRunner()\n            on_video_frame = partial(\n                workflow_runner.run_workflow,\n                workflows_parameters=workflows_parameters,\n                execution_engine=execution_engine,\n                image_input_name=image_input_name,\n                video_metadata_input_name=video_metadata_input_name,\n            )\n        except ImportError as error:\n            raise CannotInitialiseModelError(\n                f\"Could not initialise workflow processing due to lack of dependencies required. \"\n                f\"Please provide an issue report under https://github.com/roboflow/inference/issues\"\n            ) from error\n        on_pipeline_end_closure = partial(\n            on_pipeline_end,\n            thread_pool_executor=thread_pool_executor,\n            cancel_thread_pool_tasks_on_exit=cancel_thread_pool_tasks_on_exit,\n            profiler=profiler,\n            profiling_directory=profiling_directory,\n        )\n        return cls.init_with_custom_logic(\n            video_reference=video_reference,\n            on_video_frame=on_video_frame,\n            on_prediction=on_prediction,\n            on_pipeline_start=None,\n            on_pipeline_end=on_pipeline_end_closure,\n            max_fps=max_fps,\n            watchdog=watchdog,\n            status_update_handlers=status_update_handlers,\n            source_buffer_filling_strategy=source_buffer_filling_strategy,\n            source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n            video_source_properties=video_source_properties,\n            batch_collection_timeout=batch_collection_timeout,\n        )\n\n    @classmethod\n    def init_with_custom_logic(\n        cls,\n        video_reference: Union[VideoSourceIdentifier, List[VideoSourceIdentifier]],\n        on_video_frame: InferenceHandler,\n        on_prediction: SinkHandler = None,\n        on_pipeline_start: Optional[Callable[[], None]] = None,\n        on_pipeline_end: Optional[Callable[[], None]] = None,\n        max_fps: Optional[Union[float, int]] = None,\n        watchdog: Optional[PipelineWatchDog] = None,\n        status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n        source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n        source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n        video_source_properties: Optional[Dict[str, float]] = None,\n        batch_collection_timeout: Optional[float] = None,\n        sink_mode: SinkMode = SinkMode.ADAPTIVE,\n    ) -&gt; \"InferencePipeline\":\n        \"\"\"\n        This class creates the abstraction for making inferences from given workflow against video stream.\n        The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initialiser\n        method.\n\n        Args:\n            video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n                predictions against. It can be video file path, stream URL and device (like camera) id\n                (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n                it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n                `sink_mode` parameter comments.\n            on_video_frame (Callable[[VideoFrame], AnyPrediction]): function supposed to make prediction (or do another\n                kind of custom processing according to your will). Accept `VideoFrame` object and is supposed\n                to return dictionary with results of any kind.\n            on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n                once prediction is ready - passing both decoded frame, their metadata and dict with output from your\n                custom callable `on_video_frame(...)`. Logic here must be adjusted to the output of `on_video_frame`.\n            on_pipeline_start (Optional[Callable[[], None]]): Optional (parameter-free) function to be called\n                whenever pipeline starts\n            on_pipeline_end (Optional[Callable[[], None]]): Optional (parameter-free) function to be called\n                whenever pipeline ends\n            max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n                dictate max FPS of each video source.\n                The implementation details of this option has been changed in release `v0.26.0`. Prior to the release\n                this value, when applied to video files caused the processing to wait `1 / max_fps` seconds before next\n                frame is processed - the new implementation drops the intermediate frames, which seems to be more\n                aligned with peoples expectations.\n                New behaviour is now enabled in experimental mode, by setting environmental variable flag\n                `ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True`. Please note that the new behaviour will\n                be the default one end of Q4 2024!\n            watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n                inference pipeline - if not given null implementation (doing nothing) will be used.\n            status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n                status updates of all elements of the pipeline. Should be used only if detailed inspection of\n                pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n                fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n                without re-raising. Default: None.\n            source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n                video stream decoding behaviour. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n            source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n                video stream frames consumption. By default - tweaked to the type of source given.\n                Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n            video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n                Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n                cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n                It is optional and if provided can be provided as single dict (applicable for all sources) or\n                as list of configs. Then the list must be of length of `video_reference` and may also contain None\n                values to denote that specific source should remain not configured.\n                Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n            batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n                to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n                frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n                unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n            sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n                handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n                in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n                in the order of video sources - with None values in the place of vide_frames / predictions that\n                were skipped due to `batch_collection_timeout`.\n                `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n                against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n                To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n                `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n                old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n                prediction element.\n\n        Other ENV variables involved in low-level configuration:\n        * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n        * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n        Returns: Instance of InferencePipeline\n\n        Throws:\n            * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n                always if connection to stream is lost.\n        \"\"\"\n        if watchdog is None:\n            watchdog = NullPipelineWatchdog()\n        if status_update_handlers is None:\n            status_update_handlers = []\n        status_update_handlers.append(watchdog.on_status_update)\n        desired_source_fps = None\n        if ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING:\n            desired_source_fps = max_fps\n        video_sources = prepare_video_sources(\n            video_reference=video_reference,\n            video_source_properties=video_source_properties,\n            status_update_handlers=status_update_handlers,\n            source_buffer_filling_strategy=source_buffer_filling_strategy,\n            source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n            desired_source_fps=desired_source_fps,\n        )\n        watchdog.register_video_sources(video_sources=video_sources)\n        predictions_queue = Queue(maxsize=PREDICTIONS_QUEUE_SIZE)\n        return cls(\n            on_video_frame=on_video_frame,\n            video_sources=video_sources,\n            predictions_queue=predictions_queue,\n            watchdog=watchdog,\n            status_update_handlers=status_update_handlers,\n            on_prediction=on_prediction,\n            max_fps=max_fps,\n            on_pipeline_start=on_pipeline_start,\n            on_pipeline_end=on_pipeline_end,\n            batch_collection_timeout=batch_collection_timeout,\n            sink_mode=sink_mode,\n        )\n\n    def __init__(\n        self,\n        on_video_frame: InferenceHandler,\n        video_sources: List[VideoSource],\n        predictions_queue: Queue,\n        watchdog: PipelineWatchDog,\n        status_update_handlers: List[Callable[[StatusUpdate], None]],\n        on_prediction: SinkHandler = None,\n        on_pipeline_start: Optional[Callable[[], None]] = None,\n        on_pipeline_end: Optional[Callable[[], None]] = None,\n        max_fps: Optional[float] = None,\n        batch_collection_timeout: Optional[float] = None,\n        sink_mode: SinkMode = SinkMode.ADAPTIVE,\n    ):\n        self._on_video_frame = on_video_frame\n        self._video_sources = video_sources\n        self._on_prediction = on_prediction\n        self._max_fps = max_fps\n        self._predictions_queue = predictions_queue\n        self._watchdog = watchdog\n        self._command_handler_thread: Optional[Thread] = None\n        self._inference_thread: Optional[Thread] = None\n        self._dispatching_thread: Optional[Thread] = None\n        self._stop = False\n        self._camera_restart_ongoing = False\n        self._status_update_handlers = status_update_handlers\n        self._on_pipeline_start = on_pipeline_start\n        self._on_pipeline_end = on_pipeline_end\n        self._batch_collection_timeout = batch_collection_timeout\n        self._sink_mode = sink_mode\n\n    def start(self, use_main_thread: bool = True) -&gt; None:\n        self._stop = False\n        self._inference_thread = Thread(target=self._execute_inference)\n        self._inference_thread.start()\n        if self._on_pipeline_start is not None:\n            self._on_pipeline_start()\n        if use_main_thread:\n            self._dispatch_inference_results()\n        else:\n            self._dispatching_thread = Thread(target=self._dispatch_inference_results)\n            self._dispatching_thread.start()\n\n    def terminate(self) -&gt; None:\n        self._stop = True\n        for video_source in self._video_sources:\n            video_source.terminate(\n                wait_on_frames_consumption=False, purge_frames_buffer=True\n            )\n\n    def pause_stream(self, source_id: Optional[int] = None) -&gt; None:\n        for video_source in self._video_sources:\n            if video_source.source_id == source_id or source_id is None:\n                video_source.pause()\n\n    def mute_stream(self, source_id: Optional[int] = None) -&gt; None:\n        for video_source in self._video_sources:\n            if video_source.source_id == source_id or source_id is None:\n                video_source.mute()\n\n    def resume_stream(self, source_id: Optional[int] = None) -&gt; None:\n        for video_source in self._video_sources:\n            if video_source.source_id == source_id or source_id is None:\n                video_source.resume()\n\n    def join(self) -&gt; None:\n        if self._inference_thread is not None:\n            self._inference_thread.join()\n            self._inference_thread = None\n        if self._dispatching_thread is not None:\n            self._dispatching_thread.join()\n            self._dispatching_thread = None\n        if self._on_pipeline_end is not None:\n            self._on_pipeline_end()\n\n    def _execute_inference(self) -&gt; None:\n        send_inference_pipeline_status_update(\n            severity=UpdateSeverity.INFO,\n            event_type=INFERENCE_THREAD_STARTED_EVENT,\n            status_update_handlers=self._status_update_handlers,\n        )\n        logger.info(f\"Inference thread started\")\n        try:\n            for video_frames in self._generate_frames():\n                self._watchdog.on_model_inference_started(\n                    frames=video_frames,\n                )\n                predictions = self._on_video_frame(video_frames)\n                self._watchdog.on_model_prediction_ready(\n                    frames=video_frames,\n                )\n                self._predictions_queue.put((predictions, video_frames))\n                send_inference_pipeline_status_update(\n                    severity=UpdateSeverity.DEBUG,\n                    event_type=INFERENCE_COMPLETED_EVENT,\n                    payload={\n                        \"frames_ids\": [f.frame_id for f in video_frames],\n                        \"frames_timestamps\": [f.frame_timestamp for f in video_frames],\n                        \"sources_id\": [f.source_id for f in video_frames],\n                    },\n                    status_update_handlers=self._status_update_handlers,\n                )\n\n        except Exception as error:\n            payload = {\n                \"error_type\": error.__class__.__name__,\n                \"error_message\": str(error),\n                \"error_context\": \"inference_thread\",\n            }\n            send_inference_pipeline_status_update(\n                severity=UpdateSeverity.ERROR,\n                event_type=INFERENCE_ERROR_EVENT,\n                payload=payload,\n                status_update_handlers=self._status_update_handlers,\n            )\n            logger.exception(f\"Encountered inference error: {error}\")\n        finally:\n            self._predictions_queue.put(None)\n            send_inference_pipeline_status_update(\n                severity=UpdateSeverity.INFO,\n                event_type=INFERENCE_THREAD_FINISHED_EVENT,\n                status_update_handlers=self._status_update_handlers,\n            )\n            logger.info(f\"Inference thread finished\")\n\n    def _dispatch_inference_results(self) -&gt; None:\n        while True:\n            inference_results: Optional[\n                Tuple[List[AnyPrediction], List[VideoFrame]]\n            ] = self._predictions_queue.get()\n            if inference_results is None:\n                self._predictions_queue.task_done()\n                break\n            predictions, video_frames = inference_results\n            if self._on_prediction is not None:\n                self._handle_predictions_dispatching(\n                    predictions=predictions,\n                    video_frames=video_frames,\n                )\n            self._predictions_queue.task_done()\n\n    def _handle_predictions_dispatching(\n        self,\n        predictions: List[AnyPrediction],\n        video_frames: List[VideoFrame],\n    ) -&gt; None:\n        if self._should_use_batch_sink():\n            self._use_batch_sink(predictions, video_frames)\n            return None\n        for frame_predictions, video_frame in zip(predictions, video_frames):\n            self._use_sink(frame_predictions, video_frame)\n\n    def _should_use_batch_sink(self) -&gt; bool:\n        return self._sink_mode is SinkMode.BATCH or (\n            self._sink_mode is SinkMode.ADAPTIVE and len(self._video_sources) &gt; 1\n        )\n\n    def _use_batch_sink(\n        self,\n        predictions: List[AnyPrediction],\n        video_frames: List[VideoFrame],\n    ) -&gt; None:\n        # This function makes it possible to always call sinks with payloads aligned to order of\n        # video sources - marking empty frames as None\n        results_by_source_id = {\n            video_frame.source_id: (frame_predictions, video_frame)\n            for frame_predictions, video_frame in zip(predictions, video_frames)\n        }\n        source_id_aligned_sink_payload = [\n            results_by_source_id.get(video_source.source_id, (None, None))\n            for video_source in self._video_sources\n        ]\n        source_id_aligned_predictions = [e[0] for e in source_id_aligned_sink_payload]\n        source_id_aligned_frames = [e[1] for e in source_id_aligned_sink_payload]\n        self._use_sink(\n            predictions=source_id_aligned_predictions,\n            video_frames=source_id_aligned_frames,\n        )\n\n    def _use_sink(\n        self,\n        predictions: Union[AnyPrediction, List[Optional[AnyPrediction]]],\n        video_frames: Union[VideoFrame, List[Optional[VideoFrame]]],\n    ) -&gt; None:\n        try:\n            self._on_prediction(predictions, video_frames)\n        except Exception as error:\n            payload = {\n                \"error_type\": error.__class__.__name__,\n                \"error_message\": str(error),\n                \"error_context\": \"inference_results_dispatching\",\n            }\n            send_inference_pipeline_status_update(\n                severity=UpdateSeverity.ERROR,\n                event_type=INFERENCE_RESULTS_DISPATCHING_ERROR_EVENT,\n                payload=payload,\n                status_update_handlers=self._status_update_handlers,\n            )\n            logger.exception(f\"Error in results dispatching - {error}\")\n\n    def _generate_frames(\n        self,\n    ) -&gt; Generator[List[VideoFrame], None, None]:\n        for video_source in self._video_sources:\n            video_source.start()\n        max_fps = None\n        if not ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING:\n            max_fps = self._max_fps\n        yield from multiplex_videos(\n            videos=self._video_sources,\n            max_fps=max_fps,\n            batch_collection_timeout=self._batch_collection_timeout,\n            should_stop=lambda: self._stop,\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/#inference.core.interfaces.stream.inference_pipeline.InferencePipeline.init","title":"<code>init(video_reference, model_id, on_prediction=None, api_key=None, max_fps=None, watchdog=None, status_update_handlers=None, source_buffer_filling_strategy=None, source_buffer_consumption_strategy=None, class_agnostic_nms=None, confidence=None, iou_threshold=None, max_candidates=None, max_detections=None, mask_decode_mode='accurate', tradeoff_factor=0.0, active_learning_enabled=None, video_source_properties=None, active_learning_target_dataset=None, batch_collection_timeout=None, sink_mode=SinkMode.ADAPTIVE)</code>  <code>classmethod</code>","text":"<p>This class creates the abstraction for making inferences from Roboflow models against video stream. It allows to choose model from Roboflow platform and run predictions against video streams - just by the price of specifying which model to use and what to do with predictions.</p> <p>It allows to set the model post-processing parameters (via .init() or env) and intercept updates related to state of pipeline via <code>PipelineWatchDog</code> abstraction (although that is something probably useful only for advanced use-cases).</p> <p>For maximum efficiency, all separate chunks of processing: video decoding, inference, results dispatching are handled by separate threads.</p> <p>Given that reference to stream is passed and connectivity is lost - it attempts to re-connect with delay.</p> <p>Since version 0.9.11 it works not only for object detection models but is also compatible with stubs, classification, instance-segmentation and keypoint-detection models.</p> <p>Since version 0.9.18, <code>InferencePipeline</code> is capable of handling multiple video sources at once. If multiple sources are provided - source multiplexing will happen. One of the change introduced in that release is switch from <code>get_video_frames_generator(...)</code> as video frames provider into <code>multiplex_videos(...)</code>. For a single video source, the behaviour of <code>InferencePipeline</code> is remained unchanged when default parameters are used. For multiple videos - frames are multiplexed, and we can adjust the pipeline behaviour using new configuration options. <code>batch_collection_timeout</code> is one of the new option - it is the parameter of <code>multiplex_videos(...)</code> that dictates how long the batch frames collection process may wait for all sources to provide video frame. It can be set infinite (None) or with specific value representing fraction of second. We advise that value to be set in production solutions to avoid processing slow-down caused by source with unstable latency spikes. For more information on multiplexing process - please visit <code>multiplex_videos(...)</code> function docs. Another change is the way on how sinks work. They can work in <code>SinkMode.ADAPTIVE</code> - which means that video frames and predictions will be either provided to sink as list of objects, or specific elements - and the determining factor is number of sources (it will behave SEQUENTIAL for one source and BATCH if multiple ones are provided). All old sinks were adjusted to work in both modes, custom ones should be migrated to reflect changes in sink function signature.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Name and version of model on the Roboflow platform (example: \"my-model/3\")</p> required <code>video_reference</code> <code>Union[str, int, List[Union[str, int]]]</code> <p>Reference of source or sources to be used to make predictions against. It can be video file path, stream URL and device (like camera) id (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then it will trigger parallel processing of multiple sources. It has some implication on sinks. See: <code>sink_mode</code> parameter comments.</p> required <code>on_prediction</code> <code>Callable[AnyPrediction, VideoFrame], None]</code> <p>Function to be called once prediction is ready - passing both decoded frame, their metadata and dict with standard Roboflow model prediction (different for specific types of models).</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\" and \"API_KEY\" variables. API key, passed in some form is required.</p> <code>None</code> <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Specific value passed as this parameter will be used to dictate max FPS of each video source. The implementation details of this option has been changed in release <code>v0.26.0</code>. Prior to the release this value, when applied to video files caused the processing to wait <code>1 / max_fps</code> seconds before next frame is processed - the new implementation drops the intermediate frames, which seems to be more aligned with peoples expectations. New behaviour is now enabled in experimental mode, by setting environmental variable flag <code>ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True</code>. Please note that the new behaviour will be the default one end of Q4 2024!</p> <code>None</code> <code>watchdog</code> <code>Optional[PipelineWatchDog]</code> <p>Implementation of class that allows profiling of inference pipeline - if not given null implementation (doing nothing) will be used.</p> <code>None</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers to intercept status updates of all elements of the pipeline. Should be used only if detailed inspection of pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed fast - otherwise they will impair pipeline performance. All errors will be logged as warnings without re-raising. Default: None.</p> <code>None</code> <code>source_buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Parameter dictating strategy for video stream decoding behaviour. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>source_buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Parameter dictating strategy for video stream frames consumption. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>class_agnostic_nms</code> <code>Optional[bool]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"</p> <code>None</code> <code>confidence</code> <code>Optional[float]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"CONFIDENCE\" with default \"0.5\"</p> <code>None</code> <code>iou_threshold</code> <code>Optional[float]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"IOU_THRESHOLD\" with default \"0.5\"</p> <code>None</code> <code>max_candidates</code> <code>Optional[int]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"MAX_CANDIDATES\" with default \"3000\"</p> <code>None</code> <code>max_detections</code> <code>Optional[int]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"MAX_DETECTIONS\" with default \"300\"</p> <code>None</code> <code>mask_decode_mode</code> <code>Optional[str]</code> <p>(Optional[str]): Parameter of model post-processing. If not given - model \"accurate\" is used. Applicable for instance segmentation models</p> <code>'accurate'</code> <code>tradeoff_factor</code> <code>Optional[float]</code> <p>Parameter of model post-processing. If not 0.0 - model default is used. Applicable for instance segmentation models</p> <code>0.0</code> <code>active_learning_enabled</code> <code>Optional[bool]</code> <p>Flag to enable / disable Active Learning middleware (setting it true does not guarantee any data to be collected, as data collection is controlled by Roboflow backend - it just enables middleware intercepting predictions). If not given, env variable <code>ACTIVE_LEARNING_ENABLED</code> will be used. Please point out that Active Learning will be forcefully disabled in a scenario when Roboflow API key is not given, as Roboflow account is required for this feature to be operational.</p> <code>None</code> <code>video_source_properties</code> <code>Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]</code> <p>Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source will be used. It is optional and if provided can be provided as single dict (applicable for all sources) or as list of configs. Then the list must be of length of <code>video_reference</code> and may also contain None values to denote that specific source should remain not configured. Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}</p> <code>None</code> <code>active_learning_target_dataset</code> <code>Optional[str]</code> <p>Parameter to be used when Active Learning data registration should happen against different dataset than the one pointed by model_id</p> <code>None</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>Parameter of multiplex_videos(...) dictating how long process to grab frames from multiple sources can wait for batch to be filled before yielding already collected frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows unstable latency. Visit <code>multiplex_videos(...)</code> for more information about multiplexing process.</p> <code>None</code> <code>sink_mode</code> <code>SinkMode</code> <p>Parameter that controls how video frames and predictions will be passed to sink handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink, in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned in the order of video sources - with None values in the place of vide_frames / predictions that were skipped due to <code>batch_collection_timeout</code>. <code>SinkMode.ADAPTIVE</code> is a middle ground (and default mode) - all old sources will work in that mode against a single video input, as the pipeline will behave as if running in <code>SinkMode.SEQUENTIAL</code>. To handle multiple videos - sink needs to accept <code>predictions: List[Optional[dict]]</code> and <code>video_frame: List[Optional[VideoFrame]]</code>. It is also possible to process multiple videos using old sinks - but then <code>SinkMode.SEQUENTIAL</code> is to be used, causing sink to be called on each prediction element.</p> <code>ADAPTIVE</code> <p>Other ENV variables involved in low-level configuration: * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop * ACTIVE_LEARNING_ENABLED - controls Active Learning middleware if explicit parameter not given</p> <p>Returns: Instance of InferencePipeline</p> Throws <ul> <li>SourceConnectionError if source cannot be connected at start, however it attempts to reconnect     always if connection to stream is lost.</li> </ul> Source code in <code>inference/core/interfaces/stream/inference_pipeline.py</code> <pre><code>@classmethod\ndef init(\n    cls,\n    video_reference: Union[VideoSourceIdentifier, List[VideoSourceIdentifier]],\n    model_id: str,\n    on_prediction: SinkHandler = None,\n    api_key: Optional[str] = None,\n    max_fps: Optional[Union[float, int]] = None,\n    watchdog: Optional[PipelineWatchDog] = None,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    class_agnostic_nms: Optional[bool] = None,\n    confidence: Optional[float] = None,\n    iou_threshold: Optional[float] = None,\n    max_candidates: Optional[int] = None,\n    max_detections: Optional[int] = None,\n    mask_decode_mode: Optional[str] = \"accurate\",\n    tradeoff_factor: Optional[float] = 0.0,\n    active_learning_enabled: Optional[bool] = None,\n    video_source_properties: Optional[\n        Union[Dict[str, float], List[Optional[Dict[str, float]]]]\n    ] = None,\n    active_learning_target_dataset: Optional[str] = None,\n    batch_collection_timeout: Optional[float] = None,\n    sink_mode: SinkMode = SinkMode.ADAPTIVE,\n) -&gt; \"InferencePipeline\":\n    \"\"\"\n    This class creates the abstraction for making inferences from Roboflow models against video stream.\n    It allows to choose model from Roboflow platform and run predictions against\n    video streams - just by the price of specifying which model to use and what to do with predictions.\n\n    It allows to set the model post-processing parameters (via .init() or env) and intercept updates\n    related to state of pipeline via `PipelineWatchDog` abstraction (although that is something probably\n    useful only for advanced use-cases).\n\n    For maximum efficiency, all separate chunks of processing: video decoding, inference, results dispatching\n    are handled by separate threads.\n\n    Given that reference to stream is passed and connectivity is lost - it attempts to re-connect with delay.\n\n    Since version 0.9.11 it works not only for object detection models but is also compatible with stubs,\n    classification, instance-segmentation and keypoint-detection models.\n\n    Since version 0.9.18, `InferencePipeline` is capable of handling multiple video sources at once. If multiple\n    sources are provided - source multiplexing will happen. One of the change introduced in that release is switch\n    from `get_video_frames_generator(...)` as video frames provider into `multiplex_videos(...)`. For a single\n    video source, the behaviour of `InferencePipeline` is remained unchanged when default parameters are used.\n    For multiple videos - frames are multiplexed, and we can adjust the pipeline behaviour using new configuration\n    options. `batch_collection_timeout` is one of the new option - it is the parameter of `multiplex_videos(...)`\n    that dictates how long the batch frames collection process may wait for all sources to provide video frame.\n    It can be set infinite (None) or with specific value representing fraction of second. We advise that value to\n    be set in production solutions to avoid processing slow-down caused by source with unstable latency spikes.\n    For more information on multiplexing process - please visit `multiplex_videos(...)` function docs.\n    Another change is the way on how sinks work. They can work in `SinkMode.ADAPTIVE` - which means that\n    video frames and predictions will be either provided to sink as list of objects, or specific elements -\n    and the determining factor is number of sources (it will behave SEQUENTIAL for one source and BATCH if multiple\n    ones are provided). All old sinks were adjusted to work in both modes, custom ones should be migrated\n    to reflect changes in sink function signature.\n\n    Args:\n        model_id (str): Name and version of model on the Roboflow platform (example: \"my-model/3\")\n        video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n            predictions against. It can be video file path, stream URL and device (like camera) id\n            (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n            it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n            `sink_mode` parameter comments.\n        on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n            once prediction is ready - passing both decoded frame, their metadata and dict with standard\n            Roboflow model prediction (different for specific types of models).\n        api_key (Optional[str]): Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\"\n            and \"API_KEY\" variables. API key, passed in some form is required.\n        max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n            dictate max FPS of each video source.\n            The implementation details of this option has been changed in release `v0.26.0`. Prior to the release\n            this value, when applied to video files caused the processing to wait `1 / max_fps` seconds before next\n            frame is processed - the new implementation drops the intermediate frames, which seems to be more\n            aligned with peoples expectations.\n            New behaviour is now enabled in experimental mode, by setting environmental variable flag\n            `ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True`. Please note that the new behaviour will\n            be the default one end of Q4 2024!\n        watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n            inference pipeline - if not given null implementation (doing nothing) will be used.\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n            status updates of all elements of the pipeline. Should be used only if detailed inspection of\n            pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n            fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n            without re-raising. Default: None.\n        source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n            video stream decoding behaviour. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n        source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n            video stream frames consumption. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n        class_agnostic_nms (Optional[bool]): Parameter of model post-processing. If not given - value checked in\n            env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"\n        confidence (Optional[float]): Parameter of model post-processing. If not given - value checked in\n            env variable \"CONFIDENCE\" with default \"0.5\"\n        iou_threshold (Optional[float]): Parameter of model post-processing. If not given - value checked in\n            env variable \"IOU_THRESHOLD\" with default \"0.5\"\n        max_candidates (Optional[int]): Parameter of model post-processing. If not given - value checked in\n            env variable \"MAX_CANDIDATES\" with default \"3000\"\n        max_detections (Optional[int]): Parameter of model post-processing. If not given - value checked in\n            env variable \"MAX_DETECTIONS\" with default \"300\"\n        mask_decode_mode: (Optional[str]): Parameter of model post-processing. If not given - model \"accurate\" is\n            used. Applicable for instance segmentation models\n        tradeoff_factor (Optional[float]): Parameter of model post-processing. If not 0.0 - model default is used.\n            Applicable for instance segmentation models\n        active_learning_enabled (Optional[bool]): Flag to enable / disable Active Learning middleware (setting it\n            true does not guarantee any data to be collected, as data collection is controlled by Roboflow backend -\n            it just enables middleware intercepting predictions). If not given, env variable\n            `ACTIVE_LEARNING_ENABLED` will be used. Please point out that Active Learning will be forcefully\n            disabled in a scenario when Roboflow API key is not given, as Roboflow account is required\n            for this feature to be operational.\n        video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n            Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n            cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n            It is optional and if provided can be provided as single dict (applicable for all sources) or\n            as list of configs. Then the list must be of length of `video_reference` and may also contain None\n            values to denote that specific source should remain not configured.\n            Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n        active_learning_target_dataset (Optional[str]): Parameter to be used when Active Learning data registration\n            should happen against different dataset than the one pointed by model_id\n        batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n            to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n            frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n            unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n        sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n            handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n            in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n            in the order of video sources - with None values in the place of vide_frames / predictions that\n            were skipped due to `batch_collection_timeout`.\n            `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n            against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n            To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n            `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n            old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n            prediction element.\n\n    Other ENV variables involved in low-level configuration:\n    * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n    * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n    * ACTIVE_LEARNING_ENABLED - controls Active Learning middleware if explicit parameter not given\n\n    Returns: Instance of InferencePipeline\n\n    Throws:\n        * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n            always if connection to stream is lost.\n    \"\"\"\n    if api_key is None:\n        api_key = API_KEY\n    inference_config = ModelConfig.init(\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        iou_threshold=iou_threshold,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        mask_decode_mode=mask_decode_mode,\n        tradeoff_factor=tradeoff_factor,\n    )\n    model = get_model(model_id=model_id, api_key=api_key)\n    on_video_frame = partial(\n        default_process_frame, model=model, inference_config=inference_config\n    )\n    active_learning_middleware = NullActiveLearningMiddleware()\n    if active_learning_enabled is None:\n        logger.info(\n            f\"`active_learning_enabled` parameter not set - using env `ACTIVE_LEARNING_ENABLED` \"\n            f\"with value: {ACTIVE_LEARNING_ENABLED}\"\n        )\n        active_learning_enabled = ACTIVE_LEARNING_ENABLED\n    if api_key is None:\n        logger.info(\n            f\"Roboflow API key not given - Active Learning is forced to be disabled.\"\n        )\n        active_learning_enabled = False\n    if active_learning_enabled is True:\n        resolved_model_id = resolve_roboflow_model_alias(model_id=model_id)\n        target_dataset = (\n            active_learning_target_dataset or resolved_model_id.split(\"/\")[0]\n        )\n        active_learning_middleware = ThreadingActiveLearningMiddleware.init(\n            api_key=api_key,\n            target_dataset=target_dataset,\n            model_id=resolved_model_id,\n            cache=cache,\n        )\n        al_sink = partial(\n            active_learning_sink,\n            active_learning_middleware=active_learning_middleware,\n            model_type=model.task_type,\n            disable_preproc_auto_orient=DISABLE_PREPROC_AUTO_ORIENT,\n        )\n        logger.info(\n            \"AL enabled - wrapping `on_prediction` with multi_sink() and active_learning_sink()\"\n        )\n        on_prediction = partial(multi_sink, sinks=[on_prediction, al_sink])\n    on_pipeline_start = active_learning_middleware.start_registration_thread\n    on_pipeline_end = active_learning_middleware.stop_registration_thread\n    return cls.init_with_custom_logic(\n        video_reference=video_reference,\n        on_video_frame=on_video_frame,\n        on_prediction=on_prediction,\n        on_pipeline_start=on_pipeline_start,\n        on_pipeline_end=on_pipeline_end,\n        max_fps=max_fps,\n        watchdog=watchdog,\n        status_update_handlers=status_update_handlers,\n        source_buffer_filling_strategy=source_buffer_filling_strategy,\n        source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n        video_source_properties=video_source_properties,\n        batch_collection_timeout=batch_collection_timeout,\n        sink_mode=sink_mode,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/#inference.core.interfaces.stream.inference_pipeline.InferencePipeline.init_with_custom_logic","title":"<code>init_with_custom_logic(video_reference, on_video_frame, on_prediction=None, on_pipeline_start=None, on_pipeline_end=None, max_fps=None, watchdog=None, status_update_handlers=None, source_buffer_filling_strategy=None, source_buffer_consumption_strategy=None, video_source_properties=None, batch_collection_timeout=None, sink_mode=SinkMode.ADAPTIVE)</code>  <code>classmethod</code>","text":"<p>This class creates the abstraction for making inferences from given workflow against video stream. The way of how <code>InferencePipeline</code> works is displayed in <code>InferencePipeline.init(...)</code> initialiser method.</p> <p>Parameters:</p> Name Type Description Default <code>video_reference</code> <code>Union[str, int, List[Union[str, int]]]</code> <p>Reference of source or sources to be used to make predictions against. It can be video file path, stream URL and device (like camera) id (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then it will trigger parallel processing of multiple sources. It has some implication on sinks. See: <code>sink_mode</code> parameter comments.</p> required <code>on_video_frame</code> <code>Callable[[VideoFrame], AnyPrediction]</code> <p>function supposed to make prediction (or do another kind of custom processing according to your will). Accept <code>VideoFrame</code> object and is supposed to return dictionary with results of any kind.</p> required <code>on_prediction</code> <code>Callable[AnyPrediction, VideoFrame], None]</code> <p>Function to be called once prediction is ready - passing both decoded frame, their metadata and dict with output from your custom callable <code>on_video_frame(...)</code>. Logic here must be adjusted to the output of <code>on_video_frame</code>.</p> <code>None</code> <code>on_pipeline_start</code> <code>Optional[Callable[[], None]]</code> <p>Optional (parameter-free) function to be called whenever pipeline starts</p> <code>None</code> <code>on_pipeline_end</code> <code>Optional[Callable[[], None]]</code> <p>Optional (parameter-free) function to be called whenever pipeline ends</p> <code>None</code> <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Specific value passed as this parameter will be used to dictate max FPS of each video source. The implementation details of this option has been changed in release <code>v0.26.0</code>. Prior to the release this value, when applied to video files caused the processing to wait <code>1 / max_fps</code> seconds before next frame is processed - the new implementation drops the intermediate frames, which seems to be more aligned with peoples expectations. New behaviour is now enabled in experimental mode, by setting environmental variable flag <code>ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True</code>. Please note that the new behaviour will be the default one end of Q4 2024!</p> <code>None</code> <code>watchdog</code> <code>Optional[PipelineWatchDog]</code> <p>Implementation of class that allows profiling of inference pipeline - if not given null implementation (doing nothing) will be used.</p> <code>None</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers to intercept status updates of all elements of the pipeline. Should be used only if detailed inspection of pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed fast - otherwise they will impair pipeline performance. All errors will be logged as warnings without re-raising. Default: None.</p> <code>None</code> <code>source_buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Parameter dictating strategy for video stream decoding behaviour. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>source_buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Parameter dictating strategy for video stream frames consumption. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>video_source_properties</code> <code>Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]</code> <p>Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source will be used. It is optional and if provided can be provided as single dict (applicable for all sources) or as list of configs. Then the list must be of length of <code>video_reference</code> and may also contain None values to denote that specific source should remain not configured. Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}</p> <code>None</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>Parameter of multiplex_videos(...) dictating how long process to grab frames from multiple sources can wait for batch to be filled before yielding already collected frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows unstable latency. Visit <code>multiplex_videos(...)</code> for more information about multiplexing process.</p> <code>None</code> <code>sink_mode</code> <code>SinkMode</code> <p>Parameter that controls how video frames and predictions will be passed to sink handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink, in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned in the order of video sources - with None values in the place of vide_frames / predictions that were skipped due to <code>batch_collection_timeout</code>. <code>SinkMode.ADAPTIVE</code> is a middle ground (and default mode) - all old sources will work in that mode against a single video input, as the pipeline will behave as if running in <code>SinkMode.SEQUENTIAL</code>. To handle multiple videos - sink needs to accept <code>predictions: List[Optional[dict]]</code> and <code>video_frame: List[Optional[VideoFrame]]</code>. It is also possible to process multiple videos using old sinks - but then <code>SinkMode.SEQUENTIAL</code> is to be used, causing sink to be called on each prediction element.</p> <code>ADAPTIVE</code> <p>Other ENV variables involved in low-level configuration: * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop</p> <p>Returns: Instance of InferencePipeline</p> Throws <ul> <li>SourceConnectionError if source cannot be connected at start, however it attempts to reconnect     always if connection to stream is lost.</li> </ul> Source code in <code>inference/core/interfaces/stream/inference_pipeline.py</code> <pre><code>@classmethod\ndef init_with_custom_logic(\n    cls,\n    video_reference: Union[VideoSourceIdentifier, List[VideoSourceIdentifier]],\n    on_video_frame: InferenceHandler,\n    on_prediction: SinkHandler = None,\n    on_pipeline_start: Optional[Callable[[], None]] = None,\n    on_pipeline_end: Optional[Callable[[], None]] = None,\n    max_fps: Optional[Union[float, int]] = None,\n    watchdog: Optional[PipelineWatchDog] = None,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    video_source_properties: Optional[Dict[str, float]] = None,\n    batch_collection_timeout: Optional[float] = None,\n    sink_mode: SinkMode = SinkMode.ADAPTIVE,\n) -&gt; \"InferencePipeline\":\n    \"\"\"\n    This class creates the abstraction for making inferences from given workflow against video stream.\n    The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initialiser\n    method.\n\n    Args:\n        video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n            predictions against. It can be video file path, stream URL and device (like camera) id\n            (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n            it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n            `sink_mode` parameter comments.\n        on_video_frame (Callable[[VideoFrame], AnyPrediction]): function supposed to make prediction (or do another\n            kind of custom processing according to your will). Accept `VideoFrame` object and is supposed\n            to return dictionary with results of any kind.\n        on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n            once prediction is ready - passing both decoded frame, their metadata and dict with output from your\n            custom callable `on_video_frame(...)`. Logic here must be adjusted to the output of `on_video_frame`.\n        on_pipeline_start (Optional[Callable[[], None]]): Optional (parameter-free) function to be called\n            whenever pipeline starts\n        on_pipeline_end (Optional[Callable[[], None]]): Optional (parameter-free) function to be called\n            whenever pipeline ends\n        max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n            dictate max FPS of each video source.\n            The implementation details of this option has been changed in release `v0.26.0`. Prior to the release\n            this value, when applied to video files caused the processing to wait `1 / max_fps` seconds before next\n            frame is processed - the new implementation drops the intermediate frames, which seems to be more\n            aligned with peoples expectations.\n            New behaviour is now enabled in experimental mode, by setting environmental variable flag\n            `ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True`. Please note that the new behaviour will\n            be the default one end of Q4 2024!\n        watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n            inference pipeline - if not given null implementation (doing nothing) will be used.\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n            status updates of all elements of the pipeline. Should be used only if detailed inspection of\n            pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n            fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n            without re-raising. Default: None.\n        source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n            video stream decoding behaviour. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n        source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n            video stream frames consumption. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n        video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n            Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n            cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n            It is optional and if provided can be provided as single dict (applicable for all sources) or\n            as list of configs. Then the list must be of length of `video_reference` and may also contain None\n            values to denote that specific source should remain not configured.\n            Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n        batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n            to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n            frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n            unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n        sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n            handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n            in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n            in the order of video sources - with None values in the place of vide_frames / predictions that\n            were skipped due to `batch_collection_timeout`.\n            `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n            against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n            To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n            `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n            old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n            prediction element.\n\n    Other ENV variables involved in low-level configuration:\n    * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n    * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n    Returns: Instance of InferencePipeline\n\n    Throws:\n        * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n            always if connection to stream is lost.\n    \"\"\"\n    if watchdog is None:\n        watchdog = NullPipelineWatchdog()\n    if status_update_handlers is None:\n        status_update_handlers = []\n    status_update_handlers.append(watchdog.on_status_update)\n    desired_source_fps = None\n    if ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING:\n        desired_source_fps = max_fps\n    video_sources = prepare_video_sources(\n        video_reference=video_reference,\n        video_source_properties=video_source_properties,\n        status_update_handlers=status_update_handlers,\n        source_buffer_filling_strategy=source_buffer_filling_strategy,\n        source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n        desired_source_fps=desired_source_fps,\n    )\n    watchdog.register_video_sources(video_sources=video_sources)\n    predictions_queue = Queue(maxsize=PREDICTIONS_QUEUE_SIZE)\n    return cls(\n        on_video_frame=on_video_frame,\n        video_sources=video_sources,\n        predictions_queue=predictions_queue,\n        watchdog=watchdog,\n        status_update_handlers=status_update_handlers,\n        on_prediction=on_prediction,\n        max_fps=max_fps,\n        on_pipeline_start=on_pipeline_start,\n        on_pipeline_end=on_pipeline_end,\n        batch_collection_timeout=batch_collection_timeout,\n        sink_mode=sink_mode,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/#inference.core.interfaces.stream.inference_pipeline.InferencePipeline.init_with_workflow","title":"<code>init_with_workflow(video_reference, workflow_specification=None, workspace_name=None, workflow_id=None, api_key=None, image_input_name='image', workflows_parameters=None, on_prediction=None, max_fps=None, watchdog=None, status_update_handlers=None, source_buffer_filling_strategy=None, source_buffer_consumption_strategy=None, video_source_properties=None, workflow_init_parameters=None, workflows_thread_pool_workers=4, cancel_thread_pool_tasks_on_exit=True, video_metadata_input_name='video_metadata', batch_collection_timeout=None, profiling_directory='./inference_profiling', use_workflow_definition_cache=True)</code>  <code>classmethod</code>","text":"<p>This class creates the abstraction for making inferences from given workflow against video stream. The way of how <code>InferencePipeline</code> works is displayed in <code>InferencePipeline.init(...)</code> initializer method.</p> <p>Parameters:</p> Name Type Description Default <code>video_reference</code> <code>Union[str, int, List[Union[str, int]]]</code> <p>Reference of source to be used to make predictions against. It can be video file path, stream URL and device (like camera) id (we handle whatever cv2 handles). It can also be a list of references (since v0.13.0) - and then it will trigger parallel processing of multiple sources. It has some implication on sinks. See: <code>sink_mode</code> parameter comments.</p> required <code>workflow_specification</code> <code>Optional[dict]</code> <p>Valid specification of workflow. See workflow docs. It can be provided optionally, but if not given, both <code>workspace_name</code> and <code>workflow_id</code> must be provided.</p> <code>None</code> <code>workspace_name</code> <code>Optional[str]</code> <p>When using registered workflows - Roboflow workspace name needs to be given.</p> <code>None</code> <code>workflow_id</code> <code>Optional[str]</code> <p>When using registered workflows - Roboflow workflow id needs to be given.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\" and \"API_KEY\" variables. API key, passed in some form is required.</p> <code>None</code> <code>image_input_name</code> <code>str</code> <p>Name of input image defined in <code>workflow_specification</code> or Workflow definition saved on the Roboflow Platform. <code>InferencePipeline</code> will be injecting video frames to workflow through that parameter name.</p> <code>'image'</code> <code>workflows_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary with additional parameters that can be defined within <code>workflow_specification</code>.</p> <code>None</code> <code>on_prediction</code> <code>Callable[AnyPrediction, VideoFrame], None]</code> <p>Function to be called once prediction is ready - passing both decoded frame, their metadata and dict with workflow output.</p> <code>None</code> <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Specific value passed as this parameter will be used to dictate max FPS of each video source. The implementation details of this option has been changed in release <code>v0.26.0</code>. Prior to the release this value, when applied to video files caused the processing to wait <code>1 / max_fps</code> seconds before next frame is processed - the new implementation drops the intermediate frames, which seems to be more aligned with peoples expectations. New behaviour is now enabled in experimental mode, by setting environmental variable flag <code>ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True</code>. Please note that the new behaviour will be the default one end of Q4 2024!</p> <code>None</code> <code>watchdog</code> <code>Optional[PipelineWatchDog]</code> <p>Implementation of class that allows profiling of inference pipeline - if not given null implementation (doing nothing) will be used.</p> <code>None</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers to intercept status updates of all elements of the pipeline. Should be used only if detailed inspection of pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed fast - otherwise they will impair pipeline performance. All errors will be logged as warnings without re-raising. Default: None.</p> <code>None</code> <code>source_buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Parameter dictating strategy for video stream decoding behaviour. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>source_buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Parameter dictating strategy for video stream frames consumption. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>video_source_properties</code> <code>Optional[dict[str, float]]</code> <p>Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source will be used. Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}</p> <code>None</code> <code>workflow_init_parameters</code> <code>Optional[Dict[str, Any]]</code> <p>Additional init parameters to be used by workflows Execution Engine to init steps of your workflow - may be required when running workflows with custom plugins.</p> <code>None</code> <code>workflows_thread_pool_workers</code> <code>int</code> <p>Number of workers for workflows thread pool which is used by workflows blocks to run background tasks.</p> <code>4</code> <code>cancel_thread_pool_tasks_on_exit</code> <code>bool</code> <p>Flag to decide if unstated background tasks should be canceled at the end of InferencePipeline processing. By default, when video file ends or pipeline is stopped, tasks that has not started will be cancelled.</p> <code>True</code> <code>video_metadata_input_name</code> <code>str</code> <p>Name of input for video metadata defined in <code>workflow_specification</code> or Workflow definition saved  on the Roboflow Platform. <code>InferencePipeline</code> will be injecting video frames metadata to workflows through that parameter name.</p> <code>'video_metadata'</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>Parameter of multiplex_videos(...) dictating how long process to grab frames from multiple sources can wait for batch to be filled before yielding already collected frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows unstable latency. Visit <code>multiplex_videos(...)</code> for more information about multiplexing process.</p> <code>None</code> <code>profiling_directory</code> <code>str</code> <p>Directory where workflows profiler traces will be dumped. To enable profiling export <code>ENABLE_WORKFLOWS_PROFILING=True</code> environmental variable. You may specify number of workflow runs in a buffer with environmental variable <code>WORKFLOWS_PROFILER_BUFFER_SIZE=n</code> - making last <code>n</code> frames to be present in buffer on processing end.</p> <code>'./inference_profiling'</code> <code>use_workflow_definition_cache</code> <code>bool</code> <p>Controls usage of cache for workflow definitions. Set this to False when you frequently modify definition saved in Roboflow app and want to fetch the newest version for the request. Only applies for Workflows definitions saved on Roboflow platform.</p> <code>True</code> <p>Other ENV variables involved in low-level configuration: * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop</p> <p>Returns: Instance of InferencePipeline</p> Throws <ul> <li>SourceConnectionError if source cannot be connected at start, however it attempts to reconnect     always if connection to stream is lost.</li> <li>ValueError if workflow specification not provided and registered workflow not pointed out</li> <li>NotImplementedError if workflow used against multiple videos which is not supported yet</li> <li>MissingApiKeyError - if API key is not provided in situation when retrieving workflow definition     from Roboflow API is needed</li> </ul> Source code in <code>inference/core/interfaces/stream/inference_pipeline.py</code> <pre><code>@classmethod\n@experimental(\n    reason=\"Usage of workflows with `InferencePipeline` is an experimental feature. Please report any issues \"\n    \"here: https://github.com/roboflow/inference/issues\"\n)\ndef init_with_workflow(\n    cls,\n    video_reference: Union[str, int, List[Union[str, int]]],\n    workflow_specification: Optional[dict] = None,\n    workspace_name: Optional[str] = None,\n    workflow_id: Optional[str] = None,\n    api_key: Optional[str] = None,\n    image_input_name: str = \"image\",\n    workflows_parameters: Optional[Dict[str, Any]] = None,\n    on_prediction: SinkHandler = None,\n    max_fps: Optional[Union[float, int]] = None,\n    watchdog: Optional[PipelineWatchDog] = None,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    video_source_properties: Optional[Dict[str, float]] = None,\n    workflow_init_parameters: Optional[Dict[str, Any]] = None,\n    workflows_thread_pool_workers: int = 4,\n    cancel_thread_pool_tasks_on_exit: bool = True,\n    video_metadata_input_name: str = \"video_metadata\",\n    batch_collection_timeout: Optional[float] = None,\n    profiling_directory: str = \"./inference_profiling\",\n    use_workflow_definition_cache: bool = True,\n) -&gt; \"InferencePipeline\":\n    \"\"\"\n    This class creates the abstraction for making inferences from given workflow against video stream.\n    The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initializer\n    method.\n\n    Args:\n        video_reference (Union[str, int, List[Union[str, int]]]): Reference of source to be used to make predictions\n            against. It can be video file path, stream URL and device (like camera) id\n            (we handle whatever cv2 handles). It can also be a list of references (since v0.13.0) - and then\n            it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n            `sink_mode` parameter comments.\n        workflow_specification (Optional[dict]): Valid specification of workflow. See [workflow docs](https://github.com/roboflow/inference/tree/main/inference/enterprise/workflows).\n            It can be provided optionally, but if not given, both `workspace_name` and `workflow_id`\n            must be provided.\n        workspace_name (Optional[str]): When using registered workflows - Roboflow workspace name needs to be given.\n        workflow_id (Optional[str]): When using registered workflows - Roboflow workflow id needs to be given.\n        api_key (Optional[str]): Roboflow API key - if not passed - will be looked in env under \"ROBOFLOW_API_KEY\"\n            and \"API_KEY\" variables. API key, passed in some form is required.\n        image_input_name (str): Name of input image defined in `workflow_specification` or Workflow definition saved\n            on the Roboflow Platform. `InferencePipeline` will be injecting video frames to workflow through that\n            parameter name.\n        workflows_parameters (Optional[Dict[str, Any]]): Dictionary with additional parameters that can be\n            defined within `workflow_specification`.\n        on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n            once prediction is ready - passing both decoded frame, their metadata and dict with workflow output.\n        max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n            dictate max FPS of each video source.\n            The implementation details of this option has been changed in release `v0.26.0`. Prior to the release\n            this value, when applied to video files caused the processing to wait `1 / max_fps` seconds before next\n            frame is processed - the new implementation drops the intermediate frames, which seems to be more\n            aligned with peoples expectations.\n            New behaviour is now enabled in experimental mode, by setting environmental variable flag\n            `ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True`. Please note that the new behaviour will\n            be the default one end of Q4 2024!\n        watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n            inference pipeline - if not given null implementation (doing nothing) will be used.\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n            status updates of all elements of the pipeline. Should be used only if detailed inspection of\n            pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n            fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n            without re-raising. Default: None.\n        source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n            video stream decoding behaviour. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n        source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n            video stream frames consumption. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n        video_source_properties (Optional[dict[str, float]]): Optional source properties to set up the video source,\n            corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source\n            will be used.\n            Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n        workflow_init_parameters (Optional[Dict[str, Any]]): Additional init parameters to be used by\n            workflows Execution Engine to init steps of your workflow - may be required when running workflows\n            with custom plugins.\n        workflows_thread_pool_workers (int): Number of workers for workflows thread pool which is used\n            by workflows blocks to run background tasks.\n        cancel_thread_pool_tasks_on_exit (bool): Flag to decide if unstated background tasks should be\n            canceled at the end of InferencePipeline processing. By default, when video file ends or\n            pipeline is stopped, tasks that has not started will be cancelled.\n        video_metadata_input_name (str): Name of input for video metadata defined in `workflow_specification` or\n            Workflow definition saved  on the Roboflow Platform. `InferencePipeline` will be injecting video frames\n            metadata to workflows through that parameter name.\n        batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n            to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n            frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n            unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n        profiling_directory (str): Directory where workflows profiler traces will be dumped. To enable profiling\n            export `ENABLE_WORKFLOWS_PROFILING=True` environmental variable. You may specify number of workflow\n            runs in a buffer with environmental variable `WORKFLOWS_PROFILER_BUFFER_SIZE=n` - making last `n`\n            frames to be present in buffer on processing end.\n        use_workflow_definition_cache (bool): Controls usage of cache for workflow definitions. Set this to False\n            when you frequently modify definition saved in Roboflow app and want to fetch the\n            newest version for the request. Only applies for Workflows definitions saved on Roboflow platform.\n\n    Other ENV variables involved in low-level configuration:\n    * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n    * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n    Returns: Instance of InferencePipeline\n\n    Throws:\n        * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n            always if connection to stream is lost.\n        * ValueError if workflow specification not provided and registered workflow not pointed out\n        * NotImplementedError if workflow used against multiple videos which is not supported yet\n        * MissingApiKeyError - if API key is not provided in situation when retrieving workflow definition\n            from Roboflow API is needed\n    \"\"\"\n    if ENABLE_WORKFLOWS_PROFILING:\n        profiler = BaseWorkflowsProfiler.init(\n            max_runs_in_buffer=WORKFLOWS_PROFILER_BUFFER_SIZE\n        )\n    else:\n        profiler = NullWorkflowsProfiler.init()\n    if api_key is None:\n        api_key = API_KEY\n    named_workflow_specified = (workspace_name is not None) and (\n        workflow_id is not None\n    )\n    if not (named_workflow_specified != (workflow_specification is not None)):\n        raise ValueError(\n            \"Parameters (`workspace_name`, `workflow_id`) can be used mutually exclusive with \"\n            \"`workflow_specification`, but at least one must be set.\"\n        )\n    try:\n        from inference.core.interfaces.stream.model_handlers.workflows import (\n            WorkflowRunner,\n        )\n        from inference.core.roboflow_api import get_workflow_specification\n        from inference.core.workflows.execution_engine.core import ExecutionEngine\n\n        if workflow_specification is None:\n            if api_key is None:\n                raise MissingApiKeyError(\n                    \"Roboflow API key needs to be provided either as parameter or via env variable \"\n                    \"ROBOFLOW_API_KEY. If you do not know how to get API key - visit \"\n                    \"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key to learn how to \"\n                    \"retrieve one.\"\n                )\n            with profiler.profile_execution_phase(\n                name=\"workflow_definition_fetching\",\n                categories=[\"inference_package_operation\"],\n            ):\n                workflow_specification = get_workflow_specification(\n                    api_key=api_key,\n                    workspace_id=workspace_name,\n                    workflow_id=workflow_id,\n                    use_cache=use_workflow_definition_cache,\n                )\n        model_registry = RoboflowModelRegistry(ROBOFLOW_MODEL_TYPES)\n        model_manager = BackgroundTaskActiveLearningManager(\n            model_registry=model_registry, cache=cache\n        )\n        model_manager = WithFixedSizeCache(\n            model_manager,\n            max_size=MAX_ACTIVE_MODELS,\n        )\n        if api_key is None:\n            api_key = API_KEY\n        if workflow_init_parameters is None:\n            workflow_init_parameters = {}\n        thread_pool_executor = ThreadPoolExecutor(\n            max_workers=workflows_thread_pool_workers\n        )\n        workflow_init_parameters[\"workflows_core.model_manager\"] = model_manager\n        workflow_init_parameters[\"workflows_core.api_key\"] = api_key\n        workflow_init_parameters[\"workflows_core.thread_pool_executor\"] = (\n            thread_pool_executor\n        )\n        execution_engine = ExecutionEngine.init(\n            workflow_definition=workflow_specification,\n            init_parameters=workflow_init_parameters,\n            workflow_id=workflow_id,\n            profiler=profiler,\n        )\n        workflow_runner = WorkflowRunner()\n        on_video_frame = partial(\n            workflow_runner.run_workflow,\n            workflows_parameters=workflows_parameters,\n            execution_engine=execution_engine,\n            image_input_name=image_input_name,\n            video_metadata_input_name=video_metadata_input_name,\n        )\n    except ImportError as error:\n        raise CannotInitialiseModelError(\n            f\"Could not initialise workflow processing due to lack of dependencies required. \"\n            f\"Please provide an issue report under https://github.com/roboflow/inference/issues\"\n        ) from error\n    on_pipeline_end_closure = partial(\n        on_pipeline_end,\n        thread_pool_executor=thread_pool_executor,\n        cancel_thread_pool_tasks_on_exit=cancel_thread_pool_tasks_on_exit,\n        profiler=profiler,\n        profiling_directory=profiling_directory,\n    )\n    return cls.init_with_custom_logic(\n        video_reference=video_reference,\n        on_video_frame=on_video_frame,\n        on_prediction=on_prediction,\n        on_pipeline_start=None,\n        on_pipeline_end=on_pipeline_end_closure,\n        max_fps=max_fps,\n        watchdog=watchdog,\n        status_update_handlers=status_update_handlers,\n        source_buffer_filling_strategy=source_buffer_filling_strategy,\n        source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n        video_source_properties=video_source_properties,\n        batch_collection_timeout=batch_collection_timeout,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/inference_pipeline/#inference.core.interfaces.stream.inference_pipeline.InferencePipeline.init_with_yolo_world","title":"<code>init_with_yolo_world(video_reference, classes, model_size='s', on_prediction=None, max_fps=None, watchdog=None, status_update_handlers=None, source_buffer_filling_strategy=None, source_buffer_consumption_strategy=None, class_agnostic_nms=None, confidence=None, iou_threshold=None, max_candidates=None, max_detections=None, video_source_properties=None, batch_collection_timeout=None, sink_mode=SinkMode.ADAPTIVE)</code>  <code>classmethod</code>","text":"<p>This class creates the abstraction for making inferences from YoloWorld against video stream. The way of how <code>InferencePipeline</code> works is displayed in <code>InferencePipeline.init(...)</code> initializer method.</p> <p>Parameters:</p> Name Type Description Default <code>video_reference</code> <code>Union[str, int, List[Union[str, int]]]</code> <p>Reference of source or sources to be used to make predictions against. It can be video file path, stream URL and device (like camera) id (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then it will trigger parallel processing of multiple sources. It has some implication on sinks. See: <code>sink_mode</code> parameter comments.</p> required <code>classes</code> <code>List[str]</code> <p>List of classes to execute zero-shot detection against</p> required <code>model_size</code> <code>str</code> <p>version of model - to be chosen from <code>s</code>, <code>m</code>, <code>l</code></p> <code>'s'</code> <code>on_prediction</code> <code>Callable[AnyPrediction, VideoFrame], None]</code> <p>Function to be called once prediction is ready - passing both decoded frame, their metadata and dict with standard Roboflow Object Detection prediction.</p> <code>None</code> <code>max_fps</code> <code>Optional[Union[float, int]]</code> <p>Specific value passed as this parameter will be used to dictate max FPS of each video source. The implementation details of this option has been changed in release <code>v0.26.0</code>. Prior to the release this value, when applied to video files caused the processing to wait <code>1 / max_fps</code> seconds before next frame is processed - the new implementation drops the intermediate frames, which seems to be more aligned with peoples expectations. New behaviour is now enabled in experimental mode, by setting environmental variable flag <code>ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True</code>. Please note that the new behaviour will be the default one end of Q4 2024!</p> <code>None</code> <code>watchdog</code> <code>Optional[PipelineWatchDog]</code> <p>Implementation of class that allows profiling of inference pipeline - if not given null implementation (doing nothing) will be used.</p> <code>None</code> <code>status_update_handlers</code> <code>Optional[List[Callable[[StatusUpdate], None]]]</code> <p>List of handlers to intercept status updates of all elements of the pipeline. Should be used only if detailed inspection of pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed fast - otherwise they will impair pipeline performance. All errors will be logged as warnings without re-raising. Default: None.</p> <code>None</code> <code>source_buffer_filling_strategy</code> <code>Optional[BufferFillingStrategy]</code> <p>Parameter dictating strategy for video stream decoding behaviour. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>source_buffer_consumption_strategy</code> <code>Optional[BufferConsumptionStrategy]</code> <p>Parameter dictating strategy for video stream frames consumption. By default - tweaked to the type of source given. Please find detailed explanation in docs of <code>VideoSource</code></p> <code>None</code> <code>class_agnostic_nms</code> <code>Optional[bool]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"</p> <code>None</code> <code>confidence</code> <code>Optional[float]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"CONFIDENCE\" with default \"0.5\"</p> <code>None</code> <code>iou_threshold</code> <code>Optional[float]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"IOU_THRESHOLD\" with default \"0.5\"</p> <code>None</code> <code>max_candidates</code> <code>Optional[int]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"MAX_CANDIDATES\" with default \"3000\"</p> <code>None</code> <code>max_detections</code> <code>Optional[int]</code> <p>Parameter of model post-processing. If not given - value checked in env variable \"MAX_DETECTIONS\" with default \"300\"</p> <code>None</code> <code>video_source_properties</code> <code>Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]</code> <p>Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. If not given, defaults for the video source will be used. It is optional and if provided can be provided as single dict (applicable for all sources) or as list of configs. Then the list must be of length of <code>video_reference</code> and may also contain None values to denote that specific source should remain not configured. Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}</p> <code>None</code> <code>batch_collection_timeout</code> <code>Optional[float]</code> <p>Parameter of multiplex_videos(...) dictating how long process to grab frames from multiple sources can wait for batch to be filled before yielding already collected frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows unstable latency. Visit <code>multiplex_videos(...)</code> for more information about multiplexing process.</p> <code>None</code> <code>sink_mode</code> <code>SinkMode</code> <p>Parameter that controls how video frames and predictions will be passed to sink handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink, in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned in the order of video sources - with None values in the place of vide_frames / predictions that were skipped due to <code>batch_collection_timeout</code>. <code>SinkMode.ADAPTIVE</code> is a middle ground (and default mode) - all old sources will work in that mode against a single video input, as the pipeline will behave as if running in <code>SinkMode.SEQUENTIAL</code>. To handle multiple videos - sink needs to accept <code>predictions: List[Optional[dict]]</code> and <code>video_frame: List[Optional[VideoFrame]]</code>. It is also possible to process multiple videos using old sinks - but then <code>SinkMode.SEQUENTIAL</code> is to be used, causing sink to be called on each prediction element.</p> <code>ADAPTIVE</code> <p>Other ENV variables involved in low-level configuration: * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop</p> <p>Returns: Instance of InferencePipeline</p> Throws <ul> <li>SourceConnectionError if source cannot be connected at start, however it attempts to reconnect     always if connection to stream is lost.</li> </ul> Source code in <code>inference/core/interfaces/stream/inference_pipeline.py</code> <pre><code>@classmethod\ndef init_with_yolo_world(\n    cls,\n    video_reference: Union[str, int, List[Union[str, int]]],\n    classes: List[str],\n    model_size: str = \"s\",\n    on_prediction: SinkHandler = None,\n    max_fps: Optional[Union[float, int]] = None,\n    watchdog: Optional[PipelineWatchDog] = None,\n    status_update_handlers: Optional[List[Callable[[StatusUpdate], None]]] = None,\n    source_buffer_filling_strategy: Optional[BufferFillingStrategy] = None,\n    source_buffer_consumption_strategy: Optional[BufferConsumptionStrategy] = None,\n    class_agnostic_nms: Optional[bool] = None,\n    confidence: Optional[float] = None,\n    iou_threshold: Optional[float] = None,\n    max_candidates: Optional[int] = None,\n    max_detections: Optional[int] = None,\n    video_source_properties: Optional[Dict[str, float]] = None,\n    batch_collection_timeout: Optional[float] = None,\n    sink_mode: SinkMode = SinkMode.ADAPTIVE,\n) -&gt; \"InferencePipeline\":\n    \"\"\"\n    This class creates the abstraction for making inferences from YoloWorld against video stream.\n    The way of how `InferencePipeline` works is displayed in `InferencePipeline.init(...)` initializer\n    method.\n\n    Args:\n        video_reference (Union[str, int, List[Union[str, int]]]): Reference of source or sources to be used to make\n            predictions against. It can be video file path, stream URL and device (like camera) id\n            (we handle whatever cv2 handles). It can also be a list of references (since v0.9.18) - and then\n            it will trigger parallel processing of multiple sources. It has some implication on sinks. See:\n            `sink_mode` parameter comments.\n        classes (List[str]): List of classes to execute zero-shot detection against\n        model_size (str): version of model - to be chosen from `s`, `m`, `l`\n        on_prediction (Callable[AnyPrediction, VideoFrame], None]): Function to be called\n            once prediction is ready - passing both decoded frame, their metadata and dict with standard\n            Roboflow Object Detection prediction.\n        max_fps (Optional[Union[float, int]]): Specific value passed as this parameter will be used to\n            dictate max FPS of each video source.\n            The implementation details of this option has been changed in release `v0.26.0`. Prior to the release\n            this value, when applied to video files caused the processing to wait `1 / max_fps` seconds before next\n            frame is processed - the new implementation drops the intermediate frames, which seems to be more\n            aligned with peoples expectations.\n            New behaviour is now enabled in experimental mode, by setting environmental variable flag\n            `ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True`. Please note that the new behaviour will\n            be the default one end of Q4 2024!\n        watchdog (Optional[PipelineWatchDog]): Implementation of class that allows profiling of\n            inference pipeline - if not given null implementation (doing nothing) will be used.\n        status_update_handlers (Optional[List[Callable[[StatusUpdate], None]]]): List of handlers to intercept\n            status updates of all elements of the pipeline. Should be used only if detailed inspection of\n            pipeline behaviour in time is needed. Please point out that handlers should be possible to be executed\n            fast - otherwise they will impair pipeline performance. All errors will be logged as warnings\n            without re-raising. Default: None.\n        source_buffer_filling_strategy (Optional[BufferFillingStrategy]): Parameter dictating strategy for\n            video stream decoding behaviour. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n        source_buffer_consumption_strategy (Optional[BufferConsumptionStrategy]): Parameter dictating strategy for\n            video stream frames consumption. By default - tweaked to the type of source given.\n            Please find detailed explanation in docs of [`VideoSource`](/docs/reference/inference/core/interfaces/camera/video_source/#inference.core.interfaces.camera.video_source.VideoSource)\n        class_agnostic_nms (Optional[bool]): Parameter of model post-processing. If not given - value checked in\n            env variable \"CLASS_AGNOSTIC_NMS\" with default \"False\"\n        confidence (Optional[float]): Parameter of model post-processing. If not given - value checked in\n            env variable \"CONFIDENCE\" with default \"0.5\"\n        iou_threshold (Optional[float]): Parameter of model post-processing. If not given - value checked in\n            env variable \"IOU_THRESHOLD\" with default \"0.5\"\n        max_candidates (Optional[int]): Parameter of model post-processing. If not given - value checked in\n            env variable \"MAX_CANDIDATES\" with default \"3000\"\n        max_detections (Optional[int]): Parameter of model post-processing. If not given - value checked in\n            env variable \"MAX_DETECTIONS\" with default \"300\"\n        video_source_properties (Optional[Union[Dict[str, float], List[Optional[Dict[str, float]]]]]):\n            Optional source properties to set up the video source, corresponding to cv2 VideoCapture properties\n            cv2.CAP_PROP_*. If not given, defaults for the video source will be used.\n            It is optional and if provided can be provided as single dict (applicable for all sources) or\n            as list of configs. Then the list must be of length of `video_reference` and may also contain None\n            values to denote that specific source should remain not configured.\n            Example valid properties are: {\"frame_width\": 1920, \"frame_height\": 1080, \"fps\": 30.0}\n        batch_collection_timeout (Optional[float]): Parameter of multiplex_videos(...) dictating how long process\n            to grab frames from multiple sources can wait for batch to be filled before yielding already collected\n            frames. Please set this value in PRODUCTION to avoid performance drops when specific sources shows\n            unstable latency. Visit `multiplex_videos(...)` for more information about multiplexing process.\n        sink_mode (SinkMode): Parameter that controls how video frames and predictions will be passed to sink\n            handler. With SinkMode.SEQUENTIAL - each frame and prediction triggers separate call for sink,\n            in case of SinkMode.BATCH - list of frames and predictions will be provided to sink, always aligned\n            in the order of video sources - with None values in the place of vide_frames / predictions that\n            were skipped due to `batch_collection_timeout`.\n            `SinkMode.ADAPTIVE` is a middle ground (and default mode) - all old sources will work in that mode\n            against a single video input, as the pipeline will behave as if running in `SinkMode.SEQUENTIAL`.\n            To handle multiple videos - sink needs to accept `predictions: List[Optional[dict]]` and\n            `video_frame: List[Optional[VideoFrame]]`. It is also possible to process multiple videos using\n            old sinks - but then `SinkMode.SEQUENTIAL` is to be used, causing sink to be called on each\n            prediction element.\n\n    Other ENV variables involved in low-level configuration:\n    * INFERENCE_PIPELINE_PREDICTIONS_QUEUE_SIZE - size of buffer for predictions that are ready for dispatching\n    * INFERENCE_PIPELINE_RESTART_ATTEMPT_DELAY - delay for restarts on stream connection drop\n\n    Returns: Instance of InferencePipeline\n\n    Throws:\n        * SourceConnectionError if source cannot be connected at start, however it attempts to reconnect\n            always if connection to stream is lost.\n    \"\"\"\n    inference_config = ModelConfig.init(\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        iou_threshold=iou_threshold,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n    )\n    try:\n        from inference.core.interfaces.stream.model_handlers.yolo_world import (\n            build_yolo_world_inference_function,\n        )\n\n        on_video_frame = build_yolo_world_inference_function(\n            model_id=f\"yolo_world/{model_size}\",\n            classes=classes,\n            inference_config=inference_config,\n        )\n    except ImportError as error:\n        raise CannotInitialiseModelError(\n            f\"Could not initialise yolo_world/{model_size} due to lack of sufficient dependencies. \"\n            f\"Use pip install inference[yolo-world] to install missing dependencies and try again.\"\n        ) from error\n    return cls.init_with_custom_logic(\n        video_reference=video_reference,\n        on_video_frame=on_video_frame,\n        on_prediction=on_prediction,\n        on_pipeline_start=None,\n        on_pipeline_end=None,\n        max_fps=max_fps,\n        watchdog=watchdog,\n        status_update_handlers=status_update_handlers,\n        source_buffer_filling_strategy=source_buffer_filling_strategy,\n        source_buffer_consumption_strategy=source_buffer_consumption_strategy,\n        video_source_properties=video_source_properties,\n        batch_collection_timeout=batch_collection_timeout,\n        sink_mode=sink_mode,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/","title":"sinks","text":""},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.UDPSink","title":"<code>UDPSink</code>","text":"Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>class UDPSink:\n    @classmethod\n    def init(cls, ip_address: str, port: int) -&gt; \"UDPSink\":\n        \"\"\"\n        Creates `InferencePipeline` predictions sink capable of sending model predictions over network\n        using UDP socket.\n\n        As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n        Args:\n            ip_address (str): IP address to send predictions\n            port (int): Port to send predictions\n\n        Returns: Initialised object of `UDPSink` class.\n        \"\"\"\n        udp_socket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n        udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n        return cls(\n            ip_address=ip_address,\n            port=port,\n            udp_socket=udp_socket,\n        )\n\n    def __init__(self, ip_address: str, port: int, udp_socket: socket.socket):\n        self._ip_address = ip_address\n        self._port = port\n        self._socket = udp_socket\n\n    def send_predictions(\n        self,\n        predictions: Union[dict, List[Optional[dict]]],\n        video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    ) -&gt; None:\n        \"\"\"\n        Method to send predictions via UDP socket. Useful in combination with `InferencePipeline` as\n        a sink for predictions.\n\n        Args:\n            predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n                processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n                should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n            video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n                by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n                to `predictions` list). Order is expected to match with `predictions`\n\n        Returns: None\n        Side effects: Sends serialised `predictions` and `video_frame` metadata via the UDP socket as\n            JSON string. It adds key named \"inference_metadata\" into `predictions` dict (mutating its\n            state). \"inference_metadata\" contain id of the frame, frame grabbing timestamp and message\n            emission time in datetime iso format.\n\n        Example:\n            ```python\n            import cv2\n            from inference.core.interfaces.stream.inference_pipeline import InferencePipeline\n            from inference.core.interfaces.stream.sinks import UDPSink\n\n            udp_sink = UDPSink.init(ip_address=\"127.0.0.1\", port=9090)\n\n            pipeline = InferencePipeline.init(\n                 model_id=\"your-model/3\",\n                 video_reference=\"./some_file.mp4\",\n                 on_prediction=udp_sink.send_predictions,\n            )\n            pipeline.start()\n            pipeline.join()\n            ```\n            `UDPSink` used in this way will emit predictions to receiver automatically.\n        \"\"\"\n        video_frame = wrap_in_list(element=video_frame)\n        predictions = wrap_in_list(element=predictions)\n        for single_frame, frame_predictions in zip(video_frame, predictions):\n            if single_frame is None:\n                continue\n            inference_metadata = {\n                \"source_id\": single_frame.source_id,\n                \"frame_id\": single_frame.frame_id,\n                \"frame_decoding_time\": single_frame.frame_timestamp.isoformat(),\n                \"emission_time\": datetime.now().isoformat(),\n            }\n            frame_predictions[\"inference_metadata\"] = inference_metadata\n            serialised_predictions = json.dumps(frame_predictions).encode(\"utf-8\")\n            self._socket.sendto(\n                serialised_predictions,\n                (\n                    self._ip_address,\n                    self._port,\n                ),\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.UDPSink.init","title":"<code>init(ip_address, port)</code>  <code>classmethod</code>","text":"<p>Creates <code>InferencePipeline</code> predictions sink capable of sending model predictions over network using UDP socket.</p> <p>As an <code>inference</code> user, please use .init() method instead of constructor to instantiate objects. Args:     ip_address (str): IP address to send predictions     port (int): Port to send predictions</p> <p>Returns: Initialised object of <code>UDPSink</code> class.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>@classmethod\ndef init(cls, ip_address: str, port: int) -&gt; \"UDPSink\":\n    \"\"\"\n    Creates `InferencePipeline` predictions sink capable of sending model predictions over network\n    using UDP socket.\n\n    As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n    Args:\n        ip_address (str): IP address to send predictions\n        port (int): Port to send predictions\n\n    Returns: Initialised object of `UDPSink` class.\n    \"\"\"\n    udp_socket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n    udp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n    return cls(\n        ip_address=ip_address,\n        port=port,\n        udp_socket=udp_socket,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.UDPSink.send_predictions","title":"<code>send_predictions(predictions, video_frame)</code>","text":"<p>Method to send predictions via UDP socket. Useful in combination with <code>InferencePipeline</code> as a sink for predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Union[dict, List[Optional[dict]]]</code> <p>Roboflow predictions, the function support single prediction processing and batch processing since version <code>0.9.18</code>. Batch predictions elements are optional, but should occur at the same position as <code>video_frame</code> list. Order is expected to match with <code>video_frame</code>.</p> required <code>video_frame</code> <code>Union[VideoFrame, List[Optional[VideoFrame]]]</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code> or list of frames from (it is possible for empty batch frames at corresponding positions to <code>predictions</code> list). Order is expected to match with <code>predictions</code></p> required <p>Side effects: Sends serialised <code>predictions</code> and <code>video_frame</code> metadata via the UDP socket as     JSON string. It adds key named \"inference_metadata\" into <code>predictions</code> dict (mutating its     state). \"inference_metadata\" contain id of the frame, frame grabbing timestamp and message     emission time in datetime iso format.</p> Example <p><pre><code>import cv2\nfrom inference.core.interfaces.stream.inference_pipeline import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import UDPSink\n\nudp_sink = UDPSink.init(ip_address=\"127.0.0.1\", port=9090)\n\npipeline = InferencePipeline.init(\n     model_id=\"your-model/3\",\n     video_reference=\"./some_file.mp4\",\n     on_prediction=udp_sink.send_predictions,\n)\npipeline.start()\npipeline.join()\n</code></pre> <code>UDPSink</code> used in this way will emit predictions to receiver automatically.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def send_predictions(\n    self,\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    \"\"\"\n    Method to send predictions via UDP socket. Useful in combination with `InferencePipeline` as\n    a sink for predictions.\n\n    Args:\n        predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n            processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n            should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n        video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n            by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n            to `predictions` list). Order is expected to match with `predictions`\n\n    Returns: None\n    Side effects: Sends serialised `predictions` and `video_frame` metadata via the UDP socket as\n        JSON string. It adds key named \"inference_metadata\" into `predictions` dict (mutating its\n        state). \"inference_metadata\" contain id of the frame, frame grabbing timestamp and message\n        emission time in datetime iso format.\n\n    Example:\n        ```python\n        import cv2\n        from inference.core.interfaces.stream.inference_pipeline import InferencePipeline\n        from inference.core.interfaces.stream.sinks import UDPSink\n\n        udp_sink = UDPSink.init(ip_address=\"127.0.0.1\", port=9090)\n\n        pipeline = InferencePipeline.init(\n             model_id=\"your-model/3\",\n             video_reference=\"./some_file.mp4\",\n             on_prediction=udp_sink.send_predictions,\n        )\n        pipeline.start()\n        pipeline.join()\n        ```\n        `UDPSink` used in this way will emit predictions to receiver automatically.\n    \"\"\"\n    video_frame = wrap_in_list(element=video_frame)\n    predictions = wrap_in_list(element=predictions)\n    for single_frame, frame_predictions in zip(video_frame, predictions):\n        if single_frame is None:\n            continue\n        inference_metadata = {\n            \"source_id\": single_frame.source_id,\n            \"frame_id\": single_frame.frame_id,\n            \"frame_decoding_time\": single_frame.frame_timestamp.isoformat(),\n            \"emission_time\": datetime.now().isoformat(),\n        }\n        frame_predictions[\"inference_metadata\"] = inference_metadata\n        serialised_predictions = json.dumps(frame_predictions).encode(\"utf-8\")\n        self._socket.sendto(\n            serialised_predictions,\n            (\n                self._ip_address,\n                self._port,\n            ),\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.VideoFileSink","title":"<code>VideoFileSink</code>","text":"Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>class VideoFileSink:\n    @classmethod\n    def init(\n        cls,\n        video_file_name: str,\n        annotator: Optional[Union[BaseAnnotator, List[BaseAnnotator]]] = None,\n        display_size: Optional[Tuple[int, int]] = (1280, 720),\n        fps_monitor: Optional[sv.FPSMonitor] = DEFAULT_FPS_MONITOR,\n        display_statistics: bool = False,\n        output_fps: int = 25,\n        quiet: bool = False,\n        video_frame_size: Tuple[int, int] = (1280, 720),\n    ) -&gt; \"VideoFileSink\":\n        \"\"\"\n        Creates `InferencePipeline` predictions sink capable of saving model predictions into video file.\n        It works both for pipelines with single input video and multiple ones.\n\n        As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n        Args:\n            video_file_name (str): name of the video file to save predictions\n            annotator (Union[BaseAnnotator, List[BaseAnnotator]]): instance of class inheriting from supervision BaseAnnotator\n                or list of such instances. If nothing is passed chain of `sv.BoundingBoxAnnotator()` and `sv.LabelAnnotator()` is used.\n            display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output. Should\n                be set to the same value as `display_size` for InferencePipeline with single video source, otherwise\n                it represents the size of single visualisation tile (whole tiles mosaic will be scaled to\n                `video_frame_size`)\n            fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput\n            display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,\n                if enabled, throughput will only be presented if `fps_monitor` is not None\n            output_fps (int): desired FPS of output file\n            quiet (bool): Flag to decide whether to log progress\n            video_frame_size (Tuple[int, int]): The size of frame in target video file.\n\n        Attributes:\n            on_prediction (Callable[[dict, VideoFrame], None]): callable to be used as a sink for predictions\n\n        Returns: Initialized object of `VideoFileSink` class.\n\n        Example:\n            ```python\n            import cv2\n            from inference import InferencePipeline\n            from inference.core.interfaces.stream.sinks import VideoFileSink\n\n            video_sink = VideoFileSink.init(video_file_name=\"output.avi\")\n\n            pipeline = InferencePipeline.init(\n                model_id=\"your-model/3\",\n                video_reference=\"./some_file.mp4\",\n                on_prediction=video_sink.on_prediction,\n            )\n            pipeline.start()\n            pipeline.join()\n            video_sink.release()\n            ```\n\n            `VideoFileSink` used in this way will save predictions to video file automatically.\n        \"\"\"\n        return cls(\n            video_file_name=video_file_name,\n            annotator=annotator,\n            display_size=display_size,\n            fps_monitor=fps_monitor,\n            display_statistics=display_statistics,\n            output_fps=output_fps,\n            quiet=quiet,\n            video_frame_size=video_frame_size,\n        )\n\n    def __init__(\n        self,\n        video_file_name: str,\n        annotator: Union[BaseAnnotator, List[BaseAnnotator]],\n        display_size: Optional[Tuple[int, int]],\n        fps_monitor: Optional[sv.FPSMonitor],\n        display_statistics: bool,\n        output_fps: int,\n        quiet: bool,\n        video_frame_size: Tuple[int, int],\n    ):\n        self._video_file_name = video_file_name\n        self._annotator = annotator\n        self._display_size = display_size\n        self._fps_monitor = fps_monitor\n        self._display_statistics = display_statistics\n        self._output_fps = output_fps\n        self._quiet = quiet\n        self._frame_idx = 0\n        self._video_frame_size = video_frame_size\n        self._video_writer: Optional[cv2.VideoWriter] = None\n        self.on_prediction = partial(\n            render_boxes,\n            annotator=self._annotator,\n            display_size=self._display_size,\n            fps_monitor=self._fps_monitor,\n            display_statistics=self._display_statistics,\n            on_frame_rendered=self._save_predictions,\n        )\n\n    def release(self) -&gt; None:\n        \"\"\"\n        Releases VideoWriter object.\n        \"\"\"\n        if self._video_writer is not None and self._video_writer.isOpened():\n            self._video_writer.release()\n\n    def _save_predictions(\n        self,\n        frame: Union[ImageWithSourceID, List[ImageWithSourceID]],\n    ) -&gt; None:\n        if self._video_writer is None:\n            self._initialise_sink()\n        if issubclass(type(frame), list):\n            frame = create_tiles(images=[i[1] for i in frame])\n        else:\n            frame = frame[1]\n        if (frame.shape[1], frame.shape[0]) != self._video_frame_size:\n            frame = letterbox_image(image=frame, desired_size=self._video_frame_size)\n        self._video_writer.write(frame)\n        if not self._quiet:\n            print(f\"Writing frame {self._frame_idx}\", end=\"\\r\")\n        self._frame_idx += 1\n\n    def _initialise_sink(self) -&gt; None:\n        self._video_writer = cv2.VideoWriter(\n            self._video_file_name,\n            cv2.VideoWriter_fourcc(*\"MJPG\"),\n            self._output_fps,\n            self._video_frame_size,\n        )\n\n    def __enter__(self) -&gt; \"VideoFileSink\":\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        self.release()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.VideoFileSink.init","title":"<code>init(video_file_name, annotator=None, display_size=(1280, 720), fps_monitor=DEFAULT_FPS_MONITOR, display_statistics=False, output_fps=25, quiet=False, video_frame_size=(1280, 720))</code>  <code>classmethod</code>","text":"<p>Creates <code>InferencePipeline</code> predictions sink capable of saving model predictions into video file. It works both for pipelines with single input video and multiple ones.</p> <p>As an <code>inference</code> user, please use .init() method instead of constructor to instantiate objects. Args:     video_file_name (str): name of the video file to save predictions     annotator (Union[BaseAnnotator, List[BaseAnnotator]]): instance of class inheriting from supervision BaseAnnotator         or list of such instances. If nothing is passed chain of <code>sv.BoundingBoxAnnotator()</code> and <code>sv.LabelAnnotator()</code> is used.     display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output. Should         be set to the same value as <code>display_size</code> for InferencePipeline with single video source, otherwise         it represents the size of single visualisation tile (whole tiles mosaic will be scaled to         <code>video_frame_size</code>)     fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput     display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,         if enabled, throughput will only be presented if <code>fps_monitor</code> is not None     output_fps (int): desired FPS of output file     quiet (bool): Flag to decide whether to log progress     video_frame_size (Tuple[int, int]): The size of frame in target video file.</p> <p>Attributes:</p> Name Type Description <code>on_prediction</code> <code>Callable[[dict, VideoFrame], None]</code> <p>callable to be used as a sink for predictions</p> <p>Returns: Initialized object of <code>VideoFileSink</code> class.</p> Example <pre><code>import cv2\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import VideoFileSink\n\nvideo_sink = VideoFileSink.init(video_file_name=\"output.avi\")\n\npipeline = InferencePipeline.init(\n    model_id=\"your-model/3\",\n    video_reference=\"./some_file.mp4\",\n    on_prediction=video_sink.on_prediction,\n)\npipeline.start()\npipeline.join()\nvideo_sink.release()\n</code></pre> <p><code>VideoFileSink</code> used in this way will save predictions to video file automatically.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>@classmethod\ndef init(\n    cls,\n    video_file_name: str,\n    annotator: Optional[Union[BaseAnnotator, List[BaseAnnotator]]] = None,\n    display_size: Optional[Tuple[int, int]] = (1280, 720),\n    fps_monitor: Optional[sv.FPSMonitor] = DEFAULT_FPS_MONITOR,\n    display_statistics: bool = False,\n    output_fps: int = 25,\n    quiet: bool = False,\n    video_frame_size: Tuple[int, int] = (1280, 720),\n) -&gt; \"VideoFileSink\":\n    \"\"\"\n    Creates `InferencePipeline` predictions sink capable of saving model predictions into video file.\n    It works both for pipelines with single input video and multiple ones.\n\n    As an `inference` user, please use .init() method instead of constructor to instantiate objects.\n    Args:\n        video_file_name (str): name of the video file to save predictions\n        annotator (Union[BaseAnnotator, List[BaseAnnotator]]): instance of class inheriting from supervision BaseAnnotator\n            or list of such instances. If nothing is passed chain of `sv.BoundingBoxAnnotator()` and `sv.LabelAnnotator()` is used.\n        display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output. Should\n            be set to the same value as `display_size` for InferencePipeline with single video source, otherwise\n            it represents the size of single visualisation tile (whole tiles mosaic will be scaled to\n            `video_frame_size`)\n        fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput\n        display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,\n            if enabled, throughput will only be presented if `fps_monitor` is not None\n        output_fps (int): desired FPS of output file\n        quiet (bool): Flag to decide whether to log progress\n        video_frame_size (Tuple[int, int]): The size of frame in target video file.\n\n    Attributes:\n        on_prediction (Callable[[dict, VideoFrame], None]): callable to be used as a sink for predictions\n\n    Returns: Initialized object of `VideoFileSink` class.\n\n    Example:\n        ```python\n        import cv2\n        from inference import InferencePipeline\n        from inference.core.interfaces.stream.sinks import VideoFileSink\n\n        video_sink = VideoFileSink.init(video_file_name=\"output.avi\")\n\n        pipeline = InferencePipeline.init(\n            model_id=\"your-model/3\",\n            video_reference=\"./some_file.mp4\",\n            on_prediction=video_sink.on_prediction,\n        )\n        pipeline.start()\n        pipeline.join()\n        video_sink.release()\n        ```\n\n        `VideoFileSink` used in this way will save predictions to video file automatically.\n    \"\"\"\n    return cls(\n        video_file_name=video_file_name,\n        annotator=annotator,\n        display_size=display_size,\n        fps_monitor=fps_monitor,\n        display_statistics=display_statistics,\n        output_fps=output_fps,\n        quiet=quiet,\n        video_frame_size=video_frame_size,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.VideoFileSink.release","title":"<code>release()</code>","text":"<p>Releases VideoWriter object.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def release(self) -&gt; None:\n    \"\"\"\n    Releases VideoWriter object.\n    \"\"\"\n    if self._video_writer is not None and self._video_writer.isOpened():\n        self._video_writer.release()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.active_learning_sink","title":"<code>active_learning_sink(predictions, video_frame, active_learning_middleware, model_type, disable_preproc_auto_orient=False)</code>","text":"<p>Function to serve as Active Learning sink for InferencePipeline.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Union[dict, List[Optional[dict]]]</code> <p>Roboflow predictions, the function support single prediction processing and batch processing since version <code>0.9.18</code>. Batch predictions elements are optional, but should occur at the same position as <code>video_frame</code> list. Order is expected to match with <code>video_frame</code>.</p> required <code>video_frame</code> <code>Union[VideoFrame, List[Optional[VideoFrame]]]</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code> or list of frames from (it is possible for empty batch frames at corresponding positions to <code>predictions</code> list). Order is expected to match with <code>predictions</code></p> required <code>active_learning_middleware</code> <code>ActiveLearningMiddleware</code> <p>instance of middleware to register data.</p> required <code>model_type</code> <code>str</code> <p>Type of Roboflow model in use</p> required <code>disable_preproc_auto_orient</code> <code>bool</code> <p>Flag to denote how image is preprocessed which is important in Active Learning.</p> <code>False</code> <p>Returns: None Side effects: Can register data and predictions in Roboflow backend if that's the evaluation of sampling engine.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def active_learning_sink(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    active_learning_middleware: ActiveLearningMiddleware,\n    model_type: str,\n    disable_preproc_auto_orient: bool = False,\n) -&gt; None:\n    \"\"\"\n    Function to serve as Active Learning sink for InferencePipeline.\n\n    Args:\n        predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n            processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n            should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n        video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n            by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n            to `predictions` list). Order is expected to match with `predictions`\n        active_learning_middleware (ActiveLearningMiddleware): instance of middleware to register data.\n        model_type (str): Type of Roboflow model in use\n        disable_preproc_auto_orient (bool): Flag to denote how image is preprocessed which is important in\n            Active Learning.\n\n    Returns: None\n    Side effects: Can register data and predictions in Roboflow backend if that's the evaluation of sampling engine.\n    \"\"\"\n    video_frame = wrap_in_list(element=video_frame)\n    predictions = wrap_in_list(element=predictions)\n    images = [f.image for f in video_frame if f is not None]\n    predictions = [p for p in predictions if p is not None]\n    active_learning_middleware.register_batch(\n        inference_inputs=images,\n        predictions=predictions,\n        prediction_type=model_type,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.multi_sink","title":"<code>multi_sink(predictions, video_frame, sinks)</code>","text":"<p>Helper util useful to combine multiple sinks together, while using <code>InferencePipeline</code>.</p> <p>Parameters:</p> Name Type Description Default <code>video_frame</code> <code>VideoFrame</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code></p> required <code>predictions</code> <code>dict</code> <p>Roboflow object detection predictions with Bounding Boxes</p> required <code>sinks</code> <code>List[Callable[[VideoFrame, dict], None]]</code> <p>list of sinks to be used. Each will be executed one-by-one in the order pointed in input list, all errors will be caught and reported via logger, without re-raising.</p> required <p>Returns: None Side effects: Uses all sinks in context if (video_frame, predictions) input.</p> Example <pre><code>from functools import partial\nimport cv2\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import UDPSink, render_boxes\n\nudp_sink = UDPSink(ip_address=\"127.0.0.1\", port=9090)\non_prediction = partial(multi_sink, sinks=[udp_sink.send_predictions, render_boxes])\n\npipeline = InferencePipeline.init(\n    model_id=\"your-model/3\",\n    video_reference=\"./some_file.mp4\",\n    on_prediction=on_prediction,\n)\npipeline.start()\npipeline.join()\n</code></pre> <p>As a result, predictions will both be sent via UDP socket and displayed in the screen.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def multi_sink(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    sinks: List[SinkHandler],\n) -&gt; None:\n    \"\"\"\n    Helper util useful to combine multiple sinks together, while using `InferencePipeline`.\n\n    Args:\n        video_frame (VideoFrame): frame of video with its basic metadata emitted by `VideoSource`\n        predictions (dict): Roboflow object detection predictions with Bounding Boxes\n        sinks (List[Callable[[VideoFrame, dict], None]]): list of sinks to be used. Each will be executed\n            one-by-one in the order pointed in input list, all errors will be caught and reported via logger,\n            without re-raising.\n\n    Returns: None\n    Side effects: Uses all sinks in context if (video_frame, predictions) input.\n\n    Example:\n        ```python\n        from functools import partial\n        import cv2\n        from inference import InferencePipeline\n        from inference.core.interfaces.stream.sinks import UDPSink, render_boxes\n\n        udp_sink = UDPSink(ip_address=\"127.0.0.1\", port=9090)\n        on_prediction = partial(multi_sink, sinks=[udp_sink.send_predictions, render_boxes])\n\n        pipeline = InferencePipeline.init(\n            model_id=\"your-model/3\",\n            video_reference=\"./some_file.mp4\",\n            on_prediction=on_prediction,\n        )\n        pipeline.start()\n        pipeline.join()\n        ```\n\n        As a result, predictions will both be sent via UDP socket and displayed in the screen.\n    \"\"\"\n    for sink in sinks:\n        try:\n            sink(predictions, video_frame)\n        except Exception as error:\n            logger.error(\n                f\"Could not sent prediction with to sink due to error: {error}.\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.render_boxes","title":"<code>render_boxes(predictions, video_frame, annotator=None, display_size=(1280, 720), fps_monitor=DEFAULT_FPS_MONITOR, display_statistics=False, on_frame_rendered=display_image)</code>","text":"<p>Helper tool to render object detection predictions on top of video frame. It is designed to be used with <code>InferencePipeline</code>, as sink for predictions. By default it uses standard <code>sv.BoundingBoxAnnotator()</code> chained with <code>sv.LabelAnnotator()</code> to draw bounding boxes and resizes prediction to 1280x720 (keeping aspect ratio and adding black padding). One may configure default behaviour, for instance to display latency and throughput statistics. In batch mode it will display tiles of frames and overlay predictions.</p> <p>This sink is only partially compatible with stubs and classification models (it will not fail, although predictions will not be displayed).</p> <p>Since version <code>0.9.18</code>, when multi-source InferencePipeline was introduced - it support batch input, without changes to old functionality when single (predictions, video_frame) is used.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Union[dict, List[Optional[dict]]]</code> <p>Roboflow predictions, the function support single prediction processing and batch processing since version <code>0.9.18</code>. Batch predictions elements are optional, but should occur at the same position as <code>video_frame</code> list. Order is expected to match with <code>video_frame</code>.</p> required <code>video_frame</code> <code>Union[VideoFrame, List[Optional[VideoFrame]]]</code> <p>frame of video with its basic metadata emitted by <code>VideoSource</code> or list of frames from (it is possible for empty batch frames at corresponding positions to <code>predictions</code> list). Order is expected to match with <code>predictions</code></p> required <code>annotator</code> <code>Union[BaseAnnotator, List[BaseAnnotator]]</code> <p>instance of class inheriting from supervision BaseAnnotator or list of such instances. If nothing is passed chain of <code>sv.BoundingBoxAnnotator()</code> and <code>sv.LabelAnnotator()</code> is used.</p> <code>None</code> <code>display_size</code> <code>Tuple[int, int]</code> <p>tuple in format (width, height) to resize visualisation output</p> <code>(1280, 720)</code> <code>fps_monitor</code> <code>Optional[FPSMonitor]</code> <p>FPS monitor used to monitor throughput</p> <code>DEFAULT_FPS_MONITOR</code> <code>display_statistics</code> <code>bool</code> <p>Flag to decide if throughput and latency can be displayed in the result image, if enabled, throughput will only be presented if <code>fps_monitor</code> is not None</p> <code>False</code> <code>on_frame_rendered</code> <code>Callable[[Union[ImageWithSourceID, List[ImageWithSourceID]]], None]</code> <p>callback to be called once frame is rendered - by default, function will display OpenCV window. It expects optional integer identifier with np.ndarray or list of those elements. Identifier is supposed to refer to either source_id (for sequential input) or position in the batch (from 0 to batch_size-1).</p> <code>display_image</code> <p>Side effects: on_frame_rendered() is called against the tuple (stream_id, np.ndarray) produced from video     frame and predictions.</p> Example <pre><code>from functools import partial\nimport cv2\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\noutput_size = (640, 480)\nvideo_sink = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 25.0, output_size)\non_prediction = partial(\n    render_boxes,\n    display_size=output_size,\n    on_frame_rendered=lambda frame_data: video_sink.write(frame_data[1])\n)\n\npipeline = InferencePipeline.init(\n     model_id=\"your-model/3\",\n     video_reference=\"./some_file.mp4\",\n     on_prediction=on_prediction,\n)\npipeline.start()\npipeline.join()\nvideo_sink.release()\n</code></pre> <p>In this example, <code>render_boxes()</code> is used as a sink for <code>InferencePipeline</code> predictions - making frames with predictions displayed to be saved into video file. Please note that this is oversimplified example of usage which will not be robust against multiple streams - better implementation available in <code>VideoFileSink</code> class.</p> Source code in <code>inference/core/interfaces/stream/sinks.py</code> <pre><code>def render_boxes(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n    annotator: Union[BaseAnnotator, List[BaseAnnotator]] = None,\n    display_size: Optional[Tuple[int, int]] = (1280, 720),\n    fps_monitor: Optional[sv.FPSMonitor] = DEFAULT_FPS_MONITOR,\n    display_statistics: bool = False,\n    on_frame_rendered: Callable[\n        [Union[ImageWithSourceID, List[ImageWithSourceID]]], None\n    ] = display_image,\n) -&gt; None:\n    \"\"\"\n    Helper tool to render object detection predictions on top of video frame. It is designed\n    to be used with `InferencePipeline`, as sink for predictions. By default it uses\n    standard `sv.BoundingBoxAnnotator()` chained with `sv.LabelAnnotator()`\n    to draw bounding boxes and resizes prediction to 1280x720 (keeping aspect ratio and adding black padding).\n    One may configure default behaviour, for instance to display latency and throughput statistics.\n    In batch mode it will display tiles of frames and overlay predictions.\n\n    This sink is only partially compatible with stubs and classification models (it will not fail,\n    although predictions will not be displayed).\n\n    Since version `0.9.18`, when multi-source InferencePipeline was introduced - it support batch input, without\n    changes to old functionality when single (predictions, video_frame) is used.\n\n    Args:\n        predictions (Union[dict, List[Optional[dict]]]): Roboflow predictions, the function support single prediction\n            processing and batch processing since version `0.9.18`. Batch predictions elements are optional, but\n            should occur at the same position as `video_frame` list. Order is expected to match with `video_frame`.\n        video_frame (Union[VideoFrame, List[Optional[VideoFrame]]]): frame of video with its basic metadata emitted\n            by `VideoSource` or list of frames from (it is possible for empty batch frames at corresponding positions\n            to `predictions` list). Order is expected to match with `predictions`\n        annotator (Union[BaseAnnotator, List[BaseAnnotator]]): instance of class inheriting from supervision BaseAnnotator\n            or list of such instances. If nothing is passed chain of `sv.BoundingBoxAnnotator()` and `sv.LabelAnnotator()` is used.\n        display_size (Tuple[int, int]): tuple in format (width, height) to resize visualisation output\n        fps_monitor (Optional[sv.FPSMonitor]): FPS monitor used to monitor throughput\n        display_statistics (bool): Flag to decide if throughput and latency can be displayed in the result image,\n            if enabled, throughput will only be presented if `fps_monitor` is not None\n        on_frame_rendered (Callable[[Union[ImageWithSourceID, List[ImageWithSourceID]]], None]): callback to be\n            called once frame is rendered - by default, function will display OpenCV window. It expects optional integer\n            identifier with np.ndarray or list of those elements. Identifier is supposed to refer to either source_id\n            (for sequential input) or position in the batch (from 0 to batch_size-1).\n\n    Returns: None\n    Side effects: on_frame_rendered() is called against the tuple (stream_id, np.ndarray) produced from video\n        frame and predictions.\n\n    Example:\n        ```python\n        from functools import partial\n        import cv2\n        from inference import InferencePipeline\n        from inference.core.interfaces.stream.sinks import render_boxes\n\n        output_size = (640, 480)\n        video_sink = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 25.0, output_size)\n        on_prediction = partial(\n            render_boxes,\n            display_size=output_size,\n            on_frame_rendered=lambda frame_data: video_sink.write(frame_data[1])\n        )\n\n        pipeline = InferencePipeline.init(\n             model_id=\"your-model/3\",\n             video_reference=\"./some_file.mp4\",\n             on_prediction=on_prediction,\n        )\n        pipeline.start()\n        pipeline.join()\n        video_sink.release()\n        ```\n\n        In this example, `render_boxes()` is used as a sink for `InferencePipeline` predictions - making frames with\n        predictions displayed to be saved into video file. Please note that this is oversimplified example of usage\n        which will not be robust against multiple streams - better implementation available in `VideoFileSink` class.\n    \"\"\"\n    sequential_input_provided = False\n    if not isinstance(video_frame, list):\n        sequential_input_provided = True\n    video_frame = wrap_in_list(element=video_frame)\n    predictions = wrap_in_list(element=predictions)\n    if annotator is None:\n        annotator = [\n            DEFAULT_BBOX_ANNOTATOR,\n            DEFAULT_LABEL_ANNOTATOR,\n        ]\n    fps_value = None\n    if fps_monitor is not None:\n        ticks = sum(f is not None for f in video_frame)\n        for _ in range(ticks):\n            fps_monitor.tick()\n        if hasattr(fps_monitor, \"fps\"):\n            fps_value = fps_monitor.fps\n        else:\n            fps_value = fps_monitor()\n    images: List[ImageWithSourceID] = []\n    annotators = annotator if isinstance(annotator, list) else [annotator]\n    for idx, (single_frame, frame_prediction) in enumerate(\n        zip(video_frame, predictions)\n    ):\n        image = _handle_frame_rendering(\n            frame=single_frame,\n            prediction=frame_prediction,\n            annotators=annotators,\n            display_size=display_size,\n            display_statistics=display_statistics,\n            fps_value=fps_value,\n        )\n        images.append((idx, image))\n    if sequential_input_provided:\n        on_frame_rendered((video_frame[0].source_id, images[0][1]))\n    else:\n        on_frame_rendered(images)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/","title":"stream","text":""},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream","title":"<code>Stream</code>","text":"<p>               Bases: <code>BaseInterface</code></p> <p>Roboflow defined stream interface for a general-purpose inference server.</p> <p>Attributes:</p> Name Type Description <code>model_manager</code> <code>ModelManager</code> <p>The manager that handles model inference tasks.</p> <code>model_registry</code> <code>RoboflowModelRegistry</code> <p>The registry to fetch model instances.</p> <code>api_key</code> <code>str</code> <p>The API key for accessing models.</p> <code>class_agnostic_nms</code> <code>bool</code> <p>Flag for class-agnostic non-maximum suppression.</p> <code>confidence</code> <code>float</code> <p>Confidence threshold for inference.</p> <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold for detection.</p> <code>json_response</code> <code>bool</code> <p>Flag to toggle JSON response format.</p> <code>max_candidates</code> <code>float</code> <p>The maximum number of candidates for detection.</p> <code>max_detections</code> <code>float</code> <p>The maximum number of detections.</p> <code>model</code> <code>str | Callable</code> <p>The model to be used.</p> <code>stream_id</code> <code>str</code> <p>The ID of the stream to be used.</p> <code>use_bytetrack</code> <code>bool</code> <p>Flag to use bytetrack,</p> <p>Methods:</p> Name Description <code>init_infer</code> <p>Initialize the inference with a test frame.</p> <code>preprocess_thread</code> <p>Preprocess incoming frames for inference.</p> <code>inference_request_thread</code> <p>Manage the inference requests.</p> <code>run_thread</code> <p>Run the preprocessing and inference threads.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>class Stream(BaseInterface):\n    \"\"\"Roboflow defined stream interface for a general-purpose inference server.\n\n    Attributes:\n        model_manager (ModelManager): The manager that handles model inference tasks.\n        model_registry (RoboflowModelRegistry): The registry to fetch model instances.\n        api_key (str): The API key for accessing models.\n        class_agnostic_nms (bool): Flag for class-agnostic non-maximum suppression.\n        confidence (float): Confidence threshold for inference.\n        iou_threshold (float): The intersection-over-union threshold for detection.\n        json_response (bool): Flag to toggle JSON response format.\n        max_candidates (float): The maximum number of candidates for detection.\n        max_detections (float): The maximum number of detections.\n        model (str|Callable): The model to be used.\n        stream_id (str): The ID of the stream to be used.\n        use_bytetrack (bool): Flag to use bytetrack,\n\n    Methods:\n        init_infer: Initialize the inference with a test frame.\n        preprocess_thread: Preprocess incoming frames for inference.\n        inference_request_thread: Manage the inference requests.\n        run_thread: Run the preprocessing and inference threads.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str = API_KEY,\n        class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n        confidence: float = CONFIDENCE,\n        enforce_fps: bool = ENFORCE_FPS,\n        iou_threshold: float = IOU_THRESHOLD,\n        max_candidates: float = MAX_CANDIDATES,\n        max_detections: float = MAX_DETECTIONS,\n        model: Union[str, Callable] = MODEL_ID,\n        source: Union[int, str] = STREAM_ID,\n        use_bytetrack: bool = ENABLE_BYTE_TRACK,\n        use_main_thread: bool = False,\n        output_channel_order: str = \"RGB\",\n        on_prediction: Callable = None,\n        on_start: Callable = None,\n        on_stop: Callable = None,\n    ):\n        \"\"\"Initialize the stream with the given parameters.\n        Prints the server settings and initializes the inference with a test frame.\n        \"\"\"\n        logger.info(\"Initializing server\")\n\n        self.frame_count = 0\n        self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n        self.use_bytetrack = use_bytetrack\n\n        if source == \"webcam\":\n            stream_id = 0\n        else:\n            stream_id = source\n\n        self.stream_id = stream_id\n        if self.stream_id is None:\n            raise ValueError(\"STREAM_ID is not defined\")\n        self.model_id = model\n        if not self.model_id:\n            raise ValueError(\"MODEL_ID is not defined\")\n        self.api_key = api_key\n\n        self.active_learning_middleware = NullActiveLearningMiddleware()\n        if isinstance(model, str):\n            self.model = get_model(model, self.api_key)\n            if ACTIVE_LEARNING_ENABLED:\n                self.active_learning_middleware = (\n                    ThreadingActiveLearningMiddleware.init(\n                        api_key=self.api_key,\n                        model_id=self.model_id,\n                        cache=cache,\n                    )\n                )\n            self.task_type = get_model_type(\n                model_id=self.model_id, api_key=self.api_key\n            )[0]\n        else:\n            self.model = model\n            self.task_type = \"unknown\"\n\n        self.class_agnostic_nms = class_agnostic_nms\n        self.confidence = confidence\n        self.iou_threshold = iou_threshold\n        self.max_candidates = max_candidates\n        self.max_detections = max_detections\n        self.use_main_thread = use_main_thread\n        self.output_channel_order = output_channel_order\n\n        self.inference_request_type = (\n            inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n        )\n\n        self.webcam_stream = WebcamStream(\n            stream_id=self.stream_id, enforce_fps=enforce_fps\n        )\n        logger.info(\n            f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n        )\n\n        self.on_start_callbacks = []\n        self.on_stop_callbacks = [\n            lambda: self.active_learning_middleware.stop_registration_thread()\n        ]\n        self.on_prediction_callbacks = []\n\n        if on_prediction:\n            self.on_prediction_callbacks.append(on_prediction)\n\n        if on_start:\n            self.on_start_callbacks.append(on_start)\n\n        if on_stop:\n            self.on_stop_callbacks.append(on_stop)\n\n        self.init_infer()\n        self.preproc_result = None\n        self.inference_request_obj = None\n        self.queue_control = False\n        self.inference_response = None\n        self.stop = False\n\n        self.frame = None\n        self.frame_cv = None\n        self.frame_id = None\n        logger.info(\"Server initialized with settings:\")\n        logger.info(f\"Stream ID: {self.stream_id}\")\n        logger.info(f\"Model ID: {self.model_id}\")\n        logger.info(f\"Enforce FPS: {enforce_fps}\")\n        logger.info(f\"Confidence: {self.confidence}\")\n        logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n        logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n        logger.info(f\"Max Candidates: {self.max_candidates}\")\n        logger.info(f\"Max Detections: {self.max_detections}\")\n\n        self.run_thread()\n\n    def on_start(self, callback):\n        self.on_start_callbacks.append(callback)\n\n        unsubscribe = lambda: self.on_start_callbacks.remove(callback)\n        return unsubscribe\n\n    def on_stop(self, callback):\n        self.on_stop_callbacks.append(callback)\n\n        unsubscribe = lambda: self.on_stop_callbacks.remove(callback)\n        return unsubscribe\n\n    def on_prediction(self, callback):\n        self.on_prediction_callbacks.append(callback)\n\n        unsubscribe = lambda: self.on_prediction_callbacks.remove(callback)\n        return unsubscribe\n\n    def init_infer(self):\n        \"\"\"Initialize the inference with a test frame.\n\n        Creates a test frame and runs it through the entire inference process to ensure everything is working.\n        \"\"\"\n        frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n        self.model.infer(\n            frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n        )\n        self.active_learning_middleware.start_registration_thread()\n\n    def preprocess_thread(self):\n        \"\"\"Preprocess incoming frames for inference.\n\n        Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n        inference.\n        \"\"\"\n        webcam_stream = self.webcam_stream\n        webcam_stream.start()\n        # processing frames in input stream\n        try:\n            while True:\n                if webcam_stream.stopped is True or self.stop:\n                    break\n                else:\n                    self.frame_cv, frame_id = webcam_stream.read_opencv()\n                    if frame_id &gt; 0 and frame_id != self.frame_id:\n                        self.frame_id = frame_id\n                        self.frame = cv2.cvtColor(self.frame_cv, cv2.COLOR_BGR2RGB)\n                        self.preproc_result = self.model.preprocess(self.frame_cv)\n                        self.img_in, self.img_dims = self.preproc_result\n                        self.queue_control = True\n\n        except Exception as e:\n            traceback.print_exc()\n            logger.error(e)\n\n    def inference_request_thread(self):\n        \"\"\"Manage the inference requests.\n\n        Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n        to registered callbacks.\n        \"\"\"\n        last_print = time.perf_counter()\n        print_ind = 0\n        while True:\n            if self.webcam_stream.stopped is True or self.stop:\n                while len(self.on_stop_callbacks) &gt; 0:\n                    # run each onStop callback only once from this thread\n                    cb = self.on_stop_callbacks.pop()\n                    cb()\n                break\n            if self.queue_control:\n                while len(self.on_start_callbacks) &gt; 0:\n                    # run each onStart callback only once from this thread\n                    cb = self.on_start_callbacks.pop()\n                    cb()\n\n                self.queue_control = False\n                frame_id = self.frame_id\n                inference_input = np.copy(self.frame_cv)\n                start = time.perf_counter()\n                predictions = self.model.predict(\n                    self.img_in,\n                )\n                predictions = self.model.postprocess(\n                    predictions,\n                    self.img_dims,\n                    class_agnostic_nms=self.class_agnostic_nms,\n                    confidence=self.confidence,\n                    iou_threshold=self.iou_threshold,\n                    max_candidates=self.max_candidates,\n                    max_detections=self.max_detections,\n                )[0]\n\n                self.active_learning_middleware.register(\n                    inference_input=inference_input,\n                    prediction=predictions.dict(by_alias=True, exclude_none=True),\n                    prediction_type=self.task_type,\n                )\n                if self.use_bytetrack:\n                    if hasattr(sv.Detections, \"from_inference\"):\n                        detections = sv.Detections.from_inference(\n                            predictions.dict(by_alias=True, exclude_none=True)\n                        )\n                    else:\n                        detections = sv.Detections.from_inference(\n                            predictions.dict(by_alias=True, exclude_none=True)\n                        )\n                    detections = self.byte_tracker.update_with_detections(detections)\n\n                    if detections.tracker_id is None:\n                        detections.tracker_id = np.array([], dtype=int)\n\n                    for pred, detect in zip(predictions.predictions, detections):\n                        pred.tracker_id = int(detect[4])\n                predictions.frame_id = frame_id\n                predictions = predictions.dict(by_alias=True, exclude_none=True)\n\n                self.inference_response = predictions\n                self.frame_count += 1\n\n                for cb in self.on_prediction_callbacks:\n                    if self.output_channel_order == \"BGR\":\n                        cb(predictions, self.frame_cv)\n                    else:\n                        cb(predictions, np.asarray(self.frame))\n\n                current = time.perf_counter()\n                self.webcam_stream.max_fps = 1 / (current - start)\n                logger.debug(f\"FPS: {self.webcam_stream.max_fps:.2f}\")\n\n                if time.perf_counter() - last_print &gt; 1:\n                    print_ind = (print_ind + 1) % 4\n                    last_print = time.perf_counter()\n\n    def run_thread(self):\n        \"\"\"Run the preprocessing and inference threads.\n\n        Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n        \"\"\"\n        preprocess_thread = threading.Thread(target=self.preprocess_thread)\n        preprocess_thread.start()\n\n        if self.use_main_thread:\n            self.inference_request_thread()\n        else:\n            # start a thread that looks for the predictions\n            # and call the callbacks\n            inference_request_thread = threading.Thread(\n                target=self.inference_request_thread\n            )\n            inference_request_thread.start()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.__init__","title":"<code>__init__(api_key=API_KEY, class_agnostic_nms=CLASS_AGNOSTIC_NMS, confidence=CONFIDENCE, enforce_fps=ENFORCE_FPS, iou_threshold=IOU_THRESHOLD, max_candidates=MAX_CANDIDATES, max_detections=MAX_DETECTIONS, model=MODEL_ID, source=STREAM_ID, use_bytetrack=ENABLE_BYTE_TRACK, use_main_thread=False, output_channel_order='RGB', on_prediction=None, on_start=None, on_stop=None)</code>","text":"<p>Initialize the stream with the given parameters. Prints the server settings and initializes the inference with a test frame.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def __init__(\n    self,\n    api_key: str = API_KEY,\n    class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n    confidence: float = CONFIDENCE,\n    enforce_fps: bool = ENFORCE_FPS,\n    iou_threshold: float = IOU_THRESHOLD,\n    max_candidates: float = MAX_CANDIDATES,\n    max_detections: float = MAX_DETECTIONS,\n    model: Union[str, Callable] = MODEL_ID,\n    source: Union[int, str] = STREAM_ID,\n    use_bytetrack: bool = ENABLE_BYTE_TRACK,\n    use_main_thread: bool = False,\n    output_channel_order: str = \"RGB\",\n    on_prediction: Callable = None,\n    on_start: Callable = None,\n    on_stop: Callable = None,\n):\n    \"\"\"Initialize the stream with the given parameters.\n    Prints the server settings and initializes the inference with a test frame.\n    \"\"\"\n    logger.info(\"Initializing server\")\n\n    self.frame_count = 0\n    self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n    self.use_bytetrack = use_bytetrack\n\n    if source == \"webcam\":\n        stream_id = 0\n    else:\n        stream_id = source\n\n    self.stream_id = stream_id\n    if self.stream_id is None:\n        raise ValueError(\"STREAM_ID is not defined\")\n    self.model_id = model\n    if not self.model_id:\n        raise ValueError(\"MODEL_ID is not defined\")\n    self.api_key = api_key\n\n    self.active_learning_middleware = NullActiveLearningMiddleware()\n    if isinstance(model, str):\n        self.model = get_model(model, self.api_key)\n        if ACTIVE_LEARNING_ENABLED:\n            self.active_learning_middleware = (\n                ThreadingActiveLearningMiddleware.init(\n                    api_key=self.api_key,\n                    model_id=self.model_id,\n                    cache=cache,\n                )\n            )\n        self.task_type = get_model_type(\n            model_id=self.model_id, api_key=self.api_key\n        )[0]\n    else:\n        self.model = model\n        self.task_type = \"unknown\"\n\n    self.class_agnostic_nms = class_agnostic_nms\n    self.confidence = confidence\n    self.iou_threshold = iou_threshold\n    self.max_candidates = max_candidates\n    self.max_detections = max_detections\n    self.use_main_thread = use_main_thread\n    self.output_channel_order = output_channel_order\n\n    self.inference_request_type = (\n        inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n    )\n\n    self.webcam_stream = WebcamStream(\n        stream_id=self.stream_id, enforce_fps=enforce_fps\n    )\n    logger.info(\n        f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n    )\n\n    self.on_start_callbacks = []\n    self.on_stop_callbacks = [\n        lambda: self.active_learning_middleware.stop_registration_thread()\n    ]\n    self.on_prediction_callbacks = []\n\n    if on_prediction:\n        self.on_prediction_callbacks.append(on_prediction)\n\n    if on_start:\n        self.on_start_callbacks.append(on_start)\n\n    if on_stop:\n        self.on_stop_callbacks.append(on_stop)\n\n    self.init_infer()\n    self.preproc_result = None\n    self.inference_request_obj = None\n    self.queue_control = False\n    self.inference_response = None\n    self.stop = False\n\n    self.frame = None\n    self.frame_cv = None\n    self.frame_id = None\n    logger.info(\"Server initialized with settings:\")\n    logger.info(f\"Stream ID: {self.stream_id}\")\n    logger.info(f\"Model ID: {self.model_id}\")\n    logger.info(f\"Enforce FPS: {enforce_fps}\")\n    logger.info(f\"Confidence: {self.confidence}\")\n    logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n    logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n    logger.info(f\"Max Candidates: {self.max_candidates}\")\n    logger.info(f\"Max Detections: {self.max_detections}\")\n\n    self.run_thread()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.inference_request_thread","title":"<code>inference_request_thread()</code>","text":"<p>Manage the inference requests.</p> <p>Processes preprocessed frames for inference, post-processes the predictions, and sends the results to registered callbacks.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def inference_request_thread(self):\n    \"\"\"Manage the inference requests.\n\n    Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n    to registered callbacks.\n    \"\"\"\n    last_print = time.perf_counter()\n    print_ind = 0\n    while True:\n        if self.webcam_stream.stopped is True or self.stop:\n            while len(self.on_stop_callbacks) &gt; 0:\n                # run each onStop callback only once from this thread\n                cb = self.on_stop_callbacks.pop()\n                cb()\n            break\n        if self.queue_control:\n            while len(self.on_start_callbacks) &gt; 0:\n                # run each onStart callback only once from this thread\n                cb = self.on_start_callbacks.pop()\n                cb()\n\n            self.queue_control = False\n            frame_id = self.frame_id\n            inference_input = np.copy(self.frame_cv)\n            start = time.perf_counter()\n            predictions = self.model.predict(\n                self.img_in,\n            )\n            predictions = self.model.postprocess(\n                predictions,\n                self.img_dims,\n                class_agnostic_nms=self.class_agnostic_nms,\n                confidence=self.confidence,\n                iou_threshold=self.iou_threshold,\n                max_candidates=self.max_candidates,\n                max_detections=self.max_detections,\n            )[0]\n\n            self.active_learning_middleware.register(\n                inference_input=inference_input,\n                prediction=predictions.dict(by_alias=True, exclude_none=True),\n                prediction_type=self.task_type,\n            )\n            if self.use_bytetrack:\n                if hasattr(sv.Detections, \"from_inference\"):\n                    detections = sv.Detections.from_inference(\n                        predictions.dict(by_alias=True, exclude_none=True)\n                    )\n                else:\n                    detections = sv.Detections.from_inference(\n                        predictions.dict(by_alias=True, exclude_none=True)\n                    )\n                detections = self.byte_tracker.update_with_detections(detections)\n\n                if detections.tracker_id is None:\n                    detections.tracker_id = np.array([], dtype=int)\n\n                for pred, detect in zip(predictions.predictions, detections):\n                    pred.tracker_id = int(detect[4])\n            predictions.frame_id = frame_id\n            predictions = predictions.dict(by_alias=True, exclude_none=True)\n\n            self.inference_response = predictions\n            self.frame_count += 1\n\n            for cb in self.on_prediction_callbacks:\n                if self.output_channel_order == \"BGR\":\n                    cb(predictions, self.frame_cv)\n                else:\n                    cb(predictions, np.asarray(self.frame))\n\n            current = time.perf_counter()\n            self.webcam_stream.max_fps = 1 / (current - start)\n            logger.debug(f\"FPS: {self.webcam_stream.max_fps:.2f}\")\n\n            if time.perf_counter() - last_print &gt; 1:\n                print_ind = (print_ind + 1) % 4\n                last_print = time.perf_counter()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.init_infer","title":"<code>init_infer()</code>","text":"<p>Initialize the inference with a test frame.</p> <p>Creates a test frame and runs it through the entire inference process to ensure everything is working.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def init_infer(self):\n    \"\"\"Initialize the inference with a test frame.\n\n    Creates a test frame and runs it through the entire inference process to ensure everything is working.\n    \"\"\"\n    frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n    self.model.infer(\n        frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n    )\n    self.active_learning_middleware.start_registration_thread()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.preprocess_thread","title":"<code>preprocess_thread()</code>","text":"<p>Preprocess incoming frames for inference.</p> <p>Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for inference.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def preprocess_thread(self):\n    \"\"\"Preprocess incoming frames for inference.\n\n    Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n    inference.\n    \"\"\"\n    webcam_stream = self.webcam_stream\n    webcam_stream.start()\n    # processing frames in input stream\n    try:\n        while True:\n            if webcam_stream.stopped is True or self.stop:\n                break\n            else:\n                self.frame_cv, frame_id = webcam_stream.read_opencv()\n                if frame_id &gt; 0 and frame_id != self.frame_id:\n                    self.frame_id = frame_id\n                    self.frame = cv2.cvtColor(self.frame_cv, cv2.COLOR_BGR2RGB)\n                    self.preproc_result = self.model.preprocess(self.frame_cv)\n                    self.img_in, self.img_dims = self.preproc_result\n                    self.queue_control = True\n\n    except Exception as e:\n        traceback.print_exc()\n        logger.error(e)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/stream/#inference.core.interfaces.stream.stream.Stream.run_thread","title":"<code>run_thread()</code>","text":"<p>Run the preprocessing and inference threads.</p> <p>Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.</p> Source code in <code>inference/core/interfaces/stream/stream.py</code> <pre><code>def run_thread(self):\n    \"\"\"Run the preprocessing and inference threads.\n\n    Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n    \"\"\"\n    preprocess_thread = threading.Thread(target=self.preprocess_thread)\n    preprocess_thread.start()\n\n    if self.use_main_thread:\n        self.inference_request_thread()\n    else:\n        # start a thread that looks for the predictions\n        # and call the callbacks\n        inference_request_thread = threading.Thread(\n            target=self.inference_request_thread\n        )\n        inference_request_thread.start()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/interfaces/stream/watchdog/","title":"watchdog","text":"<p>This module contains component intended to use in combination with <code>InferencePipeline</code> to ensure observability. Please consider them internal details of implementation.</p>"},{"location":"docs/reference/inference/core/interfaces/stream/watchdog/#inference.core.interfaces.stream.watchdog.BasePipelineWatchDog","title":"<code>BasePipelineWatchDog</code>","text":"<p>               Bases: <code>PipelineWatchDog</code></p> <p>Implementation to be used from single inference thread, as it keeps state assumed to represent status of consecutive stage of prediction process in latency monitor.</p> Source code in <code>inference/core/interfaces/stream/watchdog.py</code> <pre><code>class BasePipelineWatchDog(PipelineWatchDog):\n    \"\"\"\n    Implementation to be used from single inference thread, as it keeps\n    state assumed to represent status of consecutive stage of prediction process\n    in latency monitor.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self._video_sources: Optional[List[VideoSource]] = None\n        self._inference_throughput_monitor = sv.FPSMonitor()\n        self._latency_monitors: Dict[Optional[int], LatencyMonitor] = {}\n        self._stream_updates = deque(maxlen=MAX_UPDATES_CONTEXT)\n\n    def register_video_sources(self, video_sources: List[VideoSource]) -&gt; None:\n        self._video_sources = video_sources\n        for source in video_sources:\n            self._latency_monitors[source.source_id] = LatencyMonitor(\n                source_id=source.source_id\n            )\n\n    def on_status_update(self, status_update: StatusUpdate) -&gt; None:\n        if status_update.severity.value &lt;= UpdateSeverity.DEBUG.value:\n            return None\n        self._stream_updates.append(status_update)\n\n    def on_model_inference_started(self, frames: List[VideoFrame]) -&gt; None:\n        for frame in frames:\n            self._latency_monitors[frame.source_id].register_inference_start(\n                frame_timestamp=frame.frame_timestamp,\n                frame_id=frame.frame_id,\n            )\n\n    def on_model_prediction_ready(self, frames: List[VideoFrame]) -&gt; None:\n        for frame in frames:\n            self._latency_monitors[frame.source_id].register_prediction_ready(\n                frame_timestamp=frame.frame_timestamp,\n                frame_id=frame.frame_id,\n            )\n            self._inference_throughput_monitor.tick()\n\n    def get_report(self) -&gt; PipelineStateReport:\n        sources_metadata = []\n        if self._video_sources is not None:\n            sources_metadata = [s.describe_source() for s in self._video_sources]\n        latency_reports = [\n            monitor.summarise_reports() for monitor in self._latency_monitors.values()\n        ]\n        if hasattr(self._inference_throughput_monitor, \"fps\"):\n            _inference_throughput_fps = self._inference_throughput_monitor.fps\n        else:\n            _inference_throughput_fps = self._inference_throughput_monitor()\n        return PipelineStateReport(\n            video_source_status_updates=list(self._stream_updates),\n            latency_reports=latency_reports,\n            inference_throughput=_inference_throughput_fps,\n            sources_metadata=sources_metadata,\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/stream/model_handlers/roboflow_models/","title":"roboflow_models","text":""},{"location":"docs/reference/inference/core/interfaces/stream/model_handlers/workflows/","title":"workflows","text":""},{"location":"docs/reference/inference/core/interfaces/stream/model_handlers/yolo_world/","title":"yolo_world","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/api/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/api/errors/","title":"errors","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/api/stream_manager_client/","title":"stream_manager_client","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/manager_app/app/","title":"app","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/manager_app/communication/","title":"communication","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/manager_app/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/manager_app/errors/","title":"errors","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/manager_app/inference_pipeline_manager/","title":"inference_pipeline_manager","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/manager_app/serialisation/","title":"serialisation","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/manager_app/tcp_server/","title":"tcp_server","text":""},{"location":"docs/reference/inference/core/interfaces/stream_manager/manager_app/webrtc/","title":"webrtc","text":""},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/","title":"udp_stream","text":""},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream","title":"<code>UdpStream</code>","text":"<p>               Bases: <code>BaseInterface</code></p> <p>Roboflow defined UDP interface for a general-purpose inference server.</p> <p>Attributes:</p> Name Type Description <code>model_manager</code> <code>ModelManager</code> <p>The manager that handles model inference tasks.</p> <code>model_registry</code> <code>RoboflowModelRegistry</code> <p>The registry to fetch model instances.</p> <code>api_key</code> <code>str</code> <p>The API key for accessing models.</p> <code>class_agnostic_nms</code> <code>bool</code> <p>Flag for class-agnostic non-maximum suppression.</p> <code>confidence</code> <code>float</code> <p>Confidence threshold for inference.</p> <code>ip_broadcast_addr</code> <code>str</code> <p>The IP address to broadcast to.</p> <code>ip_broadcast_port</code> <code>int</code> <p>The port to broadcast on.</p> <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold for detection.</p> <code>max_candidates</code> <code>float</code> <p>The maximum number of candidates for detection.</p> <code>max_detections</code> <code>float</code> <p>The maximum number of detections.</p> <code>model_id</code> <code>str</code> <p>The ID of the model to be used.</p> <code>stream_id</code> <code>str</code> <p>The ID of the stream to be used.</p> <code>use_bytetrack</code> <code>bool</code> <p>Flag to use bytetrack,</p> <p>Methods:</p> Name Description <code>init_infer</code> <p>Initialize the inference with a test frame.</p> <code>preprocess_thread</code> <p>Preprocess incoming frames for inference.</p> <code>inference_request_thread</code> <p>Manage the inference requests.</p> <code>run_thread</code> <p>Run the preprocessing and inference threads.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>class UdpStream(BaseInterface):\n    \"\"\"Roboflow defined UDP interface for a general-purpose inference server.\n\n    Attributes:\n        model_manager (ModelManager): The manager that handles model inference tasks.\n        model_registry (RoboflowModelRegistry): The registry to fetch model instances.\n        api_key (str): The API key for accessing models.\n        class_agnostic_nms (bool): Flag for class-agnostic non-maximum suppression.\n        confidence (float): Confidence threshold for inference.\n        ip_broadcast_addr (str): The IP address to broadcast to.\n        ip_broadcast_port (int): The port to broadcast on.\n        iou_threshold (float): The intersection-over-union threshold for detection.\n        max_candidates (float): The maximum number of candidates for detection.\n        max_detections (float): The maximum number of detections.\n        model_id (str): The ID of the model to be used.\n        stream_id (str): The ID of the stream to be used.\n        use_bytetrack (bool): Flag to use bytetrack,\n\n    Methods:\n        init_infer: Initialize the inference with a test frame.\n        preprocess_thread: Preprocess incoming frames for inference.\n        inference_request_thread: Manage the inference requests.\n        run_thread: Run the preprocessing and inference threads.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str = API_KEY,\n        class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n        confidence: float = CONFIDENCE,\n        enforce_fps: bool = ENFORCE_FPS,\n        ip_broadcast_addr: str = IP_BROADCAST_ADDR,\n        ip_broadcast_port: int = IP_BROADCAST_PORT,\n        iou_threshold: float = IOU_THRESHOLD,\n        max_candidates: float = MAX_CANDIDATES,\n        max_detections: float = MAX_DETECTIONS,\n        model_id: str = MODEL_ID,\n        stream_id: Union[int, str] = STREAM_ID,\n        use_bytetrack: bool = ENABLE_BYTE_TRACK,\n    ):\n        \"\"\"Initialize the UDP stream with the given parameters.\n        Prints the server settings and initializes the inference with a test frame.\n        \"\"\"\n        logger.info(\"Initializing server\")\n\n        self.frame_count = 0\n        self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n        self.use_bytetrack = use_bytetrack\n\n        self.stream_id = stream_id\n        if self.stream_id is None:\n            raise ValueError(\"STREAM_ID is not defined\")\n        self.model_id = model_id\n        if not self.model_id:\n            raise ValueError(\"MODEL_ID is not defined\")\n        self.api_key = api_key\n        if not self.api_key:\n            raise ValueError(\n                f\"API key is missing. Either pass it explicitly to constructor, or use one of env variables: \"\n                f\"{API_KEY_ENV_NAMES}. Visit \"\n                f\"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key to learn how to generate \"\n                f\"the key.\"\n            )\n\n        self.model = get_model(self.model_id, self.api_key)\n        self.task_type = get_model_type(model_id=self.model_id, api_key=self.api_key)[0]\n        self.active_learning_middleware = NullActiveLearningMiddleware()\n        if ACTIVE_LEARNING_ENABLED:\n            self.active_learning_middleware = ThreadingActiveLearningMiddleware.init(\n                api_key=self.api_key,\n                model_id=self.model_id,\n                cache=cache,\n            )\n        self.class_agnostic_nms = class_agnostic_nms\n        self.confidence = confidence\n        self.iou_threshold = iou_threshold\n        self.max_candidates = max_candidates\n        self.max_detections = max_detections\n        self.ip_broadcast_addr = ip_broadcast_addr\n        self.ip_broadcast_port = ip_broadcast_port\n\n        self.inference_request_type = (\n            inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n        )\n\n        self.UDPServerSocket = socket.socket(\n            family=socket.AF_INET, type=socket.SOCK_DGRAM\n        )\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n        self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n\n        self.webcam_stream = WebcamStream(\n            stream_id=self.stream_id, enforce_fps=enforce_fps\n        )\n        logger.info(\n            f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n        )\n\n        self.init_infer()\n        self.preproc_result = None\n        self.inference_request_obj = None\n        self.queue_control = False\n        self.inference_response = None\n        self.stop = False\n\n        self.frame_cv = None\n        self.frame_id = None\n        logger.info(\"Server initialized with settings:\")\n        logger.info(f\"Stream ID: {self.stream_id}\")\n        logger.info(f\"Model ID: {self.model_id}\")\n        logger.info(f\"Confidence: {self.confidence}\")\n        logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n        logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n        logger.info(f\"Max Candidates: {self.max_candidates}\")\n        logger.info(f\"Max Detections: {self.max_detections}\")\n\n    def init_infer(self):\n        \"\"\"Initialize the inference with a test frame.\n\n        Creates a test frame and runs it through the entire inference process to ensure everything is working.\n        \"\"\"\n        frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n        self.model.infer(\n            frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n        )\n        self.active_learning_middleware.start_registration_thread()\n\n    def preprocess_thread(self):\n        \"\"\"Preprocess incoming frames for inference.\n\n        Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n        inference.\n        \"\"\"\n        webcam_stream = self.webcam_stream\n        webcam_stream.start()\n        # processing frames in input stream\n        try:\n            while True:\n                if webcam_stream.stopped is True or self.stop:\n                    break\n                else:\n                    self.frame_cv, frame_id = webcam_stream.read_opencv()\n                    if frame_id != self.frame_id:\n                        self.frame_id = frame_id\n                        self.preproc_result = self.model.preprocess(self.frame_cv)\n                        self.img_in, self.img_dims = self.preproc_result\n                        self.queue_control = True\n\n        except Exception as e:\n            logger.error(e)\n\n    def inference_request_thread(self):\n        \"\"\"Manage the inference requests.\n\n        Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n        as a UDP broadcast.\n        \"\"\"\n        last_print = time.perf_counter()\n        print_ind = 0\n        print_chars = [\"|\", \"/\", \"-\", \"\\\\\"]\n        while True:\n            if self.stop:\n                break\n            if self.queue_control:\n                self.queue_control = False\n                frame_id = self.frame_id\n                inference_input = np.copy(self.frame_cv)\n                predictions = self.model.predict(\n                    self.img_in,\n                )\n                predictions = self.model.postprocess(\n                    predictions,\n                    self.img_dims,\n                    class_agnostic_nms=self.class_agnostic_nms,\n                    confidence=self.confidence,\n                    iou_threshold=self.iou_threshold,\n                    max_candidates=self.max_candidates,\n                    max_detections=self.max_detections,\n                )[0]\n                self.active_learning_middleware.register(\n                    inference_input=inference_input,\n                    prediction=predictions.dict(by_alias=True, exclude_none=True),\n                    prediction_type=self.task_type,\n                )\n                if self.use_bytetrack:\n                    if hasattr(sv.Detections, \"from_inference\"):\n                        detections = sv.Detections.from_inference(\n                            predictions.dict(by_alias=True), self.model.class_names\n                        )\n                    else:\n                        detections = sv.Detections.from_inference(\n                            predictions.dict(by_alias=True), self.model.class_names\n                        )\n                    detections = self.byte_tracker.update_with_detections(detections)\n                    for pred, detect in zip(predictions.predictions, detections):\n                        pred.tracker_id = int(detect[4])\n                predictions.frame_id = frame_id\n                predictions = predictions.json(exclude_none=True, by_alias=True)\n\n                self.inference_response = predictions\n                self.frame_count += 1\n\n                bytesToSend = predictions.encode(\"utf-8\")\n                self.UDPServerSocket.sendto(\n                    bytesToSend,\n                    (\n                        self.ip_broadcast_addr,\n                        self.ip_broadcast_port,\n                    ),\n                )\n                if time.perf_counter() - last_print &gt; 1:\n                    print(f\"Streaming {print_chars[print_ind]}\", end=\"\\r\")\n                    print_ind = (print_ind + 1) % 4\n                    last_print = time.perf_counter()\n\n    def run_thread(self):\n        \"\"\"Run the preprocessing and inference threads.\n\n        Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n        \"\"\"\n        preprocess_thread = threading.Thread(target=self.preprocess_thread)\n        inference_request_thread = threading.Thread(\n            target=self.inference_request_thread\n        )\n\n        preprocess_thread.start()\n        inference_request_thread.start()\n\n        while True:\n            try:\n                time.sleep(10)\n            except KeyboardInterrupt:\n                logger.info(\"Stopping server...\")\n                self.stop = True\n                self.active_learning_middleware.stop_registration_thread()\n                time.sleep(3)\n                sys.exit(0)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.__init__","title":"<code>__init__(api_key=API_KEY, class_agnostic_nms=CLASS_AGNOSTIC_NMS, confidence=CONFIDENCE, enforce_fps=ENFORCE_FPS, ip_broadcast_addr=IP_BROADCAST_ADDR, ip_broadcast_port=IP_BROADCAST_PORT, iou_threshold=IOU_THRESHOLD, max_candidates=MAX_CANDIDATES, max_detections=MAX_DETECTIONS, model_id=MODEL_ID, stream_id=STREAM_ID, use_bytetrack=ENABLE_BYTE_TRACK)</code>","text":"<p>Initialize the UDP stream with the given parameters. Prints the server settings and initializes the inference with a test frame.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def __init__(\n    self,\n    api_key: str = API_KEY,\n    class_agnostic_nms: bool = CLASS_AGNOSTIC_NMS,\n    confidence: float = CONFIDENCE,\n    enforce_fps: bool = ENFORCE_FPS,\n    ip_broadcast_addr: str = IP_BROADCAST_ADDR,\n    ip_broadcast_port: int = IP_BROADCAST_PORT,\n    iou_threshold: float = IOU_THRESHOLD,\n    max_candidates: float = MAX_CANDIDATES,\n    max_detections: float = MAX_DETECTIONS,\n    model_id: str = MODEL_ID,\n    stream_id: Union[int, str] = STREAM_ID,\n    use_bytetrack: bool = ENABLE_BYTE_TRACK,\n):\n    \"\"\"Initialize the UDP stream with the given parameters.\n    Prints the server settings and initializes the inference with a test frame.\n    \"\"\"\n    logger.info(\"Initializing server\")\n\n    self.frame_count = 0\n    self.byte_tracker = sv.ByteTrack() if use_bytetrack else None\n    self.use_bytetrack = use_bytetrack\n\n    self.stream_id = stream_id\n    if self.stream_id is None:\n        raise ValueError(\"STREAM_ID is not defined\")\n    self.model_id = model_id\n    if not self.model_id:\n        raise ValueError(\"MODEL_ID is not defined\")\n    self.api_key = api_key\n    if not self.api_key:\n        raise ValueError(\n            f\"API key is missing. Either pass it explicitly to constructor, or use one of env variables: \"\n            f\"{API_KEY_ENV_NAMES}. Visit \"\n            f\"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key to learn how to generate \"\n            f\"the key.\"\n        )\n\n    self.model = get_model(self.model_id, self.api_key)\n    self.task_type = get_model_type(model_id=self.model_id, api_key=self.api_key)[0]\n    self.active_learning_middleware = NullActiveLearningMiddleware()\n    if ACTIVE_LEARNING_ENABLED:\n        self.active_learning_middleware = ThreadingActiveLearningMiddleware.init(\n            api_key=self.api_key,\n            model_id=self.model_id,\n            cache=cache,\n        )\n    self.class_agnostic_nms = class_agnostic_nms\n    self.confidence = confidence\n    self.iou_threshold = iou_threshold\n    self.max_candidates = max_candidates\n    self.max_detections = max_detections\n    self.ip_broadcast_addr = ip_broadcast_addr\n    self.ip_broadcast_port = ip_broadcast_port\n\n    self.inference_request_type = (\n        inference.core.entities.requests.inference.ObjectDetectionInferenceRequest\n    )\n\n    self.UDPServerSocket = socket.socket(\n        family=socket.AF_INET, type=socket.SOCK_DGRAM\n    )\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1)\n    self.UDPServerSocket.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 65536)\n\n    self.webcam_stream = WebcamStream(\n        stream_id=self.stream_id, enforce_fps=enforce_fps\n    )\n    logger.info(\n        f\"Streaming from device with resolution: {self.webcam_stream.width} x {self.webcam_stream.height}\"\n    )\n\n    self.init_infer()\n    self.preproc_result = None\n    self.inference_request_obj = None\n    self.queue_control = False\n    self.inference_response = None\n    self.stop = False\n\n    self.frame_cv = None\n    self.frame_id = None\n    logger.info(\"Server initialized with settings:\")\n    logger.info(f\"Stream ID: {self.stream_id}\")\n    logger.info(f\"Model ID: {self.model_id}\")\n    logger.info(f\"Confidence: {self.confidence}\")\n    logger.info(f\"Class Agnostic NMS: {self.class_agnostic_nms}\")\n    logger.info(f\"IOU Threshold: {self.iou_threshold}\")\n    logger.info(f\"Max Candidates: {self.max_candidates}\")\n    logger.info(f\"Max Detections: {self.max_detections}\")\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.inference_request_thread","title":"<code>inference_request_thread()</code>","text":"<p>Manage the inference requests.</p> <p>Processes preprocessed frames for inference, post-processes the predictions, and sends the results as a UDP broadcast.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def inference_request_thread(self):\n    \"\"\"Manage the inference requests.\n\n    Processes preprocessed frames for inference, post-processes the predictions, and sends the results\n    as a UDP broadcast.\n    \"\"\"\n    last_print = time.perf_counter()\n    print_ind = 0\n    print_chars = [\"|\", \"/\", \"-\", \"\\\\\"]\n    while True:\n        if self.stop:\n            break\n        if self.queue_control:\n            self.queue_control = False\n            frame_id = self.frame_id\n            inference_input = np.copy(self.frame_cv)\n            predictions = self.model.predict(\n                self.img_in,\n            )\n            predictions = self.model.postprocess(\n                predictions,\n                self.img_dims,\n                class_agnostic_nms=self.class_agnostic_nms,\n                confidence=self.confidence,\n                iou_threshold=self.iou_threshold,\n                max_candidates=self.max_candidates,\n                max_detections=self.max_detections,\n            )[0]\n            self.active_learning_middleware.register(\n                inference_input=inference_input,\n                prediction=predictions.dict(by_alias=True, exclude_none=True),\n                prediction_type=self.task_type,\n            )\n            if self.use_bytetrack:\n                if hasattr(sv.Detections, \"from_inference\"):\n                    detections = sv.Detections.from_inference(\n                        predictions.dict(by_alias=True), self.model.class_names\n                    )\n                else:\n                    detections = sv.Detections.from_inference(\n                        predictions.dict(by_alias=True), self.model.class_names\n                    )\n                detections = self.byte_tracker.update_with_detections(detections)\n                for pred, detect in zip(predictions.predictions, detections):\n                    pred.tracker_id = int(detect[4])\n            predictions.frame_id = frame_id\n            predictions = predictions.json(exclude_none=True, by_alias=True)\n\n            self.inference_response = predictions\n            self.frame_count += 1\n\n            bytesToSend = predictions.encode(\"utf-8\")\n            self.UDPServerSocket.sendto(\n                bytesToSend,\n                (\n                    self.ip_broadcast_addr,\n                    self.ip_broadcast_port,\n                ),\n            )\n            if time.perf_counter() - last_print &gt; 1:\n                print(f\"Streaming {print_chars[print_ind]}\", end=\"\\r\")\n                print_ind = (print_ind + 1) % 4\n                last_print = time.perf_counter()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.init_infer","title":"<code>init_infer()</code>","text":"<p>Initialize the inference with a test frame.</p> <p>Creates a test frame and runs it through the entire inference process to ensure everything is working.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def init_infer(self):\n    \"\"\"Initialize the inference with a test frame.\n\n    Creates a test frame and runs it through the entire inference process to ensure everything is working.\n    \"\"\"\n    frame = Image.new(\"RGB\", (640, 640), color=\"black\")\n    self.model.infer(\n        frame, confidence=self.confidence, iou_threshold=self.iou_threshold\n    )\n    self.active_learning_middleware.start_registration_thread()\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.preprocess_thread","title":"<code>preprocess_thread()</code>","text":"<p>Preprocess incoming frames for inference.</p> <p>Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for inference.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def preprocess_thread(self):\n    \"\"\"Preprocess incoming frames for inference.\n\n    Reads frames from the webcam stream, converts them into the proper format, and preprocesses them for\n    inference.\n    \"\"\"\n    webcam_stream = self.webcam_stream\n    webcam_stream.start()\n    # processing frames in input stream\n    try:\n        while True:\n            if webcam_stream.stopped is True or self.stop:\n                break\n            else:\n                self.frame_cv, frame_id = webcam_stream.read_opencv()\n                if frame_id != self.frame_id:\n                    self.frame_id = frame_id\n                    self.preproc_result = self.model.preprocess(self.frame_cv)\n                    self.img_in, self.img_dims = self.preproc_result\n                    self.queue_control = True\n\n    except Exception as e:\n        logger.error(e)\n</code></pre>"},{"location":"docs/reference/inference/core/interfaces/udp/udp_stream/#inference.core.interfaces.udp.udp_stream.UdpStream.run_thread","title":"<code>run_thread()</code>","text":"<p>Run the preprocessing and inference threads.</p> <p>Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.</p> Source code in <code>inference/core/interfaces/udp/udp_stream.py</code> <pre><code>def run_thread(self):\n    \"\"\"Run the preprocessing and inference threads.\n\n    Starts the preprocessing and inference threads, and handles graceful shutdown on KeyboardInterrupt.\n    \"\"\"\n    preprocess_thread = threading.Thread(target=self.preprocess_thread)\n    inference_request_thread = threading.Thread(\n        target=self.inference_request_thread\n    )\n\n    preprocess_thread.start()\n    inference_request_thread.start()\n\n    while True:\n        try:\n            time.sleep(10)\n        except KeyboardInterrupt:\n            logger.info(\"Stopping server...\")\n            self.stop = True\n            self.active_learning_middleware.stop_registration_thread()\n            time.sleep(3)\n            sys.exit(0)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/active_learning/","title":"active_learning","text":""},{"location":"docs/reference/inference/core/managers/base/","title":"base","text":""},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager","title":"<code>ModelManager</code>","text":"<p>Model managers keep track of a dictionary of Model objects and is responsible for passing requests to the right model using the infer method.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>class ModelManager:\n    \"\"\"Model managers keep track of a dictionary of Model objects and is responsible for passing requests to the right model using the infer method.\"\"\"\n\n    def __init__(self, model_registry: ModelRegistry, models: Optional[dict] = None):\n        self.model_registry = model_registry\n        self._models: Dict[str, Model] = models if models is not None else {}\n        self.pingback = None\n\n    def init_pingback(self):\n        \"\"\"Initializes pingback mechanism.\"\"\"\n        self.num_errors = 0  # in the device\n        self.uuid = ROBOFLOW_SERVER_UUID\n        if METRICS_ENABLED:\n            self.pingback = PingbackInfo(self)\n            self.pingback.start()\n\n    def add_model(\n        self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"Adds a new model to the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n        \"\"\"\n        logger.debug(\n            f\"ModelManager - Adding model with model_id={model_id}, model_id_alias={model_id_alias}\"\n        )\n        resolved_identifier = model_id if model_id_alias is None else model_id_alias\n        if resolved_identifier in self._models:\n            logger.debug(\n                f\"ModelManager - model with model_id={resolved_identifier} is already loaded.\"\n            )\n            return\n        logger.debug(\"ModelManager - model initialisation...\")\n        model = self.model_registry.get_model(resolved_identifier, api_key)(\n            model_id=model_id,\n            api_key=api_key,\n        )\n        logger.debug(\"ModelManager - model successfully loaded.\")\n        self._models[resolved_identifier] = model\n\n    def check_for_model(self, model_id: str) -&gt; None:\n        \"\"\"Checks whether the model with the given ID is in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Raises:\n            InferenceModelNotFound: If the model is not found in the manager.\n        \"\"\"\n        if model_id not in self:\n            raise InferenceModelNotFound(f\"Model with id {model_id} not loaded.\")\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Runs inference on the specified model with the given request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        logger.debug(\n            f\"ModelManager - inference from request started for model_id={model_id}.\"\n        )\n        if METRICS_ENABLED and self.pingback:\n            logger.debug(\"ModelManager - setting pingback fallback api key...\")\n            self.pingback.fallback_api_key = request.api_key\n        try:\n            rtn_val = await self.model_infer(\n                model_id=model_id, request=request, **kwargs\n            )\n            logger.debug(\n                f\"ModelManager - inference from request finished for model_id={model_id}.\"\n            )\n            finish_time = time.time()\n            if not DISABLE_INFERENCE_CACHE:\n                logger.debug(\n                    f\"ModelManager - caching inference request started for model_id={model_id}\"\n                )\n                cache.zadd(\n                    f\"models\",\n                    value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                if (\n                    hasattr(request, \"image\")\n                    and hasattr(request.image, \"type\")\n                    and request.image.type == \"numpy\"\n                ):\n                    request.image.value = str(request.image.value)\n                cache.zadd(\n                    f\"inference:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                    value=to_cachable_inference_item(request, rtn_val),\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                logger.debug(\n                    f\"ModelManager - caching inference request finished for model_id={model_id}\"\n                )\n            return rtn_val\n        except Exception as e:\n            finish_time = time.time()\n            if not DISABLE_INFERENCE_CACHE:\n                cache.zadd(\n                    f\"models\",\n                    value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                cache.zadd(\n                    f\"error:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                    value={\n                        \"request\": jsonable_encoder(\n                            request.dict(exclude={\"image\", \"subject\", \"prompt\"})\n                        ),\n                        \"error\": str(e),\n                    },\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n            raise\n\n    def infer_from_request_sync(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Runs inference on the specified model with the given request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        logger.debug(\n            f\"ModelManager - inference from request started for model_id={model_id}.\"\n        )\n        if METRICS_ENABLED and self.pingback:\n            logger.debug(\"ModelManager - setting pingback fallback api key...\")\n            self.pingback.fallback_api_key = request.api_key\n        try:\n            rtn_val = self.model_infer_sync(\n                model_id=model_id, request=request, **kwargs\n            )\n            logger.debug(\n                f\"ModelManager - inference from request finished for model_id={model_id}.\"\n            )\n            finish_time = time.time()\n            if not DISABLE_INFERENCE_CACHE:\n                logger.debug(\n                    f\"ModelManager - caching inference request started for model_id={model_id}\"\n                )\n                cache.zadd(\n                    f\"models\",\n                    value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                if (\n                    hasattr(request, \"image\")\n                    and hasattr(request.image, \"type\")\n                    and request.image.type == \"numpy\"\n                ):\n                    request.image.value = str(request.image.value)\n                cache.zadd(\n                    f\"inference:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                    value=to_cachable_inference_item(request, rtn_val),\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                logger.debug(\n                    f\"ModelManager - caching inference request finished for model_id={model_id}\"\n                )\n            return rtn_val\n        except Exception as e:\n            finish_time = time.time()\n            if not DISABLE_INFERENCE_CACHE:\n                cache.zadd(\n                    f\"models\",\n                    value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n                cache.zadd(\n                    f\"error:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                    value={\n                        \"request\": jsonable_encoder(\n                            request.dict(exclude={\"image\", \"subject\", \"prompt\"})\n                        ),\n                        \"error\": str(e),\n                    },\n                    score=finish_time,\n                    expire=METRICS_INTERVAL * 2,\n                )\n            raise\n\n    async def model_infer(self, model_id: str, request: InferenceRequest, **kwargs):\n        self.check_for_model(model_id)\n        return self._models[model_id].infer_from_request(request)\n\n    def model_infer_sync(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n        self.check_for_model(model_id)\n        return self._models[model_id].infer_from_request(request)\n\n    def make_response(\n        self, model_id: str, predictions: List[List[float]], *args, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Creates a response object from the model's predictions.\n\n        Args:\n            model_id (str): The identifier of the model.\n            predictions (List[List[float]]): The model's predictions.\n\n        Returns:\n            InferenceResponse: The created response object.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].make_response(predictions, *args, **kwargs)\n\n    def postprocess(\n        self,\n        model_id: str,\n        predictions: Tuple[np.ndarray, ...],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        *args,\n        **kwargs,\n    ) -&gt; List[List[float]]:\n        \"\"\"Processes the model's predictions after inference.\n\n        Args:\n            model_id (str): The identifier of the model.\n            predictions (np.ndarray): The model's predictions.\n\n        Returns:\n            List[List[float]]: The post-processed predictions.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].postprocess(\n            predictions, preprocess_return_metadata, *args, **kwargs\n        )\n\n    def predict(self, model_id: str, *args, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n        \"\"\"Runs prediction on the specified model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            np.ndarray: The predictions from the model.\n        \"\"\"\n        self.check_for_model(model_id)\n        self._models[model_id].metrics[\"num_inferences\"] += 1\n        tic = time.perf_counter()\n        res = self._models[model_id].predict(*args, **kwargs)\n        toc = time.perf_counter()\n        self._models[model_id].metrics[\"avg_inference_time\"] += toc - tic\n        return res\n\n    def preprocess(\n        self, model_id: str, request: InferenceRequest\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        \"\"\"Preprocesses the request before inference.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to preprocess.\n\n        Returns:\n            Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].preprocess(**request.dict())\n\n    def get_class_names(self, model_id):\n        \"\"\"Retrieves the class names for a given model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            List[str]: The class names of the model.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].class_names\n\n    def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n        \"\"\"Retrieves the task type for a given model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            str: The task type of the model.\n        \"\"\"\n        self.check_for_model(model_id)\n        return self._models[model_id].task_type\n\n    def remove(self, model_id: str) -&gt; None:\n        \"\"\"Removes a model from the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n        \"\"\"\n        try:\n            logger.debug(f\"Removing model {model_id} from base model manager\")\n            self.check_for_model(model_id)\n            self._models[model_id].clear_cache()\n            del self._models[model_id]\n        except InferenceModelNotFound:\n            logger.warning(\n                f\"Attempted to remove model with id {model_id}, but it is not loaded. Skipping...\"\n            )\n\n    def clear(self) -&gt; None:\n        \"\"\"Removes all models from the manager.\"\"\"\n        for model_id in list(self.keys()):\n            self.remove(model_id)\n\n    def __contains__(self, model_id: str) -&gt; bool:\n        \"\"\"Checks if the model is contained in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            bool: Whether the model is in the manager.\n        \"\"\"\n        return model_id in self._models\n\n    def __getitem__(self, key: str) -&gt; Model:\n        \"\"\"Retrieve a model from the manager by key.\n\n        Args:\n            key (str): The identifier of the model.\n\n        Returns:\n            Model: The model corresponding to the key.\n        \"\"\"\n        self.check_for_model(model_id=key)\n        return self._models[key]\n\n    def __len__(self) -&gt; int:\n        \"\"\"Retrieve the number of models in the manager.\n\n        Returns:\n            int: The number of models in the manager.\n        \"\"\"\n        return len(self._models)\n\n    def keys(self):\n        \"\"\"Retrieve the keys (model identifiers) from the manager.\n\n        Returns:\n            List[str]: The keys of the models in the manager.\n        \"\"\"\n        return self._models.keys()\n\n    def models(self) -&gt; Dict[str, Model]:\n        \"\"\"Retrieve the models dictionary from the manager.\n\n        Returns:\n            Dict[str, Model]: The keys of the models in the manager.\n        \"\"\"\n        return self._models\n\n    def describe_models(self) -&gt; List[ModelDescription]:\n        return [\n            ModelDescription(\n                model_id=model_id,\n                task_type=model.task_type,\n                batch_size=getattr(model, \"batch_size\", None),\n                input_width=getattr(model, \"img_size_w\", None),\n                input_height=getattr(model, \"img_size_h\", None),\n            )\n            for model_id, model in self._models.items()\n        ]\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.__contains__","title":"<code>__contains__(model_id)</code>","text":"<p>Checks if the model is contained in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the model is in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __contains__(self, model_id: str) -&gt; bool:\n    \"\"\"Checks if the model is contained in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        bool: Whether the model is in the manager.\n    \"\"\"\n    return model_id in self._models\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieve a model from the manager by key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model corresponding to the key.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Model:\n    \"\"\"Retrieve a model from the manager by key.\n\n    Args:\n        key (str): The identifier of the model.\n\n    Returns:\n        Model: The model corresponding to the key.\n    \"\"\"\n    self.check_for_model(model_id=key)\n    return self._models[key]\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.__len__","title":"<code>__len__()</code>","text":"<p>Retrieve the number of models in the manager.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Retrieve the number of models in the manager.\n\n    Returns:\n        int: The number of models in the manager.\n    \"\"\"\n    return len(self._models)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None)</code>","text":"<p>Adds a new model to the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required Source code in <code>inference/core/managers/base.py</code> <pre><code>def add_model(\n    self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n) -&gt; None:\n    \"\"\"Adds a new model to the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n    \"\"\"\n    logger.debug(\n        f\"ModelManager - Adding model with model_id={model_id}, model_id_alias={model_id_alias}\"\n    )\n    resolved_identifier = model_id if model_id_alias is None else model_id_alias\n    if resolved_identifier in self._models:\n        logger.debug(\n            f\"ModelManager - model with model_id={resolved_identifier} is already loaded.\"\n        )\n        return\n    logger.debug(\"ModelManager - model initialisation...\")\n    model = self.model_registry.get_model(resolved_identifier, api_key)(\n        model_id=model_id,\n        api_key=api_key,\n    )\n    logger.debug(\"ModelManager - model successfully loaded.\")\n    self._models[resolved_identifier] = model\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.check_for_model","title":"<code>check_for_model(model_id)</code>","text":"<p>Checks whether the model with the given ID is in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Raises:</p> Type Description <code>InferenceModelNotFound</code> <p>If the model is not found in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def check_for_model(self, model_id: str) -&gt; None:\n    \"\"\"Checks whether the model with the given ID is in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Raises:\n        InferenceModelNotFound: If the model is not found in the manager.\n    \"\"\"\n    if model_id not in self:\n        raise InferenceModelNotFound(f\"Model with id {model_id} not loaded.\")\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.clear","title":"<code>clear()</code>","text":"<p>Removes all models from the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Removes all models from the manager.\"\"\"\n    for model_id in list(self.keys()):\n        self.remove(model_id)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.get_class_names","title":"<code>get_class_names(model_id)</code>","text":"<p>Retrieves the class names for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <p>List[str]: The class names of the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def get_class_names(self, model_id):\n    \"\"\"Retrieves the class names for a given model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        List[str]: The class names of the model.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].class_names\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.get_task_type","title":"<code>get_task_type(model_id, api_key=None)</code>","text":"<p>Retrieves the task type for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The task type of the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n    \"\"\"Retrieves the task type for a given model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        str: The task type of the model.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].task_type\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Runs inference on the specified model with the given request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Runs inference on the specified model with the given request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    logger.debug(\n        f\"ModelManager - inference from request started for model_id={model_id}.\"\n    )\n    if METRICS_ENABLED and self.pingback:\n        logger.debug(\"ModelManager - setting pingback fallback api key...\")\n        self.pingback.fallback_api_key = request.api_key\n    try:\n        rtn_val = await self.model_infer(\n            model_id=model_id, request=request, **kwargs\n        )\n        logger.debug(\n            f\"ModelManager - inference from request finished for model_id={model_id}.\"\n        )\n        finish_time = time.time()\n        if not DISABLE_INFERENCE_CACHE:\n            logger.debug(\n                f\"ModelManager - caching inference request started for model_id={model_id}\"\n            )\n            cache.zadd(\n                f\"models\",\n                value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n            if (\n                hasattr(request, \"image\")\n                and hasattr(request.image, \"type\")\n                and request.image.type == \"numpy\"\n            ):\n                request.image.value = str(request.image.value)\n            cache.zadd(\n                f\"inference:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                value=to_cachable_inference_item(request, rtn_val),\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n            logger.debug(\n                f\"ModelManager - caching inference request finished for model_id={model_id}\"\n            )\n        return rtn_val\n    except Exception as e:\n        finish_time = time.time()\n        if not DISABLE_INFERENCE_CACHE:\n            cache.zadd(\n                f\"models\",\n                value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n            cache.zadd(\n                f\"error:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                value={\n                    \"request\": jsonable_encoder(\n                        request.dict(exclude={\"image\", \"subject\", \"prompt\"})\n                    ),\n                    \"error\": str(e),\n                },\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n        raise\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.infer_from_request_sync","title":"<code>infer_from_request_sync(model_id, request, **kwargs)</code>","text":"<p>Runs inference on the specified model with the given request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def infer_from_request_sync(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Runs inference on the specified model with the given request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    logger.debug(\n        f\"ModelManager - inference from request started for model_id={model_id}.\"\n    )\n    if METRICS_ENABLED and self.pingback:\n        logger.debug(\"ModelManager - setting pingback fallback api key...\")\n        self.pingback.fallback_api_key = request.api_key\n    try:\n        rtn_val = self.model_infer_sync(\n            model_id=model_id, request=request, **kwargs\n        )\n        logger.debug(\n            f\"ModelManager - inference from request finished for model_id={model_id}.\"\n        )\n        finish_time = time.time()\n        if not DISABLE_INFERENCE_CACHE:\n            logger.debug(\n                f\"ModelManager - caching inference request started for model_id={model_id}\"\n            )\n            cache.zadd(\n                f\"models\",\n                value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n            if (\n                hasattr(request, \"image\")\n                and hasattr(request.image, \"type\")\n                and request.image.type == \"numpy\"\n            ):\n                request.image.value = str(request.image.value)\n            cache.zadd(\n                f\"inference:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                value=to_cachable_inference_item(request, rtn_val),\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n            logger.debug(\n                f\"ModelManager - caching inference request finished for model_id={model_id}\"\n            )\n        return rtn_val\n    except Exception as e:\n        finish_time = time.time()\n        if not DISABLE_INFERENCE_CACHE:\n            cache.zadd(\n                f\"models\",\n                value=f\"{GLOBAL_INFERENCE_SERVER_ID}:{request.api_key}:{model_id}\",\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n            cache.zadd(\n                f\"error:{GLOBAL_INFERENCE_SERVER_ID}:{model_id}\",\n                value={\n                    \"request\": jsonable_encoder(\n                        request.dict(exclude={\"image\", \"subject\", \"prompt\"})\n                    ),\n                    \"error\": str(e),\n                },\n                score=finish_time,\n                expire=METRICS_INTERVAL * 2,\n            )\n        raise\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.init_pingback","title":"<code>init_pingback()</code>","text":"<p>Initializes pingback mechanism.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def init_pingback(self):\n    \"\"\"Initializes pingback mechanism.\"\"\"\n    self.num_errors = 0  # in the device\n    self.uuid = ROBOFLOW_SERVER_UUID\n    if METRICS_ENABLED:\n        self.pingback = PingbackInfo(self)\n        self.pingback.start()\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.keys","title":"<code>keys()</code>","text":"<p>Retrieve the keys (model identifiers) from the manager.</p> <p>Returns:</p> Type Description <p>List[str]: The keys of the models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def keys(self):\n    \"\"\"Retrieve the keys (model identifiers) from the manager.\n\n    Returns:\n        List[str]: The keys of the models in the manager.\n    \"\"\"\n    return self._models.keys()\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.make_response","title":"<code>make_response(model_id, predictions, *args, **kwargs)</code>","text":"<p>Creates a response object from the model's predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>predictions</code> <code>List[List[float]]</code> <p>The model's predictions.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The created response object.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def make_response(\n    self, model_id: str, predictions: List[List[float]], *args, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Creates a response object from the model's predictions.\n\n    Args:\n        model_id (str): The identifier of the model.\n        predictions (List[List[float]]): The model's predictions.\n\n    Returns:\n        InferenceResponse: The created response object.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].make_response(predictions, *args, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.models","title":"<code>models()</code>","text":"<p>Retrieve the models dictionary from the manager.</p> <p>Returns:</p> Type Description <code>Dict[str, Model]</code> <p>Dict[str, Model]: The keys of the models in the manager.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def models(self) -&gt; Dict[str, Model]:\n    \"\"\"Retrieve the models dictionary from the manager.\n\n    Returns:\n        Dict[str, Model]: The keys of the models in the manager.\n    \"\"\"\n    return self._models\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.postprocess","title":"<code>postprocess(model_id, predictions, preprocess_return_metadata, *args, **kwargs)</code>","text":"<p>Processes the model's predictions after inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>predictions</code> <code>ndarray</code> <p>The model's predictions.</p> required <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>List[List[float]]: The post-processed predictions.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def postprocess(\n    self,\n    model_id: str,\n    predictions: Tuple[np.ndarray, ...],\n    preprocess_return_metadata: PreprocessReturnMetadata,\n    *args,\n    **kwargs,\n) -&gt; List[List[float]]:\n    \"\"\"Processes the model's predictions after inference.\n\n    Args:\n        model_id (str): The identifier of the model.\n        predictions (np.ndarray): The model's predictions.\n\n    Returns:\n        List[List[float]]: The post-processed predictions.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].postprocess(\n        predictions, preprocess_return_metadata, *args, **kwargs\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.predict","title":"<code>predict(model_id, *args, **kwargs)</code>","text":"<p>Runs prediction on the specified model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ...]</code> <p>np.ndarray: The predictions from the model.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def predict(self, model_id: str, *args, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n    \"\"\"Runs prediction on the specified model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        np.ndarray: The predictions from the model.\n    \"\"\"\n    self.check_for_model(model_id)\n    self._models[model_id].metrics[\"num_inferences\"] += 1\n    tic = time.perf_counter()\n    res = self._models[model_id].predict(*args, **kwargs)\n    toc = time.perf_counter()\n    self._models[model_id].metrics[\"avg_inference_time\"] += toc - tic\n    return res\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.preprocess","title":"<code>preprocess(model_id, request)</code>","text":"<p>Preprocesses the request before inference.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to preprocess.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, PreprocessReturnMetadata]</code> <p>Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.</p> Source code in <code>inference/core/managers/base.py</code> <pre><code>def preprocess(\n    self, model_id: str, request: InferenceRequest\n) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n    \"\"\"Preprocesses the request before inference.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to preprocess.\n\n    Returns:\n        Tuple[np.ndarray, List[Tuple[int, int]]]: The preprocessed data.\n    \"\"\"\n    self.check_for_model(model_id)\n    return self._models[model_id].preprocess(**request.dict())\n</code></pre>"},{"location":"docs/reference/inference/core/managers/base/#inference.core.managers.base.ModelManager.remove","title":"<code>remove(model_id)</code>","text":"<p>Removes a model from the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required Source code in <code>inference/core/managers/base.py</code> <pre><code>def remove(self, model_id: str) -&gt; None:\n    \"\"\"Removes a model from the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n    \"\"\"\n    try:\n        logger.debug(f\"Removing model {model_id} from base model manager\")\n        self.check_for_model(model_id)\n        self._models[model_id].clear_cache()\n        del self._models[model_id]\n    except InferenceModelNotFound:\n        logger.warning(\n            f\"Attempted to remove model with id {model_id}, but it is not loaded. Skipping...\"\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/managers/metrics/","title":"metrics","text":""},{"location":"docs/reference/inference/core/managers/metrics/#inference.core.managers.metrics.get_container_stats","title":"<code>get_container_stats(docker_socket_path)</code>","text":"<p>Gets the container stats.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the container stats.</p> Source code in <code>inference/core/managers/metrics.py</code> <pre><code>def get_container_stats(docker_socket_path: str) -&gt; dict:\n    \"\"\"\n    Gets the container stats.\n\n    Returns:\n        dict: A dictionary containing the container stats.\n    \"\"\"\n    try:\n        container_id = socket.gethostname()\n        result = subprocess.run(\n            [\n                \"curl\",\n                \"--unix-socket\",\n                docker_socket_path,\n                f\"http://localhost/containers/{container_id}/stats?stream=false\",\n            ],\n            capture_output=True,\n            text=True,\n        )\n        if result.returncode != 0:\n            raise Exception(result.stderr)\n        stats = json.loads(result.stdout.strip())\n        return {\"stats\": stats}\n    except Exception as e:\n        logger.exception(e)\n        raise Exception(\"An error occurred while fetching container stats.\")\n</code></pre>"},{"location":"docs/reference/inference/core/managers/metrics/#inference.core.managers.metrics.get_model_metrics","title":"<code>get_model_metrics(inference_server_id, model_id, min=-1, max=float('inf'))</code>","text":"<p>Gets the metrics for a given model between a specified time range.</p> <p>Parameters:</p> Name Type Description Default <code>device_id</code> <code>str</code> <p>The identifier of the device.</p> required <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>start</code> <code>float</code> <p>The starting timestamp of the time range. Defaults to -1.</p> required <code>stop</code> <code>float</code> <p>The ending timestamp of the time range. Defaults to float(\"inf\").</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing the metrics of the model:   - num_inferences (int): The number of inferences made.   - avg_inference_time (float): The average inference time.   - num_errors (int): The number of errors occurred.</p> Source code in <code>inference/core/managers/metrics.py</code> <pre><code>def get_model_metrics(\n    inference_server_id: str, model_id: str, min: float = -1, max: float = float(\"inf\")\n) -&gt; dict:\n    \"\"\"\n    Gets the metrics for a given model between a specified time range.\n\n    Args:\n        device_id (str): The identifier of the device.\n        model_id (str): The identifier of the model.\n        start (float, optional): The starting timestamp of the time range. Defaults to -1.\n        stop (float, optional): The ending timestamp of the time range. Defaults to float(\"inf\").\n\n    Returns:\n        dict: A dictionary containing the metrics of the model:\n              - num_inferences (int): The number of inferences made.\n              - avg_inference_time (float): The average inference time.\n              - num_errors (int): The number of errors occurred.\n    \"\"\"\n    now = time.time()\n    inferences_with_times = cache.zrangebyscore(\n        f\"inference:{inference_server_id}:{model_id}\", min=min, max=max, withscores=True\n    )\n    num_inferences = len(inferences_with_times)\n    inference_times = []\n    for inference, t in inferences_with_times:\n        response = inference[\"response\"]\n        if isinstance(response, list):\n            times = [r[\"time\"] for r in response if \"time\" in r]\n            inference_times.extend(times)\n        else:\n            if \"time\" in response:\n                inference_times.append(response[\"time\"])\n    avg_inference_time = (\n        sum(inference_times) / len(inference_times) if len(inference_times) &gt; 0 else 0\n    )\n    errors_with_times = cache.zrangebyscore(\n        f\"error:{inference_server_id}:{model_id}\", min=min, max=max, withscores=True\n    )\n    num_errors = len(errors_with_times)\n    return {\n        \"num_inferences\": num_inferences,\n        \"avg_inference_time\": avg_inference_time,\n        \"num_errors\": num_errors,\n    }\n</code></pre>"},{"location":"docs/reference/inference/core/managers/metrics/#inference.core.managers.metrics.get_system_info","title":"<code>get_system_info()</code>","text":"<p>Collects system information such as platform, architecture, hostname, IP address, MAC address, and processor details.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary containing detailed system information.</p> Source code in <code>inference/core/managers/metrics.py</code> <pre><code>def get_system_info() -&gt; dict:\n    \"\"\"Collects system information such as platform, architecture, hostname, IP address, MAC address, and processor details.\n\n    Returns:\n        dict: A dictionary containing detailed system information.\n    \"\"\"\n    info = {}\n    try:\n        info[\"platform\"] = platform.system()\n        info[\"platform_release\"] = platform.release()\n        info[\"platform_version\"] = platform.version()\n        info[\"architecture\"] = platform.machine()\n        info[\"hostname\"] = socket.gethostname()\n        info[\"ip_address\"] = socket.gethostbyname(socket.gethostname())\n        info[\"mac_address\"] = \":\".join(re.findall(\"..\", \"%012x\" % uuid.getnode()))\n        info[\"processor\"] = platform.processor()\n        return info\n    except Exception as e:\n        logger.exception(e)\n    finally:\n        return info\n</code></pre>"},{"location":"docs/reference/inference/core/managers/pingback/","title":"pingback","text":""},{"location":"docs/reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo","title":"<code>PingbackInfo</code>","text":"<p>Class responsible for managing pingback information for Roboflow.</p> <p>This class initializes a scheduler to periodically post data to Roboflow, containing information about the models, container, and device.</p> <p>Attributes:</p> Name Type Description <code>scheduler</code> <code>BackgroundScheduler</code> <p>A scheduler for running jobs in the background.</p> <code>model_manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> <code>process_startup_time</code> <code>str</code> <p>Unix timestamp indicating when the process started.</p> <code>METRICS_URL</code> <code>str</code> <p>URL to send the pingback data to.</p> <code>system_info</code> <code>dict</code> <p>Information about the system.</p> <code>window_start_timestamp</code> <code>str</code> <p>Unix timestamp indicating the start of the current window.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>class PingbackInfo:\n    \"\"\"Class responsible for managing pingback information for Roboflow.\n\n    This class initializes a scheduler to periodically post data to Roboflow, containing information about the models,\n    container, and device.\n\n    Attributes:\n        scheduler (BackgroundScheduler): A scheduler for running jobs in the background.\n        model_manager (ModelManager): Reference to the model manager object.\n        process_startup_time (str): Unix timestamp indicating when the process started.\n        METRICS_URL (str): URL to send the pingback data to.\n        system_info (dict): Information about the system.\n        window_start_timestamp (str): Unix timestamp indicating the start of the current window.\n    \"\"\"\n\n    def __init__(self, manager):\n        \"\"\"Initializes PingbackInfo with the given manager.\n\n        Args:\n            manager (ModelManager): Reference to the model manager object.\n        \"\"\"\n        try:\n            self.scheduler = BackgroundScheduler(\n                job_defaults={\"coalesce\": True, \"max_instances\": 1}\n            )\n            self.model_manager = manager\n            self.process_startup_time = str(int(time.time()))\n            logger.debug(\n                \"UUID: \" + self.model_manager.uuid\n            )  # To correlate with UI container view\n            self.window_start_timestamp = str(int(time.time()))\n            context = {\n                \"api_key\": API_KEY,\n                \"timestamp\": str(int(time.time())),\n                \"device_id\": GLOBAL_DEVICE_ID,\n                \"inference_server_id\": GLOBAL_INFERENCE_SERVER_ID,\n                \"inference_server_version\": __version__,\n                \"tags\": TAGS,\n            }\n            self.environment_info = context | get_system_info()\n\n            # we will set this from model manager when a new api key is used\n            # to use in case there is no global ENV api key configured\n            self.fallback_api_key = None\n\n        except Exception as e:\n            logger.debug(\n                \"Error sending pingback to Roboflow, if you want to disable this feature unset the ROBOFLOW_ENABLED environment variable. \"\n                + str(e)\n            )\n\n    def start(self):\n        \"\"\"Starts the scheduler to periodically post data to Roboflow.\n\n        If METRICS_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.\n        \"\"\"\n        if METRICS_ENABLED == False:\n            logger.warning(\n                \"Metrics reporting to Roboflow is disabled; not sending back stats to Roboflow.\"\n            )\n            return\n        try:\n            self.scheduler.add_job(\n                self.post_data,\n                \"interval\",\n                seconds=METRICS_INTERVAL,\n                args=[self.model_manager],\n                replace_existing=True,\n            )\n            self.scheduler.start()\n        except Exception as e:\n            logger.debug(e)\n\n    def stop(self):\n        \"\"\"Stops the scheduler.\"\"\"\n        self.scheduler.shutdown()\n\n    def post_data(self, model_manager):\n        \"\"\"Posts data to Roboflow about the models, container, device, and other relevant metrics.\n\n        Args:\n            model_manager (ModelManager): Reference to the model manager object.\n\n        The data is collected and reset for the next window, and a POST request is made to the pingback URL.\n        \"\"\"\n        all_data = self.environment_info.copy()\n        all_data[\"inference_results\"] = []\n\n        # use fallback api key if env didn't have one\n        if self.fallback_api_key and not all_data.get(\"api_key\"):\n            all_data[\"api_key\"] = self.fallback_api_key\n\n        try:\n            now = time.time()\n            start = now - METRICS_INTERVAL\n            for model_id in model_manager.models():\n                results = get_inference_results_for_model(\n                    GLOBAL_INFERENCE_SERVER_ID, model_id, min=start, max=now\n                )\n                all_data[\"inference_results\"] = all_data[\"inference_results\"] + results\n            res = requests.post(wrap_url(METRICS_URL), json=all_data, timeout=10)\n            try:\n                api_key_safe_raise_for_status(response=res)\n                logger.debug(\n                    \"Sent metrics to Roboflow {} at {}.\".format(\n                        METRICS_URL, str(all_data)\n                    )\n                )\n            except Exception as e:\n                logger.debug(\n                    f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable.\"\n                )\n\n        except Exception as e:\n            try:\n                logger.debug(\n                    f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}. Data was: {all_data}\"\n                )\n                traceback.print_exc()\n\n            except Exception as e2:\n                logger.debug(\n                    f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}.\"\n                )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.__init__","title":"<code>__init__(manager)</code>","text":"<p>Initializes PingbackInfo with the given manager.</p> <p>Parameters:</p> Name Type Description Default <code>manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> required Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def __init__(self, manager):\n    \"\"\"Initializes PingbackInfo with the given manager.\n\n    Args:\n        manager (ModelManager): Reference to the model manager object.\n    \"\"\"\n    try:\n        self.scheduler = BackgroundScheduler(\n            job_defaults={\"coalesce\": True, \"max_instances\": 1}\n        )\n        self.model_manager = manager\n        self.process_startup_time = str(int(time.time()))\n        logger.debug(\n            \"UUID: \" + self.model_manager.uuid\n        )  # To correlate with UI container view\n        self.window_start_timestamp = str(int(time.time()))\n        context = {\n            \"api_key\": API_KEY,\n            \"timestamp\": str(int(time.time())),\n            \"device_id\": GLOBAL_DEVICE_ID,\n            \"inference_server_id\": GLOBAL_INFERENCE_SERVER_ID,\n            \"inference_server_version\": __version__,\n            \"tags\": TAGS,\n        }\n        self.environment_info = context | get_system_info()\n\n        # we will set this from model manager when a new api key is used\n        # to use in case there is no global ENV api key configured\n        self.fallback_api_key = None\n\n    except Exception as e:\n        logger.debug(\n            \"Error sending pingback to Roboflow, if you want to disable this feature unset the ROBOFLOW_ENABLED environment variable. \"\n            + str(e)\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.post_data","title":"<code>post_data(model_manager)</code>","text":"<p>Posts data to Roboflow about the models, container, device, and other relevant metrics.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>Reference to the model manager object.</p> required <p>The data is collected and reset for the next window, and a POST request is made to the pingback URL.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def post_data(self, model_manager):\n    \"\"\"Posts data to Roboflow about the models, container, device, and other relevant metrics.\n\n    Args:\n        model_manager (ModelManager): Reference to the model manager object.\n\n    The data is collected and reset for the next window, and a POST request is made to the pingback URL.\n    \"\"\"\n    all_data = self.environment_info.copy()\n    all_data[\"inference_results\"] = []\n\n    # use fallback api key if env didn't have one\n    if self.fallback_api_key and not all_data.get(\"api_key\"):\n        all_data[\"api_key\"] = self.fallback_api_key\n\n    try:\n        now = time.time()\n        start = now - METRICS_INTERVAL\n        for model_id in model_manager.models():\n            results = get_inference_results_for_model(\n                GLOBAL_INFERENCE_SERVER_ID, model_id, min=start, max=now\n            )\n            all_data[\"inference_results\"] = all_data[\"inference_results\"] + results\n        res = requests.post(wrap_url(METRICS_URL), json=all_data, timeout=10)\n        try:\n            api_key_safe_raise_for_status(response=res)\n            logger.debug(\n                \"Sent metrics to Roboflow {} at {}.\".format(\n                    METRICS_URL, str(all_data)\n                )\n            )\n        except Exception as e:\n            logger.debug(\n                f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable.\"\n            )\n\n    except Exception as e:\n        try:\n            logger.debug(\n                f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}. Data was: {all_data}\"\n            )\n            traceback.print_exc()\n\n        except Exception as e2:\n            logger.debug(\n                f\"Error sending metrics to Roboflow, if you want to disable this feature unset the METRICS_ENABLED environment variable. Error was: {e}.\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.start","title":"<code>start()</code>","text":"<p>Starts the scheduler to periodically post data to Roboflow.</p> <p>If METRICS_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def start(self):\n    \"\"\"Starts the scheduler to periodically post data to Roboflow.\n\n    If METRICS_ENABLED is False, a warning is logged, and the method returns without starting the scheduler.\n    \"\"\"\n    if METRICS_ENABLED == False:\n        logger.warning(\n            \"Metrics reporting to Roboflow is disabled; not sending back stats to Roboflow.\"\n        )\n        return\n    try:\n        self.scheduler.add_job(\n            self.post_data,\n            \"interval\",\n            seconds=METRICS_INTERVAL,\n            args=[self.model_manager],\n            replace_existing=True,\n        )\n        self.scheduler.start()\n    except Exception as e:\n        logger.debug(e)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/pingback/#inference.core.managers.pingback.PingbackInfo.stop","title":"<code>stop()</code>","text":"<p>Stops the scheduler.</p> Source code in <code>inference/core/managers/pingback.py</code> <pre><code>def stop(self):\n    \"\"\"Stops the scheduler.\"\"\"\n    self.scheduler.shutdown()\n</code></pre>"},{"location":"docs/reference/inference/core/managers/prometheus/","title":"prometheus","text":""},{"location":"docs/reference/inference/core/managers/prometheus/#inference.core.managers.prometheus.InferenceInstrumentator","title":"<code>InferenceInstrumentator</code>","text":"<p>Class responsible for managing the Prometheus metrics for the inference server.</p> <p>This class inititalizes the Prometheus Instrumentator and exposes the metrics endpoint.</p> Source code in <code>inference/core/managers/prometheus.py</code> <pre><code>class InferenceInstrumentator:\n    \"\"\"\n    Class responsible for managing the Prometheus metrics for the inference server.\n\n    This class inititalizes the Prometheus Instrumentator and exposes the metrics endpoint.\n\n    \"\"\"\n\n    def __init__(self, app, model_manager, endpoint: str = \"/metrics\"):\n        self.instrumentator = Instrumentator()\n        self.instrumentator.instrument(app).expose(app, endpoint)\n        self.collector = CustomCollector(model_manager)\n        REGISTRY.register(self.collector)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/stub_loader/","title":"stub_loader","text":""},{"location":"docs/reference/inference/core/managers/stub_loader/#inference.core.managers.stub_loader.StubLoaderManager","title":"<code>StubLoaderManager</code>","text":"<p>               Bases: <code>ModelManager</code></p> Source code in <code>inference/core/managers/stub_loader.py</code> <pre><code>class StubLoaderManager(ModelManager):\n    def add_model(self, model_id: str, api_key: str, model_id_alias=None) -&gt; None:\n        \"\"\"Adds a new model to the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n        \"\"\"\n        if model_id in self._models:\n            return\n        model_class = self.model_registry.get_model(\n            model_id_alias if model_id_alias is not None else model_id, api_key\n        )\n        model = model_class(model_id=model_id, api_key=api_key, load_weights=False)\n        self._models[model_id] = model\n</code></pre>"},{"location":"docs/reference/inference/core/managers/stub_loader/#inference.core.managers.stub_loader.StubLoaderManager.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None)</code>","text":"<p>Adds a new model to the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required Source code in <code>inference/core/managers/stub_loader.py</code> <pre><code>def add_model(self, model_id: str, api_key: str, model_id_alias=None) -&gt; None:\n    \"\"\"Adds a new model to the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n    \"\"\"\n    if model_id in self._models:\n        return\n    model_class = self.model_registry.get_model(\n        model_id_alias if model_id_alias is not None else model_id, api_key\n    )\n    model = model_class(model_id=model_id, api_key=api_key, load_weights=False)\n    self._models[model_id] = model\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/","title":"base","text":""},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator","title":"<code>ModelManagerDecorator</code>","text":"<p>               Bases: <code>ModelManager</code></p> <p>Basic decorator, it acts like a <code>ModelManager</code> and contains a <code>ModelManager</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>Instance of a ModelManager.</p> required <p>Methods:</p> Name Description <code>add_model</code> <p>Adds a model to the manager.</p> <code>infer</code> <p>Processes a complete inference request.</p> <code>infer_only</code> <p>Performs only the inference part of a request.</p> <code>preprocess</code> <p>Processes the preprocessing part of a request.</p> <code>get_task_type</code> <p>Gets the task type associated with a model.</p> <code>get_class_names</code> <p>Gets the class names for a given model.</p> <code>remove</code> <p>Removes a model from the manager.</p> <code>__len__</code> <p>Returns the number of models in the manager.</p> <code>__getitem__</code> <p>Retrieves a model by its ID.</p> <code>__contains__</code> <p>Checks if a model exists in the manager.</p> <code>keys</code> <p>Returns the keys (model IDs) from the manager.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>class ModelManagerDecorator(ModelManager):\n    \"\"\"Basic decorator, it acts like a `ModelManager` and contains a `ModelManager`.\n\n    Args:\n        model_manager (ModelManager): Instance of a ModelManager.\n\n    Methods:\n        add_model: Adds a model to the manager.\n        infer: Processes a complete inference request.\n        infer_only: Performs only the inference part of a request.\n        preprocess: Processes the preprocessing part of a request.\n        get_task_type: Gets the task type associated with a model.\n        get_class_names: Gets the class names for a given model.\n        remove: Removes a model from the manager.\n        __len__: Returns the number of models in the manager.\n        __getitem__: Retrieves a model by its ID.\n        __contains__: Checks if a model exists in the manager.\n        keys: Returns the keys (model IDs) from the manager.\n    \"\"\"\n\n    @property\n    def _models(self):\n        raise ValueError(\"Should only be accessing self.model_manager._models\")\n\n    @property\n    def model_registry(self):\n        raise ValueError(\"Should only be accessing self.model_manager.model_registry\")\n\n    def __init__(self, model_manager: ModelManager):\n        \"\"\"Initializes the decorator with an instance of a ModelManager.\"\"\"\n        self.model_manager = model_manager\n\n    def init_pingback(self):\n        self.model_manager.init_pingback()\n\n    @property\n    def pingback(self):\n        return self.model_manager.pingback\n\n    def add_model(\n        self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n    ):\n        \"\"\"Adds a model to the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n        \"\"\"\n        if model_id in self:\n            return\n        self.model_manager.add_model(model_id, api_key, model_id_alias=model_id_alias)\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes a complete inference request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        return await self.model_manager.infer_from_request(model_id, request, **kwargs)\n\n    def infer_from_request_sync(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes a complete inference request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        return self.model_manager.infer_from_request_sync(model_id, request, **kwargs)\n\n    def infer_only(self, model_id: str, request, img_in, img_dims, batch_size=None):\n        \"\"\"Performs only the inference part of a request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request: The request to process.\n            img_in: Input image.\n            img_dims: Image dimensions.\n            batch_size (int, optional): Batch size.\n\n        Returns:\n            Response from the inference-only operation.\n        \"\"\"\n        return self.model_manager.infer_only(\n            model_id, request, img_in, img_dims, batch_size\n        )\n\n    def preprocess(self, model_id: str, request: InferenceRequest):\n        \"\"\"Processes the preprocessing part of a request.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to preprocess.\n        \"\"\"\n        return self.model_manager.preprocess(model_id, request)\n\n    def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n        \"\"\"Gets the task type associated with a model.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            str: The task type.\n        \"\"\"\n        if api_key is None:\n            api_key = API_KEY\n        return self.model_manager.get_task_type(model_id, api_key=api_key)\n\n    def get_class_names(self, model_id):\n        \"\"\"Gets the class names for a given model.\n\n        Args:\n            model_id: The identifier of the model.\n\n        Returns:\n            List of class names.\n        \"\"\"\n        return self.model_manager.get_class_names(model_id)\n\n    def remove(self, model_id: str) -&gt; Model:\n        \"\"\"Removes a model from the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            Model: The removed model.\n        \"\"\"\n        return self.model_manager.remove(model_id)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of models in the manager.\n\n        Returns:\n            int: Number of models.\n        \"\"\"\n        return len(self.model_manager)\n\n    def __getitem__(self, key: str) -&gt; Model:\n        \"\"\"Retrieves a model by its ID.\n\n        Args:\n            key (str): The identifier of the model.\n\n        Returns:\n            Model: The model instance.\n        \"\"\"\n        return self.model_manager[key]\n\n    def __contains__(self, model_id: str):\n        \"\"\"Checks if a model exists in the manager.\n\n        Args:\n            model_id (str): The identifier of the model.\n\n        Returns:\n            bool: True if the model exists, False otherwise.\n        \"\"\"\n        return model_id in self.model_manager\n\n    def keys(self):\n        \"\"\"Returns the keys (model IDs) from the manager.\n\n        Returns:\n            List of keys (model IDs).\n        \"\"\"\n        return self.model_manager.keys()\n\n    def models(self):\n        return self.model_manager.models()\n\n    def predict(self, model_id: str, *args, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n        return self.model_manager.predict(model_id, *args, **kwargs)\n\n    def postprocess(\n        self,\n        model_id: str,\n        predictions: Tuple[np.ndarray, ...],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        *args,\n        **kwargs\n    ) -&gt; List[List[float]]:\n        return self.model_manager.postprocess(\n            model_id, predictions, preprocess_return_metadata, *args, **kwargs\n        )\n\n    def make_response(\n        self, model_id: str, predictions: List[List[float]], *args, **kwargs\n    ) -&gt; InferenceResponse:\n        return self.model_manager.make_response(model_id, predictions, *args, **kwargs)\n\n    @property\n    def num_errors(self):\n        return self.model_manager.num_errors\n\n    @num_errors.setter\n    def num_errors(self, value):\n        self.model_manager.num_errors = value\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__contains__","title":"<code>__contains__(model_id)</code>","text":"<p>Checks if a model exists in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the model exists, False otherwise.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __contains__(self, model_id: str):\n    \"\"\"Checks if a model exists in the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        bool: True if the model exists, False otherwise.\n    \"\"\"\n    return model_id in self.model_manager\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieves a model by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model instance.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Model:\n    \"\"\"Retrieves a model by its ID.\n\n    Args:\n        key (str): The identifier of the model.\n\n    Returns:\n        Model: The model instance.\n    \"\"\"\n    return self.model_manager[key]\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__init__","title":"<code>__init__(model_manager)</code>","text":"<p>Initializes the decorator with an instance of a ModelManager.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __init__(self, model_manager: ModelManager):\n    \"\"\"Initializes the decorator with an instance of a ModelManager.\"\"\"\n    self.model_manager = model_manager\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of models in the manager.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of models.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of models in the manager.\n\n    Returns:\n        int: Number of models.\n    \"\"\"\n    return len(self.model_manager)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None)</code>","text":"<p>Adds a model to the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def add_model(\n    self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n):\n    \"\"\"Adds a model to the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n    \"\"\"\n    if model_id in self:\n        return\n    self.model_manager.add_model(model_id, api_key, model_id_alias=model_id_alias)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.get_class_names","title":"<code>get_class_names(model_id)</code>","text":"<p>Gets the class names for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <p>The identifier of the model.</p> required <p>Returns:</p> Type Description <p>List of class names.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def get_class_names(self, model_id):\n    \"\"\"Gets the class names for a given model.\n\n    Args:\n        model_id: The identifier of the model.\n\n    Returns:\n        List of class names.\n    \"\"\"\n    return self.model_manager.get_class_names(model_id)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.get_task_type","title":"<code>get_task_type(model_id, api_key=None)</code>","text":"<p>Gets the task type associated with a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The task type.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def get_task_type(self, model_id: str, api_key: str = None) -&gt; str:\n    \"\"\"Gets the task type associated with a model.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        str: The task type.\n    \"\"\"\n    if api_key is None:\n        api_key = API_KEY\n    return self.model_manager.get_task_type(model_id, api_key=api_key)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Processes a complete inference request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Processes a complete inference request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    return await self.model_manager.infer_from_request(model_id, request, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.infer_from_request_sync","title":"<code>infer_from_request_sync(model_id, request, **kwargs)</code>","text":"<p>Processes a complete inference request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def infer_from_request_sync(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Processes a complete inference request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    return self.model_manager.infer_from_request_sync(model_id, request, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.infer_only","title":"<code>infer_only(model_id, request, img_in, img_dims, batch_size=None)</code>","text":"<p>Performs only the inference part of a request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <p>The request to process.</p> required <code>img_in</code> <p>Input image.</p> required <code>img_dims</code> <p>Image dimensions.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>None</code> <p>Returns:</p> Type Description <p>Response from the inference-only operation.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def infer_only(self, model_id: str, request, img_in, img_dims, batch_size=None):\n    \"\"\"Performs only the inference part of a request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request: The request to process.\n        img_in: Input image.\n        img_dims: Image dimensions.\n        batch_size (int, optional): Batch size.\n\n    Returns:\n        Response from the inference-only operation.\n    \"\"\"\n    return self.model_manager.infer_only(\n        model_id, request, img_in, img_dims, batch_size\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.keys","title":"<code>keys()</code>","text":"<p>Returns the keys (model IDs) from the manager.</p> <p>Returns:</p> Type Description <p>List of keys (model IDs).</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def keys(self):\n    \"\"\"Returns the keys (model IDs) from the manager.\n\n    Returns:\n        List of keys (model IDs).\n    \"\"\"\n    return self.model_manager.keys()\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.preprocess","title":"<code>preprocess(model_id, request)</code>","text":"<p>Processes the preprocessing part of a request.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to preprocess.</p> required Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def preprocess(self, model_id: str, request: InferenceRequest):\n    \"\"\"Processes the preprocessing part of a request.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to preprocess.\n    \"\"\"\n    return self.model_manager.preprocess(model_id, request)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/base/#inference.core.managers.decorators.base.ModelManagerDecorator.remove","title":"<code>remove(model_id)</code>","text":"<p>Removes a model from the manager.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The removed model.</p> Source code in <code>inference/core/managers/decorators/base.py</code> <pre><code>def remove(self, model_id: str) -&gt; Model:\n    \"\"\"Removes a model from the manager.\n\n    Args:\n        model_id (str): The identifier of the model.\n\n    Returns:\n        Model: The removed model.\n    \"\"\"\n    return self.model_manager.remove(model_id)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/","title":"fixed_size_cache","text":""},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache","title":"<code>WithFixedSizeCache</code>","text":"<p>               Bases: <code>ModelManagerDecorator</code></p> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>class WithFixedSizeCache(ModelManagerDecorator):\n    def __init__(self, model_manager: ModelManager, max_size: int = 8):\n        \"\"\"Cache decorator, models will be evicted based on the last utilization (`.infer` call). Internally, a [double-ended queue](https://docs.python.org/3/library/collections.html#collections.deque) is used to keep track of model utilization.\n\n        Args:\n            model_manager (ModelManager): Instance of a ModelManager.\n            max_size (int, optional): Max number of models at the same time. Defaults to 8.\n        \"\"\"\n        super().__init__(model_manager)\n        self.max_size = max_size\n        self._key_queue = deque(self.model_manager.keys())\n\n    def add_model(\n        self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"Adds a model to the manager and evicts the least recently used if the cache is full.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n        \"\"\"\n        queue_id = self._resolve_queue_id(\n            model_id=model_id, model_id_alias=model_id_alias\n        )\n        if queue_id in self:\n            logger.debug(\n                f\"Detected {queue_id} in WithFixedSizeCache models queue -&gt; marking as most recently used.\"\n            )\n            self._key_queue.remove(queue_id)\n            self._key_queue.append(queue_id)\n            return None\n\n        logger.debug(f\"Current capacity of ModelManager: {len(self)}/{self.max_size}\")\n        while len(self) &gt;= self.max_size:\n            to_remove_model_id = self._key_queue.popleft()\n            logger.debug(\n                f\"Reached maximum capacity of ModelManager. Unloading model {to_remove_model_id}\"\n            )\n            super().remove(to_remove_model_id)\n            logger.debug(f\"Model {to_remove_model_id} successfully unloaded.\")\n        logger.debug(f\"Marking new model {queue_id} as most recently used.\")\n        self._key_queue.append(queue_id)\n        try:\n            return super().add_model(model_id, api_key, model_id_alias=model_id_alias)\n        except Exception as error:\n            logger.debug(\n                f\"Could not initialise model {queue_id}. Removing from WithFixedSizeCache models queue.\"\n            )\n            self._key_queue.remove(queue_id)\n            raise error\n\n    def clear(self) -&gt; None:\n        \"\"\"Removes all models from the manager.\"\"\"\n        for model_id in list(self.keys()):\n            self.remove(model_id)\n\n    def remove(self, model_id: str) -&gt; Model:\n        try:\n            self._key_queue.remove(model_id)\n        except ValueError:\n            logger.warning(\n                f\"Could not successfully purge model {model_id} from  WithFixedSizeCache models queue\"\n            )\n        return super().remove(model_id)\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes a complete inference request and updates the cache.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        self._key_queue.remove(model_id)\n        self._key_queue.append(model_id)\n        return await super().infer_from_request(model_id, request, **kwargs)\n\n    def infer_from_request_sync(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes a complete inference request and updates the cache.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        self._key_queue.remove(model_id)\n        self._key_queue.append(model_id)\n        return super().infer_from_request_sync(model_id, request, **kwargs)\n\n    def infer_only(self, model_id: str, request, img_in, img_dims, batch_size=None):\n        \"\"\"Performs only the inference part of a request and updates the cache.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request: The request to process.\n            img_in: Input image.\n            img_dims: Image dimensions.\n            batch_size (int, optional): Batch size.\n\n        Returns:\n            Response from the inference-only operation.\n        \"\"\"\n        self._key_queue.remove(model_id)\n        self._key_queue.append(model_id)\n        return super().infer_only(model_id, request, img_in, img_dims, batch_size)\n\n    def preprocess(self, model_id: str, request):\n        \"\"\"Processes the preprocessing part of a request and updates the cache.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to preprocess.\n        \"\"\"\n        self._key_queue.remove(model_id)\n        self._key_queue.append(model_id)\n        return super().preprocess(model_id, request)\n\n    def describe_models(self) -&gt; List[ModelDescription]:\n        return self.model_manager.describe_models()\n\n    def _resolve_queue_id(\n        self, model_id: str, model_id_alias: Optional[str] = None\n    ) -&gt; str:\n        return model_id if model_id_alias is None else model_id_alias\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.__init__","title":"<code>__init__(model_manager, max_size=8)</code>","text":"<p>Cache decorator, models will be evicted based on the last utilization (<code>.infer</code> call). Internally, a double-ended queue is used to keep track of model utilization.</p> <p>Parameters:</p> Name Type Description Default <code>model_manager</code> <code>ModelManager</code> <p>Instance of a ModelManager.</p> required <code>max_size</code> <code>int</code> <p>Max number of models at the same time. Defaults to 8.</p> <code>8</code> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def __init__(self, model_manager: ModelManager, max_size: int = 8):\n    \"\"\"Cache decorator, models will be evicted based on the last utilization (`.infer` call). Internally, a [double-ended queue](https://docs.python.org/3/library/collections.html#collections.deque) is used to keep track of model utilization.\n\n    Args:\n        model_manager (ModelManager): Instance of a ModelManager.\n        max_size (int, optional): Max number of models at the same time. Defaults to 8.\n    \"\"\"\n    super().__init__(model_manager)\n    self.max_size = max_size\n    self._key_queue = deque(self.model_manager.keys())\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None)</code>","text":"<p>Adds a model to the manager and evicts the least recently used if the cache is full.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def add_model(\n    self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n) -&gt; None:\n    \"\"\"Adds a model to the manager and evicts the least recently used if the cache is full.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n    \"\"\"\n    queue_id = self._resolve_queue_id(\n        model_id=model_id, model_id_alias=model_id_alias\n    )\n    if queue_id in self:\n        logger.debug(\n            f\"Detected {queue_id} in WithFixedSizeCache models queue -&gt; marking as most recently used.\"\n        )\n        self._key_queue.remove(queue_id)\n        self._key_queue.append(queue_id)\n        return None\n\n    logger.debug(f\"Current capacity of ModelManager: {len(self)}/{self.max_size}\")\n    while len(self) &gt;= self.max_size:\n        to_remove_model_id = self._key_queue.popleft()\n        logger.debug(\n            f\"Reached maximum capacity of ModelManager. Unloading model {to_remove_model_id}\"\n        )\n        super().remove(to_remove_model_id)\n        logger.debug(f\"Model {to_remove_model_id} successfully unloaded.\")\n    logger.debug(f\"Marking new model {queue_id} as most recently used.\")\n    self._key_queue.append(queue_id)\n    try:\n        return super().add_model(model_id, api_key, model_id_alias=model_id_alias)\n    except Exception as error:\n        logger.debug(\n            f\"Could not initialise model {queue_id}. Removing from WithFixedSizeCache models queue.\"\n        )\n        self._key_queue.remove(queue_id)\n        raise error\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.clear","title":"<code>clear()</code>","text":"<p>Removes all models from the manager.</p> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Removes all models from the manager.\"\"\"\n    for model_id in list(self.keys()):\n        self.remove(model_id)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Processes a complete inference request and updates the cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Processes a complete inference request and updates the cache.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    self._key_queue.remove(model_id)\n    self._key_queue.append(model_id)\n    return await super().infer_from_request(model_id, request, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.infer_from_request_sync","title":"<code>infer_from_request_sync(model_id, request, **kwargs)</code>","text":"<p>Processes a complete inference request and updates the cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def infer_from_request_sync(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Processes a complete inference request and updates the cache.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    self._key_queue.remove(model_id)\n    self._key_queue.append(model_id)\n    return super().infer_from_request_sync(model_id, request, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.infer_only","title":"<code>infer_only(model_id, request, img_in, img_dims, batch_size=None)</code>","text":"<p>Performs only the inference part of a request and updates the cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <p>The request to process.</p> required <code>img_in</code> <p>Input image.</p> required <code>img_dims</code> <p>Image dimensions.</p> required <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>None</code> <p>Returns:</p> Type Description <p>Response from the inference-only operation.</p> Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def infer_only(self, model_id: str, request, img_in, img_dims, batch_size=None):\n    \"\"\"Performs only the inference part of a request and updates the cache.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request: The request to process.\n        img_in: Input image.\n        img_dims: Image dimensions.\n        batch_size (int, optional): Batch size.\n\n    Returns:\n        Response from the inference-only operation.\n    \"\"\"\n    self._key_queue.remove(model_id)\n    self._key_queue.append(model_id)\n    return super().infer_only(model_id, request, img_in, img_dims, batch_size)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/fixed_size_cache/#inference.core.managers.decorators.fixed_size_cache.WithFixedSizeCache.preprocess","title":"<code>preprocess(model_id, request)</code>","text":"<p>Processes the preprocessing part of a request and updates the cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to preprocess.</p> required Source code in <code>inference/core/managers/decorators/fixed_size_cache.py</code> <pre><code>def preprocess(self, model_id: str, request):\n    \"\"\"Processes the preprocessing part of a request and updates the cache.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to preprocess.\n    \"\"\"\n    self._key_queue.remove(model_id)\n    self._key_queue.append(model_id)\n    return super().preprocess(model_id, request)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/locked_load/","title":"locked_load","text":""},{"location":"docs/reference/inference/core/managers/decorators/locked_load/#inference.core.managers.decorators.locked_load.LockedLoadModelManagerDecorator","title":"<code>LockedLoadModelManagerDecorator</code>","text":"<p>               Bases: <code>ModelManagerDecorator</code></p> <p>Must acquire lock to load model</p> Source code in <code>inference/core/managers/decorators/locked_load.py</code> <pre><code>class LockedLoadModelManagerDecorator(ModelManagerDecorator):\n    \"\"\"Must acquire lock to load model\"\"\"\n\n    def add_model(self, model_id: str, api_key: str, model_id_alias=None):\n        with cache.lock(lock_str(model_id), expire=180.0):\n            return super().add_model(model_id, api_key, model_id_alias=model_id_alias)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/logger/","title":"logger","text":""},{"location":"docs/reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger","title":"<code>WithLogger</code>","text":"<p>               Bases: <code>ModelManagerDecorator</code></p> <p>Logger Decorator, it logs what's going on inside the manager.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>class WithLogger(ModelManagerDecorator):\n    \"\"\"Logger Decorator, it logs what's going on inside the manager.\"\"\"\n\n    def add_model(\n        self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n    ):\n        \"\"\"Adds a model to the manager and logs the action.\n\n        Args:\n            model_id (str): The identifier of the model.\n            model (Model): The model instance.\n\n        Returns:\n            The result of the add_model method from the superclass.\n        \"\"\"\n        logger.info(f\"\ud83e\udd16 {model_id} added.\")\n        return super().add_model(model_id, api_key, model_id_alias=model_id_alias)\n\n    async def infer_from_request(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes a complete inference request and logs both the request and response.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        logger.info(f\"\ud83d\udce5 [{model_id}] request={request}.\")\n        res = await super().infer_from_request(model_id, request, **kwargs)\n        logger.info(f\"\ud83d\udce5 [{model_id}] res={res}.\")\n        return res\n\n    def infer_from_request_sync(\n        self, model_id: str, request: InferenceRequest, **kwargs\n    ) -&gt; InferenceResponse:\n        \"\"\"Processes a complete inference request and logs both the request and response.\n\n        Args:\n            model_id (str): The identifier of the model.\n            request (InferenceRequest): The request to process.\n\n        Returns:\n            InferenceResponse: The response from the inference.\n        \"\"\"\n        logger.info(f\"\ud83d\udce5 [{model_id}] request={request}.\")\n        res = super().infer_from_request_sync(model_id, request, **kwargs)\n        logger.info(f\"\ud83d\udce5 [{model_id}] res={res}.\")\n        return res\n\n    def remove(self, model_id: str) -&gt; Model:\n        \"\"\"Removes a model from the manager and logs the action.\n\n        Args:\n            model_id (str): The identifier of the model to remove.\n\n        Returns:\n            Model: The removed model.\n        \"\"\"\n        res = super().remove(model_id)\n        logger.info(f\"\u274c removed {model_id}\")\n        return res\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.add_model","title":"<code>add_model(model_id, api_key, model_id_alias=None)</code>","text":"<p>Adds a model to the manager and logs the action.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>model</code> <code>Model</code> <p>The model instance.</p> required <p>Returns:</p> Type Description <p>The result of the add_model method from the superclass.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>def add_model(\n    self, model_id: str, api_key: str, model_id_alias: Optional[str] = None\n):\n    \"\"\"Adds a model to the manager and logs the action.\n\n    Args:\n        model_id (str): The identifier of the model.\n        model (Model): The model instance.\n\n    Returns:\n        The result of the add_model method from the superclass.\n    \"\"\"\n    logger.info(f\"\ud83e\udd16 {model_id} added.\")\n    return super().add_model(model_id, api_key, model_id_alias=model_id_alias)\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.infer_from_request","title":"<code>infer_from_request(model_id, request, **kwargs)</code>  <code>async</code>","text":"<p>Processes a complete inference request and logs both the request and response.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>async def infer_from_request(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Processes a complete inference request and logs both the request and response.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    logger.info(f\"\ud83d\udce5 [{model_id}] request={request}.\")\n    res = await super().infer_from_request(model_id, request, **kwargs)\n    logger.info(f\"\ud83d\udce5 [{model_id}] res={res}.\")\n    return res\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.infer_from_request_sync","title":"<code>infer_from_request_sync(model_id, request, **kwargs)</code>","text":"<p>Processes a complete inference request and logs both the request and response.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model.</p> required <code>request</code> <code>InferenceRequest</code> <p>The request to process.</p> required <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>InferenceResponse</code> <p>The response from the inference.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>def infer_from_request_sync(\n    self, model_id: str, request: InferenceRequest, **kwargs\n) -&gt; InferenceResponse:\n    \"\"\"Processes a complete inference request and logs both the request and response.\n\n    Args:\n        model_id (str): The identifier of the model.\n        request (InferenceRequest): The request to process.\n\n    Returns:\n        InferenceResponse: The response from the inference.\n    \"\"\"\n    logger.info(f\"\ud83d\udce5 [{model_id}] request={request}.\")\n    res = super().infer_from_request_sync(model_id, request, **kwargs)\n    logger.info(f\"\ud83d\udce5 [{model_id}] res={res}.\")\n    return res\n</code></pre>"},{"location":"docs/reference/inference/core/managers/decorators/logger/#inference.core.managers.decorators.logger.WithLogger.remove","title":"<code>remove(model_id)</code>","text":"<p>Removes a model from the manager and logs the action.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier of the model to remove.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The removed model.</p> Source code in <code>inference/core/managers/decorators/logger.py</code> <pre><code>def remove(self, model_id: str) -&gt; Model:\n    \"\"\"Removes a model from the manager and logs the action.\n\n    Args:\n        model_id (str): The identifier of the model to remove.\n\n    Returns:\n        Model: The removed model.\n    \"\"\"\n    res = super().remove(model_id)\n    logger.info(f\"\u274c removed {model_id}\")\n    return res\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/","title":"base","text":""},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.BaseInference","title":"<code>BaseInference</code>","text":"<p>General inference class.</p> <p>This class provides a basic interface for inference tasks.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>class BaseInference:\n    \"\"\"General inference class.\n\n    This class provides a basic interface for inference tasks.\n    \"\"\"\n\n    @usage_collector\n    def infer(self, image: Any, **kwargs) -&gt; Any:\n        \"\"\"Runs inference on given data.\n        - image:\n            can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        \"\"\"\n        preproc_image, returned_metadata = self.preprocess(image, **kwargs)\n        logger.debug(\n            f\"Preprocessed input shape: {getattr(preproc_image, 'shape', None)}\"\n        )\n        predicted_arrays = self.predict(preproc_image, **kwargs)\n        postprocessed = self.postprocess(predicted_arrays, returned_metadata, **kwargs)\n\n        return postprocessed\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        raise NotImplementedError\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n        raise NotImplementedError\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Any:\n        raise NotImplementedError\n\n    def infer_from_request(\n        self, request: InferenceRequest\n    ) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n        \"\"\"Runs inference on a request\n\n        Args:\n            request (InferenceRequest): The request object.\n\n        Returns:\n            Union[CVInferenceResponse, List[CVInferenceResponse]]: The response object(s).\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n\n    def make_response(\n        self, *args, **kwargs\n    ) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n        \"\"\"Constructs an object detection response.\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.BaseInference.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Runs inference on given data. - image:     can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>@usage_collector\ndef infer(self, image: Any, **kwargs) -&gt; Any:\n    \"\"\"Runs inference on given data.\n    - image:\n        can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n    \"\"\"\n    preproc_image, returned_metadata = self.preprocess(image, **kwargs)\n    logger.debug(\n        f\"Preprocessed input shape: {getattr(preproc_image, 'shape', None)}\"\n    )\n    predicted_arrays = self.predict(preproc_image, **kwargs)\n    postprocessed = self.postprocess(predicted_arrays, returned_metadata, **kwargs)\n\n    return postprocessed\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.BaseInference.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Runs inference on a request</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InferenceRequest</code> <p>The request object.</p> required <p>Returns:</p> Type Description <code>Union[InferenceResponse, List[InferenceResponse]]</code> <p>Union[CVInferenceResponse, List[CVInferenceResponse]]: The response object(s).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def infer_from_request(\n    self, request: InferenceRequest\n) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n    \"\"\"Runs inference on a request\n\n    Args:\n        request (InferenceRequest): The request object.\n\n    Returns:\n        Union[CVInferenceResponse, List[CVInferenceResponse]]: The response object(s).\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.BaseInference.make_response","title":"<code>make_response(*args, **kwargs)</code>","text":"<p>Constructs an object detection response.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def make_response(\n    self, *args, **kwargs\n) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n    \"\"\"Constructs an object detection response.\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.Model","title":"<code>Model</code>","text":"<p>               Bases: <code>BaseInference</code></p> <p>Base Inference Model (Inherits from BaseInference to define the needed methods)</p> <p>This class provides the foundational methods for inference and logging, and can be extended by specific models.</p> <p>Methods:</p> Name Description <code>log</code> <p>Print the given message.</p> <code>clear_cache</code> <p>Clears any cache if necessary.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>class Model(BaseInference):\n    \"\"\"Base Inference Model (Inherits from BaseInference to define the needed methods)\n\n    This class provides the foundational methods for inference and logging, and can be extended by specific models.\n\n    Methods:\n        log(m): Print the given message.\n        clear_cache(): Clears any cache if necessary.\n    \"\"\"\n\n    def log(self, m):\n        \"\"\"Prints the given message.\n\n        Args:\n            m (str): The message to print.\n        \"\"\"\n        print(m)\n\n    def clear_cache(self):\n        \"\"\"Clears any cache if necessary. This method should be implemented in derived classes as needed.\"\"\"\n        pass\n\n    def infer_from_request(\n        self,\n        request: InferenceRequest,\n    ) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n        \"\"\"\n        Perform inference based on the details provided in the request, and return the associated responses.\n        The function can handle both single and multiple image inference requests. Optionally, it also provides\n        a visualization of the predictions if requested.\n\n        Args:\n            request (InferenceRequest): The request object containing details for inference, such as the image or\n                images to process, any classes to filter by, and whether or not to visualize the predictions.\n\n        Returns:\n            Union[List[InferenceResponse], InferenceResponse]: A list of response objects if the request contains\n            multiple images, or a single response object if the request contains one image. Each response object\n            contains details about the segmented instances, the time taken for inference, and optionally, a visualization.\n\n        Examples:\n            &gt;&gt;&gt; request = InferenceRequest(image=my_image, visualize_predictions=True)\n            &gt;&gt;&gt; response = infer_from_request(request)\n            &gt;&gt;&gt; print(response.time)  # Prints the time taken for inference\n            0.125\n            &gt;&gt;&gt; print(response.visualization)  # Accesses the visualization of the prediction if available\n\n        Notes:\n            - The processing time for each response is included within the response itself.\n            - If `visualize_predictions` is set to True in the request, a visualization of the prediction\n              is also included in the response.\n        \"\"\"\n        t1 = perf_counter()\n        responses = self.infer(**request.dict(), return_image_dims=False)\n        for response in responses:\n            response.time = perf_counter() - t1\n            if request.id:\n                response.inference_id = request.id\n\n        if request.visualize_predictions:\n            for response in responses:\n                response.visualization = self.draw_predictions(request, response)\n\n        if not isinstance(request.image, list) and len(responses) &gt; 0:\n            responses = responses[0]\n\n        return responses\n\n    def make_response(\n        self, *args, **kwargs\n    ) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n        \"\"\"Makes an inference response from the given arguments.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            InferenceResponse: The inference response.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".make_response\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.Model.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clears any cache if necessary. This method should be implemented in derived classes as needed.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clears any cache if necessary. This method should be implemented in derived classes as needed.\"\"\"\n    pass\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.Model.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Perform inference based on the details provided in the request, and return the associated responses. The function can handle both single and multiple image inference requests. Optionally, it also provides a visualization of the predictions if requested.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>InferenceRequest</code> <p>The request object containing details for inference, such as the image or images to process, any classes to filter by, and whether or not to visualize the predictions.</p> required <p>Returns:</p> Type Description <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>Union[List[InferenceResponse], InferenceResponse]: A list of response objects if the request contains</p> <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>multiple images, or a single response object if the request contains one image. Each response object</p> <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>contains details about the segmented instances, the time taken for inference, and optionally, a visualization.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; request = InferenceRequest(image=my_image, visualize_predictions=True)\n&gt;&gt;&gt; response = infer_from_request(request)\n&gt;&gt;&gt; print(response.time)  # Prints the time taken for inference\n0.125\n&gt;&gt;&gt; print(response.visualization)  # Accesses the visualization of the prediction if available\n</code></pre> Notes <ul> <li>The processing time for each response is included within the response itself.</li> <li>If <code>visualize_predictions</code> is set to True in the request, a visualization of the prediction   is also included in the response.</li> </ul> Source code in <code>inference/core/models/base.py</code> <pre><code>def infer_from_request(\n    self,\n    request: InferenceRequest,\n) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n    \"\"\"\n    Perform inference based on the details provided in the request, and return the associated responses.\n    The function can handle both single and multiple image inference requests. Optionally, it also provides\n    a visualization of the predictions if requested.\n\n    Args:\n        request (InferenceRequest): The request object containing details for inference, such as the image or\n            images to process, any classes to filter by, and whether or not to visualize the predictions.\n\n    Returns:\n        Union[List[InferenceResponse], InferenceResponse]: A list of response objects if the request contains\n        multiple images, or a single response object if the request contains one image. Each response object\n        contains details about the segmented instances, the time taken for inference, and optionally, a visualization.\n\n    Examples:\n        &gt;&gt;&gt; request = InferenceRequest(image=my_image, visualize_predictions=True)\n        &gt;&gt;&gt; response = infer_from_request(request)\n        &gt;&gt;&gt; print(response.time)  # Prints the time taken for inference\n        0.125\n        &gt;&gt;&gt; print(response.visualization)  # Accesses the visualization of the prediction if available\n\n    Notes:\n        - The processing time for each response is included within the response itself.\n        - If `visualize_predictions` is set to True in the request, a visualization of the prediction\n          is also included in the response.\n    \"\"\"\n    t1 = perf_counter()\n    responses = self.infer(**request.dict(), return_image_dims=False)\n    for response in responses:\n        response.time = perf_counter() - t1\n        if request.id:\n            response.inference_id = request.id\n\n    if request.visualize_predictions:\n        for response in responses:\n            response.visualization = self.draw_predictions(request, response)\n\n    if not isinstance(request.image, list) and len(responses) &gt; 0:\n        responses = responses[0]\n\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.Model.log","title":"<code>log(m)</code>","text":"<p>Prints the given message.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>str</code> <p>The message to print.</p> required Source code in <code>inference/core/models/base.py</code> <pre><code>def log(self, m):\n    \"\"\"Prints the given message.\n\n    Args:\n        m (str): The message to print.\n    \"\"\"\n    print(m)\n</code></pre>"},{"location":"docs/reference/inference/core/models/base/#inference.core.models.base.Model.make_response","title":"<code>make_response(*args, **kwargs)</code>","text":"<p>Makes an inference response from the given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>InferenceResponse</code> <code>Union[InferenceResponse, List[InferenceResponse]]</code> <p>The inference response.</p> Source code in <code>inference/core/models/base.py</code> <pre><code>def make_response(\n    self, *args, **kwargs\n) -&gt; Union[InferenceResponse, List[InferenceResponse]]:\n    \"\"\"Makes an inference response from the given arguments.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        InferenceResponse: The inference response.\n    \"\"\"\n    raise NotImplementedError(self.__class__.__name__ + \".make_response\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/","title":"classification_base","text":""},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel","title":"<code>ClassificationBaseOnnxRoboflowInferenceModel</code>","text":"<p>               Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Base class for ONNX models for Roboflow classification inference.</p> <p>Attributes:</p> Name Type Description <code>multiclass</code> <code>bool</code> <p>Whether the classification is multi-class or not.</p> <p>Methods:</p> Name Description <code>get_infer_bucket_file_list</code> <p>Get the list of required files for inference.</p> <code>softmax</code> <p>Compute softmax values for a given set of scores.</p> <code>infer</code> <p>ClassificationInferenceRequest) -&gt; Union[List[Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]], Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]]: Perform inference on a given request and return the response.</p> <code>draw_predictions</code> <p>Draw prediction visuals on an image.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>class ClassificationBaseOnnxRoboflowInferenceModel(OnnxRoboflowInferenceModel):\n    \"\"\"Base class for ONNX models for Roboflow classification inference.\n\n    Attributes:\n        multiclass (bool): Whether the classification is multi-class or not.\n\n    Methods:\n        get_infer_bucket_file_list() -&gt; list: Get the list of required files for inference.\n        softmax(x): Compute softmax values for a given set of scores.\n        infer(request: ClassificationInferenceRequest) -&gt; Union[List[Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]], Union[ClassificationInferenceResponse, MultiLabelClassificationInferenceResponse]]: Perform inference on a given request and return the response.\n        draw_predictions(inference_request, inference_response): Draw prediction visuals on an image.\n    \"\"\"\n\n    task_type = \"classification\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize the model, setting whether it is multiclass or not.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    def draw_predictions(self, inference_request, inference_response):\n        \"\"\"Draw prediction visuals on an image.\n\n        This method overlays the predictions on the input image, including drawing rectangles and text to visualize the predicted classes.\n\n        Args:\n            inference_request: The request object containing the image and parameters.\n            inference_response: The response object containing the predictions and other details.\n\n        Returns:\n            bytes: The bytes of the visualized image in JPEG format.\n        \"\"\"\n        image = load_image_rgb(inference_request.image)\n        image = Image.fromarray(image)\n        draw = ImageDraw.Draw(image)\n        font = ImageFont.load_default()\n        if isinstance(inference_response.predictions, list):\n            prediction = inference_response.predictions[0]\n            color = self.colors.get(prediction.class_name, \"#4892EA\")\n            draw.rectangle(\n                [0, 0, image.size[1], image.size[0]],\n                outline=color,\n                width=inference_request.visualization_stroke_width,\n            )\n            text = f\"{prediction.class_id} - {prediction.class_name} {prediction.confidence:.2f}\"\n            text_size = font.getbbox(text)\n\n            # set button size + 10px margins\n            button_size = (text_size[2] + 20, text_size[3] + 20)\n            button_img = Image.new(\"RGBA\", button_size, color)\n            # put text on button with 10px margins\n            button_draw = ImageDraw.Draw(button_img)\n            button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n            # put button on source image in position (0, 0)\n            image.paste(button_img, (0, 0))\n        else:\n            if len(inference_response.predictions) &gt; 0:\n                box_color = \"#4892EA\"\n                draw.rectangle(\n                    [0, 0, image.size[1], image.size[0]],\n                    outline=box_color,\n                    width=inference_request.visualization_stroke_width,\n                )\n            row = 0\n            predictions = [\n                (cls_name, pred)\n                for cls_name, pred in inference_response.predictions.items()\n            ]\n            predictions = sorted(\n                predictions, key=lambda x: x[1].confidence, reverse=True\n            )\n            for i, (cls_name, pred) in enumerate(predictions):\n                color = self.colors.get(cls_name, \"#4892EA\")\n                text = f\"{cls_name} {pred.confidence:.2f}\"\n                text_size = font.getbbox(text)\n\n                # set button size + 10px margins\n                button_size = (text_size[2] + 20, text_size[3] + 20)\n                button_img = Image.new(\"RGBA\", button_size, color)\n                # put text on button with 10px margins\n                button_draw = ImageDraw.Draw(button_img)\n                button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n                # put button on source image in position (0, 0)\n                image.paste(button_img, (0, row))\n                row += button_size[1]\n\n        buffered = BytesIO()\n        image = image.convert(\"RGB\")\n        image.save(buffered, format=\"JPEG\")\n        return buffered.getvalue()\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"environment.json\"].\n        \"\"\"\n        return [\"environment.json\"]\n\n    def infer(\n        self,\n        image: Any,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        return_image_dims: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Perform inference on the provided image(s) and return the predictions.\n\n        Args:\n            image (Any): The image or list of images to be processed.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n            return_image_dims (bool, optional): If set to True, the function will also return the dimensions of the image. Defaults to False.\n            **kwargs: Additional parameters to customize the inference process.\n\n        Returns:\n            Union[List[np.array], np.array, Tuple[List[np.array], List[Tuple[int, int]]], Tuple[np.array, Tuple[int, int]]]:\n            If `return_image_dims` is True and a list of images is provided, a tuple containing a list of prediction arrays and a list of image dimensions (width, height) is returned.\n            If `return_image_dims` is True and a single image is provided, a tuple containing the prediction array and image dimensions (width, height) is returned.\n            If `return_image_dims` is False and a list of images is provided, only the list of prediction arrays is returned.\n            If `return_image_dims` is False and a single image is provided, only the prediction array is returned.\n\n        Notes:\n            - The input image(s) will be preprocessed (normalized and reshaped) before inference.\n            - This function uses an ONNX session to perform inference on the input image(s).\n        \"\"\"\n        return super().infer(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n            return_image_dims=return_image_dims,\n        )\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        return_image_dims=False,\n        **kwargs,\n    ) -&gt; Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]:\n        predictions = predictions[0]\n        return self.make_response(\n            predictions, preprocess_return_metadata[\"img_dims\"], **kwargs\n        )\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        return (predictions,)\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        if isinstance(image, list):\n            imgs_with_dims = [\n                self.preproc_image(\n                    i,\n                    disable_preproc_auto_orient=kwargs.get(\n                        \"disable_preproc_auto_orient\", False\n                    ),\n                    disable_preproc_contrast=kwargs.get(\n                        \"disable_preproc_contrast\", False\n                    ),\n                    disable_preproc_grayscale=kwargs.get(\n                        \"disable_preproc_grayscale\", False\n                    ),\n                    disable_preproc_static_crop=kwargs.get(\n                        \"disable_preproc_static_crop\", False\n                    ),\n                )\n                for i in image\n            ]\n            imgs, img_dims = zip(*imgs_with_dims)\n            img_in = np.concatenate(imgs, axis=0)\n        else:\n            img_in, img_dims = self.preproc_image(\n                image,\n                disable_preproc_auto_orient=kwargs.get(\n                    \"disable_preproc_auto_orient\", False\n                ),\n                disable_preproc_contrast=kwargs.get(\"disable_preproc_contrast\", False),\n                disable_preproc_grayscale=kwargs.get(\n                    \"disable_preproc_grayscale\", False\n                ),\n                disable_preproc_static_crop=kwargs.get(\n                    \"disable_preproc_static_crop\", False\n                ),\n            )\n            img_dims = [img_dims]\n\n        img_in /= 255.0\n\n        mean = (0.5, 0.5, 0.5)\n        std = (0.5, 0.5, 0.5)\n\n        img_in = img_in.astype(np.float32)\n\n        img_in[:, 0, :, :] = (img_in[:, 0, :, :] - mean[0]) / std[0]\n        img_in[:, 1, :, :] = (img_in[:, 1, :, :] - mean[1]) / std[1]\n        img_in[:, 2, :, :] = (img_in[:, 2, :, :] - mean[2]) / std[2]\n        return img_in, PreprocessReturnMetadata({\"img_dims\": img_dims})\n\n    def infer_from_request(\n        self,\n        request: ClassificationInferenceRequest,\n    ) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n        \"\"\"\n        Handle an inference request to produce an appropriate response.\n\n        Args:\n            request (ClassificationInferenceRequest): The request object encapsulating the image(s) and relevant parameters.\n\n        Returns:\n            Union[List[InferenceResponse], InferenceResponse]: The response object(s) containing the predictions, visualization, and other pertinent details. If a list of images was provided, a list of responses is returned. Otherwise, a single response is returned.\n\n        Notes:\n            - Starts a timer at the beginning to calculate inference time.\n            - Processes the image(s) through the `infer` method.\n            - Generates the appropriate response object(s) using `make_response`.\n            - Calculates and sets the time taken for inference.\n            - If visualization is requested, the predictions are drawn on the image.\n        \"\"\"\n        t1 = perf_counter()\n        responses = self.infer(**request.dict(), return_image_dims=True)\n        for response in responses:\n            response.time = perf_counter() - t1\n            response.inference_id = getattr(request, \"id\", None)\n\n        if request.visualize_predictions:\n            for response in responses:\n                response.visualization = self.draw_predictions(request, response)\n\n        if not isinstance(request.image, list):\n            responses = responses[0]\n\n        return responses\n\n    def make_response(\n        self,\n        predictions,\n        img_dims,\n        confidence: float = 0.5,\n        **kwargs,\n    ) -&gt; Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]:\n        \"\"\"\n        Create response objects for the given predictions and image dimensions.\n\n        Args:\n            predictions (list): List of prediction arrays from the inference process.\n            img_dims (list): List of tuples indicating the dimensions (width, height) of each image.\n            confidence (float, optional): Confidence threshold for filtering predictions. Defaults to 0.5.\n            **kwargs: Additional parameters to influence the response creation process.\n\n        Returns:\n            Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]: A response object or a list of response objects encapsulating the prediction details.\n\n        Notes:\n            - If the model is multiclass, a `MultiLabelClassificationInferenceResponse` is generated for each image.\n            - If the model is not multiclass, a `ClassificationInferenceResponse` is generated for each image.\n            - Predictions below the confidence threshold are filtered out.\n        \"\"\"\n        responses = []\n        confidence_threshold = float(confidence)\n        for ind, prediction in enumerate(predictions):\n            if self.multiclass:\n                preds = prediction[0]\n                results = dict()\n                predicted_classes = []\n                for i, o in enumerate(preds):\n                    cls_name = self.class_names[i]\n                    score = float(o)\n                    results[cls_name] = {\"confidence\": score, \"class_id\": i}\n                    if score &gt; confidence_threshold:\n                        predicted_classes.append(cls_name)\n                response = MultiLabelClassificationInferenceResponse(\n                    image=InferenceResponseImage(\n                        width=img_dims[ind][0], height=img_dims[ind][1]\n                    ),\n                    predicted_classes=predicted_classes,\n                    predictions=results,\n                )\n            else:\n                preds = prediction[0]\n                preds = self.softmax(preds)\n                results = []\n                for i, cls_name in enumerate(self.class_names):\n                    score = float(preds[i])\n                    pred = {\n                        \"class_id\": i,\n                        \"class\": cls_name,\n                        \"confidence\": round(score, 4),\n                    }\n                    results.append(pred)\n                results = sorted(results, key=lambda x: x[\"confidence\"], reverse=True)\n\n                response = ClassificationInferenceResponse(\n                    image=InferenceResponseImage(\n                        width=img_dims[ind][1], height=img_dims[ind][0]\n                    ),\n                    predictions=results,\n                    top=results[0][\"class\"],\n                    confidence=results[0][\"confidence\"],\n                )\n            responses.append(response)\n\n        return responses\n\n    @staticmethod\n    def softmax(x):\n        \"\"\"Compute softmax values for each set of scores in x.\n\n        Args:\n            x (np.array): The input array containing the scores.\n\n        Returns:\n            np.array: The softmax values for each set of scores.\n        \"\"\"\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()\n\n    def get_model_output_shape(self) -&gt; Tuple[int, int, int]:\n        test_image = (np.random.rand(1024, 1024, 3) * 255).astype(np.uint8)\n        test_image, _ = self.preprocess(test_image)\n        output = np.array(self.predict(test_image))\n        return output.shape\n\n    def validate_model_classes(self) -&gt; None:\n        output_shape = self.get_model_output_shape()\n        num_classes = output_shape[3]\n        try:\n            assert num_classes == self.num_classes\n        except AssertionError:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the model, setting whether it is multiclass or not.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initialize the model, setting whether it is multiclass or not.\"\"\"\n    super().__init__(*args, **kwargs)\n    self.multiclass = self.environment.get(\"MULTICLASS\", False)\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.draw_predictions","title":"<code>draw_predictions(inference_request, inference_response)</code>","text":"<p>Draw prediction visuals on an image.</p> <p>This method overlays the predictions on the input image, including drawing rectangles and text to visualize the predicted classes.</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <p>The request object containing the image and parameters.</p> required <code>inference_response</code> <p>The response object containing the predictions and other details.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <p>The bytes of the visualized image in JPEG format.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def draw_predictions(self, inference_request, inference_response):\n    \"\"\"Draw prediction visuals on an image.\n\n    This method overlays the predictions on the input image, including drawing rectangles and text to visualize the predicted classes.\n\n    Args:\n        inference_request: The request object containing the image and parameters.\n        inference_response: The response object containing the predictions and other details.\n\n    Returns:\n        bytes: The bytes of the visualized image in JPEG format.\n    \"\"\"\n    image = load_image_rgb(inference_request.image)\n    image = Image.fromarray(image)\n    draw = ImageDraw.Draw(image)\n    font = ImageFont.load_default()\n    if isinstance(inference_response.predictions, list):\n        prediction = inference_response.predictions[0]\n        color = self.colors.get(prediction.class_name, \"#4892EA\")\n        draw.rectangle(\n            [0, 0, image.size[1], image.size[0]],\n            outline=color,\n            width=inference_request.visualization_stroke_width,\n        )\n        text = f\"{prediction.class_id} - {prediction.class_name} {prediction.confidence:.2f}\"\n        text_size = font.getbbox(text)\n\n        # set button size + 10px margins\n        button_size = (text_size[2] + 20, text_size[3] + 20)\n        button_img = Image.new(\"RGBA\", button_size, color)\n        # put text on button with 10px margins\n        button_draw = ImageDraw.Draw(button_img)\n        button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n        # put button on source image in position (0, 0)\n        image.paste(button_img, (0, 0))\n    else:\n        if len(inference_response.predictions) &gt; 0:\n            box_color = \"#4892EA\"\n            draw.rectangle(\n                [0, 0, image.size[1], image.size[0]],\n                outline=box_color,\n                width=inference_request.visualization_stroke_width,\n            )\n        row = 0\n        predictions = [\n            (cls_name, pred)\n            for cls_name, pred in inference_response.predictions.items()\n        ]\n        predictions = sorted(\n            predictions, key=lambda x: x[1].confidence, reverse=True\n        )\n        for i, (cls_name, pred) in enumerate(predictions):\n            color = self.colors.get(cls_name, \"#4892EA\")\n            text = f\"{cls_name} {pred.confidence:.2f}\"\n            text_size = font.getbbox(text)\n\n            # set button size + 10px margins\n            button_size = (text_size[2] + 20, text_size[3] + 20)\n            button_img = Image.new(\"RGBA\", button_size, color)\n            # put text on button with 10px margins\n            button_draw = ImageDraw.Draw(button_img)\n            button_draw.text((10, 10), text, font=font, fill=(255, 255, 255, 255))\n\n            # put button on source image in position (0, 0)\n            image.paste(button_img, (0, row))\n            row += button_size[1]\n\n    buffered = BytesIO()\n    image = image.convert(\"RGB\")\n    image.save(buffered, format=\"JPEG\")\n    return buffered.getvalue()\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"environment.json\"].</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"environment.json\"].\n    \"\"\"\n    return [\"environment.json\"]\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.infer","title":"<code>infer(image, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, return_image_dims=False, **kwargs)</code>","text":"<p>Perform inference on the provided image(s) and return the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image or list of images to be processed. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>return_image_dims</code> <code>bool</code> <p>If set to True, the function will also return the dimensions of the image. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters to customize the inference process.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Union[List[np.array], np.array, Tuple[List[np.array], List[Tuple[int, int]]], Tuple[np.array, Tuple[int, int]]]:</p> <p>If <code>return_image_dims</code> is True and a list of images is provided, a tuple containing a list of prediction arrays and a list of image dimensions (width, height) is returned.</p> <p>If <code>return_image_dims</code> is True and a single image is provided, a tuple containing the prediction array and image dimensions (width, height) is returned.</p> <p>If <code>return_image_dims</code> is False and a list of images is provided, only the list of prediction arrays is returned.</p> <p>If <code>return_image_dims</code> is False and a single image is provided, only the prediction array is returned.</p> Notes <ul> <li>The input image(s) will be preprocessed (normalized and reshaped) before inference.</li> <li>This function uses an ONNX session to perform inference on the input image(s).</li> </ul> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    return_image_dims: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Perform inference on the provided image(s) and return the predictions.\n\n    Args:\n        image (Any): The image or list of images to be processed.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        return_image_dims (bool, optional): If set to True, the function will also return the dimensions of the image. Defaults to False.\n        **kwargs: Additional parameters to customize the inference process.\n\n    Returns:\n        Union[List[np.array], np.array, Tuple[List[np.array], List[Tuple[int, int]]], Tuple[np.array, Tuple[int, int]]]:\n        If `return_image_dims` is True and a list of images is provided, a tuple containing a list of prediction arrays and a list of image dimensions (width, height) is returned.\n        If `return_image_dims` is True and a single image is provided, a tuple containing the prediction array and image dimensions (width, height) is returned.\n        If `return_image_dims` is False and a list of images is provided, only the list of prediction arrays is returned.\n        If `return_image_dims` is False and a single image is provided, only the prediction array is returned.\n\n    Notes:\n        - The input image(s) will be preprocessed (normalized and reshaped) before inference.\n        - This function uses an ONNX session to perform inference on the input image(s).\n    \"\"\"\n    return super().infer(\n        image,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n        return_image_dims=return_image_dims,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Handle an inference request to produce an appropriate response.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ClassificationInferenceRequest</code> <p>The request object encapsulating the image(s) and relevant parameters.</p> required <p>Returns:</p> Type Description <code>Union[List[InferenceResponse], InferenceResponse]</code> <p>Union[List[InferenceResponse], InferenceResponse]: The response object(s) containing the predictions, visualization, and other pertinent details. If a list of images was provided, a list of responses is returned. Otherwise, a single response is returned.</p> Notes <ul> <li>Starts a timer at the beginning to calculate inference time.</li> <li>Processes the image(s) through the <code>infer</code> method.</li> <li>Generates the appropriate response object(s) using <code>make_response</code>.</li> <li>Calculates and sets the time taken for inference.</li> <li>If visualization is requested, the predictions are drawn on the image.</li> </ul> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def infer_from_request(\n    self,\n    request: ClassificationInferenceRequest,\n) -&gt; Union[List[InferenceResponse], InferenceResponse]:\n    \"\"\"\n    Handle an inference request to produce an appropriate response.\n\n    Args:\n        request (ClassificationInferenceRequest): The request object encapsulating the image(s) and relevant parameters.\n\n    Returns:\n        Union[List[InferenceResponse], InferenceResponse]: The response object(s) containing the predictions, visualization, and other pertinent details. If a list of images was provided, a list of responses is returned. Otherwise, a single response is returned.\n\n    Notes:\n        - Starts a timer at the beginning to calculate inference time.\n        - Processes the image(s) through the `infer` method.\n        - Generates the appropriate response object(s) using `make_response`.\n        - Calculates and sets the time taken for inference.\n        - If visualization is requested, the predictions are drawn on the image.\n    \"\"\"\n    t1 = perf_counter()\n    responses = self.infer(**request.dict(), return_image_dims=True)\n    for response in responses:\n        response.time = perf_counter() - t1\n        response.inference_id = getattr(request, \"id\", None)\n\n    if request.visualize_predictions:\n        for response in responses:\n            response.visualization = self.draw_predictions(request, response)\n\n    if not isinstance(request.image, list):\n        responses = responses[0]\n\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, img_dims, confidence=0.5, **kwargs)</code>","text":"<p>Create response objects for the given predictions and image dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>list</code> <p>List of prediction arrays from the inference process.</p> required <code>img_dims</code> <code>list</code> <p>List of tuples indicating the dimensions (width, height) of each image.</p> required <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering predictions. Defaults to 0.5.</p> <code>0.5</code> <code>**kwargs</code> <p>Additional parameters to influence the response creation process.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]</code> <p>Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]: A response object or a list of response objects encapsulating the prediction details.</p> Notes <ul> <li>If the model is multiclass, a <code>MultiLabelClassificationInferenceResponse</code> is generated for each image.</li> <li>If the model is not multiclass, a <code>ClassificationInferenceResponse</code> is generated for each image.</li> <li>Predictions below the confidence threshold are filtered out.</li> </ul> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>def make_response(\n    self,\n    predictions,\n    img_dims,\n    confidence: float = 0.5,\n    **kwargs,\n) -&gt; Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]:\n    \"\"\"\n    Create response objects for the given predictions and image dimensions.\n\n    Args:\n        predictions (list): List of prediction arrays from the inference process.\n        img_dims (list): List of tuples indicating the dimensions (width, height) of each image.\n        confidence (float, optional): Confidence threshold for filtering predictions. Defaults to 0.5.\n        **kwargs: Additional parameters to influence the response creation process.\n\n    Returns:\n        Union[ClassificationInferenceResponse, List[ClassificationInferenceResponse]]: A response object or a list of response objects encapsulating the prediction details.\n\n    Notes:\n        - If the model is multiclass, a `MultiLabelClassificationInferenceResponse` is generated for each image.\n        - If the model is not multiclass, a `ClassificationInferenceResponse` is generated for each image.\n        - Predictions below the confidence threshold are filtered out.\n    \"\"\"\n    responses = []\n    confidence_threshold = float(confidence)\n    for ind, prediction in enumerate(predictions):\n        if self.multiclass:\n            preds = prediction[0]\n            results = dict()\n            predicted_classes = []\n            for i, o in enumerate(preds):\n                cls_name = self.class_names[i]\n                score = float(o)\n                results[cls_name] = {\"confidence\": score, \"class_id\": i}\n                if score &gt; confidence_threshold:\n                    predicted_classes.append(cls_name)\n            response = MultiLabelClassificationInferenceResponse(\n                image=InferenceResponseImage(\n                    width=img_dims[ind][0], height=img_dims[ind][1]\n                ),\n                predicted_classes=predicted_classes,\n                predictions=results,\n            )\n        else:\n            preds = prediction[0]\n            preds = self.softmax(preds)\n            results = []\n            for i, cls_name in enumerate(self.class_names):\n                score = float(preds[i])\n                pred = {\n                    \"class_id\": i,\n                    \"class\": cls_name,\n                    \"confidence\": round(score, 4),\n                }\n                results.append(pred)\n            results = sorted(results, key=lambda x: x[\"confidence\"], reverse=True)\n\n            response = ClassificationInferenceResponse(\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n                predictions=results,\n                top=results[0][\"class\"],\n                confidence=results[0][\"confidence\"],\n            )\n        responses.append(response)\n\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/classification_base/#inference.core.models.classification_base.ClassificationBaseOnnxRoboflowInferenceModel.softmax","title":"<code>softmax(x)</code>  <code>staticmethod</code>","text":"<p>Compute softmax values for each set of scores in x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>The input array containing the scores.</p> required <p>Returns:</p> Type Description <p>np.array: The softmax values for each set of scores.</p> Source code in <code>inference/core/models/classification_base.py</code> <pre><code>@staticmethod\ndef softmax(x):\n    \"\"\"Compute softmax values for each set of scores in x.\n\n    Args:\n        x (np.array): The input array containing the scores.\n\n    Returns:\n        np.array: The softmax values for each set of scores.\n    \"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n</code></pre>"},{"location":"docs/reference/inference/core/models/defaults/","title":"defaults","text":""},{"location":"docs/reference/inference/core/models/instance_segmentation_base/","title":"instance_segmentation_base","text":""},{"location":"docs/reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel","title":"<code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code>","text":"<p>               Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Instance Segmentation model.</p> <p>This class implements an instance segmentation specific inference method for ONNX models provided by Roboflow.</p> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>class InstanceSegmentationBaseOnnxRoboflowInferenceModel(OnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Instance Segmentation model.\n\n    This class implements an instance segmentation specific inference method\n    for ONNX models provided by Roboflow.\n    \"\"\"\n\n    task_type = \"instance-segmentation\"\n    num_masks = 32\n\n    def infer(\n        self,\n        image: Any,\n        class_agnostic_nms: bool = False,\n        confidence: float = DEFAULT_CONFIDENCE,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        mask_decode_mode: str = DEFAULT_MASK_DECODE_MODE,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        tradeoff_factor: float = DEFAULT_TRADEOFF_FACTOR,\n        **kwargs,\n    ) -&gt; Union[PREDICTIONS_TYPE, Tuple[PREDICTIONS_TYPE, List[Tuple[int, int]]]]:\n        \"\"\"\n        Process an image or list of images for instance segmentation.\n\n        Args:\n            image (Any): An image or a list of images for processing.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n            confidence (float, optional): Confidence threshold for predictions. Defaults to 0.5.\n            iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.5.\n            mask_decode_mode (str, optional): Decoding mode for masks. Choices are \"accurate\", \"tradeoff\", and \"fast\". Defaults to \"accurate\".\n            max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n            max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n            return_image_dims (bool, optional): Whether to return the dimensions of the processed images. Defaults to False.\n            tradeoff_factor (float, optional): Tradeoff factor used when `mask_decode_mode` is set to \"tradeoff\". Must be in [0.0, 1.0]. Defaults to 0.5.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n            **kwargs: Additional parameters to customize the inference process.\n\n        Returns:\n            Union[List[List[List[float]]], Tuple[List[List[List[float]]], List[Tuple[int, int]]]]: The list of predictions, with each prediction being a list of lists. Optionally, also returns the dimensions of the processed images.\n\n        Raises:\n            InvalidMaskDecodeArgument: If an invalid `mask_decode_mode` is provided or if the `tradeoff_factor` is outside the allowed range.\n\n        Notes:\n            - Processes input images and normalizes them.\n            - Makes predictions using the ONNX runtime.\n            - Applies non-maximum suppression to the predictions.\n            - Decodes the masks according to the specified mode.\n        \"\"\"\n        return super().infer(\n            image,\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n            iou_threshold=iou_threshold,\n            mask_decode_mode=mask_decode_mode,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            return_image_dims=return_image_dims,\n            tradeoff_factor=tradeoff_factor,\n        )\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Union[\n        InstanceSegmentationInferenceResponse,\n        List[InstanceSegmentationInferenceResponse],\n    ]:\n        predictions, protos = predictions\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=kwargs[\"confidence\"],\n            iou_thresh=kwargs[\"iou_threshold\"],\n            class_agnostic=kwargs[\"class_agnostic_nms\"],\n            max_detections=kwargs[\"max_detections\"],\n            max_candidate_detections=kwargs[\"max_candidates\"],\n            num_masks=self.num_masks,\n        )\n        infer_shape = (self.img_size_h, self.img_size_w)\n        masks = []\n        mask_decode_mode = kwargs[\"mask_decode_mode\"]\n        tradeoff_factor = kwargs[\"tradeoff_factor\"]\n        img_in_shape = preprocess_return_metadata[\"im_shape\"]\n\n        predictions = [np.array(p) for p in predictions]\n\n        for pred, proto, img_dim in zip(\n            predictions, protos, preprocess_return_metadata[\"img_dims\"]\n        ):\n            if pred.size == 0:\n                masks.append([])\n                continue\n            if mask_decode_mode == \"accurate\":\n                batch_masks = process_mask_accurate(\n                    proto, pred[:, 7:], pred[:, :4], img_in_shape[2:]\n                )\n                output_mask_shape = img_in_shape[2:]\n            elif mask_decode_mode == \"tradeoff\":\n                if not 0 &lt;= tradeoff_factor &lt;= 1:\n                    raise InvalidMaskDecodeArgument(\n                        f\"Invalid tradeoff_factor: {tradeoff_factor}. Must be in [0.0, 1.0]\"\n                    )\n                batch_masks = process_mask_tradeoff(\n                    proto,\n                    pred[:, 7:],\n                    pred[:, :4],\n                    img_in_shape[2:],\n                    tradeoff_factor,\n                )\n                output_mask_shape = batch_masks.shape[1:]\n            elif mask_decode_mode == \"fast\":\n                batch_masks = process_mask_fast(\n                    proto, pred[:, 7:], pred[:, :4], img_in_shape[2:]\n                )\n                output_mask_shape = batch_masks.shape[1:]\n            else:\n                raise InvalidMaskDecodeArgument(\n                    f\"Invalid mask_decode_mode: {mask_decode_mode}. Must be one of ['accurate', 'fast', 'tradeoff']\"\n                )\n            polys = masks2poly(batch_masks)\n            pred[:, :4] = post_process_bboxes(\n                [pred[:, :4]],\n                infer_shape,\n                [img_dim],\n                self.preproc,\n                resize_method=self.resize_method,\n                disable_preproc_static_crop=preprocess_return_metadata[\n                    \"disable_preproc_static_crop\"\n                ],\n            )[0]\n            polys = post_process_polygons(\n                img_dim,\n                polys,\n                output_mask_shape,\n                self.preproc,\n                resize_method=self.resize_method,\n            )\n            masks.append(polys)\n        return self.make_response(\n            predictions, masks, preprocess_return_metadata[\"img_dims\"], **kwargs\n        )\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        img_in, img_dims = self.load_image(\n            image,\n            disable_preproc_auto_orient=kwargs.get(\"disable_preproc_auto_orient\"),\n            disable_preproc_contrast=kwargs.get(\"disable_preproc_contrast\"),\n            disable_preproc_grayscale=kwargs.get(\"disable_preproc_grayscale\"),\n            disable_preproc_static_crop=kwargs.get(\"disable_preproc_static_crop\"),\n        )\n\n        img_in /= 255.0\n        return img_in, PreprocessReturnMetadata(\n            {\n                \"img_dims\": img_dims,\n                \"im_shape\": img_in.shape,\n                \"disable_preproc_static_crop\": kwargs.get(\n                    \"disable_preproc_static_crop\"\n                ),\n            }\n        )\n\n    def make_response(\n        self,\n        predictions: List[List[List[float]]],\n        masks: List[List[List[float]]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: List[str] = [],\n        **kwargs,\n    ) -&gt; Union[\n        InstanceSegmentationInferenceResponse,\n        List[InstanceSegmentationInferenceResponse],\n    ]:\n        \"\"\"\n        Create instance segmentation inference response objects for the provided predictions and masks.\n\n        Args:\n            predictions (List[List[List[float]]]): List of prediction data, one for each image.\n            masks (List[List[List[float]]]): List of masks corresponding to the predictions.\n            img_dims (List[Tuple[int, int]]): List of image dimensions corresponding to the processed images.\n            class_filter (List[str], optional): List of class names to filter predictions by. Defaults to an empty list (no filtering).\n\n        Returns:\n            Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]: A single instance segmentation response or a list of instance segmentation responses based on the number of processed images.\n\n        Notes:\n            - For each image, constructs an `InstanceSegmentationInferenceResponse` object.\n            - Each response contains a list of `InstanceSegmentationPrediction` objects.\n        \"\"\"\n        responses = []\n        for ind, (batch_predictions, batch_masks) in enumerate(zip(predictions, masks)):\n            predictions = []\n            for pred, mask in zip(batch_predictions, batch_masks):\n                if class_filter and self.class_names[int(pred[6])] in class_filter:\n                    # TODO: logger.debug\n                    continue\n                # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                predictions.append(\n                    InstanceSegmentationPrediction(\n                        **{\n                            \"x\": pred[0] + (pred[2] - pred[0]) / 2,\n                            \"y\": pred[1] + (pred[3] - pred[1]) / 2,\n                            \"width\": pred[2] - pred[0],\n                            \"height\": pred[3] - pred[1],\n                            \"points\": [Point(x=point[0], y=point[1]) for point in mask],\n                            \"confidence\": pred[4],\n                            \"class\": self.class_names[int(pred[6])],\n                            \"class_id\": int(pred[6]),\n                        }\n                    )\n                )\n            response = InstanceSegmentationInferenceResponse(\n                predictions=predictions,\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n            )\n            responses.append(response)\n        return responses\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Runs inference on the ONNX model.\n\n        Args:\n            img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: The ONNX model predictions and the ONNX model protos.\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError(\"predict must be implemented by a subclass\")\n\n    def validate_model_classes(self) -&gt; None:\n        output_shape = self.get_model_output_shape()\n        num_classes = get_num_classes_from_model_prediction_shape(\n            output_shape[2], masks=self.num_masks\n        )\n        try:\n            assert num_classes == self.num_classes\n        except AssertionError:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel.infer","title":"<code>infer(image, class_agnostic_nms=False, confidence=DEFAULT_CONFIDENCE, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, iou_threshold=DEFAULT_IOU_THRESH, mask_decode_mode=DEFAULT_MASK_DECODE_MODE, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, tradeoff_factor=DEFAULT_TRADEOFF_FACTOR, **kwargs)</code>","text":"<p>Process an image or list of images for instance segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>An image or a list of images for processing. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to use class-agnostic non-maximum suppression. Defaults to False.</p> <code>False</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-maximum suppression. Defaults to 0.5.</p> <code>DEFAULT_IOU_THRESH</code> <code>mask_decode_mode</code> <code>str</code> <p>Decoding mode for masks. Choices are \"accurate\", \"tradeoff\", and \"fast\". Defaults to \"accurate\".</p> <code>DEFAULT_MASK_DECODE_MODE</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Defaults to 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections after non-maximum suppression. Defaults to 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <code>return_image_dims</code> <code>bool</code> <p>Whether to return the dimensions of the processed images. Defaults to False.</p> <code>False</code> <code>tradeoff_factor</code> <code>float</code> <p>Tradeoff factor used when <code>mask_decode_mode</code> is set to \"tradeoff\". Must be in [0.0, 1.0]. Defaults to 0.5.</p> <code>DEFAULT_TRADEOFF_FACTOR</code> <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters to customize the inference process.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[PREDICTIONS_TYPE, Tuple[PREDICTIONS_TYPE, List[Tuple[int, int]]]]</code> <p>Union[List[List[List[float]]], Tuple[List[List[List[float]]], List[Tuple[int, int]]]]: The list of predictions, with each prediction being a list of lists. Optionally, also returns the dimensions of the processed images.</p> <p>Raises:</p> Type Description <code>InvalidMaskDecodeArgument</code> <p>If an invalid <code>mask_decode_mode</code> is provided or if the <code>tradeoff_factor</code> is outside the allowed range.</p> Notes <ul> <li>Processes input images and normalizes them.</li> <li>Makes predictions using the ONNX runtime.</li> <li>Applies non-maximum suppression to the predictions.</li> <li>Decodes the masks according to the specified mode.</li> </ul> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    class_agnostic_nms: bool = False,\n    confidence: float = DEFAULT_CONFIDENCE,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    mask_decode_mode: str = DEFAULT_MASK_DECODE_MODE,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    tradeoff_factor: float = DEFAULT_TRADEOFF_FACTOR,\n    **kwargs,\n) -&gt; Union[PREDICTIONS_TYPE, Tuple[PREDICTIONS_TYPE, List[Tuple[int, int]]]]:\n    \"\"\"\n    Process an image or list of images for instance segmentation.\n\n    Args:\n        image (Any): An image or a list of images for processing.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n        confidence (float, optional): Confidence threshold for predictions. Defaults to 0.5.\n        iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.5.\n        mask_decode_mode (str, optional): Decoding mode for masks. Choices are \"accurate\", \"tradeoff\", and \"fast\". Defaults to \"accurate\".\n        max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n        max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n        return_image_dims (bool, optional): Whether to return the dimensions of the processed images. Defaults to False.\n        tradeoff_factor (float, optional): Tradeoff factor used when `mask_decode_mode` is set to \"tradeoff\". Must be in [0.0, 1.0]. Defaults to 0.5.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        **kwargs: Additional parameters to customize the inference process.\n\n    Returns:\n        Union[List[List[List[float]]], Tuple[List[List[List[float]]], List[Tuple[int, int]]]]: The list of predictions, with each prediction being a list of lists. Optionally, also returns the dimensions of the processed images.\n\n    Raises:\n        InvalidMaskDecodeArgument: If an invalid `mask_decode_mode` is provided or if the `tradeoff_factor` is outside the allowed range.\n\n    Notes:\n        - Processes input images and normalizes them.\n        - Makes predictions using the ONNX runtime.\n        - Applies non-maximum suppression to the predictions.\n        - Decodes the masks according to the specified mode.\n    \"\"\"\n    return super().infer(\n        image,\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n        iou_threshold=iou_threshold,\n        mask_decode_mode=mask_decode_mode,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        return_image_dims=return_image_dims,\n        tradeoff_factor=tradeoff_factor,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, masks, img_dims, class_filter=[], **kwargs)</code>","text":"<p>Create instance segmentation inference response objects for the provided predictions and masks.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[List[float]]]</code> <p>List of prediction data, one for each image.</p> required <code>masks</code> <code>List[List[List[float]]]</code> <p>List of masks corresponding to the predictions.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>List of image dimensions corresponding to the processed images.</p> required <code>class_filter</code> <code>List[str]</code> <p>List of class names to filter predictions by. Defaults to an empty list (no filtering).</p> <code>[]</code> <p>Returns:</p> Type Description <code>Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]</code> <p>Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]: A single instance segmentation response or a list of instance segmentation responses based on the number of processed images.</p> Notes <ul> <li>For each image, constructs an <code>InstanceSegmentationInferenceResponse</code> object.</li> <li>Each response contains a list of <code>InstanceSegmentationPrediction</code> objects.</li> </ul> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[List[float]]],\n    masks: List[List[List[float]]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: List[str] = [],\n    **kwargs,\n) -&gt; Union[\n    InstanceSegmentationInferenceResponse,\n    List[InstanceSegmentationInferenceResponse],\n]:\n    \"\"\"\n    Create instance segmentation inference response objects for the provided predictions and masks.\n\n    Args:\n        predictions (List[List[List[float]]]): List of prediction data, one for each image.\n        masks (List[List[List[float]]]): List of masks corresponding to the predictions.\n        img_dims (List[Tuple[int, int]]): List of image dimensions corresponding to the processed images.\n        class_filter (List[str], optional): List of class names to filter predictions by. Defaults to an empty list (no filtering).\n\n    Returns:\n        Union[InstanceSegmentationInferenceResponse, List[InstanceSegmentationInferenceResponse]]: A single instance segmentation response or a list of instance segmentation responses based on the number of processed images.\n\n    Notes:\n        - For each image, constructs an `InstanceSegmentationInferenceResponse` object.\n        - Each response contains a list of `InstanceSegmentationPrediction` objects.\n    \"\"\"\n    responses = []\n    for ind, (batch_predictions, batch_masks) in enumerate(zip(predictions, masks)):\n        predictions = []\n        for pred, mask in zip(batch_predictions, batch_masks):\n            if class_filter and self.class_names[int(pred[6])] in class_filter:\n                # TODO: logger.debug\n                continue\n            # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n            predictions.append(\n                InstanceSegmentationPrediction(\n                    **{\n                        \"x\": pred[0] + (pred[2] - pred[0]) / 2,\n                        \"y\": pred[1] + (pred[3] - pred[1]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"points\": [Point(x=point[0], y=point[1]) for point in mask],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                    }\n                )\n            )\n        response = InstanceSegmentationInferenceResponse(\n            predictions=predictions,\n            image=InferenceResponseImage(\n                width=img_dims[ind][1], height=img_dims[ind][0]\n            ),\n        )\n        responses.append(response)\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/instance_segmentation_base/#inference.core.models.instance_segmentation_base.InstanceSegmentationBaseOnnxRoboflowInferenceModel.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Runs inference on the ONNX model.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>The preprocessed image(s) to run inference on.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: The ONNX model predictions and the ONNX model protos.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/instance_segmentation_base.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Runs inference on the ONNX model.\n\n    Args:\n        img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The ONNX model predictions and the ONNX model protos.\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError(\"predict must be implemented by a subclass\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/keypoints_detection_base/","title":"keypoints_detection_base","text":""},{"location":"docs/reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel","title":"<code>KeypointsDetectionBaseOnnxRoboflowInferenceModel</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model. This class implements an object detection specific infer method.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>class KeypointsDetectionBaseOnnxRoboflowInferenceModel(\n    ObjectDetectionBaseOnnxRoboflowInferenceModel\n):\n    \"\"\"Roboflow ONNX Object detection model. This class implements an object detection specific infer method.\"\"\"\n\n    task_type = \"keypoint-detection\"\n\n    def __init__(self, model_id: str, *args, **kwargs):\n        super().__init__(model_id, *args, **kwargs)\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n        Returns:\n            list: A list of filenames specific to ONNX models.\n        \"\"\"\n        return [\"environment.json\", \"class_names.txt\", \"keypoints_metadata.json\"]\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray],\n        preproc_return_metadata: PreprocessReturnMetadata,\n        class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n        confidence: float = DEFAULT_CONFIDENCE,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; List[KeypointsDetectionInferenceResponse]:\n        \"\"\"Postprocesses the object detection predictions.\n\n        Args:\n            predictions (np.ndarray): Raw predictions from the model.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n            confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n            iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n            max_candidates (int): Maximum number of candidate detections. Default is 3000.\n            max_detections (int): Maximum number of final detections. Default is 300.\n\n        Returns:\n            List[KeypointsDetectionInferenceResponse]: The post-processed predictions.\n        \"\"\"\n        predictions = predictions[0]\n        number_of_classes = len(self.get_class_names)\n        num_masks = predictions.shape[2] - 5 - number_of_classes\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=confidence,\n            iou_thresh=iou_threshold,\n            class_agnostic=class_agnostic_nms,\n            max_detections=max_detections,\n            max_candidate_detections=max_candidates,\n            num_masks=num_masks,\n        )\n\n        infer_shape = (self.img_size_h, self.img_size_w)\n        img_dims = preproc_return_metadata[\"img_dims\"]\n        predictions = post_process_bboxes(\n            predictions=predictions,\n            infer_shape=infer_shape,\n            img_dims=img_dims,\n            preproc=self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        predictions = post_process_keypoints(\n            predictions=predictions,\n            keypoints_start_index=-num_masks,\n            infer_shape=infer_shape,\n            img_dims=img_dims,\n            preproc=self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        return self.make_response(predictions, img_dims, **kwargs)\n\n    def make_response(\n        self,\n        predictions: List[List[float]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: Optional[List[str]] = None,\n        *args,\n        **kwargs,\n    ) -&gt; List[KeypointsDetectionInferenceResponse]:\n        \"\"\"Constructs object detection response objects based on predictions.\n\n        Args:\n            predictions (List[List[float]]): The list of predictions.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            List[KeypointsDetectionInferenceResponse]: A list of response objects containing keypoints detection predictions.\n        \"\"\"\n        if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n            img_dims = img_dims[\"img_dims\"]\n        keypoint_confidence_threshold = 0.0\n        if \"request\" in kwargs:\n            keypoint_confidence_threshold = kwargs[\"request\"].keypoint_confidence\n        responses = [\n            KeypointsDetectionInferenceResponse(\n                predictions=[\n                    KeypointsPrediction(\n                        # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                        **{\n                            \"x\": (pred[0] + pred[2]) / 2,\n                            \"y\": (pred[1] + pred[3]) / 2,\n                            \"width\": pred[2] - pred[0],\n                            \"height\": pred[3] - pred[1],\n                            \"confidence\": pred[4],\n                            \"class\": self.class_names[int(pred[6])],\n                            \"class_id\": int(pred[6]),\n                            \"keypoints\": model_keypoints_to_response(\n                                keypoints_metadata=self.keypoints_metadata,\n                                keypoints=pred[7:],\n                                predicted_object_class_id=int(pred[6]),\n                                keypoint_confidence_threshold=keypoint_confidence_threshold,\n                            ),\n                        }\n                    )\n                    for pred in batch_predictions\n                    if not class_filter\n                    or self.class_names[int(pred[6])] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n            )\n            for ind, batch_predictions in enumerate(predictions)\n        ]\n        return responses\n\n    def keypoints_count(self) -&gt; int:\n        raise NotImplementedError\n\n    def validate_model_classes(self) -&gt; None:\n        num_keypoints = self.keypoints_count()\n        output_shape = self.get_model_output_shape()\n        num_classes = get_num_classes_from_model_prediction_shape(\n            len_prediction=output_shape[2], keypoints=num_keypoints\n        )\n        if num_classes != self.num_classes:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Returns the list of files to be downloaded from the inference bucket for ONNX model.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of filenames specific to ONNX models.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n    Returns:\n        list: A list of filenames specific to ONNX models.\n    \"\"\"\n    return [\"environment.json\", \"class_names.txt\", \"keypoints_metadata.json\"]\n</code></pre>"},{"location":"docs/reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, img_dims, class_filter=None, *args, **kwargs)</code>","text":"<p>Constructs object detection response objects based on predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[float]]</code> <p>The list of predictions.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[KeypointsDetectionInferenceResponse]</code> <p>List[KeypointsDetectionInferenceResponse]: A list of response objects containing keypoints detection predictions.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[float]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: Optional[List[str]] = None,\n    *args,\n    **kwargs,\n) -&gt; List[KeypointsDetectionInferenceResponse]:\n    \"\"\"Constructs object detection response objects based on predictions.\n\n    Args:\n        predictions (List[List[float]]): The list of predictions.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        List[KeypointsDetectionInferenceResponse]: A list of response objects containing keypoints detection predictions.\n    \"\"\"\n    if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n        img_dims = img_dims[\"img_dims\"]\n    keypoint_confidence_threshold = 0.0\n    if \"request\" in kwargs:\n        keypoint_confidence_threshold = kwargs[\"request\"].keypoint_confidence\n    responses = [\n        KeypointsDetectionInferenceResponse(\n            predictions=[\n                KeypointsPrediction(\n                    # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                        \"keypoints\": model_keypoints_to_response(\n                            keypoints_metadata=self.keypoints_metadata,\n                            keypoints=pred[7:],\n                            predicted_object_class_id=int(pred[6]),\n                            keypoint_confidence_threshold=keypoint_confidence_threshold,\n                        ),\n                    }\n                )\n                for pred in batch_predictions\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[ind][1], height=img_dims[ind][0]\n            ),\n        )\n        for ind, batch_predictions in enumerate(predictions)\n    ]\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/keypoints_detection_base/#inference.core.models.keypoints_detection_base.KeypointsDetectionBaseOnnxRoboflowInferenceModel.postprocess","title":"<code>postprocess(predictions, preproc_return_metadata, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, confidence=DEFAULT_CONFIDENCE, iou_threshold=DEFAULT_IOU_THRESH, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, **kwargs)</code>","text":"<p>Postprocesses the object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Raw predictions from the model.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to apply class-agnostic non-max suppression. Default is False.</p> <code>DEFAULT_CLASS_AGNOSTIC_NMS</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering detections. Default is 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-max suppression. Default is 0.5.</p> <code>DEFAULT_IOU_THRESH</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Default is 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of final detections. Default is 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <p>Returns:</p> Type Description <code>List[KeypointsDetectionInferenceResponse]</code> <p>List[KeypointsDetectionInferenceResponse]: The post-processed predictions.</p> Source code in <code>inference/core/models/keypoints_detection_base.py</code> <pre><code>def postprocess(\n    self,\n    predictions: Tuple[np.ndarray],\n    preproc_return_metadata: PreprocessReturnMetadata,\n    class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n    confidence: float = DEFAULT_CONFIDENCE,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; List[KeypointsDetectionInferenceResponse]:\n    \"\"\"Postprocesses the object detection predictions.\n\n    Args:\n        predictions (np.ndarray): Raw predictions from the model.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n        confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n        iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n        max_candidates (int): Maximum number of candidate detections. Default is 3000.\n        max_detections (int): Maximum number of final detections. Default is 300.\n\n    Returns:\n        List[KeypointsDetectionInferenceResponse]: The post-processed predictions.\n    \"\"\"\n    predictions = predictions[0]\n    number_of_classes = len(self.get_class_names)\n    num_masks = predictions.shape[2] - 5 - number_of_classes\n    predictions = w_np_non_max_suppression(\n        predictions,\n        conf_thresh=confidence,\n        iou_thresh=iou_threshold,\n        class_agnostic=class_agnostic_nms,\n        max_detections=max_detections,\n        max_candidate_detections=max_candidates,\n        num_masks=num_masks,\n    )\n\n    infer_shape = (self.img_size_h, self.img_size_w)\n    img_dims = preproc_return_metadata[\"img_dims\"]\n    predictions = post_process_bboxes(\n        predictions=predictions,\n        infer_shape=infer_shape,\n        img_dims=img_dims,\n        preproc=self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    predictions = post_process_keypoints(\n        predictions=predictions,\n        keypoints_start_index=-num_masks,\n        infer_shape=infer_shape,\n        img_dims=img_dims,\n        preproc=self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    return self.make_response(predictions, img_dims, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/","title":"object_detection_base","text":""},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel","title":"<code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code>","text":"<p>               Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model. This class implements an object detection specific infer method.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>class ObjectDetectionBaseOnnxRoboflowInferenceModel(OnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model. This class implements an object detection specific infer method.\"\"\"\n\n    task_type = \"object-detection\"\n    box_format = \"xywh\"\n\n    def infer(\n        self,\n        image: Any,\n        class_agnostic_nms: bool = DEFAULT_CLASS_AGNOSTIC_NMS,\n        confidence: float = DEFAULT_CONFIDENCE,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        fix_batch_size: bool = False,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"\n        Runs object detection inference on one or multiple images and returns the detections.\n\n        Args:\n            image (Any): The input image or a list of images to process.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n            confidence (float, optional): Confidence threshold for predictions. Defaults to 0.5.\n            iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.5.\n            fix_batch_size (bool, optional): If True, fix the batch size for predictions. Useful when the model requires a fixed batch size. Defaults to False.\n            max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n            max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n            return_image_dims (bool, optional): Whether to return the dimensions of the processed images along with the predictions. Defaults to False.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n\n        Returns:\n            Union[List[ObjectDetectionInferenceResponse], ObjectDetectionInferenceResponse]: One or multiple object detection inference responses based on the number of processed images. Each response contains a list of predictions. If `return_image_dims` is True, it will return a tuple with predictions and image dimensions.\n\n        Raises:\n            ValueError: If batching is not enabled for the model and more than one image is passed for processing.\n        \"\"\"\n        return super().infer(\n            image,\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n            iou_threshold=iou_threshold,\n            fix_batch_size=fix_batch_size,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            return_image_dims=return_image_dims,\n            **kwargs,\n        )\n\n    def make_response(\n        self,\n        predictions: List[List[float]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: Optional[List[str]] = None,\n        *args,\n        **kwargs,\n    ) -&gt; List[ObjectDetectionInferenceResponse]:\n        \"\"\"Constructs object detection response objects based on predictions.\n\n        Args:\n            predictions (List[List[float]]): The list of predictions.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.\n        \"\"\"\n\n        if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n            img_dims = img_dims[\"img_dims\"]\n\n        predictions = predictions[\n            : len(img_dims)\n        ]  # If the batch size was fixed we have empty preds at the end\n        responses = [\n            ObjectDetectionInferenceResponse(\n                predictions=[\n                    ObjectDetectionPrediction(\n                        # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                        **{\n                            \"x\": (pred[0] + pred[2]) / 2,\n                            \"y\": (pred[1] + pred[3]) / 2,\n                            \"width\": pred[2] - pred[0],\n                            \"height\": pred[3] - pred[1],\n                            \"confidence\": pred[4],\n                            \"class\": self.class_names[int(pred[6])],\n                            \"class_id\": int(pred[6]),\n                        }\n                    )\n                    for pred in batch_predictions\n                    if not class_filter\n                    or self.class_names[int(pred[6])] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[ind][1], height=img_dims[ind][0]\n                ),\n            )\n            for ind, batch_predictions in enumerate(predictions)\n        ]\n        return responses\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preproc_return_metadata: PreprocessReturnMetadata,\n        class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n        confidence: float = DEFAULT_CONFIDENCE,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; List[ObjectDetectionInferenceResponse]:\n        \"\"\"Postprocesses the object detection predictions.\n\n        Args:\n            predictions (np.ndarray): Raw predictions from the model.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n            confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n            iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n            max_candidates (int): Maximum number of candidate detections. Default is 3000.\n            max_detections (int): Maximum number of final detections. Default is 300.\n\n        Returns:\n            List[ObjectDetectionInferenceResponse]: The post-processed predictions.\n        \"\"\"\n        predictions = predictions[0]\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=confidence,\n            iou_thresh=iou_threshold,\n            class_agnostic=class_agnostic_nms,\n            max_detections=max_detections,\n            max_candidate_detections=max_candidates,\n            box_format=self.box_format,\n        )\n\n        infer_shape = (self.img_size_h, self.img_size_w)\n        img_dims = preproc_return_metadata[\"img_dims\"]\n        predictions = post_process_bboxes(\n            predictions,\n            infer_shape,\n            img_dims,\n            self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        return self.make_response(predictions, img_dims, **kwargs)\n\n    def preprocess(\n        self,\n        image: Any,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n        fix_batch_size: bool = False,\n        **kwargs,\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        \"\"\"Preprocesses an object detection inference request.\n\n        Args:\n            request (ObjectDetectionInferenceRequest): The request object containing images.\n\n        Returns:\n            Tuple[np.ndarray, List[Tuple[int, int]]]: Preprocessed image inputs and corresponding dimensions.\n        \"\"\"\n        img_in, img_dims = self.load_image(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n\n        img_in /= 255.0\n\n        if self.batching_enabled:\n            batch_padding = 0\n            if FIX_BATCH_SIZE or fix_batch_size:\n                if MAX_BATCH_SIZE == float(\"inf\"):\n                    logger.warn(\n                        \"Requested fix_batch_size but MAX_BATCH_SIZE is not set. Using dynamic batching.\"\n                    )\n                    batch_padding = 0\n                else:\n                    batch_padding = MAX_BATCH_SIZE - img_in.shape[0]\n            if batch_padding &lt; 0:\n                raise ValueError(\n                    f\"Requested fix_batch_size but passed in {img_in.shape[0]} images \"\n                    f\"when the model's batch size is {MAX_BATCH_SIZE}\\n\"\n                    f\"Consider turning off fix_batch_size, changing `MAX_BATCH_SIZE` in\"\n                    f\"your inference server config, or passing at most {MAX_BATCH_SIZE} images at a time\"\n                )\n            width_remainder = img_in.shape[2] % 32\n            height_remainder = img_in.shape[3] % 32\n            if width_remainder &gt; 0:\n                width_padding = 32 - width_remainder\n            else:\n                width_padding = 0\n            if height_remainder &gt; 0:\n                height_padding = 32 - height_remainder\n            else:\n                height_padding = 0\n            img_in = np.pad(\n                img_in,\n                ((0, batch_padding), (0, 0), (0, width_padding), (0, height_padding)),\n                \"constant\",\n            )\n\n        return img_in, PreprocessReturnMetadata(\n            {\n                \"img_dims\": img_dims,\n                \"disable_preproc_static_crop\": disable_preproc_static_crop,\n            }\n        )\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Runs inference on the ONNX model.\n\n        Args:\n            img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n        Returns:\n            Tuple[np.ndarray]: The ONNX model predictions.\n\n        Raises:\n            NotImplementedError: This method must be implemented by a subclass.\n        \"\"\"\n        raise NotImplementedError(\"predict must be implemented by a subclass\")\n\n    def validate_model_classes(self) -&gt; None:\n        output_shape = self.get_model_output_shape()\n        num_classes = get_num_classes_from_model_prediction_shape(\n            output_shape[2], masks=0\n        )\n        try:\n            assert num_classes == self.num_classes\n        except AssertionError:\n            raise ValueError(\n                f\"Number of classes in model ({num_classes}) does not match the number of classes in the environment ({self.num_classes})\"\n            )\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.infer","title":"<code>infer(image, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, confidence=DEFAULT_CONFIDENCE, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, iou_threshold=DEFAULT_IOU_THRESH, fix_batch_size=False, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, **kwargs)</code>","text":"<p>Runs object detection inference on one or multiple images and returns the detections.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The input image or a list of images to process. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to use class-agnostic non-maximum suppression. Defaults to False.</p> <code>DEFAULT_CLASS_AGNOSTIC_NMS</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for predictions. Defaults to 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-maximum suppression. Defaults to 0.5.</p> <code>DEFAULT_IOU_THRESH</code> <code>fix_batch_size</code> <code>bool</code> <p>If True, fix the batch size for predictions. Useful when the model requires a fixed batch size. Defaults to False.</p> <code>False</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Defaults to 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections after non-maximum suppression. Defaults to 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <code>return_image_dims</code> <code>bool</code> <p>Whether to return the dimensions of the processed images along with the predictions. Defaults to False.</p> <code>False</code> <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the auto contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>*args</code> <p>Variable length argument list.</p> required <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Union[List[ObjectDetectionInferenceResponse], ObjectDetectionInferenceResponse]: One or multiple object detection inference responses based on the number of processed images. Each response contains a list of predictions. If <code>return_image_dims</code> is True, it will return a tuple with predictions and image dimensions.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If batching is not enabled for the model and more than one image is passed for processing.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    class_agnostic_nms: bool = DEFAULT_CLASS_AGNOSTIC_NMS,\n    confidence: float = DEFAULT_CONFIDENCE,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    fix_batch_size: bool = False,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"\n    Runs object detection inference on one or multiple images and returns the detections.\n\n    Args:\n        image (Any): The input image or a list of images to process.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_agnostic_nms (bool, optional): Whether to use class-agnostic non-maximum suppression. Defaults to False.\n        confidence (float, optional): Confidence threshold for predictions. Defaults to 0.5.\n        iou_threshold (float, optional): IoU threshold for non-maximum suppression. Defaults to 0.5.\n        fix_batch_size (bool, optional): If True, fix the batch size for predictions. Useful when the model requires a fixed batch size. Defaults to False.\n        max_candidates (int, optional): Maximum number of candidate detections. Defaults to 3000.\n        max_detections (int, optional): Maximum number of detections after non-maximum suppression. Defaults to 300.\n        return_image_dims (bool, optional): Whether to return the dimensions of the processed images along with the predictions. Defaults to False.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the auto contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        Union[List[ObjectDetectionInferenceResponse], ObjectDetectionInferenceResponse]: One or multiple object detection inference responses based on the number of processed images. Each response contains a list of predictions. If `return_image_dims` is True, it will return a tuple with predictions and image dimensions.\n\n    Raises:\n        ValueError: If batching is not enabled for the model and more than one image is passed for processing.\n    \"\"\"\n    return super().infer(\n        image,\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n        iou_threshold=iou_threshold,\n        fix_batch_size=fix_batch_size,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        return_image_dims=return_image_dims,\n        **kwargs,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.make_response","title":"<code>make_response(predictions, img_dims, class_filter=None, *args, **kwargs)</code>","text":"<p>Constructs object detection response objects based on predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[float]]</code> <p>The list of predictions.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ObjectDetectionInferenceResponse]</code> <p>List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[float]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: Optional[List[str]] = None,\n    *args,\n    **kwargs,\n) -&gt; List[ObjectDetectionInferenceResponse]:\n    \"\"\"Constructs object detection response objects based on predictions.\n\n    Args:\n        predictions (List[List[float]]): The list of predictions.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        List[ObjectDetectionInferenceResponse]: A list of response objects containing object detection predictions.\n    \"\"\"\n\n    if isinstance(img_dims, dict) and \"img_dims\" in img_dims:\n        img_dims = img_dims[\"img_dims\"]\n\n    predictions = predictions[\n        : len(img_dims)\n    ]  # If the batch size was fixed we have empty preds at the end\n    responses = [\n        ObjectDetectionInferenceResponse(\n            predictions=[\n                ObjectDetectionPrediction(\n                    # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                    }\n                )\n                for pred in batch_predictions\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[ind][1], height=img_dims[ind][0]\n            ),\n        )\n        for ind, batch_predictions in enumerate(predictions)\n    ]\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.postprocess","title":"<code>postprocess(predictions, preproc_return_metadata, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, confidence=DEFAULT_CONFIDENCE, iou_threshold=DEFAULT_IOU_THRESH, max_candidates=DEFAULT_MAX_CANDIDATES, max_detections=DEFAUlT_MAX_DETECTIONS, return_image_dims=False, **kwargs)</code>","text":"<p>Postprocesses the object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Raw predictions from the model.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to apply class-agnostic non-max suppression. Default is False.</p> <code>DEFAULT_CLASS_AGNOSTIC_NMS</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering detections. Default is 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>iou_threshold</code> <code>float</code> <p>IoU threshold for non-max suppression. Default is 0.5.</p> <code>DEFAULT_IOU_THRESH</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections. Default is 3000.</p> <code>DEFAULT_MAX_CANDIDATES</code> <code>max_detections</code> <code>int</code> <p>Maximum number of final detections. Default is 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <p>Returns:</p> Type Description <code>List[ObjectDetectionInferenceResponse]</code> <p>List[ObjectDetectionInferenceResponse]: The post-processed predictions.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def postprocess(\n    self,\n    predictions: Tuple[np.ndarray, ...],\n    preproc_return_metadata: PreprocessReturnMetadata,\n    class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n    confidence: float = DEFAULT_CONFIDENCE,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; List[ObjectDetectionInferenceResponse]:\n    \"\"\"Postprocesses the object detection predictions.\n\n    Args:\n        predictions (np.ndarray): Raw predictions from the model.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        class_agnostic_nms (bool): Whether to apply class-agnostic non-max suppression. Default is False.\n        confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n        iou_threshold (float): IoU threshold for non-max suppression. Default is 0.5.\n        max_candidates (int): Maximum number of candidate detections. Default is 3000.\n        max_detections (int): Maximum number of final detections. Default is 300.\n\n    Returns:\n        List[ObjectDetectionInferenceResponse]: The post-processed predictions.\n    \"\"\"\n    predictions = predictions[0]\n    predictions = w_np_non_max_suppression(\n        predictions,\n        conf_thresh=confidence,\n        iou_thresh=iou_threshold,\n        class_agnostic=class_agnostic_nms,\n        max_detections=max_detections,\n        max_candidate_detections=max_candidates,\n        box_format=self.box_format,\n    )\n\n    infer_shape = (self.img_size_h, self.img_size_w)\n    img_dims = preproc_return_metadata[\"img_dims\"]\n    predictions = post_process_bboxes(\n        predictions,\n        infer_shape,\n        img_dims,\n        self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    return self.make_response(predictions, img_dims, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Runs inference on the ONNX model.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>The preprocessed image(s) to run inference on.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: The ONNX model predictions.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented by a subclass.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Runs inference on the ONNX model.\n\n    Args:\n        img_in (np.ndarray): The preprocessed image(s) to run inference on.\n\n    Returns:\n        Tuple[np.ndarray]: The ONNX model predictions.\n\n    Raises:\n        NotImplementedError: This method must be implemented by a subclass.\n    \"\"\"\n    raise NotImplementedError(\"predict must be implemented by a subclass\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/object_detection_base/#inference.core.models.object_detection_base.ObjectDetectionBaseOnnxRoboflowInferenceModel.preprocess","title":"<code>preprocess(image, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False, fix_batch_size=False, **kwargs)</code>","text":"<p>Preprocesses an object detection inference request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ObjectDetectionInferenceRequest</code> <p>The request object containing images.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, PreprocessReturnMetadata]</code> <p>Tuple[np.ndarray, List[Tuple[int, int]]]: Preprocessed image inputs and corresponding dimensions.</p> Source code in <code>inference/core/models/object_detection_base.py</code> <pre><code>def preprocess(\n    self,\n    image: Any,\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n    fix_batch_size: bool = False,\n    **kwargs,\n) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n    \"\"\"Preprocesses an object detection inference request.\n\n    Args:\n        request (ObjectDetectionInferenceRequest): The request object containing images.\n\n    Returns:\n        Tuple[np.ndarray, List[Tuple[int, int]]]: Preprocessed image inputs and corresponding dimensions.\n    \"\"\"\n    img_in, img_dims = self.load_image(\n        image,\n        disable_preproc_auto_orient=disable_preproc_auto_orient,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n    )\n\n    img_in /= 255.0\n\n    if self.batching_enabled:\n        batch_padding = 0\n        if FIX_BATCH_SIZE or fix_batch_size:\n            if MAX_BATCH_SIZE == float(\"inf\"):\n                logger.warn(\n                    \"Requested fix_batch_size but MAX_BATCH_SIZE is not set. Using dynamic batching.\"\n                )\n                batch_padding = 0\n            else:\n                batch_padding = MAX_BATCH_SIZE - img_in.shape[0]\n        if batch_padding &lt; 0:\n            raise ValueError(\n                f\"Requested fix_batch_size but passed in {img_in.shape[0]} images \"\n                f\"when the model's batch size is {MAX_BATCH_SIZE}\\n\"\n                f\"Consider turning off fix_batch_size, changing `MAX_BATCH_SIZE` in\"\n                f\"your inference server config, or passing at most {MAX_BATCH_SIZE} images at a time\"\n            )\n        width_remainder = img_in.shape[2] % 32\n        height_remainder = img_in.shape[3] % 32\n        if width_remainder &gt; 0:\n            width_padding = 32 - width_remainder\n        else:\n            width_padding = 0\n        if height_remainder &gt; 0:\n            height_padding = 32 - height_remainder\n        else:\n            height_padding = 0\n        img_in = np.pad(\n            img_in,\n            ((0, batch_padding), (0, 0), (0, width_padding), (0, height_padding)),\n            \"constant\",\n        )\n\n    return img_in, PreprocessReturnMetadata(\n        {\n            \"img_dims\": img_dims,\n            \"disable_preproc_static_crop\": disable_preproc_static_crop,\n        }\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/","title":"roboflow","text":""},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowCoreModel","title":"<code>OnnxRoboflowCoreModel</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>Roboflow Inference Model that operates using an ONNX model file.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class OnnxRoboflowCoreModel(RoboflowCoreModel):\n    \"\"\"Roboflow Inference Model that operates using an ONNX model file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel","title":"<code>OnnxRoboflowInferenceModel</code>","text":"<p>               Bases: <code>RoboflowInferenceModel</code></p> <p>Roboflow Inference Model that operates using an ONNX model file.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class OnnxRoboflowInferenceModel(RoboflowInferenceModel):\n    \"\"\"Roboflow Inference Model that operates using an ONNX model file.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        onnxruntime_execution_providers: List[\n            str\n        ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Initializes the OnnxRoboflowInferenceModel instance.\n\n        Args:\n            model_id (str): The identifier for the specific ONNX model.\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(model_id, *args, **kwargs)\n        if self.load_weights or not self.has_model_metadata:\n            self.onnxruntime_execution_providers = onnxruntime_execution_providers\n            expanded_execution_providers = []\n            for ep in self.onnxruntime_execution_providers:\n                if ep == \"TensorrtExecutionProvider\":\n                    ep = (\n                        \"TensorrtExecutionProvider\",\n                        {\n                            \"trt_engine_cache_enable\": True,\n                            \"trt_engine_cache_path\": os.path.join(\n                                TENSORRT_CACHE_PATH, self.endpoint\n                            ),\n                            \"trt_fp16_enable\": True,\n                        },\n                    )\n                expanded_execution_providers.append(ep)\n            self.onnxruntime_execution_providers = expanded_execution_providers\n\n        self.initialize_model()\n        self.image_loader_threadpool = ThreadPoolExecutor(max_workers=None)\n        try:\n            self.validate_model()\n        except ModelArtefactError as e:\n            logger.error(f\"Unable to validate model artifacts, clearing cache: {e}\")\n            self.clear_cache()\n            raise ModelArtefactError from e\n\n    def infer(self, image: Any, **kwargs) -&gt; Any:\n        \"\"\"Runs inference on given data.\n        - image:\n            can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        \"\"\"\n        input_elements = len(image) if isinstance(image, list) else 1\n        max_batch_size = MAX_BATCH_SIZE if self.batching_enabled else self.batch_size\n        if (input_elements == 1) or (max_batch_size == float(\"inf\")):\n            return super().infer(image, **kwargs)\n        logger.debug(\n            f\"Inference will be executed in batches, as there is {input_elements} input elements and \"\n            f\"maximum batch size for a model is set to: {max_batch_size}\"\n        )\n        inference_results = []\n        for batch_input in create_batches(sequence=image, batch_size=max_batch_size):\n            batch_inference_results = super().infer(batch_input, **kwargs)\n            inference_results.append(batch_inference_results)\n        return self.merge_inference_results(inference_results=inference_results)\n\n    def merge_inference_results(self, inference_results: List[Any]) -&gt; Any:\n        return list(itertools.chain(*inference_results))\n\n    def validate_model(self) -&gt; None:\n        if MODEL_VALIDATION_DISABLED:\n            logger.debug(\"Model validation disabled.\")\n            return None\n        logger.debug(\"Starting model validation\")\n        if not self.load_weights:\n            return\n        try:\n            assert self.onnx_session is not None\n        except AssertionError as e:\n            raise ModelArtefactError(\n                \"ONNX session not initialized. Check that the model weights are available.\"\n            ) from e\n        try:\n            self.run_test_inference()\n        except Exception as e:\n            raise ModelArtefactError(f\"Unable to run test inference. Cause: {e}\") from e\n        try:\n            self.validate_model_classes()\n        except Exception as e:\n            raise ModelArtefactError(\n                f\"Unable to validate model classes. Cause: {e}\"\n            ) from e\n        logger.debug(\"Model validation finished\")\n\n    def run_test_inference(self) -&gt; None:\n        test_image = (np.random.rand(1024, 1024, 3) * 255).astype(np.uint8)\n        logger.debug(f\"Running test inference. Image size: {test_image.shape}\")\n        result = self.infer(test_image, usage_inference_test_run=True)\n        logger.debug(f\"Test inference finished.\")\n        return result\n\n    def get_model_output_shape(self) -&gt; Tuple[int, int, int]:\n        test_image = (np.random.rand(1024, 1024, 3) * 255).astype(np.uint8)\n        logger.debug(f\"Getting model output shape. Image size: {test_image.shape}\")\n        test_image, _ = self.preprocess(test_image)\n        output = self.predict(test_image)[0]\n        logger.debug(f\"Model output shape test finished.\")\n        return output.shape\n\n    def validate_model_classes(self) -&gt; None:\n        pass\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n        Returns:\n            list: A list of filenames specific to ONNX models.\n        \"\"\"\n        return [\"environment.json\", \"class_names.txt\"]\n\n    def initialize_model(self) -&gt; None:\n        \"\"\"Initializes the ONNX model, setting up the inference session and other necessary properties.\"\"\"\n        logger.debug(\"Getting model artefacts\")\n        self.get_model_artifacts()\n        logger.debug(\"Creating inference session\")\n        if self.load_weights or not self.has_model_metadata:\n            t1_session = perf_counter()\n            # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n            providers = self.onnxruntime_execution_providers\n\n            if not self.load_weights:\n                providers = [\"OpenVINOExecutionProvider\", \"CPUExecutionProvider\"]\n            try:\n                session_options = onnxruntime.SessionOptions()\n                # TensorRT does better graph optimization for its EP than onnx\n                if has_trt(providers):\n                    session_options.graph_optimization_level = (\n                        onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL\n                    )\n                self.onnx_session = onnxruntime.InferenceSession(\n                    self.cache_file(self.weights_file),\n                    providers=providers,\n                    sess_options=session_options,\n                )\n            except Exception as e:\n                self.clear_cache()\n                raise ModelArtefactError(\n                    f\"Unable to load ONNX session. Cause: {e}\"\n                ) from e\n            logger.debug(f\"Session created in {perf_counter() - t1_session} seconds\")\n\n            if REQUIRED_ONNX_PROVIDERS:\n                available_providers = onnxruntime.get_available_providers()\n                for provider in REQUIRED_ONNX_PROVIDERS:\n                    if provider not in available_providers:\n                        raise OnnxProviderNotAvailable(\n                            f\"Required ONNX Execution Provider {provider} is not availble. \"\n                            \"Check that you are using the correct docker image on a supported device. \"\n                            \"Export list of available providers as ONNXRUNTIME_EXECUTION_PROVIDERS environmental variable, \"\n                            \"consult documentation for more details.\"\n                        )\n\n            inputs = self.onnx_session.get_inputs()[0]\n            input_shape = inputs.shape\n            self.batch_size = input_shape[0]\n            self.img_size_h = input_shape[2]\n            self.img_size_w = input_shape[3]\n            self.input_name = inputs.name\n            if isinstance(self.img_size_h, str) or isinstance(self.img_size_w, str):\n                if \"resize\" in self.preproc:\n                    self.img_size_h = int(self.preproc[\"resize\"][\"height\"])\n                    self.img_size_w = int(self.preproc[\"resize\"][\"width\"])\n                else:\n                    self.img_size_h = 640\n                    self.img_size_w = 640\n\n            if isinstance(self.batch_size, str):\n                self.batching_enabled = True\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n                )\n            else:\n                self.batching_enabled = False\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n                )\n\n            model_metadata = {\n                \"batch_size\": self.batch_size,\n                \"img_size_h\": self.img_size_h,\n                \"img_size_w\": self.img_size_w,\n            }\n            logger.debug(f\"Writing model metadata to memcache\")\n            self.write_model_metadata_to_memcache(model_metadata)\n            if not self.load_weights:  # had to load weights to get metadata\n                del self.onnx_session\n        else:\n            if not self.has_model_metadata:\n                raise ValueError(\n                    \"This should be unreachable, should get weights if we don't have model metadata\"\n                )\n            logger.debug(f\"Loading model metadata from memcache\")\n            metadata = self.model_metadata_from_memcache()\n            self.batch_size = metadata[\"batch_size\"]\n            self.img_size_h = metadata[\"img_size_h\"]\n            self.img_size_w = metadata[\"img_size_w\"]\n            if isinstance(self.batch_size, str):\n                self.batching_enabled = True\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n                )\n            else:\n                self.batching_enabled = False\n                logger.debug(\n                    f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n                )\n        logger.debug(\"Model initialisation finished.\")\n\n    def load_image(\n        self,\n        image: Any,\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n    ) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n        if isinstance(image, list):\n            preproc_image = partial(\n                self.preproc_image,\n                disable_preproc_auto_orient=disable_preproc_auto_orient,\n                disable_preproc_contrast=disable_preproc_contrast,\n                disable_preproc_grayscale=disable_preproc_grayscale,\n                disable_preproc_static_crop=disable_preproc_static_crop,\n            )\n            imgs_with_dims = self.image_loader_threadpool.map(preproc_image, image)\n            imgs, img_dims = zip(*imgs_with_dims)\n            img_in = np.concatenate(imgs, axis=0)\n        else:\n            img_in, img_dims = self.preproc_image(\n                image,\n                disable_preproc_auto_orient=disable_preproc_auto_orient,\n                disable_preproc_contrast=disable_preproc_contrast,\n                disable_preproc_grayscale=disable_preproc_grayscale,\n                disable_preproc_static_crop=disable_preproc_static_crop,\n            )\n            img_dims = [img_dims]\n        return img_in, img_dims\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Returns the file containing the ONNX model weights.\n\n        Returns:\n            str: The file path to the weights file.\n        \"\"\"\n        return \"weights.onnx\"\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Returns the file containing the ONNX model weights.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The file path to the weights file.</p>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.__init__","title":"<code>__init__(model_id, onnxruntime_execution_providers=get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS), *args, **kwargs)</code>","text":"<p>Initializes the OnnxRoboflowInferenceModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier for the specific ONNX model.</p> required <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    onnxruntime_execution_providers: List[\n        str\n    ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n    *args,\n    **kwargs,\n):\n    \"\"\"Initializes the OnnxRoboflowInferenceModel instance.\n\n    Args:\n        model_id (str): The identifier for the specific ONNX model.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(model_id, *args, **kwargs)\n    if self.load_weights or not self.has_model_metadata:\n        self.onnxruntime_execution_providers = onnxruntime_execution_providers\n        expanded_execution_providers = []\n        for ep in self.onnxruntime_execution_providers:\n            if ep == \"TensorrtExecutionProvider\":\n                ep = (\n                    \"TensorrtExecutionProvider\",\n                    {\n                        \"trt_engine_cache_enable\": True,\n                        \"trt_engine_cache_path\": os.path.join(\n                            TENSORRT_CACHE_PATH, self.endpoint\n                        ),\n                        \"trt_fp16_enable\": True,\n                    },\n                )\n            expanded_execution_providers.append(ep)\n        self.onnxruntime_execution_providers = expanded_execution_providers\n\n    self.initialize_model()\n    self.image_loader_threadpool = ThreadPoolExecutor(max_workers=None)\n    try:\n        self.validate_model()\n    except ModelArtefactError as e:\n        logger.error(f\"Unable to validate model artifacts, clearing cache: {e}\")\n        self.clear_cache()\n        raise ModelArtefactError from e\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Returns the list of files to be downloaded from the inference bucket for ONNX model.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of filenames specific to ONNX models.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Returns the list of files to be downloaded from the inference bucket for ONNX model.\n\n    Returns:\n        list: A list of filenames specific to ONNX models.\n    \"\"\"\n    return [\"environment.json\", \"class_names.txt\"]\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Runs inference on given data. - image:     can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def infer(self, image: Any, **kwargs) -&gt; Any:\n    \"\"\"Runs inference on given data.\n    - image:\n        can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n    \"\"\"\n    input_elements = len(image) if isinstance(image, list) else 1\n    max_batch_size = MAX_BATCH_SIZE if self.batching_enabled else self.batch_size\n    if (input_elements == 1) or (max_batch_size == float(\"inf\")):\n        return super().infer(image, **kwargs)\n    logger.debug(\n        f\"Inference will be executed in batches, as there is {input_elements} input elements and \"\n        f\"maximum batch size for a model is set to: {max_batch_size}\"\n    )\n    inference_results = []\n    for batch_input in create_batches(sequence=image, batch_size=max_batch_size):\n        batch_inference_results = super().infer(batch_input, **kwargs)\n        inference_results.append(batch_inference_results)\n    return self.merge_inference_results(inference_results=inference_results)\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.OnnxRoboflowInferenceModel.initialize_model","title":"<code>initialize_model()</code>","text":"<p>Initializes the ONNX model, setting up the inference session and other necessary properties.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def initialize_model(self) -&gt; None:\n    \"\"\"Initializes the ONNX model, setting up the inference session and other necessary properties.\"\"\"\n    logger.debug(\"Getting model artefacts\")\n    self.get_model_artifacts()\n    logger.debug(\"Creating inference session\")\n    if self.load_weights or not self.has_model_metadata:\n        t1_session = perf_counter()\n        # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n        providers = self.onnxruntime_execution_providers\n\n        if not self.load_weights:\n            providers = [\"OpenVINOExecutionProvider\", \"CPUExecutionProvider\"]\n        try:\n            session_options = onnxruntime.SessionOptions()\n            # TensorRT does better graph optimization for its EP than onnx\n            if has_trt(providers):\n                session_options.graph_optimization_level = (\n                    onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL\n                )\n            self.onnx_session = onnxruntime.InferenceSession(\n                self.cache_file(self.weights_file),\n                providers=providers,\n                sess_options=session_options,\n            )\n        except Exception as e:\n            self.clear_cache()\n            raise ModelArtefactError(\n                f\"Unable to load ONNX session. Cause: {e}\"\n            ) from e\n        logger.debug(f\"Session created in {perf_counter() - t1_session} seconds\")\n\n        if REQUIRED_ONNX_PROVIDERS:\n            available_providers = onnxruntime.get_available_providers()\n            for provider in REQUIRED_ONNX_PROVIDERS:\n                if provider not in available_providers:\n                    raise OnnxProviderNotAvailable(\n                        f\"Required ONNX Execution Provider {provider} is not availble. \"\n                        \"Check that you are using the correct docker image on a supported device. \"\n                        \"Export list of available providers as ONNXRUNTIME_EXECUTION_PROVIDERS environmental variable, \"\n                        \"consult documentation for more details.\"\n                    )\n\n        inputs = self.onnx_session.get_inputs()[0]\n        input_shape = inputs.shape\n        self.batch_size = input_shape[0]\n        self.img_size_h = input_shape[2]\n        self.img_size_w = input_shape[3]\n        self.input_name = inputs.name\n        if isinstance(self.img_size_h, str) or isinstance(self.img_size_w, str):\n            if \"resize\" in self.preproc:\n                self.img_size_h = int(self.preproc[\"resize\"][\"height\"])\n                self.img_size_w = int(self.preproc[\"resize\"][\"width\"])\n            else:\n                self.img_size_h = 640\n                self.img_size_w = 640\n\n        if isinstance(self.batch_size, str):\n            self.batching_enabled = True\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n            )\n        else:\n            self.batching_enabled = False\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n            )\n\n        model_metadata = {\n            \"batch_size\": self.batch_size,\n            \"img_size_h\": self.img_size_h,\n            \"img_size_w\": self.img_size_w,\n        }\n        logger.debug(f\"Writing model metadata to memcache\")\n        self.write_model_metadata_to_memcache(model_metadata)\n        if not self.load_weights:  # had to load weights to get metadata\n            del self.onnx_session\n    else:\n        if not self.has_model_metadata:\n            raise ValueError(\n                \"This should be unreachable, should get weights if we don't have model metadata\"\n            )\n        logger.debug(f\"Loading model metadata from memcache\")\n        metadata = self.model_metadata_from_memcache()\n        self.batch_size = metadata[\"batch_size\"]\n        self.img_size_h = metadata[\"img_size_h\"]\n        self.img_size_w = metadata[\"img_size_w\"]\n        if isinstance(self.batch_size, str):\n            self.batching_enabled = True\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching enabled\"\n            )\n        else:\n            self.batching_enabled = False\n            logger.debug(\n                f\"Model {self.endpoint} is loaded with dynamic batching disabled\"\n            )\n    logger.debug(\"Model initialisation finished.\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel","title":"<code>RoboflowCoreModel</code>","text":"<p>               Bases: <code>RoboflowInferenceModel</code></p> <p>Base Roboflow inference model (Inherits from CvModel since all Roboflow models are CV models currently).</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class RoboflowCoreModel(RoboflowInferenceModel):\n    \"\"\"Base Roboflow inference model (Inherits from CvModel since all Roboflow models are CV models currently).\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        api_key=None,\n    ):\n        \"\"\"Initializes the RoboflowCoreModel instance.\n\n        Args:\n            model_id (str): The identifier for the specific model.\n            api_key ([type], optional): The API key for authentication. Defaults to None.\n        \"\"\"\n        super().__init__(model_id, api_key=api_key)\n        self.download_weights()\n\n    def download_weights(self) -&gt; None:\n        \"\"\"Downloads the model weights from the configured source.\n\n        This method includes handling for AWS access keys and error handling.\n        \"\"\"\n        infer_bucket_files = self.get_infer_bucket_file_list()\n        if are_all_files_cached(files=infer_bucket_files, model_id=self.endpoint):\n            logger.debug(\"Model artifacts already downloaded, loading from cache\")\n            return None\n        if is_model_artefacts_bucket_available():\n            self.download_model_artefacts_from_s3()\n            return None\n        self.download_model_from_roboflow_api()\n\n    def download_model_from_roboflow_api(self) -&gt; None:\n        api_data = get_roboflow_model_data(\n            api_key=self.api_key,\n            model_id=self.endpoint,\n            endpoint_type=ModelEndpointType.CORE_MODEL,\n            device_id=self.device_id,\n        )\n        if \"weights\" not in api_data:\n            raise ModelArtefactError(\n                f\"`weights` key not available in Roboflow API response while downloading model weights.\"\n            )\n        for weights_url_key in api_data[\"weights\"]:\n            weights_url = api_data[\"weights\"][weights_url_key]\n            t1 = perf_counter()\n            model_weights_response = get_from_url(weights_url, json_response=False)\n            filename = weights_url.split(\"?\")[0].split(\"/\")[-1]\n            save_bytes_in_cache(\n                content=model_weights_response.content,\n                file=filename,\n                model_id=self.endpoint,\n            )\n            if perf_counter() - t1 &gt; 120:\n                logger.debug(\n                    \"Weights download took longer than 120 seconds, refreshing API request\"\n                )\n                api_data = get_roboflow_model_data(\n                    api_key=self.api_key,\n                    model_id=self.endpoint,\n                    endpoint_type=ModelEndpointType.CORE_MODEL,\n                    device_id=self.device_id,\n                )\n\n    def get_device_id(self) -&gt; str:\n        \"\"\"Returns the device ID associated with this model.\n\n        Returns:\n            str: The device ID.\n        \"\"\"\n        return self.device_id\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Abstract method to get the list of files to be downloaded from the inference bucket.\n\n        Raises:\n            NotImplementedError: This method must be implemented in subclasses.\n\n        Returns:\n            List[str]: A list of filenames.\n        \"\"\"\n        raise NotImplementedError(\n            \"get_infer_bucket_file_list not implemented for OnnxRoboflowCoreModel\"\n        )\n\n    def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n        \"\"\"Abstract method to preprocess an image.\n\n        Raises:\n            NotImplementedError: This method must be implemented in subclasses.\n\n        Returns:\n            Image.Image: The preprocessed PIL image.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".preprocess_image\")\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Abstract property representing the file containing the model weights. For core models, all model artifacts are handled through get_infer_bucket_file_list method.\"\"\"\n        return None\n\n    @property\n    def model_artifact_bucket(self):\n        return CORE_MODEL_BUCKET\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Abstract property representing the file containing the model weights. For core models, all model artifacts are handled through get_infer_bucket_file_list method.</p>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.__init__","title":"<code>__init__(model_id, api_key=None)</code>","text":"<p>Initializes the RoboflowCoreModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The identifier for the specific model.</p> required <code>api_key</code> <code>[type]</code> <p>The API key for authentication. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    api_key=None,\n):\n    \"\"\"Initializes the RoboflowCoreModel instance.\n\n    Args:\n        model_id (str): The identifier for the specific model.\n        api_key ([type], optional): The API key for authentication. Defaults to None.\n    \"\"\"\n    super().__init__(model_id, api_key=api_key)\n    self.download_weights()\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.download_weights","title":"<code>download_weights()</code>","text":"<p>Downloads the model weights from the configured source.</p> <p>This method includes handling for AWS access keys and error handling.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def download_weights(self) -&gt; None:\n    \"\"\"Downloads the model weights from the configured source.\n\n    This method includes handling for AWS access keys and error handling.\n    \"\"\"\n    infer_bucket_files = self.get_infer_bucket_file_list()\n    if are_all_files_cached(files=infer_bucket_files, model_id=self.endpoint):\n        logger.debug(\"Model artifacts already downloaded, loading from cache\")\n        return None\n    if is_model_artefacts_bucket_available():\n        self.download_model_artefacts_from_s3()\n        return None\n    self.download_model_from_roboflow_api()\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.get_device_id","title":"<code>get_device_id()</code>","text":"<p>Returns the device ID associated with this model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The device ID.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_device_id(self) -&gt; str:\n    \"\"\"Returns the device ID associated with this model.\n\n    Returns:\n        str: The device ID.\n    \"\"\"\n    return self.device_id\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Abstract method to get the list of files to be downloaded from the inference bucket.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented in subclasses.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of filenames.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Abstract method to get the list of files to be downloaded from the inference bucket.\n\n    Raises:\n        NotImplementedError: This method must be implemented in subclasses.\n\n    Returns:\n        List[str]: A list of filenames.\n    \"\"\"\n    raise NotImplementedError(\n        \"get_infer_bucket_file_list not implemented for OnnxRoboflowCoreModel\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowCoreModel.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>Abstract method to preprocess an image.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method must be implemented in subclasses.</p> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The preprocessed PIL image.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n    \"\"\"Abstract method to preprocess an image.\n\n    Raises:\n        NotImplementedError: This method must be implemented in subclasses.\n\n    Returns:\n        Image.Image: The preprocessed PIL image.\n    \"\"\"\n    raise NotImplementedError(self.__class__.__name__ + \".preprocess_image\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel","title":"<code>RoboflowInferenceModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>Base Roboflow inference model.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>class RoboflowInferenceModel(Model):\n    \"\"\"Base Roboflow inference model.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        cache_dir_root=MODEL_CACHE_DIR,\n        api_key=None,\n        load_weights=True,\n    ):\n        \"\"\"\n        Initialize the RoboflowInferenceModel object.\n\n        Args:\n            model_id (str): The unique identifier for the model.\n            cache_dir_root (str, optional): The root directory for the cache. Defaults to MODEL_CACHE_DIR.\n            api_key (str, optional): API key for authentication. Defaults to None.\n        \"\"\"\n        super().__init__()\n        self.load_weights = load_weights\n        self.metrics = {\"num_inferences\": 0, \"avg_inference_time\": 0.0}\n        self.api_key = api_key if api_key else API_KEY\n        model_id = resolve_roboflow_model_alias(model_id=model_id)\n        self.dataset_id, self.version_id = model_id.split(\"/\")\n        self.endpoint = model_id\n        self.device_id = GLOBAL_DEVICE_ID\n        self.cache_dir = os.path.join(cache_dir_root, self.endpoint)\n        self.keypoints_metadata: Optional[dict] = None\n        initialise_cache(model_id=self.endpoint)\n\n    def cache_file(self, f: str) -&gt; str:\n        \"\"\"Get the cache file path for a given file.\n\n        Args:\n            f (str): Filename.\n\n        Returns:\n            str: Full path to the cached file.\n        \"\"\"\n        return get_cache_file_path(file=f, model_id=self.endpoint)\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear the cache directory.\"\"\"\n        clear_cache(model_id=self.endpoint)\n\n    def draw_predictions(\n        self,\n        inference_request: InferenceRequest,\n        inference_response: InferenceResponse,\n    ) -&gt; bytes:\n        \"\"\"Draw predictions from an inference response onto the original image provided by an inference request\n\n        Args:\n            inference_request (ObjectDetectionInferenceRequest): The inference request containing the image on which to draw predictions\n            inference_response (ObjectDetectionInferenceResponse): The inference response containing predictions to be drawn\n\n        Returns:\n            str: A base64 encoded image string\n        \"\"\"\n        return draw_detection_predictions(\n            inference_request=inference_request,\n            inference_response=inference_response,\n            colors=self.colors,\n        )\n\n    @property\n    def get_class_names(self):\n        return self.class_names\n\n    def get_device_id(self) -&gt; str:\n        \"\"\"\n        Get the device identifier on which the model is deployed.\n\n        Returns:\n            str: Device identifier.\n        \"\"\"\n        return self.device_id\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Get a list of inference bucket files.\n\n        Raises:\n            NotImplementedError: If the method is not implemented.\n\n        Returns:\n            List[str]: A list of inference bucket files.\n        \"\"\"\n        raise NotImplementedError(\n            self.__class__.__name__ + \".get_infer_bucket_file_list\"\n        )\n\n    @property\n    def cache_key(self):\n        return f\"metadata:{self.endpoint}\"\n\n    @staticmethod\n    def model_metadata_from_memcache_endpoint(endpoint):\n        model_metadata = cache.get(f\"metadata:{endpoint}\")\n        return model_metadata\n\n    def model_metadata_from_memcache(self):\n        model_metadata = cache.get(self.cache_key)\n        return model_metadata\n\n    def write_model_metadata_to_memcache(self, metadata):\n        cache.set(\n            self.cache_key, metadata, expire=MODEL_METADATA_CACHE_EXPIRATION_TIMEOUT\n        )\n\n    @property\n    def has_model_metadata(self):\n        return self.model_metadata_from_memcache() is not None\n\n    def get_model_artifacts(self) -&gt; None:\n        \"\"\"Fetch or load the model artifacts.\n\n        Downloads the model artifacts from S3 or the Roboflow API if they are not already cached.\n        \"\"\"\n        self.cache_model_artefacts()\n        self.load_model_artifacts_from_cache()\n\n    def cache_model_artefacts(self) -&gt; None:\n        infer_bucket_files = self.get_all_required_infer_bucket_file()\n        if are_all_files_cached(files=infer_bucket_files, model_id=self.endpoint):\n            return None\n        if is_model_artefacts_bucket_available():\n            self.download_model_artefacts_from_s3()\n            return None\n        self.download_model_artifacts_from_roboflow_api()\n\n    def get_all_required_infer_bucket_file(self) -&gt; List[str]:\n        infer_bucket_files = self.get_infer_bucket_file_list()\n        infer_bucket_files.append(self.weights_file)\n        logger.debug(f\"List of files required to load model: {infer_bucket_files}\")\n        return [f for f in infer_bucket_files if f is not None]\n\n    def download_model_artefacts_from_s3(self) -&gt; None:\n        try:\n            logger.debug(\"Downloading model artifacts from S3\")\n            infer_bucket_files = self.get_all_required_infer_bucket_file()\n            cache_directory = get_cache_dir()\n            s3_keys = [f\"{self.endpoint}/{file}\" for file in infer_bucket_files]\n            download_s3_files_to_directory(\n                bucket=self.model_artifact_bucket,\n                keys=s3_keys,\n                target_dir=cache_directory,\n                s3_client=S3_CLIENT,\n            )\n        except Exception as error:\n            raise ModelArtefactError(\n                f\"Could not obtain model artefacts from S3 with keys {s3_keys}. Cause: {error}\"\n            ) from error\n\n    @property\n    def model_artifact_bucket(self):\n        return INFER_BUCKET\n\n    def download_model_artifacts_from_roboflow_api(self) -&gt; None:\n        logger.debug(\"Downloading model artifacts from Roboflow API\")\n        api_data = get_roboflow_model_data(\n            api_key=self.api_key,\n            model_id=self.endpoint,\n            endpoint_type=ModelEndpointType.ORT,\n            device_id=self.device_id,\n        )\n        if \"ort\" not in api_data.keys():\n            raise ModelArtefactError(\n                \"Could not find `ort` key in roboflow API model description response.\"\n            )\n        api_data = api_data[\"ort\"]\n        if \"classes\" in api_data:\n            save_text_lines_in_cache(\n                content=api_data[\"classes\"],\n                file=\"class_names.txt\",\n                model_id=self.endpoint,\n            )\n        if \"model\" not in api_data:\n            raise ModelArtefactError(\n                \"Could not find `model` key in roboflow API model description response.\"\n            )\n        if \"environment\" not in api_data:\n            raise ModelArtefactError(\n                \"Could not find `environment` key in roboflow API model description response.\"\n            )\n        environment = get_from_url(api_data[\"environment\"])\n        model_weights_response = get_from_url(api_data[\"model\"], json_response=False)\n        save_bytes_in_cache(\n            content=model_weights_response.content,\n            file=self.weights_file,\n            model_id=self.endpoint,\n        )\n        if \"colors\" in api_data:\n            environment[\"COLORS\"] = api_data[\"colors\"]\n        save_json_in_cache(\n            content=environment,\n            file=\"environment.json\",\n            model_id=self.endpoint,\n        )\n        if \"keypoints_metadata\" in api_data:\n            # TODO: make sure backend provides that\n            save_json_in_cache(\n                content=api_data[\"keypoints_metadata\"],\n                file=\"keypoints_metadata.json\",\n                model_id=self.endpoint,\n            )\n\n    def load_model_artifacts_from_cache(self) -&gt; None:\n        logger.debug(\"Model artifacts already downloaded, loading model from cache\")\n        infer_bucket_files = self.get_all_required_infer_bucket_file()\n        if \"environment.json\" in infer_bucket_files:\n            self.environment = load_json_from_cache(\n                file=\"environment.json\",\n                model_id=self.endpoint,\n                object_pairs_hook=OrderedDict,\n            )\n        if \"class_names.txt\" in infer_bucket_files:\n            self.class_names = load_text_file_from_cache(\n                file=\"class_names.txt\",\n                model_id=self.endpoint,\n                split_lines=True,\n                strip_white_chars=True,\n            )\n        else:\n            self.class_names = get_class_names_from_environment_file(\n                environment=self.environment\n            )\n        self.colors = get_color_mapping_from_environment(\n            environment=self.environment,\n            class_names=self.class_names,\n        )\n        if \"keypoints_metadata.json\" in infer_bucket_files:\n            self.keypoints_metadata = parse_keypoints_metadata(\n                load_json_from_cache(\n                    file=\"keypoints_metadata.json\",\n                    model_id=self.endpoint,\n                    object_pairs_hook=OrderedDict,\n                )\n            )\n        self.num_classes = len(self.class_names)\n        if \"PREPROCESSING\" not in self.environment:\n            raise ModelArtefactError(\n                \"Could not find `PREPROCESSING` key in environment file.\"\n            )\n        if issubclass(type(self.environment[\"PREPROCESSING\"]), dict):\n            self.preproc = self.environment[\"PREPROCESSING\"]\n        else:\n            self.preproc = json.loads(self.environment[\"PREPROCESSING\"])\n        if self.preproc.get(\"resize\"):\n            self.resize_method = self.preproc[\"resize\"].get(\"format\", \"Stretch to\")\n            if self.resize_method not in [\n                \"Stretch to\",\n                \"Fit (black edges) in\",\n                \"Fit (white edges) in\",\n                \"Fit (grey edges) in\",\n            ]:\n                self.resize_method = \"Stretch to\"\n        else:\n            self.resize_method = \"Stretch to\"\n        logger.debug(f\"Resize method is '{self.resize_method}'\")\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    def initialize_model(self) -&gt; None:\n        \"\"\"Initialize the model.\n\n        Raises:\n            NotImplementedError: If the method is not implemented.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".initialize_model\")\n\n    def preproc_image(\n        self,\n        image: Union[Any, InferenceRequestImage],\n        disable_preproc_auto_orient: bool = False,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n    ) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n        \"\"\"\n        Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.\n\n        Args:\n            image (Union[Any, InferenceRequestImage]): An object containing information necessary to load the image for inference.\n            disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n            disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n        Returns:\n            Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.\n        \"\"\"\n        np_image, is_bgr = load_image(\n            image,\n            disable_preproc_auto_orient=disable_preproc_auto_orient\n            or \"auto-orient\" not in self.preproc.keys()\n            or DISABLE_PREPROC_AUTO_ORIENT,\n        )\n        preprocessed_image, img_dims = self.preprocess_image(\n            np_image,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n\n        if self.resize_method == \"Stretch to\":\n            resized = cv2.resize(\n                preprocessed_image, (self.img_size_w, self.img_size_h), cv2.INTER_CUBIC\n            )\n        elif self.resize_method == \"Fit (black edges) in\":\n            resized = letterbox_image(\n                preprocessed_image, (self.img_size_w, self.img_size_h)\n            )\n        elif self.resize_method == \"Fit (white edges) in\":\n            resized = letterbox_image(\n                preprocessed_image,\n                (self.img_size_w, self.img_size_h),\n                color=(255, 255, 255),\n            )\n        elif self.resize_method == \"Fit (grey edges) in\":\n            resized = letterbox_image(\n                preprocessed_image,\n                (self.img_size_w, self.img_size_h),\n                color=(114, 114, 114),\n            )\n\n        if is_bgr:\n            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n        img_in = np.transpose(resized, (2, 0, 1))\n        img_in = img_in.astype(np.float32)\n        img_in = np.expand_dims(img_in, axis=0)\n\n        return img_in, img_dims\n\n    def preprocess_image(\n        self,\n        image: np.ndarray,\n        disable_preproc_contrast: bool = False,\n        disable_preproc_grayscale: bool = False,\n        disable_preproc_static_crop: bool = False,\n    ) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n        \"\"\"\n        Preprocesses the given image using specified preprocessing steps.\n\n        Args:\n            image (Image.Image): The PIL image to preprocess.\n            disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n            disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n            disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n        Returns:\n            Image.Image: The preprocessed PIL image.\n        \"\"\"\n        return prepare(\n            image,\n            self.preproc,\n            disable_preproc_contrast=disable_preproc_contrast,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Abstract property representing the file containing the model weights.\n\n        Raises:\n            NotImplementedError: This property must be implemented in subclasses.\n\n        Returns:\n            str: The file path to the weights file.\n        \"\"\"\n        raise NotImplementedError(self.__class__.__name__ + \".weights_file\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Abstract property representing the file containing the model weights.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This property must be implemented in subclasses.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The file path to the weights file.</p>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.__init__","title":"<code>__init__(model_id, cache_dir_root=MODEL_CACHE_DIR, api_key=None, load_weights=True)</code>","text":"<p>Initialize the RoboflowInferenceModel object.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The unique identifier for the model.</p> required <code>cache_dir_root</code> <code>str</code> <p>The root directory for the cache. Defaults to MODEL_CACHE_DIR.</p> <code>MODEL_CACHE_DIR</code> <code>api_key</code> <code>str</code> <p>API key for authentication. Defaults to None.</p> <code>None</code> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    cache_dir_root=MODEL_CACHE_DIR,\n    api_key=None,\n    load_weights=True,\n):\n    \"\"\"\n    Initialize the RoboflowInferenceModel object.\n\n    Args:\n        model_id (str): The unique identifier for the model.\n        cache_dir_root (str, optional): The root directory for the cache. Defaults to MODEL_CACHE_DIR.\n        api_key (str, optional): API key for authentication. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self.load_weights = load_weights\n    self.metrics = {\"num_inferences\": 0, \"avg_inference_time\": 0.0}\n    self.api_key = api_key if api_key else API_KEY\n    model_id = resolve_roboflow_model_alias(model_id=model_id)\n    self.dataset_id, self.version_id = model_id.split(\"/\")\n    self.endpoint = model_id\n    self.device_id = GLOBAL_DEVICE_ID\n    self.cache_dir = os.path.join(cache_dir_root, self.endpoint)\n    self.keypoints_metadata: Optional[dict] = None\n    initialise_cache(model_id=self.endpoint)\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.cache_file","title":"<code>cache_file(f)</code>","text":"<p>Get the cache file path for a given file.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>str</code> <p>Filename.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Full path to the cached file.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def cache_file(self, f: str) -&gt; str:\n    \"\"\"Get the cache file path for a given file.\n\n    Args:\n        f (str): Filename.\n\n    Returns:\n        str: Full path to the cached file.\n    \"\"\"\n    return get_cache_file_path(file=f, model_id=self.endpoint)\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the cache directory.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear the cache directory.\"\"\"\n    clear_cache(model_id=self.endpoint)\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.draw_predictions","title":"<code>draw_predictions(inference_request, inference_response)</code>","text":"<p>Draw predictions from an inference response onto the original image provided by an inference request</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <code>ObjectDetectionInferenceRequest</code> <p>The inference request containing the image on which to draw predictions</p> required <code>inference_response</code> <code>ObjectDetectionInferenceResponse</code> <p>The inference response containing predictions to be drawn</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>bytes</code> <p>A base64 encoded image string</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def draw_predictions(\n    self,\n    inference_request: InferenceRequest,\n    inference_response: InferenceResponse,\n) -&gt; bytes:\n    \"\"\"Draw predictions from an inference response onto the original image provided by an inference request\n\n    Args:\n        inference_request (ObjectDetectionInferenceRequest): The inference request containing the image on which to draw predictions\n        inference_response (ObjectDetectionInferenceResponse): The inference response containing predictions to be drawn\n\n    Returns:\n        str: A base64 encoded image string\n    \"\"\"\n    return draw_detection_predictions(\n        inference_request=inference_request,\n        inference_response=inference_response,\n        colors=self.colors,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.get_device_id","title":"<code>get_device_id()</code>","text":"<p>Get the device identifier on which the model is deployed.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Device identifier.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_device_id(self) -&gt; str:\n    \"\"\"\n    Get the device identifier on which the model is deployed.\n\n    Returns:\n        str: Device identifier.\n    \"\"\"\n    return self.device_id\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get a list of inference bucket files.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of inference bucket files.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Get a list of inference bucket files.\n\n    Raises:\n        NotImplementedError: If the method is not implemented.\n\n    Returns:\n        List[str]: A list of inference bucket files.\n    \"\"\"\n    raise NotImplementedError(\n        self.__class__.__name__ + \".get_infer_bucket_file_list\"\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.get_model_artifacts","title":"<code>get_model_artifacts()</code>","text":"<p>Fetch or load the model artifacts.</p> <p>Downloads the model artifacts from S3 or the Roboflow API if they are not already cached.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def get_model_artifacts(self) -&gt; None:\n    \"\"\"Fetch or load the model artifacts.\n\n    Downloads the model artifacts from S3 or the Roboflow API if they are not already cached.\n    \"\"\"\n    self.cache_model_artefacts()\n    self.load_model_artifacts_from_cache()\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.initialize_model","title":"<code>initialize_model()</code>","text":"<p>Initialize the model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def initialize_model(self) -&gt; None:\n    \"\"\"Initialize the model.\n\n    Raises:\n        NotImplementedError: If the method is not implemented.\n    \"\"\"\n    raise NotImplementedError(self.__class__.__name__ + \".initialize_model\")\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.preproc_image","title":"<code>preproc_image(image, disable_preproc_auto_orient=False, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False)</code>","text":"<p>Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[Any, InferenceRequestImage]</code> <p>An object containing information necessary to load the image for inference.</p> required <code>disable_preproc_auto_orient</code> <code>bool</code> <p>If true, the auto orient preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Tuple[int, int]]</code> <p>Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def preproc_image(\n    self,\n    image: Union[Any, InferenceRequestImage],\n    disable_preproc_auto_orient: bool = False,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n    \"\"\"\n    Preprocesses an inference request image by loading it, then applying any pre-processing specified by the Roboflow platform, then scaling it to the inference input dimensions.\n\n    Args:\n        image (Union[Any, InferenceRequestImage]): An object containing information necessary to load the image for inference.\n        disable_preproc_auto_orient (bool, optional): If true, the auto orient preprocessing step is disabled for this call. Default is False.\n        disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        Tuple[np.ndarray, Tuple[int, int]]: A tuple containing a numpy array of the preprocessed image pixel data and a tuple of the images original size.\n    \"\"\"\n    np_image, is_bgr = load_image(\n        image,\n        disable_preproc_auto_orient=disable_preproc_auto_orient\n        or \"auto-orient\" not in self.preproc.keys()\n        or DISABLE_PREPROC_AUTO_ORIENT,\n    )\n    preprocessed_image, img_dims = self.preprocess_image(\n        np_image,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n    )\n\n    if self.resize_method == \"Stretch to\":\n        resized = cv2.resize(\n            preprocessed_image, (self.img_size_w, self.img_size_h), cv2.INTER_CUBIC\n        )\n    elif self.resize_method == \"Fit (black edges) in\":\n        resized = letterbox_image(\n            preprocessed_image, (self.img_size_w, self.img_size_h)\n        )\n    elif self.resize_method == \"Fit (white edges) in\":\n        resized = letterbox_image(\n            preprocessed_image,\n            (self.img_size_w, self.img_size_h),\n            color=(255, 255, 255),\n        )\n    elif self.resize_method == \"Fit (grey edges) in\":\n        resized = letterbox_image(\n            preprocessed_image,\n            (self.img_size_w, self.img_size_h),\n            color=(114, 114, 114),\n        )\n\n    if is_bgr:\n        resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    img_in = np.transpose(resized, (2, 0, 1))\n    img_in = img_in.astype(np.float32)\n    img_in = np.expand_dims(img_in, axis=0)\n\n    return img_in, img_dims\n</code></pre>"},{"location":"docs/reference/inference/core/models/roboflow/#inference.core.models.roboflow.RoboflowInferenceModel.preprocess_image","title":"<code>preprocess_image(image, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False)</code>","text":"<p>Preprocesses the given image using specified preprocessing steps.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The PIL image to preprocess.</p> required <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, Tuple[int, int]]</code> <p>Image.Image: The preprocessed PIL image.</p> Source code in <code>inference/core/models/roboflow.py</code> <pre><code>def preprocess_image(\n    self,\n    image: np.ndarray,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n    \"\"\"\n    Preprocesses the given image using specified preprocessing steps.\n\n    Args:\n        image (Image.Image): The PIL image to preprocess.\n        disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        Image.Image: The preprocessed PIL image.\n    \"\"\"\n    return prepare(\n        image,\n        self.preproc,\n        disable_preproc_contrast=disable_preproc_contrast,\n        disable_preproc_grayscale=disable_preproc_grayscale,\n        disable_preproc_static_crop=disable_preproc_static_crop,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/models/stubs/","title":"stubs","text":""},{"location":"docs/reference/inference/core/models/types/","title":"types","text":""},{"location":"docs/reference/inference/core/models/utils/batching/","title":"batching","text":""},{"location":"docs/reference/inference/core/models/utils/keypoints/","title":"keypoints","text":""},{"location":"docs/reference/inference/core/models/utils/keypoints/#inference.core.models.utils.keypoints.superset_keypoints_count","title":"<code>superset_keypoints_count(keypoints_metadata={})</code>","text":"<p>Returns the number of keypoints in the superset.</p> Source code in <code>inference/core/models/utils/keypoints.py</code> <pre><code>def superset_keypoints_count(keypoints_metadata={}) -&gt; int:\n    \"\"\"Returns the number of keypoints in the superset.\"\"\"\n    max_keypoints = 0\n    for keypoints in keypoints_metadata.values():\n        if len(keypoints) &gt; max_keypoints:\n            max_keypoints = len(keypoints)\n    return max_keypoints\n</code></pre>"},{"location":"docs/reference/inference/core/models/utils/onnx/","title":"onnx","text":""},{"location":"docs/reference/inference/core/models/utils/validate/","title":"validate","text":""},{"location":"docs/reference/inference/core/registries/base/","title":"base","text":""},{"location":"docs/reference/inference/core/registries/base/#inference.core.registries.base.ModelRegistry","title":"<code>ModelRegistry</code>","text":"<p>An object which is able to return model classes based on given model IDs and model types.</p> <p>Attributes:</p> Name Type Description <code>registry_dict</code> <code>dict</code> <p>A dictionary mapping model types to model classes.</p> Source code in <code>inference/core/registries/base.py</code> <pre><code>class ModelRegistry:\n    \"\"\"An object which is able to return model classes based on given model IDs and model types.\n\n    Attributes:\n        registry_dict (dict): A dictionary mapping model types to model classes.\n    \"\"\"\n\n    def __init__(self, registry_dict) -&gt; None:\n        \"\"\"Initializes the ModelRegistry with the given dictionary of registered models.\n\n        Args:\n            registry_dict (dict): A dictionary mapping model types to model classes.\n        \"\"\"\n        self.registry_dict = registry_dict\n\n    def get_model(self, model_type: str, model_id: str) -&gt; Model:\n        \"\"\"Returns the model class based on the given model type.\n\n        Args:\n            model_type (str): The type of the model to be retrieved.\n            model_id (str): The ID of the model to be retrieved (unused in the current implementation).\n\n        Returns:\n            Model: The model class corresponding to the given model type.\n\n        Raises:\n            ModelNotRecognisedError: If the model_type is not found in the registry_dict.\n        \"\"\"\n        if model_type not in self.registry_dict:\n            raise ModelNotRecognisedError(\n                f\"Could not find model of type: {model_type} in configured registry.\"\n            )\n        return self.registry_dict[model_type]\n</code></pre>"},{"location":"docs/reference/inference/core/registries/base/#inference.core.registries.base.ModelRegistry.__init__","title":"<code>__init__(registry_dict)</code>","text":"<p>Initializes the ModelRegistry with the given dictionary of registered models.</p> <p>Parameters:</p> Name Type Description Default <code>registry_dict</code> <code>dict</code> <p>A dictionary mapping model types to model classes.</p> required Source code in <code>inference/core/registries/base.py</code> <pre><code>def __init__(self, registry_dict) -&gt; None:\n    \"\"\"Initializes the ModelRegistry with the given dictionary of registered models.\n\n    Args:\n        registry_dict (dict): A dictionary mapping model types to model classes.\n    \"\"\"\n    self.registry_dict = registry_dict\n</code></pre>"},{"location":"docs/reference/inference/core/registries/base/#inference.core.registries.base.ModelRegistry.get_model","title":"<code>get_model(model_type, model_id)</code>","text":"<p>Returns the model class based on the given model type.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The type of the model to be retrieved.</p> required <code>model_id</code> <code>str</code> <p>The ID of the model to be retrieved (unused in the current implementation).</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model class corresponding to the given model type.</p> <p>Raises:</p> Type Description <code>ModelNotRecognisedError</code> <p>If the model_type is not found in the registry_dict.</p> Source code in <code>inference/core/registries/base.py</code> <pre><code>def get_model(self, model_type: str, model_id: str) -&gt; Model:\n    \"\"\"Returns the model class based on the given model type.\n\n    Args:\n        model_type (str): The type of the model to be retrieved.\n        model_id (str): The ID of the model to be retrieved (unused in the current implementation).\n\n    Returns:\n        Model: The model class corresponding to the given model type.\n\n    Raises:\n        ModelNotRecognisedError: If the model_type is not found in the registry_dict.\n    \"\"\"\n    if model_type not in self.registry_dict:\n        raise ModelNotRecognisedError(\n            f\"Could not find model of type: {model_type} in configured registry.\"\n        )\n    return self.registry_dict[model_type]\n</code></pre>"},{"location":"docs/reference/inference/core/registries/roboflow/","title":"roboflow","text":""},{"location":"docs/reference/inference/core/registries/roboflow/#inference.core.registries.roboflow.RoboflowModelRegistry","title":"<code>RoboflowModelRegistry</code>","text":"<p>               Bases: <code>ModelRegistry</code></p> <p>A Roboflow-specific model registry which gets the model type using the model id, then returns a model class based on the model type.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>class RoboflowModelRegistry(ModelRegistry):\n    \"\"\"A Roboflow-specific model registry which gets the model type using the model id,\n    then returns a model class based on the model type.\n    \"\"\"\n\n    def get_model(self, model_id: str, api_key: str) -&gt; Model:\n        \"\"\"Returns the model class based on the given model id and API key.\n\n        Args:\n            model_id (str): The ID of the model to be retrieved.\n            api_key (str): The API key used to authenticate.\n\n        Returns:\n            Model: The model class corresponding to the given model ID and type.\n\n        Raises:\n            ModelNotRecognisedError: If the model type is not supported or found.\n        \"\"\"\n        model_type = get_model_type(model_id, api_key)\n        logger.debug(f\"Model type: {model_type}\")\n        if model_type not in self.registry_dict:\n            raise ModelNotRecognisedError(f\"Model type not supported: {model_type}\")\n        return self.registry_dict[model_type]\n</code></pre>"},{"location":"docs/reference/inference/core/registries/roboflow/#inference.core.registries.roboflow.RoboflowModelRegistry.get_model","title":"<code>get_model(model_id, api_key)</code>","text":"<p>Returns the model class based on the given model id and API key.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model to be retrieved.</p> required <code>api_key</code> <code>str</code> <p>The API key used to authenticate.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>The model class corresponding to the given model ID and type.</p> <p>Raises:</p> Type Description <code>ModelNotRecognisedError</code> <p>If the model type is not supported or found.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>def get_model(self, model_id: str, api_key: str) -&gt; Model:\n    \"\"\"Returns the model class based on the given model id and API key.\n\n    Args:\n        model_id (str): The ID of the model to be retrieved.\n        api_key (str): The API key used to authenticate.\n\n    Returns:\n        Model: The model class corresponding to the given model ID and type.\n\n    Raises:\n        ModelNotRecognisedError: If the model type is not supported or found.\n    \"\"\"\n    model_type = get_model_type(model_id, api_key)\n    logger.debug(f\"Model type: {model_type}\")\n    if model_type not in self.registry_dict:\n        raise ModelNotRecognisedError(f\"Model type not supported: {model_type}\")\n    return self.registry_dict[model_type]\n</code></pre>"},{"location":"docs/reference/inference/core/registries/roboflow/#inference.core.registries.roboflow.get_model_type","title":"<code>get_model_type(model_id, api_key=None)</code>","text":"<p>Retrieves the model type based on the given model ID and API key.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model.</p> required <code>api_key</code> <code>str</code> <p>The API key used to authenticate.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[TaskType, ModelType]</code> <p>The project task type and the model type.</p> <p>Raises:</p> Type Description <code>WorkspaceLoadError</code> <p>If the workspace could not be loaded or if the API key is invalid.</p> <code>DatasetLoadError</code> <p>If the dataset could not be loaded due to invalid ID, workspace ID or version ID.</p> <code>MissingDefaultModelError</code> <p>If default model is not configured and API does not provide this info</p> <code>MalformedRoboflowAPIResponseError</code> <p>Roboflow API responds in invalid format.</p> Source code in <code>inference/core/registries/roboflow.py</code> <pre><code>def get_model_type(\n    model_id: str,\n    api_key: Optional[str] = None,\n) -&gt; Tuple[TaskType, ModelType]:\n    \"\"\"Retrieves the model type based on the given model ID and API key.\n\n    Args:\n        model_id (str): The ID of the model.\n        api_key (str): The API key used to authenticate.\n\n    Returns:\n        tuple: The project task type and the model type.\n\n    Raises:\n        WorkspaceLoadError: If the workspace could not be loaded or if the API key is invalid.\n        DatasetLoadError: If the dataset could not be loaded due to invalid ID, workspace ID or version ID.\n        MissingDefaultModelError: If default model is not configured and API does not provide this info\n        MalformedRoboflowAPIResponseError: Roboflow API responds in invalid format.\n    \"\"\"\n    model_id = resolve_roboflow_model_alias(model_id=model_id)\n    dataset_id, version_id = get_model_id_chunks(model_id=model_id)\n    if dataset_id in GENERIC_MODELS:\n        logger.debug(f\"Loading generic model: {dataset_id}.\")\n        return GENERIC_MODELS[dataset_id]\n    cached_metadata = get_model_metadata_from_cache(\n        dataset_id=dataset_id, version_id=version_id\n    )\n    if cached_metadata is not None:\n        return cached_metadata[0], cached_metadata[1]\n    if version_id == STUB_VERSION_ID:\n        if api_key is None:\n            raise MissingApiKeyError(\n                \"Stub model version provided but no API key was provided. API key is required to load stub models.\"\n            )\n        workspace_id = get_roboflow_workspace(api_key=api_key)\n        project_task_type = get_roboflow_dataset_type(\n            api_key=api_key, workspace_id=workspace_id, dataset_id=dataset_id\n        )\n        model_type = \"stub\"\n        save_model_metadata_in_cache(\n            dataset_id=dataset_id,\n            version_id=version_id,\n            project_task_type=project_task_type,\n            model_type=model_type,\n        )\n        return project_task_type, model_type\n    api_data = get_roboflow_model_data(\n        api_key=api_key,\n        model_id=model_id,\n        endpoint_type=ModelEndpointType.ORT,\n        device_id=GLOBAL_DEVICE_ID,\n    ).get(\"ort\")\n    if api_data is None:\n        raise ModelArtefactError(\"Error loading model artifacts from Roboflow API.\")\n    # some older projects do not have type field - hence defaulting\n    project_task_type = api_data.get(\"type\", \"object-detection\")\n    model_type = api_data.get(\"modelType\")\n    if model_type is None or model_type == \"ort\":\n        # some very old model versions do not have modelType reported - and API respond in a generic way -\n        # then we shall attempt using default model for given task type\n        model_type = MODEL_TYPE_DEFAULTS.get(project_task_type)\n    if model_type is None or project_task_type is None:\n        raise ModelArtefactError(\"Error loading model artifacts from Roboflow API.\")\n    save_model_metadata_in_cache(\n        dataset_id=dataset_id,\n        version_id=version_id,\n        project_task_type=project_task_type,\n        model_type=model_type,\n    )\n\n    return project_task_type, model_type\n</code></pre>"},{"location":"docs/reference/inference/core/utils/async_utils/","title":"async_utils","text":""},{"location":"docs/reference/inference/core/utils/container/","title":"container","text":""},{"location":"docs/reference/inference/core/utils/container/#inference.core.utils.container.is_docker_socket_mounted","title":"<code>is_docker_socket_mounted(docker_socket_path)</code>","text":"<p>Check if the given path is a mounted Docker socket.</p> <p>Parameters:</p> Name Type Description Default <code>docker_socket_path</code> <code>str</code> <p>The path to the socket file.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the path is a Unix socket, False otherwise.</p> Source code in <code>inference/core/utils/container.py</code> <pre><code>def is_docker_socket_mounted(docker_socket_path: str) -&gt; bool:\n    \"\"\"\n    Check if the given path is a mounted Docker socket.\n\n    Args:\n        docker_socket_path (str): The path to the socket file.\n\n    Returns:\n        bool: True if the path is a Unix socket, False otherwise.\n    \"\"\"\n    if os.path.exists(docker_socket_path):\n        socket_stat = os.stat(docker_socket_path)\n        if stat.S_ISSOCK(socket_stat.st_mode):\n            return True\n    return False\n</code></pre>"},{"location":"docs/reference/inference/core/utils/drawing/","title":"drawing","text":""},{"location":"docs/reference/inference/core/utils/environment/","title":"environment","text":""},{"location":"docs/reference/inference/core/utils/environment/#inference.core.utils.environment.safe_env_to_type","title":"<code>safe_env_to_type(variable_name, default_value=None, type_constructor=None)</code>","text":"<p>Converts env variable to specified type, but only if variable is set - otherwise default is returned. If <code>type_constructor</code> is not given - value of type str will be returned.</p> Source code in <code>inference/core/utils/environment.py</code> <pre><code>def safe_env_to_type(\n    variable_name: str,\n    default_value: Optional[T] = None,\n    type_constructor: Optional[Union[Type[T], Callable[[str], T]]] = None,\n) -&gt; Optional[T]:\n    \"\"\"\n    Converts env variable to specified type, but only if variable is set - otherwise default is returned.\n    If `type_constructor` is not given - value of type str will be returned.\n    \"\"\"\n    if variable_name not in os.environ:\n        return default_value\n    variable_value = os.environ[variable_name]\n    if type_constructor is None:\n        return variable_value\n    return type_constructor(variable_value)\n</code></pre>"},{"location":"docs/reference/inference/core/utils/environment/#inference.core.utils.environment.safe_split_value","title":"<code>safe_split_value(value, delimiter=',')</code>","text":"<p>Splits a separated environment variable into a list.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The environment variable value to be split.</p> required <code>delimiter(str)</code> <p>Delimiter to be used</p> required <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>list or None: The split values as a list, or None if the input is None.</p> Source code in <code>inference/core/utils/environment.py</code> <pre><code>def safe_split_value(value: Optional[str], delimiter: str = \",\") -&gt; Optional[List[str]]:\n    \"\"\"\n    Splits a separated environment variable into a list.\n\n    Args:\n        value (str): The environment variable value to be split.\n        delimiter(str): Delimiter to be used\n\n    Returns:\n        list or None: The split values as a list, or None if the input is None.\n    \"\"\"\n    if value is None:\n        return None\n    else:\n        return value.split(delimiter)\n</code></pre>"},{"location":"docs/reference/inference/core/utils/environment/#inference.core.utils.environment.str2bool","title":"<code>str2bool(value)</code>","text":"<p>Converts an environment variable to a boolean value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str or bool</code> <p>The environment variable value to be converted.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>The converted boolean value.</p> <p>Raises:</p> Type Description <code>InvalidEnvironmentVariableError</code> <p>If the value is not 'true', 'false', or a boolean.</p> Source code in <code>inference/core/utils/environment.py</code> <pre><code>def str2bool(value: Any) -&gt; bool:\n    \"\"\"\n    Converts an environment variable to a boolean value.\n\n    Args:\n        value (str or bool): The environment variable value to be converted.\n\n    Returns:\n        bool: The converted boolean value.\n\n    Raises:\n        InvalidEnvironmentVariableError: If the value is not 'true', 'false', or a boolean.\n    \"\"\"\n    if isinstance(value, bool):\n        return value\n    if not issubclass(type(value), str):\n        raise InvalidEnvironmentVariableError(\n            f\"Expected a boolean environment variable (true or false) but got '{value}'\"\n        )\n    if value.lower() == \"true\":\n        return True\n    elif value.lower() == \"false\":\n        return False\n    else:\n        raise InvalidEnvironmentVariableError(\n            f\"Expected a boolean environment variable (true or false) but got '{value}'\"\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/file_system/","title":"file_system","text":""},{"location":"docs/reference/inference/core/utils/function/","title":"function","text":""},{"location":"docs/reference/inference/core/utils/hash/","title":"hash","text":""},{"location":"docs/reference/inference/core/utils/image_utils/","title":"image_utils","text":""},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.attempt_loading_image_from_string","title":"<code>attempt_loading_image_from_string(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Attempt to load an image from a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, bytes, bytearray, _IOBase]</code> <p>The image data in string format.</p> required <code>cv_imread_flags</code> <code>int</code> <p>OpenCV flags used for image reading.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Tuple[np.ndarray, bool]: A tuple of the loaded image in numpy array format and a boolean flag indicating if the image is in BGR format.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def attempt_loading_image_from_string(\n    value: Union[str, bytes, bytearray, _IOBase],\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; Tuple[np.ndarray, bool]:\n    \"\"\"\n    Attempt to load an image from a string.\n\n    Args:\n        value (Union[str, bytes, bytearray, _IOBase]): The image data in string format.\n        cv_imread_flags (int): OpenCV flags used for image reading.\n\n    Returns:\n        Tuple[np.ndarray, bool]: A tuple of the loaded image in numpy array format and a boolean flag indicating if the image is in BGR format.\n    \"\"\"\n    try:\n        return load_image_base64(value=value, cv_imread_flags=cv_imread_flags), True\n    except:\n        pass\n    try:\n        return (\n            load_image_from_encoded_bytes(value=value, cv_imread_flags=cv_imread_flags),\n            True,\n        )\n    except:\n        pass\n    try:\n        return (\n            load_image_from_buffer(value=value, cv_imread_flags=cv_imread_flags),\n            True,\n        )\n    except:\n        pass\n    try:\n        return load_image_from_numpy_str(value=value), True\n    except InvalidImageTypeDeclared as error:\n        raise error\n    except InvalidNumpyInput as error:\n        raise InputFormatInferenceFailed(\n            message=\"Input image format could not be inferred from string.\",\n            public_message=\"Input image format could not be inferred from string.\",\n        ) from error\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.choose_image_decoding_flags","title":"<code>choose_image_decoding_flags(disable_preproc_auto_orient)</code>","text":"<p>Choose the appropriate OpenCV image decoding flags.</p> <p>Parameters:</p> Name Type Description Default <code>disable_preproc_auto_orient</code> <code>bool</code> <p>Flag to disable preprocessing auto-orientation.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>OpenCV image decoding flags.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def choose_image_decoding_flags(disable_preproc_auto_orient: bool) -&gt; int:\n    \"\"\"Choose the appropriate OpenCV image decoding flags.\n\n    Args:\n        disable_preproc_auto_orient (bool): Flag to disable preprocessing auto-orientation.\n\n    Returns:\n        int: OpenCV image decoding flags.\n    \"\"\"\n    cv_imread_flags = cv2.IMREAD_COLOR\n    if disable_preproc_auto_orient:\n        cv_imread_flags = cv_imread_flags | cv2.IMREAD_IGNORE_ORIENTATION\n    return cv_imread_flags\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.convert_gray_image_to_bgr","title":"<code>convert_gray_image_to_bgr(image)</code>","text":"<p>Convert a grayscale image to BGR format.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The grayscale image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The converted BGR image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def convert_gray_image_to_bgr(image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Convert a grayscale image to BGR format.\n\n    Args:\n        image (np.ndarray): The grayscale image.\n\n    Returns:\n        np.ndarray: The converted BGR image.\n    \"\"\"\n\n    if len(image.shape) == 2 or image.shape[2] == 1:\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    return image\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.encode_image_to_jpeg_bytes","title":"<code>encode_image_to_jpeg_bytes(image, jpeg_quality=90)</code>","text":"<p>Encode a numpy image to JPEG format in bytes.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The numpy array representing a BGR image.</p> required <code>jpeg_quality</code> <code>int</code> <p>Quality of the JPEG image.</p> <code>90</code> <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The JPEG encoded image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def encode_image_to_jpeg_bytes(image: np.ndarray, jpeg_quality: int = 90) -&gt; bytes:\n    \"\"\"\n    Encode a numpy image to JPEG format in bytes.\n\n    Args:\n        image (np.ndarray): The numpy array representing a BGR image.\n        jpeg_quality (int): Quality of the JPEG image.\n\n    Returns:\n        bytes: The JPEG encoded image.\n    \"\"\"\n    encoding_param = [int(cv2.IMWRITE_JPEG_QUALITY), jpeg_quality]\n    _, img_encoded = cv2.imencode(\".jpg\", image, encoding_param)\n    return np.array(img_encoded).tobytes()\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.extract_image_payload_and_type","title":"<code>extract_image_payload_and_type(value)</code>","text":"<p>Extract the image payload and type from the given value.</p> <p>This function supports different types of image inputs (e.g., InferenceRequestImage, dict, etc.) and extracts the relevant data and image type for further processing.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The input value which can be an image or information to derive the image.</p> required <p>Returns:</p> Type Description <code>Tuple[Any, Optional[ImageType]]</code> <p>Tuple[Any, Optional[ImageType]]: A tuple containing the extracted image data and the corresponding image type.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def extract_image_payload_and_type(value: Any) -&gt; Tuple[Any, Optional[ImageType]]:\n    \"\"\"Extract the image payload and type from the given value.\n\n    This function supports different types of image inputs (e.g., InferenceRequestImage, dict, etc.)\n    and extracts the relevant data and image type for further processing.\n\n    Args:\n        value (Any): The input value which can be an image or information to derive the image.\n\n    Returns:\n        Tuple[Any, Optional[ImageType]]: A tuple containing the extracted image data and the corresponding image type.\n    \"\"\"\n    image_type = None\n    if issubclass(type(value), InferenceRequestImage):\n        image_type = value.type\n        value = value.value\n    elif issubclass(type(value), dict):\n        image_type = value.get(\"type\")\n        value = value.get(\"value\")\n    allowed_payload_types = {e.value for e in ImageType}\n    if image_type is None:\n        return value, image_type\n    if image_type.lower() not in allowed_payload_types:\n        raise InvalidImageTypeDeclared(\n            message=f\"Declared image type: {image_type.lower()} which is not in allowed types: {allowed_payload_types}.\",\n            public_message=\"Image declaration contains not recognised image type.\",\n        )\n    return value, ImageType(image_type.lower())\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image","title":"<code>load_image(value, disable_preproc_auto_orient=False)</code>","text":"<p>Loads an image based on the specified type and value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Image value which could be an instance of InferenceRequestImage, a dict with 'type' and 'value' keys, or inferred based on the value's content.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Image.Image: The loaded PIL image, converted to RGB.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the specified image type is not supported.</p> <code>InvalidNumpyInput</code> <p>If the numpy input method is used and the input data is invalid.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image(\n    value: Any,\n    disable_preproc_auto_orient: bool = False,\n) -&gt; Tuple[np.ndarray, bool]:\n    \"\"\"Loads an image based on the specified type and value.\n\n    Args:\n        value (Any): Image value which could be an instance of InferenceRequestImage,\n            a dict with 'type' and 'value' keys, or inferred based on the value's content.\n\n    Returns:\n        Image.Image: The loaded PIL image, converted to RGB.\n\n    Raises:\n        NotImplementedError: If the specified image type is not supported.\n        InvalidNumpyInput: If the numpy input method is used and the input data is invalid.\n    \"\"\"\n    cv_imread_flags = choose_image_decoding_flags(\n        disable_preproc_auto_orient=disable_preproc_auto_orient\n    )\n    value, image_type = extract_image_payload_and_type(value=value)\n    if image_type is not None:\n        np_image, is_bgr = load_image_with_known_type(\n            value=value,\n            image_type=image_type,\n            cv_imread_flags=cv_imread_flags,\n        )\n    else:\n        np_image, is_bgr = load_image_with_inferred_type(\n            value, cv_imread_flags=cv_imread_flags\n        )\n    np_image = convert_gray_image_to_bgr(image=np_image)\n    logger.debug(f\"Loaded inference image. Shape: {getattr(np_image, 'shape', None)}\")\n    return np_image, is_bgr\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_base64","title":"<code>load_image_base64(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Loads an image from a base64 encoded string using OpenCV.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Base64 encoded string representing the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The loaded image as a numpy array.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_base64(\n    value: Union[str, bytes], cv_imread_flags=cv2.IMREAD_COLOR\n) -&gt; np.ndarray:\n    \"\"\"Loads an image from a base64 encoded string using OpenCV.\n\n    Args:\n        value (str): Base64 encoded string representing the image.\n\n    Returns:\n        np.ndarray: The loaded image as a numpy array.\n    \"\"\"\n    # New routes accept images via json body (str), legacy routes accept bytes which need to be decoded as strings\n    if not isinstance(value, str):\n        value = value.decode(\"utf-8\")\n    value = BASE64_DATA_TYPE_PATTERN.sub(\"\", value)\n    try:\n        value = pybase64.b64decode(value)\n    except binascii.Error as error:\n        raise InputImageLoadError(\n            message=\"Could not load valid image from base64 string.\",\n            public_message=\"Malformed base64 input image.\",\n        ) from error\n    if len(value) == 0:\n        raise InputImageLoadError(\n            message=\"Could not load valid image from base64 string.\",\n            public_message=\"Empty image payload.\",\n        )\n    image_np = np.frombuffer(value, np.uint8)\n    result = cv2.imdecode(image_np, cv_imread_flags)\n    if result is None:\n        raise InputImageLoadError(\n            message=\"Could not load valid image from base64 string.\",\n            public_message=\"Malformed base64 input image.\",\n        )\n    return result\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_buffer","title":"<code>load_image_from_buffer(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Loads an image from a multipart-encoded input.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Multipart-encoded input representing the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image.Image: The loaded PIL image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_buffer(\n    value: _IOBase,\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; np.ndarray:\n    \"\"\"Loads an image from a multipart-encoded input.\n\n    Args:\n        value (Any): Multipart-encoded input representing the image.\n\n    Returns:\n        Image.Image: The loaded PIL image.\n    \"\"\"\n    value.seek(0)\n    image_np = np.frombuffer(value.read(), np.uint8)\n    result = cv2.imdecode(image_np, cv_imread_flags)\n    if result is None:\n        raise InputImageLoadError(\n            message=\"Could not load valid image from buffer.\",\n            public_message=\"Could not decode bytes into image.\",\n        )\n    return result\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_encoded_bytes","title":"<code>load_image_from_encoded_bytes(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Load an image from encoded bytes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>The byte sequence representing the image.</p> required <code>cv_imread_flags</code> <code>int</code> <p>OpenCV flags used for image reading.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The loaded image as a numpy array.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_encoded_bytes(\n    value: bytes, cv_imread_flags: int = cv2.IMREAD_COLOR\n) -&gt; np.ndarray:\n    \"\"\"\n    Load an image from encoded bytes.\n\n    Args:\n        value (bytes): The byte sequence representing the image.\n        cv_imread_flags (int): OpenCV flags used for image reading.\n\n    Returns:\n        np.ndarray: The loaded image as a numpy array.\n    \"\"\"\n    image_np = np.asarray(bytearray(value), dtype=np.uint8)\n    image = cv2.imdecode(image_np, cv_imread_flags)\n    if image is None:\n        raise InputImageLoadError(\n            message=f\"Could not decode bytes as image.\",\n            public_message=\"Data is not image.\",\n        )\n    return image\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_numpy_str","title":"<code>load_image_from_numpy_str(value)</code>","text":"<p>Loads an image from a numpy array string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[bytes, str]</code> <p>Base64 string or byte sequence representing the pickled numpy array of the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image.Image: The loaded PIL image.</p> <p>Raises:</p> Type Description <code>InvalidNumpyInput</code> <p>If the numpy data is invalid.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_numpy_str(value: Union[bytes, str]) -&gt; np.ndarray:\n    \"\"\"Loads an image from a numpy array string.\n\n    Args:\n        value (Union[bytes, str]): Base64 string or byte sequence representing the pickled numpy array of the image.\n\n    Returns:\n        Image.Image: The loaded PIL image.\n\n    Raises:\n        InvalidNumpyInput: If the numpy data is invalid.\n    \"\"\"\n    if not ALLOW_NUMPY_INPUT:\n        raise InvalidImageTypeDeclared(\n            message=f\"NumPy image type is not supported in this configuration of `inference`.\",\n            public_message=f\"NumPy image type is not supported in this configuration of `inference`.\",\n        )\n    try:\n        if isinstance(value, str):\n            value = pybase64.b64decode(value)\n        data = pickle.loads(value)\n    except (EOFError, TypeError, pickle.UnpicklingError, binascii.Error) as error:\n        raise InvalidNumpyInput(\n            message=f\"Could not unpickle image data. Cause: {error}\",\n            public_message=\"Could not deserialize pickle payload.\",\n        ) from error\n    validate_numpy_image(data=data)\n    return data\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_from_url","title":"<code>load_image_from_url(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Loads an image from a given URL.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>URL of the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Image.Image: The loaded PIL image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_from_url(\n    value: str, cv_imread_flags: int = cv2.IMREAD_COLOR\n) -&gt; np.ndarray:\n    \"\"\"Loads an image from a given URL.\n\n    Args:\n        value (str): URL of the image.\n\n    Returns:\n        Image.Image: The loaded PIL image.\n    \"\"\"\n    _ensure_url_input_allowed()\n    try:\n        parsed_url = urllib.parse.urlparse(value)\n    except ValueError as error:\n        message = \"Provided image URL is invalid\"\n        raise InputImageLoadError(\n            message=message,\n            public_message=message,\n        ) from error\n    _ensure_resource_schema_allowed(schema=parsed_url.scheme)\n    domain_extraction_result = tldextract.TLDExtract(suffix_list_urls=())(\n        parsed_url.netloc\n    )  # we get rid of potential ports and parse FQDNs\n    _ensure_resource_fqdn_allowed(fqdn=domain_extraction_result.fqdn)\n    address_parts_concatenated = _concatenate_chunks_of_network_location(\n        extraction_result=domain_extraction_result\n    )  # concatenation of chunks - even if there is no FQDN, but address\n    # it allows white-/black-list verification\n    _ensure_location_matches_destination_whitelist(\n        destination=address_parts_concatenated\n    )\n    _ensure_location_matches_destination_blacklist(\n        destination=address_parts_concatenated\n    )\n    try:\n        response = requests.get(value, stream=True)\n        api_key_safe_raise_for_status(response=response)\n        return load_image_from_encoded_bytes(\n            value=response.content, cv_imread_flags=cv_imread_flags\n        )\n    except (RequestException, ConnectionError) as error:\n        raise InputImageLoadError(\n            message=f\"Could not load image from url: {value}. Details: {error}\",\n            public_message=\"Data pointed by URL could not be decoded into image.\",\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_with_inferred_type","title":"<code>load_image_with_inferred_type(value, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Load an image by inferring its type.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The image data.</p> required <code>cv_imread_flags</code> <code>int</code> <p>Flags used for OpenCV's imread function.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Tuple[np.ndarray, bool]: Loaded image as a numpy array and a boolean indicating if the image is in BGR format.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the image type could not be inferred.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_with_inferred_type(\n    value: Any,\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; Tuple[np.ndarray, bool]:\n    \"\"\"Load an image by inferring its type.\n\n    Args:\n        value (Any): The image data.\n        cv_imread_flags (int): Flags used for OpenCV's imread function.\n\n    Returns:\n        Tuple[np.ndarray, bool]: Loaded image as a numpy array and a boolean indicating if the image is in BGR format.\n\n    Raises:\n        NotImplementedError: If the image type could not be inferred.\n    \"\"\"\n    if isinstance(value, (np.ndarray, np.generic)):\n        validate_numpy_image(data=value)\n        return value, True\n    elif isinstance(value, Image.Image):\n        return np.asarray(value.convert(\"RGB\")), False\n    elif isinstance(value, str) and (value.startswith(\"http\")):\n        return load_image_from_url(value=value, cv_imread_flags=cv_imread_flags), True\n    elif isinstance(value, str) and os.path.isfile(value):\n        return cv2.imread(value, cv_imread_flags), True\n    else:\n        return attempt_loading_image_from_string(\n            value=value, cv_imread_flags=cv_imread_flags\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.load_image_with_known_type","title":"<code>load_image_with_known_type(value, image_type, cv_imread_flags=cv2.IMREAD_COLOR)</code>","text":"<p>Load an image using the known image type.</p> <p>Supports various image types (e.g., NUMPY, PILLOW, etc.) and loads them into a numpy array format.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The image data.</p> required <code>image_type</code> <code>ImageType</code> <p>The type of the image.</p> required <code>cv_imread_flags</code> <code>int</code> <p>Flags used for OpenCV's imread function.</p> <code>IMREAD_COLOR</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, bool]</code> <p>Tuple[np.ndarray, bool]: A tuple of the loaded image as a numpy array and a boolean indicating if the image is in BGR format.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def load_image_with_known_type(\n    value: Any,\n    image_type: ImageType,\n    cv_imread_flags: int = cv2.IMREAD_COLOR,\n) -&gt; Tuple[np.ndarray, bool]:\n    \"\"\"Load an image using the known image type.\n\n    Supports various image types (e.g., NUMPY, PILLOW, etc.) and loads them into a numpy array format.\n\n    Args:\n        value (Any): The image data.\n        image_type (ImageType): The type of the image.\n        cv_imread_flags (int): Flags used for OpenCV's imread function.\n\n    Returns:\n        Tuple[np.ndarray, bool]: A tuple of the loaded image as a numpy array and a boolean indicating if the image is in BGR format.\n    \"\"\"\n    loader = IMAGE_LOADERS[image_type]\n    is_bgr = True if image_type is not ImageType.PILLOW else False\n    image = loader(value, cv_imread_flags)\n    return image, is_bgr\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.np_image_to_base64","title":"<code>np_image_to_base64(image)</code>","text":"<p>TODO: This function is broken: https://github.com/roboflow/inference/issues/439 Convert a numpy image to a base64 encoded byte string.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The numpy array representing an image.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The base64 encoded image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>@deprecated(\n    reason=\"Method replaced with inference.core.utils.image_utils.encode_image_to_jpeg_bytes\"\n)\ndef np_image_to_base64(image: np.ndarray) -&gt; bytes:\n    \"\"\"\n    TODO: This function is broken: https://github.com/roboflow/inference/issues/439\n    Convert a numpy image to a base64 encoded byte string.\n\n    Args:\n        image (np.ndarray): The numpy array representing an image.\n\n    Returns:\n        bytes: The base64 encoded image.\n    \"\"\"\n    image = Image.fromarray(image)\n    with BytesIO() as buffer:\n        image = image.convert(\"RGB\")\n        image.save(buffer, format=\"JPEG\")\n        buffer.seek(0)\n        return buffer.getvalue()\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.validate_numpy_image","title":"<code>validate_numpy_image(data)</code>","text":"<p>Validate if the provided data is a valid numpy image.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The numpy array representing an image.</p> required <p>Raises:</p> Type Description <code>InvalidNumpyInput</code> <p>If the provided data is not a valid numpy image.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def validate_numpy_image(data: np.ndarray) -&gt; None:\n    \"\"\"\n    Validate if the provided data is a valid numpy image.\n\n    Args:\n        data (np.ndarray): The numpy array representing an image.\n\n    Raises:\n        InvalidNumpyInput: If the provided data is not a valid numpy image.\n    \"\"\"\n    if not issubclass(type(data), np.ndarray):\n        raise InvalidNumpyInput(\n            message=f\"Data provided as input could not be decoded into np.ndarray object.\",\n            public_message=f\"Data provided as input could not be decoded into np.ndarray object.\",\n        )\n    if len(data.shape) != 3 and len(data.shape) != 2:\n        raise InvalidNumpyInput(\n            message=f\"For image given as np.ndarray expected 2 or 3 dimensions, got {len(data.shape)} dimensions.\",\n            public_message=f\"For image given as np.ndarray expected 2 or 3 dimensions.\",\n        )\n    if data.shape[-1] != 3 and data.shape[-1] != 1:\n        raise InvalidNumpyInput(\n            message=f\"For image given as np.ndarray expected 1 or 3 channels, got {data.shape[-1]} channels.\",\n            public_message=\"For image given as np.ndarray expected 1 or 3 channels.\",\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/image_utils/#inference.core.utils.image_utils.xyxy_to_xywh","title":"<code>xyxy_to_xywh(xyxy)</code>","text":"<p>Convert bounding box format from (xmin, ymin, xmax, ymax) to (xcenter, ycenter, width, height).</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>List[int]</code> <p>List containing the coordinates in (xmin, ymin, xmax, ymax) format.</p> required <p>Returns:</p> Type Description <p>List[int]: List containing the converted coordinates in (xcenter, ycenter, width, height) format.</p> Source code in <code>inference/core/utils/image_utils.py</code> <pre><code>def xyxy_to_xywh(xyxy):\n    \"\"\"\n    Convert bounding box format from (xmin, ymin, xmax, ymax) to (xcenter, ycenter, width, height).\n\n    Args:\n        xyxy (List[int]): List containing the coordinates in (xmin, ymin, xmax, ymax) format.\n\n    Returns:\n        List[int]: List containing the converted coordinates in (xcenter, ycenter, width, height) format.\n    \"\"\"\n    x_temp = (xyxy[0] + xyxy[2]) / 2\n    y_temp = (xyxy[1] + xyxy[3]) / 2\n    w_temp = abs(xyxy[0] - xyxy[2])\n    h_temp = abs(xyxy[1] - xyxy[3])\n\n    return [int(x_temp), int(y_temp), int(w_temp), int(h_temp)]\n</code></pre>"},{"location":"docs/reference/inference/core/utils/notebooks/","title":"notebooks","text":""},{"location":"docs/reference/inference/core/utils/onnx/","title":"onnx","text":""},{"location":"docs/reference/inference/core/utils/onnx/#inference.core.utils.onnx.get_onnxruntime_execution_providers","title":"<code>get_onnxruntime_execution_providers(value)</code>","text":"<p>Extracts the ONNX runtime execution providers from the given string.</p> <p>The input string is expected to be a comma-separated list, possibly enclosed within square brackets and containing single quotes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The string containing the list of ONNX runtime execution providers.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings representing each execution provider.</p> Source code in <code>inference/core/utils/onnx.py</code> <pre><code>def get_onnxruntime_execution_providers(value: str) -&gt; List[str]:\n    \"\"\"Extracts the ONNX runtime execution providers from the given string.\n\n    The input string is expected to be a comma-separated list, possibly enclosed\n    within square brackets and containing single quotes.\n\n    Args:\n        value (str): The string containing the list of ONNX runtime execution providers.\n\n    Returns:\n        List[str]: A list of strings representing each execution provider.\n    \"\"\"\n    if len(value) == 0:\n        return []\n    value = value.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").replace(\" \", \"\")\n    return value.split(\",\")\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/","title":"postprocess","text":""},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.cosine_similarity","title":"<code>cosine_similarity(a, b)</code>","text":"<p>Compute the cosine similarity between two vectors.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>Vector A.</p> required <code>b</code> <code>ndarray</code> <p>Vector B.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>Union[number, ndarray]</code> <p>Cosine similarity between vectors A and B.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def cosine_similarity(a: np.ndarray, b: np.ndarray) -&gt; Union[np.number, np.ndarray]:\n    \"\"\"\n    Compute the cosine similarity between two vectors.\n\n    Args:\n        a (np.ndarray): Vector A.\n        b (np.ndarray): Vector B.\n\n    Returns:\n        float: Cosine similarity between vectors A and B.\n    \"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.crop_mask","title":"<code>crop_mask(masks, boxes)</code>","text":"<p>\"Crop\" predicted masks by zeroing out everything not in the predicted bbox. Vectorized by Chong (thanks Chong).</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def crop_mask(masks: np.ndarray, boxes: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    \"Crop\" predicted masks by zeroing out everything not in the predicted bbox.\n    Vectorized by Chong (thanks Chong).\n\n    Args:\n        - masks should be a size [h, w, n] tensor of masks\n        - boxes should be a size [n, 4] tensor of bbox coords in relative point form\n    \"\"\"\n\n    n, h, w = masks.shape\n    x1, y1, x2, y2 = np.split(boxes[:, :, None], 4, 1)  # x1 shape(1,1,n)\n    r = np.arange(w, dtype=x1.dtype)[None, None, :]  # rows shape(1,w,1)\n    c = np.arange(h, dtype=x1.dtype)[None, :, None]  # cols shape(h,1,1)\n\n    masks = masks * ((r &gt;= x1) * (r &lt; x2) * (c &gt;= y1) * (c &lt; y2))\n    return masks\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.get_static_crop_dimensions","title":"<code>get_static_crop_dimensions(orig_shape, preproc, disable_preproc_static_crop=False)</code>","text":"<p>Generates a transformation based on preprocessing configuration.</p> <p>Parameters:</p> Name Type Description Default <code>orig_shape</code> <code>tuple</code> <p>The original shape of the object (e.g., image) - (height, width).</p> required <code>preproc</code> <code>dict</code> <p>Preprocessing configuration dictionary, containing information such as static cropping.</p> required <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Tuple[int, int], Tuple[int, int]]</code> <p>A tuple containing the shift in the x and y directions, and the updated original shape after cropping.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def get_static_crop_dimensions(\n    orig_shape: Tuple[int, int],\n    preproc: dict,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[Tuple[int, int], Tuple[int, int]]:\n    \"\"\"\n    Generates a transformation based on preprocessing configuration.\n\n    Args:\n        orig_shape (tuple): The original shape of the object (e.g., image) - (height, width).\n        preproc (dict): Preprocessing configuration dictionary, containing information such as static cropping.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        tuple: A tuple containing the shift in the x and y directions, and the updated original shape after cropping.\n    \"\"\"\n    try:\n        if static_crop_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        ):\n            x_min, y_min, x_max, y_max = standardise_static_crop(\n                static_crop_config=preproc[STATIC_CROP_KEY]\n            )\n        else:\n            x_min, y_min, x_max, y_max = 0, 0, 1, 1\n        crop_shift_x, crop_shift_y = (\n            round(x_min * orig_shape[1]),\n            round(y_min * orig_shape[0]),\n        )\n        cropped_percent_x = x_max - x_min\n        cropped_percent_y = y_max - y_min\n        orig_shape = (\n            round(orig_shape[0] * cropped_percent_y),\n            round(orig_shape[1] * cropped_percent_x),\n        )\n        return (crop_shift_x, crop_shift_y), orig_shape\n    except KeyError as error:\n        raise PostProcessingError(\n            f\"Could not find a proper configuration key {error} in post-processing.\"\n        )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.mask2multipoly","title":"<code>mask2multipoly(mask)</code>","text":"<p>Find all contours in the mask and return them as a float32 array.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>A binary mask.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Contours represented as a float32 array.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def mask2multipoly(mask: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Find all contours in the mask and return them as a float32 array.\n\n    Args:\n        mask (np.ndarray): A binary mask.\n\n    Returns:\n        np.ndarray: Contours represented as a float32 array.\n    \"\"\"\n    contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n    if contours:\n        contours = [c.reshape(-1, 2).astype(\"float32\") for c in contours]\n    else:\n        contours = [np.zeros((0, 2)).astype(\"float32\")]\n    return contours\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.mask2poly","title":"<code>mask2poly(mask)</code>","text":"<p>Find contours in the mask and return them as a float32 array.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>A binary mask.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Contours represented as a float32 array.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def mask2poly(mask: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Find contours in the mask and return them as a float32 array.\n\n    Args:\n        mask (np.ndarray): A binary mask.\n\n    Returns:\n        np.ndarray: Contours represented as a float32 array.\n    \"\"\"\n    contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n    if contours:\n        contours = np.array(\n            contours[np.array([len(x) for x in contours]).argmax()]\n        ).reshape(-1, 2)\n    else:\n        contours = np.zeros((0, 2))\n    return contours.astype(\"float32\")\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.masks2multipoly","title":"<code>masks2multipoly(masks)</code>","text":"<p>Converts binary masks to polygonal segments.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>A set of binary masks, where masks are multiplied by 255 and converted to uint8 type.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[ndarray]</code> <p>A list of segments, where each segment is obtained by converting the corresponding mask.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def masks2multipoly(masks: np.ndarray) -&gt; List[np.ndarray]:\n    \"\"\"Converts binary masks to polygonal segments.\n\n    Args:\n        masks (numpy.ndarray): A set of binary masks, where masks are multiplied by 255 and converted to uint8 type.\n\n    Returns:\n        list: A list of segments, where each segment is obtained by converting the corresponding mask.\n    \"\"\"\n    segments = []\n    masks = (masks * 255.0).astype(np.uint8)\n    for mask in masks:\n        segments.append(mask2multipoly(mask))\n    return segments\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.masks2poly","title":"<code>masks2poly(masks)</code>","text":"<p>Converts binary masks to polygonal segments.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>A set of binary masks, where masks are multiplied by 255 and converted to uint8 type.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[ndarray]</code> <p>A list of segments, where each segment is obtained by converting the corresponding mask.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def masks2poly(masks: np.ndarray) -&gt; List[np.ndarray]:\n    \"\"\"Converts binary masks to polygonal segments.\n\n    Args:\n        masks (numpy.ndarray): A set of binary masks, where masks are multiplied by 255 and converted to uint8 type.\n\n    Returns:\n        list: A list of segments, where each segment is obtained by converting the corresponding mask.\n    \"\"\"\n    segments = []\n    masks = (masks * 255.0).astype(np.uint8)\n    for mask in masks:\n        segments.append(mask2poly(mask))\n    return segments\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.post_process_bboxes","title":"<code>post_process_bboxes(predictions, infer_shape, img_dims, preproc, disable_preproc_static_crop=False, resize_method='Stretch to')</code>","text":"<p>Postprocesses each patch of detections by scaling them to the original image coordinates and by shifting them based on a static crop preproc (if applied).</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[List[float]]]</code> <p>The predictions output from NMS, indices are: batch x prediction x [x1, y1, x2, y2, ...].</p> required <code>infer_shape</code> <code>Tuple[int, int]</code> <p>The shape of the inference image.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>The dimensions of the original image for each batch, indices are: batch x [height, width].</p> required <code>preproc</code> <code>dict</code> <p>Preprocessing configuration dictionary.</p> required <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>resize_method</code> <code>str</code> <p>Resize method for image. Defaults to \"Stretch to\".</p> <code>'Stretch to'</code> <p>Returns:</p> Type Description <code>List[List[List[float]]]</code> <p>List[List[List[float]]]: The scaled and shifted predictions, indices are: batch x prediction x [x1, y1, x2, y2, ...].</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def post_process_bboxes(\n    predictions: List[List[List[float]]],\n    infer_shape: Tuple[int, int],\n    img_dims: List[Tuple[int, int]],\n    preproc: dict,\n    disable_preproc_static_crop: bool = False,\n    resize_method: str = \"Stretch to\",\n) -&gt; List[List[List[float]]]:\n    \"\"\"\n    Postprocesses each patch of detections by scaling them to the original image coordinates and by shifting them based on a static crop preproc (if applied).\n\n    Args:\n        predictions (List[List[List[float]]]): The predictions output from NMS, indices are: batch x prediction x [x1, y1, x2, y2, ...].\n        infer_shape (Tuple[int, int]): The shape of the inference image.\n        img_dims (List[Tuple[int, int]]): The dimensions of the original image for each batch, indices are: batch x [height, width].\n        preproc (dict): Preprocessing configuration dictionary.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n        resize_method (str, optional): Resize method for image. Defaults to \"Stretch to\".\n\n    Returns:\n        List[List[List[float]]]: The scaled and shifted predictions, indices are: batch x prediction x [x1, y1, x2, y2, ...].\n    \"\"\"\n\n    # Get static crop params\n    scaled_predictions = []\n    # Loop through batches\n    for i, batch_predictions in enumerate(predictions):\n        if len(batch_predictions) == 0:\n            scaled_predictions.append([])\n            continue\n        np_batch_predictions = np.array(batch_predictions)\n        # Get bboxes from predictions (x1,y1,x2,y2)\n        predicted_bboxes = np_batch_predictions[:, :4]\n        (crop_shift_x, crop_shift_y), origin_shape = get_static_crop_dimensions(\n            img_dims[i],\n            preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n        if resize_method == \"Stretch to\":\n            predicted_bboxes = stretch_bboxes(\n                predicted_bboxes=predicted_bboxes,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        elif (\n            resize_method == \"Fit (black edges) in\"\n            or resize_method == \"Fit (white edges) in\"\n            or resize_method == \"Fit (grey edges) in\"\n        ):\n            predicted_bboxes = undo_image_padding_for_predicted_boxes(\n                predicted_bboxes=predicted_bboxes,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        predicted_bboxes = clip_boxes_coordinates(\n            predicted_bboxes=predicted_bboxes,\n            origin_shape=origin_shape,\n        )\n        predicted_bboxes = shift_bboxes(\n            bboxes=predicted_bboxes,\n            shift_x=crop_shift_x,\n            shift_y=crop_shift_y,\n        )\n        np_batch_predictions[:, :4] = predicted_bboxes\n        scaled_predictions.append(np_batch_predictions.tolist())\n    return scaled_predictions\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.post_process_keypoints","title":"<code>post_process_keypoints(predictions, keypoints_start_index, infer_shape, img_dims, preproc, disable_preproc_static_crop=False, resize_method='Stretch to')</code>","text":"<p>Scales and shifts keypoints based on the given image shapes and preprocessing method.</p> <p>This function performs polygon scaling and shifting based on the specified resizing method and pre-processing steps. The polygons are transformed according to the ratio and padding between two images.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[List[float]]]</code> <p>predictions from model</p> required <code>keypoints_start_index</code> <code>int</code> <p>offset in the 3rd dimension pointing where in the prediction start keypoints [(x, y, cfg), ...] for each keypoint class</p> required <code>img_dims</code> <code>list of (tuple of int</code> <p>Shape of the source image (height, width).</p> required <code>infer_shape</code> <code>tuple of int</code> <p>Shape of the target image (height, width).</p> required <code>preproc</code> <code>object</code> <p>Preprocessing details used for generating the transformation.</p> required <code>resize_method</code> <code>str</code> <p>Resizing method, either \"Stretch to\", \"Fit (black edges) in\", \"Fit (white edges) in\", or \"Fit (grey edges) in\". Defaults to \"Stretch to\".</p> <code>'Stretch to'</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>flag to disable static crop</p> <code>False</code> <p>Returns:     list of list of list: predictions with post-processed keypoints</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def post_process_keypoints(\n    predictions: List[List[List[float]]],\n    keypoints_start_index: int,\n    infer_shape: Tuple[int, int],\n    img_dims: List[Tuple[int, int]],\n    preproc: dict,\n    disable_preproc_static_crop: bool = False,\n    resize_method: str = \"Stretch to\",\n) -&gt; List[List[List[float]]]:\n    \"\"\"Scales and shifts keypoints based on the given image shapes and preprocessing method.\n\n    This function performs polygon scaling and shifting based on the specified resizing method and\n    pre-processing steps. The polygons are transformed according to the ratio and padding between two images.\n\n    Args:\n        predictions: predictions from model\n        keypoints_start_index: offset in the 3rd dimension pointing where in the prediction start keypoints [(x, y, cfg), ...] for each keypoint class\n        img_dims list of (tuple of int): Shape of the source image (height, width).\n        infer_shape (tuple of int): Shape of the target image (height, width).\n        preproc (object): Preprocessing details used for generating the transformation.\n        resize_method (str, optional): Resizing method, either \"Stretch to\", \"Fit (black edges) in\", \"Fit (white edges) in\", or \"Fit (grey edges) in\". Defaults to \"Stretch to\".\n        disable_preproc_static_crop: flag to disable static crop\n    Returns:\n        list of list of list: predictions with post-processed keypoints\n    \"\"\"\n    # Get static crop params\n    scaled_predictions = []\n    # Loop through batches\n    for i, batch_predictions in enumerate(predictions):\n        if len(batch_predictions) == 0:\n            scaled_predictions.append([])\n            continue\n        np_batch_predictions = np.array(batch_predictions)\n        keypoints = np_batch_predictions[:, keypoints_start_index:]\n        (crop_shift_x, crop_shift_y), origin_shape = get_static_crop_dimensions(\n            img_dims[i],\n            preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        )\n        if resize_method == \"Stretch to\":\n            keypoints = stretch_keypoints(\n                keypoints=keypoints,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        elif (\n            resize_method == \"Fit (black edges) in\"\n            or resize_method == \"Fit (white edges) in\"\n            or resize_method == \"Fit (grey edges) in\"\n        ):\n            keypoints = undo_image_padding_for_predicted_keypoints(\n                keypoints=keypoints,\n                infer_shape=infer_shape,\n                origin_shape=origin_shape,\n            )\n        keypoints = clip_keypoints_coordinates(\n            keypoints=keypoints, origin_shape=origin_shape\n        )\n        keypoints = shift_keypoints(\n            keypoints=keypoints, shift_x=crop_shift_x, shift_y=crop_shift_y\n        )\n        np_batch_predictions[:, keypoints_start_index:] = keypoints\n        scaled_predictions.append(np_batch_predictions.tolist())\n    return scaled_predictions\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.post_process_polygons","title":"<code>post_process_polygons(origin_shape, polys, infer_shape, preproc, resize_method='Stretch to')</code>","text":"<p>Scales and shifts polygons based on the given image shapes and preprocessing method.</p> <p>This function performs polygon scaling and shifting based on the specified resizing method and pre-processing steps. The polygons are transformed according to the ratio and padding between two images.</p> <p>Parameters:</p> Name Type Description Default <code>origin_shape</code> <code>tuple of int</code> <p>Shape of the source image (height, width).</p> required <code>infer_shape</code> <code>tuple of int</code> <p>Shape of the target image (height, width).</p> required <code>polys</code> <code>list of list of tuple</code> <p>List of polygons, where each polygon is represented by a list of (x, y) coordinates.</p> required <code>preproc</code> <code>object</code> <p>Preprocessing details used for generating the transformation.</p> required <code>resize_method</code> <code>str</code> <p>Resizing method, either \"Stretch to\", \"Fit (black edges) in\", \"Fit (white edges) in\", or \"Fit (grey edges) in\". Defaults to \"Stretch to\".</p> <code>'Stretch to'</code> <p>Returns:</p> Type Description <code>List[List[Tuple[float, float]]]</code> <p>list of list of tuple: A list of shifted and scaled polygons.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def post_process_polygons(\n    origin_shape: Tuple[int, int],\n    polys: List[List[Tuple[float, float]]],\n    infer_shape: Tuple[int, int],\n    preproc: dict,\n    resize_method: str = \"Stretch to\",\n) -&gt; List[List[Tuple[float, float]]]:\n    \"\"\"Scales and shifts polygons based on the given image shapes and preprocessing method.\n\n    This function performs polygon scaling and shifting based on the specified resizing method and\n    pre-processing steps. The polygons are transformed according to the ratio and padding between two images.\n\n    Args:\n        origin_shape (tuple of int): Shape of the source image (height, width).\n        infer_shape (tuple of int): Shape of the target image (height, width).\n        polys (list of list of tuple): List of polygons, where each polygon is represented by a list of (x, y) coordinates.\n        preproc (object): Preprocessing details used for generating the transformation.\n        resize_method (str, optional): Resizing method, either \"Stretch to\", \"Fit (black edges) in\", \"Fit (white edges) in\", or \"Fit (grey edges) in\". Defaults to \"Stretch to\".\n\n    Returns:\n        list of list of tuple: A list of shifted and scaled polygons.\n    \"\"\"\n    (crop_shift_x, crop_shift_y), origin_shape = get_static_crop_dimensions(\n        origin_shape, preproc\n    )\n    new_polys = []\n    if resize_method == \"Stretch to\":\n        width_ratio = origin_shape[1] / infer_shape[1]\n        height_ratio = origin_shape[0] / infer_shape[0]\n        new_polys = scale_polygons(\n            polygons=polys,\n            x_scale=width_ratio,\n            y_scale=height_ratio,\n        )\n    elif resize_method in {\n        \"Fit (black edges) in\",\n        \"Fit (white edges) in\",\n        \"Fit (grey edges) in\",\n    }:\n        new_polys = undo_image_padding_for_predicted_polygons(\n            polygons=polys,\n            infer_shape=infer_shape,\n            origin_shape=origin_shape,\n        )\n    shifted_polys = []\n    for poly in new_polys:\n        poly = [(p[0] + crop_shift_x, p[1] + crop_shift_y) for p in poly]\n        shifted_polys.append(poly)\n    return shifted_polys\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.process_mask_accurate","title":"<code>process_mask_accurate(protos, masks_in, bboxes, shape)</code>","text":"<p>Returns masks that are the size of the original image.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>ndarray</code> <p>Prototype masks.</p> required <code>masks_in</code> <code>ndarray</code> <p>Input masks.</p> required <code>bboxes</code> <code>ndarray</code> <p>Bounding boxes.</p> required <code>shape</code> <code>tuple</code> <p>Target shape.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Processed masks.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def process_mask_accurate(\n    protos: np.ndarray,\n    masks_in: np.ndarray,\n    bboxes: np.ndarray,\n    shape: Tuple[int, int],\n) -&gt; np.ndarray:\n    \"\"\"Returns masks that are the size of the original image.\n\n    Args:\n        protos (numpy.ndarray): Prototype masks.\n        masks_in (numpy.ndarray): Input masks.\n        bboxes (numpy.ndarray): Bounding boxes.\n        shape (tuple): Target shape.\n\n    Returns:\n        numpy.ndarray: Processed masks.\n    \"\"\"\n    masks = preprocess_segmentation_masks(\n        protos=protos,\n        masks_in=masks_in,\n        shape=shape,\n    )\n\n    # Order = 1 -&gt; bilinear\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=0)\n    masks = masks.transpose((1, 2, 0))\n    masks = cv2.resize(masks, (shape[1], shape[0]), cv2.INTER_LINEAR)\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=2)\n    masks = masks.transpose((2, 0, 1))\n    masks = crop_mask(masks, bboxes)\n    masks[masks &lt; 0.5] = 0\n    return masks\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.process_mask_fast","title":"<code>process_mask_fast(protos, masks_in, bboxes, shape)</code>","text":"<p>Returns masks in their original size.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>ndarray</code> <p>Prototype masks.</p> required <code>masks_in</code> <code>ndarray</code> <p>Input masks.</p> required <code>bboxes</code> <code>ndarray</code> <p>Bounding boxes.</p> required <code>shape</code> <code>tuple</code> <p>Target shape.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Processed masks.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def process_mask_fast(\n    protos: np.ndarray,\n    masks_in: np.ndarray,\n    bboxes: np.ndarray,\n    shape: Tuple[int, int],\n) -&gt; np.ndarray:\n    \"\"\"Returns masks in their original size.\n\n    Args:\n        protos (numpy.ndarray): Prototype masks.\n        masks_in (numpy.ndarray): Input masks.\n        bboxes (numpy.ndarray): Bounding boxes.\n        shape (tuple): Target shape.\n\n    Returns:\n        numpy.ndarray: Processed masks.\n    \"\"\"\n    ih, iw = shape\n    c, mh, mw = protos.shape  # CHW\n    masks = preprocess_segmentation_masks(\n        protos=protos,\n        masks_in=masks_in,\n        shape=shape,\n    )\n    down_sampled_boxes = scale_bboxes(\n        bboxes=deepcopy(bboxes),\n        scale_x=mw / iw,\n        scale_y=mh / ih,\n    )\n    masks = crop_mask(masks, down_sampled_boxes)\n    masks[masks &lt; 0.5] = 0\n    return masks\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.process_mask_tradeoff","title":"<code>process_mask_tradeoff(protos, masks_in, bboxes, shape, tradeoff_factor)</code>","text":"<p>Returns masks that are the size of the original image with a tradeoff factor applied.</p> <p>Parameters:</p> Name Type Description Default <code>protos</code> <code>ndarray</code> <p>Prototype masks.</p> required <code>masks_in</code> <code>ndarray</code> <p>Input masks.</p> required <code>bboxes</code> <code>ndarray</code> <p>Bounding boxes.</p> required <code>shape</code> <code>tuple</code> <p>Target shape.</p> required <code>tradeoff_factor</code> <code>float</code> <p>Tradeoff factor for resizing masks.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: Processed masks.</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def process_mask_tradeoff(\n    protos: np.ndarray,\n    masks_in: np.ndarray,\n    bboxes: np.ndarray,\n    shape: Tuple[int, int],\n    tradeoff_factor: float,\n) -&gt; np.ndarray:\n    \"\"\"Returns masks that are the size of the original image with a tradeoff factor applied.\n\n    Args:\n        protos (numpy.ndarray): Prototype masks.\n        masks_in (numpy.ndarray): Input masks.\n        bboxes (numpy.ndarray): Bounding boxes.\n        shape (tuple): Target shape.\n        tradeoff_factor (float): Tradeoff factor for resizing masks.\n\n    Returns:\n        numpy.ndarray: Processed masks.\n    \"\"\"\n    c, mh, mw = protos.shape  # CHW\n    masks = preprocess_segmentation_masks(\n        protos=protos,\n        masks_in=masks_in,\n        shape=shape,\n    )\n\n    # Order = 1 -&gt; bilinear\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=0)\n    masks = masks.transpose((1, 2, 0))\n    ih, iw = shape\n    h = int(mh * (1 - tradeoff_factor) + ih * tradeoff_factor)\n    w = int(mw * (1 - tradeoff_factor) + iw * tradeoff_factor)\n    size = (h, w)\n    if tradeoff_factor != 0:\n        masks = cv2.resize(masks, size, cv2.INTER_LINEAR)\n    if len(masks.shape) == 2:\n        masks = np.expand_dims(masks, axis=2)\n    masks = masks.transpose((2, 0, 1))\n    c, mh, mw = masks.shape\n    down_sampled_boxes = scale_bboxes(\n        bboxes=deepcopy(bboxes),\n        scale_x=mw / iw,\n        scale_y=mh / ih,\n    )\n    masks = crop_mask(masks, down_sampled_boxes)\n    masks[masks &lt; 0.5] = 0\n    return masks\n</code></pre>"},{"location":"docs/reference/inference/core/utils/postprocess/#inference.core.utils.postprocess.sigmoid","title":"<code>sigmoid(x)</code>","text":"<p>Computes the sigmoid function for the given input.</p> <p>The sigmoid function is defined as: f(x) = 1 / (1 + exp(-x))</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float or ndarray</code> <p>Input value or array for which the sigmoid function is to be computed.</p> required <p>Returns:</p> Type Description <code>Union[float, number, ndarray]</code> <p>float or numpy.ndarray: The computed sigmoid value(s).</p> Source code in <code>inference/core/utils/postprocess.py</code> <pre><code>def sigmoid(x: Union[float, np.ndarray]) -&gt; Union[float, np.number, np.ndarray]:\n    \"\"\"Computes the sigmoid function for the given input.\n\n    The sigmoid function is defined as:\n    f(x) = 1 / (1 + exp(-x))\n\n    Args:\n        x (float or numpy.ndarray): Input value or array for which the sigmoid function is to be computed.\n\n    Returns:\n        float or numpy.ndarray: The computed sigmoid value(s).\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n</code></pre>"},{"location":"docs/reference/inference/core/utils/preprocess/","title":"preprocess","text":""},{"location":"docs/reference/inference/core/utils/preprocess/#inference.core.utils.preprocess.letterbox_image","title":"<code>letterbox_image(image, desired_size, color=(0, 0, 0))</code>","text":"<p>Resize and pad image to fit the desired size, preserving its aspect ratio.</p> <p>Parameters: - image: numpy array representing the image. - desired_size: tuple (width, height) representing the target dimensions. - color: tuple (B, G, R) representing the color to pad with.</p> <p>Returns: - letterboxed image.</p> Source code in <code>inference/core/utils/preprocess.py</code> <pre><code>def letterbox_image(\n    image: np.ndarray,\n    desired_size: Tuple[int, int],\n    color: Tuple[int, int, int] = (0, 0, 0),\n) -&gt; np.ndarray:\n    \"\"\"\n    Resize and pad image to fit the desired size, preserving its aspect ratio.\n\n    Parameters:\n    - image: numpy array representing the image.\n    - desired_size: tuple (width, height) representing the target dimensions.\n    - color: tuple (B, G, R) representing the color to pad with.\n\n    Returns:\n    - letterboxed image.\n    \"\"\"\n    resized_img = resize_image_keeping_aspect_ratio(\n        image=image,\n        desired_size=desired_size,\n    )\n    new_height, new_width = resized_img.shape[:2]\n    top_padding = (desired_size[1] - new_height) // 2\n    bottom_padding = desired_size[1] - new_height - top_padding\n    left_padding = (desired_size[0] - new_width) // 2\n    right_padding = desired_size[0] - new_width - left_padding\n    return cv2.copyMakeBorder(\n        resized_img,\n        top_padding,\n        bottom_padding,\n        left_padding,\n        right_padding,\n        cv2.BORDER_CONSTANT,\n        value=color,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/utils/preprocess/#inference.core.utils.preprocess.prepare","title":"<code>prepare(image, preproc, disable_preproc_contrast=False, disable_preproc_grayscale=False, disable_preproc_static_crop=False)</code>","text":"<p>Prepares an image by applying a series of preprocessing steps defined in the <code>preproc</code> dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input PIL image object.</p> required <code>preproc</code> <code>dict</code> <p>Dictionary containing preprocessing steps. Example: {     \"resize\": {\"enabled\": true, \"width\": 416, \"height\": 416, \"format\": \"Stretch to\"},     \"static-crop\": {\"y_min\": 25, \"x_max\": 75, \"y_max\": 75, \"enabled\": true, \"x_min\": 25},     \"auto-orient\": {\"enabled\": true},     \"grayscale\": {\"enabled\": true},     \"contrast\": {\"enabled\": true, \"type\": \"Adaptive Equalization\"} }</p> required <code>disable_preproc_contrast</code> <code>bool</code> <p>If true, the contrast preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_grayscale</code> <code>bool</code> <p>If true, the grayscale preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <code>disable_preproc_static_crop</code> <code>bool</code> <p>If true, the static crop preprocessing step is disabled for this call. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>ndarray</code> <p>PIL.Image.Image: The preprocessed image object.</p> <code>tuple</code> <code>Tuple[int, int]</code> <p>The dimensions of the image.</p> Note <p>The function uses global flags like <code>DISABLE_PREPROC_AUTO_ORIENT</code>, <code>DISABLE_PREPROC_STATIC_CROP</code>, etc. to conditionally enable or disable certain preprocessing steps.</p> Source code in <code>inference/core/utils/preprocess.py</code> <pre><code>def prepare(\n    image: np.ndarray,\n    preproc,\n    disable_preproc_contrast: bool = False,\n    disable_preproc_grayscale: bool = False,\n    disable_preproc_static_crop: bool = False,\n) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n    \"\"\"\n    Prepares an image by applying a series of preprocessing steps defined in the `preproc` dictionary.\n\n    Args:\n        image (PIL.Image.Image): The input PIL image object.\n        preproc (dict): Dictionary containing preprocessing steps. Example:\n            {\n                \"resize\": {\"enabled\": true, \"width\": 416, \"height\": 416, \"format\": \"Stretch to\"},\n                \"static-crop\": {\"y_min\": 25, \"x_max\": 75, \"y_max\": 75, \"enabled\": true, \"x_min\": 25},\n                \"auto-orient\": {\"enabled\": true},\n                \"grayscale\": {\"enabled\": true},\n                \"contrast\": {\"enabled\": true, \"type\": \"Adaptive Equalization\"}\n            }\n        disable_preproc_contrast (bool, optional): If true, the contrast preprocessing step is disabled for this call. Default is False.\n        disable_preproc_grayscale (bool, optional): If true, the grayscale preprocessing step is disabled for this call. Default is False.\n        disable_preproc_static_crop (bool, optional): If true, the static crop preprocessing step is disabled for this call. Default is False.\n\n    Returns:\n        PIL.Image.Image: The preprocessed image object.\n        tuple: The dimensions of the image.\n\n    Note:\n        The function uses global flags like `DISABLE_PREPROC_AUTO_ORIENT`, `DISABLE_PREPROC_STATIC_CROP`, etc.\n        to conditionally enable or disable certain preprocessing steps.\n    \"\"\"\n    try:\n        h, w = image.shape[0:2]\n        img_dims = (h, w)\n        if static_crop_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_static_crop=disable_preproc_static_crop,\n        ):\n            image = take_static_crop(\n                image=image, crop_parameters=preproc[STATIC_CROP_KEY]\n            )\n        if contrast_adjustments_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_contrast=disable_preproc_contrast,\n        ):\n            adjustment_type = ContrastAdjustmentType(preproc[CONTRAST_KEY][TYPE_KEY])\n            image = apply_contrast_adjustment(\n                image=image, adjustment_type=adjustment_type\n            )\n        if grayscale_conversion_should_be_applied(\n            preprocessing_config=preproc,\n            disable_preproc_grayscale=disable_preproc_grayscale,\n        ):\n            image = apply_grayscale_conversion(image=image)\n        return image, img_dims\n    except KeyError as error:\n        raise PreProcessingError(\n            f\"Pre-processing of image failed due to misconfiguration. Missing key: {error}.\"\n        ) from error\n</code></pre>"},{"location":"docs/reference/inference/core/utils/preprocess/#inference.core.utils.preprocess.resize_image_keeping_aspect_ratio","title":"<code>resize_image_keeping_aspect_ratio(image, desired_size)</code>","text":"<p>Resize reserving its aspect ratio.</p> <p>Parameters: - image: numpy array representing the image. - desired_size: tuple (width, height) representing the target dimensions.</p> Source code in <code>inference/core/utils/preprocess.py</code> <pre><code>def resize_image_keeping_aspect_ratio(\n    image: np.ndarray,\n    desired_size: Tuple[int, int],\n) -&gt; np.ndarray:\n    \"\"\"\n    Resize reserving its aspect ratio.\n\n    Parameters:\n    - image: numpy array representing the image.\n    - desired_size: tuple (width, height) representing the target dimensions.\n    \"\"\"\n    img_ratio = image.shape[1] / image.shape[0]\n    desired_ratio = desired_size[0] / desired_size[1]\n\n    # Determine the new dimensions\n    if img_ratio &gt;= desired_ratio:\n        # Resize by width\n        new_width = desired_size[0]\n        new_height = int(desired_size[0] / img_ratio)\n    else:\n        # Resize by height\n        new_height = desired_size[1]\n        new_width = int(desired_size[1] * img_ratio)\n\n    # Resize the image to new dimensions\n    return cv2.resize(image, (new_width, new_height))\n</code></pre>"},{"location":"docs/reference/inference/core/utils/requests/","title":"requests","text":""},{"location":"docs/reference/inference/core/utils/roboflow/","title":"roboflow","text":""},{"location":"docs/reference/inference/core/utils/s3/","title":"s3","text":""},{"location":"docs/reference/inference/core/utils/sqlite_wrapper/","title":"sqlite_wrapper","text":""},{"location":"docs/reference/inference/core/utils/url_utils/","title":"url_utils","text":""},{"location":"docs/reference/inference/core/utils/visualisation/","title":"visualisation","text":""},{"location":"docs/reference/inference/core/workflows/errors/","title":"errors","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/loader/","title":"loader","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/analytics/data_aggregator/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/analytics/line_counter/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/analytics/line_counter/v2/","title":"v2","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/analytics/path_deviation/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/analytics/path_deviation/v2/","title":"v2","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/analytics/time_in_zone/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/analytics/time_in_zone/v2/","title":"v2","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/camera_focus/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/camera_focus/v1/#inference.core.workflows.core_steps.classical_cv.camera_focus.v1.calculate_brenner_measure","title":"<code>calculate_brenner_measure(input_image, text_color=(255, 255, 255), text_thickness=2)</code>","text":"<p>Brenner's focus measure.</p>"},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/camera_focus/v1/#inference.core.workflows.core_steps.classical_cv.camera_focus.v1.calculate_brenner_measure--parameters","title":"Parameters","text":"<p>input_image : np.ndarray     The input image in grayscale. text_color : Tuple[int, int, int], optional     The color of the text displaying the Brenner value, in BGR format. Default is white (255, 255, 255). text_thickness : int, optional     The thickness of the text displaying the Brenner value. Default is 2.</p>"},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/camera_focus/v1/#inference.core.workflows.core_steps.classical_cv.camera_focus.v1.calculate_brenner_measure--returns","title":"Returns","text":"<p>Tuple[np.ndarray, float]     The Brenner image and the Brenner value.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/camera_focus/v1.py</code> <pre><code>def calculate_brenner_measure(\n    input_image: np.ndarray,\n    text_color: Tuple[int, int, int] = (255, 255, 255),\n    text_thickness: int = 2,\n) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"\n    Brenner's focus measure.\n\n    Parameters\n    ----------\n    input_image : np.ndarray\n        The input image in grayscale.\n    text_color : Tuple[int, int, int], optional\n        The color of the text displaying the Brenner value, in BGR format. Default is white (255, 255, 255).\n    text_thickness : int, optional\n        The thickness of the text displaying the Brenner value. Default is 2.\n\n    Returns\n    -------\n    Tuple[np.ndarray, float]\n        The Brenner image and the Brenner value.\n    \"\"\"\n    # Convert image to grayscale if it has 3 channels\n    if len(input_image.shape) == 3:\n        input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n\n    # Convert image to 16-bit integer format\n    converted_image = input_image.astype(np.int16)\n\n    # Get the dimensions of the image\n    height, width = converted_image.shape\n\n    # Initialize two matrices for horizontal and vertical focus measures\n    horizontal_diff = np.zeros((height, width))\n    vertical_diff = np.zeros((height, width))\n\n    # Calculate horizontal and vertical focus measures\n    horizontal_diff[:, : width - 2] = np.clip(\n        converted_image[:, 2:] - converted_image[:, :-2], 0, None\n    )\n    vertical_diff[: height - 2, :] = np.clip(\n        converted_image[2:, :] - converted_image[:-2, :], 0, None\n    )\n\n    # Calculate final focus measure\n    focus_measure = np.max((horizontal_diff, vertical_diff), axis=0) ** 2\n\n    # Convert focus measure matrix to 8-bit for visualization\n    focus_measure_image = ((focus_measure / focus_measure.max()) * 255).astype(np.uint8)\n\n    # Display the Brenner value on the top left of the image\n    cv2.putText(\n        focus_measure_image,\n        f\"Focus value: {focus_measure.mean():.2f}\",\n        (10, 30),\n        cv2.FONT_HERSHEY_SIMPLEX,\n        1,\n        text_color,\n        text_thickness,\n    )\n\n    return focus_measure_image, focus_measure.mean()\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/contours/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/contours/v1/#inference.core.workflows.core_steps.classical_cv.contours.v1.find_and_draw_contours","title":"<code>find_and_draw_contours(image, color=(255, 0, 255), thickness=3)</code>","text":"<p>Finds and draws contours on the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input thresholded image.</p> required <code>color</code> <code>tuple</code> <p>Color of the contour lines in BGR. Defaults to purple (255, 0, 255).</p> <code>(255, 0, 255)</code> <code>thickness</code> <code>int</code> <p>Thickness of the contour lines. Defaults to 3.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[ndarray, int]</code> <p>Image with contours drawn and number of contours.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/contours/v1.py</code> <pre><code>def find_and_draw_contours(\n    image: np.ndarray, color: Tuple[int, int, int] = (255, 0, 255), thickness: int = 3\n) -&gt; Tuple[np.ndarray, int]:\n    \"\"\"\n    Finds and draws contours on the image.\n\n    Args:\n        image (np.ndarray): Input thresholded image.\n        color (tuple, optional): Color of the contour lines in BGR. Defaults to purple (255, 0, 255).\n        thickness (int, optional): Thickness of the contour lines. Defaults to 3.\n\n    Returns:\n        tuple: Image with contours drawn and number of contours.\n    \"\"\"\n    # If not in grayscale, convert to grayscale\n    if len(image.shape) == 3 and image.shape[2] == 3:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Find contours\n    contours, hierarchy = cv2.findContours(\n        image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n\n    # Draw contours on a copy of the original image\n    contour_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    cv2.drawContours(contour_image, contours, -1, color, thickness)\n\n    # Return the image with contours and the number of contours\n    return contour_image, contours, hierarchy\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/convert_grayscale/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/distance_measurement/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/distance_measurement/v1/#inference.core.workflows.core_steps.classical_cv.distance_measurement.v1.has_overlap","title":"<code>has_overlap(bbox1, bbox2)</code>","text":"<p>Check if two bounding boxes overlap.</p> <p>Parameters:</p> Name Type Description Default <code>bbox1</code> <code>Tuple[int, int, int, int]</code> <p>A tuple of (x_min, y_min, x_max, y_max) for the first bounding box.</p> required <code>bbox2</code> <code>Tuple[int, int, int, int]</code> <p>A tuple of (x_min, y_min, x_max, y_max) for the second bounding box.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the bounding boxes overlap, False otherwise.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/distance_measurement/v1.py</code> <pre><code>def has_overlap(\n    bbox1: Tuple[int, int, int, int], bbox2: Tuple[int, int, int, int]\n) -&gt; bool:\n    \"\"\"\n    Check if two bounding boxes overlap.\n\n    Args:\n        bbox1: A tuple of (x_min, y_min, x_max, y_max) for the first bounding box.\n        bbox2: A tuple of (x_min, y_min, x_max, y_max) for the second bounding box.\n\n    Returns:\n        True if the bounding boxes overlap, False otherwise.\n    \"\"\"\n    x1_min, y1_min, x1_max, y1_max = bbox1\n    x2_min, y2_min, x2_max, y2_max = bbox2\n\n    if x1_max &lt; x2_min or x2_max &lt; x1_min:\n        return False\n    if y1_max &lt; y2_min or y2_max &lt; y1_min:\n        return False\n\n    return True\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/dominant_color/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/image_blur/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/image_blur/v1/#inference.core.workflows.core_steps.classical_cv.image_blur.v1.apply_blur","title":"<code>apply_blur(image, blur_type, ksize=5)</code>","text":"<p>Applies the specified blur to the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image.</p> required <code>blur_type</code> <code>str</code> <p>Type of blur ('average', 'gaussian', 'median', 'bilateral').</p> required <code>ksize</code> <code>int</code> <p>Kernel size for the blur. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Blurred image.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/image_blur/v1.py</code> <pre><code>def apply_blur(image: np.ndarray, blur_type: str, ksize: int = 5) -&gt; np.ndarray:\n    \"\"\"\n    Applies the specified blur to the image.\n\n    Args:\n        image: Input image.\n        blur_type (str): Type of blur ('average', 'gaussian', 'median', 'bilateral').\n        ksize (int, optional): Kernel size for the blur. Defaults to 5.\n\n    Returns:\n        np.ndarray: Blurred image.\n    \"\"\"\n\n    if blur_type == \"average\":\n        blurred_image = cv2.blur(image, (ksize, ksize))\n    elif blur_type == \"gaussian\":\n        blurred_image = cv2.GaussianBlur(image, (ksize, ksize), 0)\n    elif blur_type == \"median\":\n        blurred_image = cv2.medianBlur(image, ksize)\n    elif blur_type == \"bilateral\":\n        blurred_image = cv2.bilateralFilter(image, ksize, 75, 75)\n    else:\n        raise ValueError(f\"Unknown blur type: {blur_type}\")\n\n    return blurred_image\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/image_preprocessing/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/pixel_color_count/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/pixel_color_count/v1/#inference.core.workflows.core_steps.classical_cv.pixel_color_count.v1.count_specific_color_pixels","title":"<code>count_specific_color_pixels(image, target_color, tolerance)</code>","text":"<p>Counts the number of pixels that match the target color within the given tolerance.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image.</p> required <code>target_color</code> <code>Union[str, tuple]</code> <p>Target color in hex format (e.g., '#431112') or BGR tuple (e.g., (18, 17, 67)).</p> required <code>tolerance</code> <code>int</code> <p>Tolerance for color matching. Defaults to 10.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of pixels that match the target color.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/pixel_color_count/v1.py</code> <pre><code>def count_specific_color_pixels(\n    image: np.ndarray,\n    target_color: Union[str, Tuple[int, int, int]],\n    tolerance: int,\n) -&gt; int:\n    \"\"\"\n    Counts the number of pixels that match the target color within the given tolerance.\n\n    Args:\n        image: Input image.\n        target_color (Union[str, tuple]): Target color in hex format (e.g., '#431112') or BGR tuple (e.g., (18, 17, 67)).\n        tolerance (int, optional): Tolerance for color matching. Defaults to 10.\n\n    Returns:\n        int: Number of pixels that match the target color.\n    \"\"\"\n    target_color_bgr = convert_color_to_bgr_tuple(color=target_color)\n    lower_bound = np.array(\n        [\n            target_color_bgr[0] - tolerance,\n            target_color_bgr[1] - tolerance,\n            target_color_bgr[2] - tolerance,\n        ]\n    )\n    upper_bound = np.array(\n        [\n            target_color_bgr[0] + tolerance,\n            target_color_bgr[1] + tolerance,\n            target_color_bgr[2] + tolerance,\n        ]\n    )\n    mask = cv2.inRange(image, lower_bound, upper_bound)\n    return int(np.sum(mask &gt; 0))\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/sift/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/sift/v1/#inference.core.workflows.core_steps.classical_cv.sift.v1.apply_sift","title":"<code>apply_sift(image)</code>","text":"<p>Applies SIFT to the image. Args:     image: Input image. Returns:     np.ndarray: Image with keypoints drawn.     list: Keypoints detected.     np.ndarray: Descriptors of the keypoints.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/sift/v1.py</code> <pre><code>def apply_sift(image: np.ndarray) -&gt; (np.ndarray, list, np.ndarray):\n    \"\"\"\n    Applies SIFT to the image.\n    Args:\n        image: Input image.\n    Returns:\n        np.ndarray: Image with keypoints drawn.\n        list: Keypoints detected.\n        np.ndarray: Descriptors of the keypoints.\n    \"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    sift = cv2.SIFT_create()\n    kp, des = sift.detectAndCompute(gray, None)\n    img_with_kp = cv2.drawKeypoints(gray, kp, image)\n    # Convert keypoints to the desired format\n    keypoints = [\n        {\n            \"pt\": (point.pt[0], point.pt[1]),\n            \"size\": point.size,\n            \"angle\": point.angle,\n            \"response\": point.response,\n            \"octave\": point.octave,\n            \"class_id\": point.class_id,\n        }\n        for point in kp\n    ]\n    return img_with_kp, keypoints, des\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/sift_comparison/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/sift_comparison/v2/","title":"v2","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/sift_comparison/v2/#inference.core.workflows.core_steps.classical_cv.sift_comparison.v2.apply_sift","title":"<code>apply_sift(image, visualize=False)</code>","text":"<p>Applies SIFT to the image. Args:     image: Input image.     visualize: Whether to visualize keypoints on the image. Returns:     img_with_kp: Image with keypoints drawn (if visualize is True).     kp: List of cv2.KeyPoint objects.     keypoints_dicts: List of keypoints as dictionaries.     des: Descriptors of the keypoints.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/sift_comparison/v2.py</code> <pre><code>def apply_sift(\n    image: np.ndarray, visualize=False\n) -&gt; (Optional[np.ndarray], list, list, np.ndarray):\n    \"\"\"\n    Applies SIFT to the image.\n    Args:\n        image: Input image.\n        visualize: Whether to visualize keypoints on the image.\n    Returns:\n        img_with_kp: Image with keypoints drawn (if visualize is True).\n        kp: List of cv2.KeyPoint objects.\n        keypoints_dicts: List of keypoints as dictionaries.\n        des: Descriptors of the keypoints.\n    \"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    sift = cv2.SIFT_create()\n    kp, des = sift.detectAndCompute(gray, None)\n    img_with_kp = None\n    if visualize:\n        img_with_kp = cv2.drawKeypoints(gray, kp, None)\n    # Convert keypoints to the desired format\n    keypoints_dicts = [\n        {\n            \"pt\": (point.pt[0], point.pt[1]),\n            \"size\": point.size,\n            \"angle\": point.angle,\n            \"response\": point.response,\n            \"octave\": point.octave,\n            \"class_id\": point.class_id,\n        }\n        for point in kp\n    ]\n    return img_with_kp, kp, keypoints_dicts, des\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/size_measurement/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/template_matching/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/threshold/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/classical_cv/threshold/v1/#inference.core.workflows.core_steps.classical_cv.threshold.v1.apply_thresholding","title":"<code>apply_thresholding(image, threshold_type, thresh_value, max_value)</code>","text":"<p>Applies the specified thresholding to the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image in grayscale.</p> required <code>threshold_type</code> <code>str</code> <p>Type of thresholding ('binary', 'binary_inv', 'trunc', 'tozero', 'tozero_inv', 'adaptive_mean', 'adaptive_gaussian', 'otsu').</p> required <code>thresh_value</code> <code>int</code> <p>Threshold value.</p> required <code>max_value</code> <code>int</code> <p>Maximum value to use with the THRESH_BINARY and THRESH_BINARY_INV thresholding types.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Image with thresholding applied.</p> Source code in <code>inference/core/workflows/core_steps/classical_cv/threshold/v1.py</code> <pre><code>def apply_thresholding(\n    image: np.ndarray, threshold_type: str, thresh_value: int, max_value: int\n) -&gt; np.ndarray:\n    \"\"\"\n    Applies the specified thresholding to the image.\n\n    Args:\n        image (np.ndarray): Input image in grayscale.\n        threshold_type (str): Type of thresholding ('binary', 'binary_inv', 'trunc', 'tozero', 'tozero_inv', 'adaptive_mean', 'adaptive_gaussian', 'otsu').\n        thresh_value (int, optional): Threshold value.\n        max_value (int, optional): Maximum value to use with the THRESH_BINARY and THRESH_BINARY_INV thresholding types.\n\n    Returns:\n        np.ndarray: Image with thresholding applied.\n    \"\"\"\n    if threshold_type == \"binary\":\n        _, thresh_image = cv2.threshold(\n            image, thresh_value, max_value, cv2.THRESH_BINARY\n        )\n    elif threshold_type == \"binary_inv\":\n        _, thresh_image = cv2.threshold(\n            image, thresh_value, max_value, cv2.THRESH_BINARY_INV\n        )\n    elif threshold_type == \"trunc\":\n        _, thresh_image = cv2.threshold(\n            image, thresh_value, max_value, cv2.THRESH_TRUNC\n        )\n    elif threshold_type == \"tozero\":\n        _, thresh_image = cv2.threshold(\n            image, thresh_value, max_value, cv2.THRESH_TOZERO\n        )\n    elif threshold_type == \"tozero_inv\":\n        _, thresh_image = cv2.threshold(\n            image, thresh_value, max_value, cv2.THRESH_TOZERO_INV\n        )\n    elif threshold_type == \"adaptive_mean\":\n        thresh_image = cv2.adaptiveThreshold(\n            image, max_value, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2\n        )\n    elif threshold_type == \"adaptive_gaussian\":\n        thresh_image = cv2.adaptiveThreshold(\n            image,\n            max_value,\n            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n            cv2.THRESH_BINARY,\n            11,\n            2,\n        )\n    elif threshold_type == \"otsu\":\n        _, thresh_image = cv2.threshold(\n            image, 0, max_value, cv2.THRESH_BINARY + cv2.THRESH_OTSU\n        )\n    else:\n        raise ValueError(f\"Unknown threshold type: {threshold_type}\")\n\n    return thresh_image\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/common/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/operators/","title":"operators","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/serializers/","title":"serializers","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/utils/#inference.core.workflows.core_steps.common.utils.remove_unexpected_keys_from_dictionary","title":"<code>remove_unexpected_keys_from_dictionary(dictionary, expected_keys)</code>","text":"<p>This function mutates input <code>dictionary</code></p> Source code in <code>inference/core/workflows/core_steps/common/utils.py</code> <pre><code>def remove_unexpected_keys_from_dictionary(\n    dictionary: dict,\n    expected_keys: set,\n) -&gt; dict:\n    \"\"\"This function mutates input `dictionary`\"\"\"\n    unexpected_keys = set(dictionary.keys()).difference(expected_keys)\n    for unexpected_key in unexpected_keys:\n        del dictionary[unexpected_key]\n    return dictionary\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/common/vlms/","title":"vlms","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/errors/","title":"errors","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/entities/enums/","title":"enums","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/entities/introspection/","title":"introspection","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/entities/operations/","title":"operations","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/entities/types/","title":"types","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/evaluation_engine/core/","title":"core","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/evaluation_engine/detection/geometry/","title":"geometry","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/introspection/core/","title":"core","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/core/","title":"core","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/booleans/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/classification_results/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/detection/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/detections/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/dictionaries/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/generic/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/images/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/numbers/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/sequences/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/common/query_language/operations/strings/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/flow_control/continue_if/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/flow_control/rate_limiter/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/formatters/csv/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/formatters/expression/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/formatters/first_non_empty_or_default/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/formatters/json_parser/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/formatters/property_definition/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/formatters/vlm_as_classifier/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/formatters/vlm_as_detector/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/fusion/detections_classes_replacement/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/fusion/detections_consensus/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/fusion/detections_stitch/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/fusion/detections_stitch/v1/#inference.core.workflows.core_steps.fusion.detections_stitch.v1.move_detections","title":"<code>move_detections(detections, offset, resolution_wh)</code>","text":"<p>Copied from: https://github.com/roboflow/supervision/blob/5123085037ec594524fc8f9d9b71b1cd9f487e8d/supervision/detection/tools/inference_slicer.py#L17-L16 to avoid fragile contract with supervision, as this function is not element of public API.</p> Source code in <code>inference/core/workflows/core_steps/fusion/detections_stitch/v1.py</code> <pre><code>def move_detections(\n    detections: sv.Detections,\n    offset: Optional[np.ndarray],\n    resolution_wh: Optional[Tuple[int, int]],\n) -&gt; sv.Detections:\n    \"\"\"\n    Copied from: https://github.com/roboflow/supervision/blob/5123085037ec594524fc8f9d9b71b1cd9f487e8d/supervision/detection/tools/inference_slicer.py#L17-L16\n    to avoid fragile contract with supervision, as this function is not element of public\n    API.\n    \"\"\"\n    if len(detections) == 0:\n        return detections\n    if offset is None:\n        raise ValueError(\"To move non-empty detections offset is needed, but not given\")\n    detections.xyxy = move_boxes(xyxy=detections.xyxy, offset=offset)\n    if detections.mask is not None:\n        if resolution_wh is None:\n            raise ValueError(\n                \"To move non-empty detections with segmentation mask, resolution_wh is needed, but not given.\"\n            )\n        detections.mask = move_masks(\n            masks=detections.mask, offset=offset, resolution_wh=resolution_wh\n        )\n    return detections\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/fusion/dimension_collapse/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/anthropic_claude/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/clip_comparison/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/clip_comparison/v2/","title":"v2","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/cog_vlm/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/florence2/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/google_gemini/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/google_vision_ocr/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/lmm/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/lmm_classifier/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/ocr/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/openai/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/openai/v2/","title":"v2","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/segment_anything2/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/stability_ai/inpainting/v1/","title":"v1","text":"<p>Credits to: https://github.com/Fafruch for origin idea</p>"},{"location":"docs/reference/inference/core/workflows/core_steps/models/foundation/yolo_world/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/roboflow/instance_segmentation/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/roboflow/keypoint_detection/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/roboflow/multi_class_classification/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/roboflow/multi_label_classification/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/roboflow/object_detection/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/third_party/barcode_detection/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/models/third_party/qr_code_detection/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/sinks/email_notification/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/sinks/local_file/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/sinks/roboflow/custom_metadata/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/sinks/roboflow/dataset_upload/v1/","title":"v1","text":"<ul> <li>WARNING!                            *</li> </ul> <p>This module contains the utility functions used by RoboflowDatasetUploadBlockV2.</p> <p>We do not recommend making multiple blocks dependent on the same code, but the change between v1 and v2 was basically the default value of some parameter - hence we decided not to replicate the code.</p> <p>If you need to modify this module beware that you may introduce change to RoboflowDatasetUploadBlockV2! If that happens, probably that's the time to disentangle those blocks and copy the code.</p>"},{"location":"docs/reference/inference/core/workflows/core_steps/sinks/roboflow/dataset_upload/v2/","title":"v2","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/sinks/webhook/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/absolute_static_crop/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/bounding_rect/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/byte_tracker/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/byte_tracker/v2/","title":"v2","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/byte_tracker/v3/","title":"v3","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/detection_offset/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/detections_filter/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/detections_transformation/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/dynamic_crop/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/dynamic_zones/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/image_slicer/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/image_slicer/v1/#inference.core.workflows.core_steps.transformations.image_slicer.v1.generate_offsets","title":"<code>generate_offsets(resolution_wh, slice_wh, overlap_ratio_wh)</code>","text":"<p>Original code: https://github.com/roboflow/supervision/blob/5123085037ec594524fc8f9d9b71b1cd9f487e8d/supervision/detection/tools/inference_slicer.py#L204-L203 to avoid fragile contract with supervision, as this function is not element of public API.</p> <p>Generate offset coordinates for slicing an image based on the given resolution, slice dimensions, and overlap ratios.</p> <p>Parameters:</p> Name Type Description Default <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>A tuple representing the width and height of the image to be sliced.</p> required <code>slice_wh</code> <code>Tuple[int, int]</code> <p>Dimensions of each slice measured in pixels. The</p> required <code>overlap_ratio_wh</code> <code>Optional[Tuple[float, float]]</code> <p>A tuple representing the desired overlap ratio for width and height between consecutive slices. Each value should be in the range [0, 1), where 0 means no overlap and a value close to 1 means high overlap.</p> required <p>Returns:     np.ndarray: An array of shape <code>(n, 4)</code> containing coordinates for each         slice in the format <code>[xmin, ymin, xmax, ymax]</code>.</p> Note <p>The function ensures that slices do not exceed the boundaries of the     original image. As a result, the final slices in the row and column     dimensions might be smaller than the specified slice dimensions if the     image's width or height is not a multiple of the slice's width or     height minus the overlap.</p> Source code in <code>inference/core/workflows/core_steps/transformations/image_slicer/v1.py</code> <pre><code>def generate_offsets(\n    resolution_wh: Tuple[int, int],\n    slice_wh: Tuple[int, int],\n    overlap_ratio_wh: Optional[Tuple[float, float]],\n) -&gt; np.ndarray:\n    \"\"\"\n    Original code: https://github.com/roboflow/supervision/blob/5123085037ec594524fc8f9d9b71b1cd9f487e8d/supervision/detection/tools/inference_slicer.py#L204-L203\n    to avoid fragile contract with supervision, as this function is not element of public\n    API.\n\n    Generate offset coordinates for slicing an image based on the given resolution,\n    slice dimensions, and overlap ratios.\n\n    Args:\n        resolution_wh (Tuple[int, int]): A tuple representing the width and height\n            of the image to be sliced.\n        slice_wh (Tuple[int, int]): Dimensions of each slice measured in pixels. The\n        tuple should be in the format `(width, height)`.\n        overlap_ratio_wh (Optional[Tuple[float, float]]): A tuple representing the\n            desired overlap ratio for width and height between consecutive slices.\n            Each value should be in the range [0, 1), where 0 means no overlap and\n            a value close to 1 means high overlap.\n    Returns:\n        np.ndarray: An array of shape `(n, 4)` containing coordinates for each\n            slice in the format `[xmin, ymin, xmax, ymax]`.\n\n    Note:\n        The function ensures that slices do not exceed the boundaries of the\n            original image. As a result, the final slices in the row and column\n            dimensions might be smaller than the specified slice dimensions if the\n            image's width or height is not a multiple of the slice's width or\n            height minus the overlap.\n    \"\"\"\n    slice_width, slice_height = slice_wh\n    image_width, image_height = resolution_wh\n    overlap_width = int(overlap_ratio_wh[0] * slice_width)\n    overlap_height = int(overlap_ratio_wh[1] * slice_height)\n    width_stride = slice_width - overlap_width\n    height_stride = slice_height - overlap_height\n    ws = np.arange(0, image_width, width_stride)\n    hs = np.arange(0, image_height, height_stride)\n    xmin, ymin = np.meshgrid(ws, hs)\n    xmax = np.clip(xmin + slice_width, 0, image_width)\n    ymax = np.clip(ymin + slice_height, 0, image_height)\n    return np.stack([xmin, ymin, xmax, ymax], axis=-1).reshape(-1, 4)\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/perspective_correction/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/relative_static_crop/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/stabilize_detections/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/stitch_images/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/#inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.get_line_separator","title":"<code>get_line_separator(reading_direction)</code>","text":"<p>Get the appropriate separator based on reading direction.</p> Source code in <code>inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1.py</code> <pre><code>def get_line_separator(reading_direction: str) -&gt; str:\n    \"\"\"Get the appropriate separator based on reading direction.\"\"\"\n    return \"\\n\" if reading_direction in [\"left_to_right\", \"right_to_left\"] else \" \"\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/#inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.group_detections_by_line","title":"<code>group_detections_by_line(xyxy, reading_direction, tolerance)</code>","text":"<p>Group detections into lines based on primary coordinate.</p> Source code in <code>inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1.py</code> <pre><code>def group_detections_by_line(\n    xyxy: np.ndarray,\n    reading_direction: str,\n    tolerance: int,\n) -&gt; Dict[float, Dict[str, List]]:\n    \"\"\"Group detections into lines based on primary coordinate.\"\"\"\n    # After prepare_coordinates swap, we always group by y ([:, 1])\n    primary_coord = xyxy[:, 1]  # This is y for horizontal, swapped x for vertical\n\n    # Round primary coordinate to group into lines\n    rounded_primary = np.round(primary_coord / tolerance) * tolerance\n\n    boxes_by_line = {}\n    # Group bounding boxes and associated indices by line\n    for i, (bbox, line_pos) in enumerate(zip(xyxy, rounded_primary)):\n        if line_pos not in boxes_by_line:\n            boxes_by_line[line_pos] = {\"xyxy\": [bbox], \"idx\": [i]}\n        else:\n            boxes_by_line[line_pos][\"xyxy\"].append(bbox)\n            boxes_by_line[line_pos][\"idx\"].append(i)\n\n    return boxes_by_line\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/#inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.prepare_coordinates","title":"<code>prepare_coordinates(xyxy, reading_direction)</code>","text":"<p>Prepare coordinates based on reading direction.</p> Source code in <code>inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1.py</code> <pre><code>def prepare_coordinates(\n    xyxy: np.ndarray,\n    reading_direction: str,\n) -&gt; np.ndarray:\n    \"\"\"Prepare coordinates based on reading direction.\"\"\"\n    if reading_direction in [\"vertical_top_to_bottom\", \"vertical_bottom_to_top\"]:\n        # Swap x and y coordinates: [x1,y1,x2,y2] -&gt; [y1,x1,y2,x2]\n        return xyxy[:, [1, 0, 3, 2]]\n    return xyxy\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/#inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.sort_line_detections","title":"<code>sort_line_detections(line_xyxy, reading_direction)</code>","text":"<p>Sort detections within a line based on reading direction.</p> Source code in <code>inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1.py</code> <pre><code>def sort_line_detections(\n    line_xyxy: np.ndarray,\n    reading_direction: str,\n) -&gt; np.ndarray:\n    \"\"\"Sort detections within a line based on reading direction.\"\"\"\n    # After prepare_coordinates swap, we always sort by x ([:, 0])\n    if reading_direction in [\"left_to_right\", \"vertical_top_to_bottom\"]:\n        return line_xyxy[:, 0].argsort()  # Sort by x1 (original x or swapped y)\n    else:  # right_to_left or vertical_bottom_to_top\n        return (-line_xyxy[:, 0]).argsort()  # Sort by -x1 (original -x or swapped -y)\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1/#inference.core.workflows.core_steps.transformations.stitch_ocr_detections.v1.stitch_ocr_detections","title":"<code>stitch_ocr_detections(detections, reading_direction='left_to_right', tolerance=10)</code>","text":"<p>Stitch OCR detections into coherent text based on spatial arrangement.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>Supervision Detections object containing OCR results</p> required <code>reading_direction</code> <code>str</code> <p>Direction to read text (\"left_to_right\", \"right_to_left\",              \"vertical_top_to_bottom\", \"vertical_bottom_to_top\")</p> <code>'left_to_right'</code> <code>tolerance</code> <code>int</code> <p>Vertical tolerance for grouping text into lines</p> <code>10</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict containing stitched OCR text under 'ocr_text' key</p> Source code in <code>inference/core/workflows/core_steps/transformations/stitch_ocr_detections/v1.py</code> <pre><code>def stitch_ocr_detections(\n    detections: sv.Detections,\n    reading_direction: str = \"left_to_right\",\n    tolerance: int = 10,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Stitch OCR detections into coherent text based on spatial arrangement.\n\n    Args:\n        detections: Supervision Detections object containing OCR results\n        reading_direction: Direction to read text (\"left_to_right\", \"right_to_left\",\n                         \"vertical_top_to_bottom\", \"vertical_bottom_to_top\")\n        tolerance: Vertical tolerance for grouping text into lines\n\n    Returns:\n        Dict containing stitched OCR text under 'ocr_text' key\n    \"\"\"\n    if len(detections) == 0:\n        return {\"ocr_text\": \"\"}\n\n    xyxy = detections.xyxy.round().astype(dtype=int)\n    class_names = detections.data[\"class_name\"]\n\n    # Prepare coordinates based on reading direction\n    xyxy = prepare_coordinates(xyxy, reading_direction)\n\n    # Group detections into lines\n    boxes_by_line = group_detections_by_line(xyxy, reading_direction, tolerance)\n    # Sort lines based on reading direction\n    lines = sorted(\n        boxes_by_line.keys(), reverse=reading_direction in [\"vertical_bottom_to_top\"]\n    )\n\n    # Build final text\n    ordered_class_names = []\n    for i, key in enumerate(lines):\n        line_data = boxes_by_line[key]\n        line_xyxy = np.array(line_data[\"xyxy\"])\n        line_idx = np.array(line_data[\"idx\"])\n\n        # Sort detections within line\n        sort_idx = sort_line_detections(line_xyxy, reading_direction)\n\n        # Add sorted class names for this line\n        ordered_class_names.extend(class_names[line_idx[sort_idx]])\n\n        # Add line separator if not last line\n        if i &lt; len(lines) - 1:\n            ordered_class_names.append(get_line_separator(reading_direction))\n\n    return {\"ocr_text\": \"\".join(ordered_class_names)}\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/background_color/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/blur/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/bounding_box/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/circle/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/color/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/base_colorable/","title":"base_colorable","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/background_color/","title":"background_color","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/background_color/#inference.core.workflows.core_steps.visualizations.common.annotators.background_color.BackgroundColorAnnotator","title":"<code>BackgroundColorAnnotator</code>","text":"<p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing background colors outside of detected box or mask regions.</p> <p>Warning</p> <p>This annotator uses <code>sv.Detections.mask</code>.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/background_color.py</code> <pre><code>class BackgroundColorAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing background colors outside of detected box or mask regions.\n    !!! warning\n        This annotator uses `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Color = Color.BLACK,\n        opacity: float = 0.5,\n        force_box: bool = False,\n    ):\n        \"\"\"\n        Args:\n            color (Color): The color to use for annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        \"\"\"\n        self.color: Color = color\n        self.opacity = opacity\n        self.force_box = force_box\n\n    def annotate(self, scene: np.ndarray, detections: Detections) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with masks based on the provided detections.\n        Args:\n            scene (ImageType): The image where masks will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n        Example:\n            ```python\n            import supervision as sv\n            image = ...\n            detections = sv.Detections(...)\n            background_color_annotator = sv.BackgroundColorAnnotator()\n            annotated_frame = background_color_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n        ![background-color-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/background-color-annotator-example-purple.png)\n        \"\"\"\n\n        colored_mask = np.full_like(scene, self.color.as_bgr(), dtype=np.uint8)\n\n        cv2.addWeighted(\n            scene, 1 - self.opacity, colored_mask, self.opacity, 0, dst=colored_mask\n        )\n\n        if detections.mask is None or self.force_box:\n            for detection_idx in range(len(detections)):\n                x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n                colored_mask[y1:y2, x1:x2] = scene[y1:y2, x1:x2]\n        else:\n            for mask in detections.mask:\n                colored_mask[mask] = scene[mask]\n\n        return colored_mask\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/background_color/#inference.core.workflows.core_steps.visualizations.common.annotators.background_color.BackgroundColorAnnotator.__init__","title":"<code>__init__(color=Color.BLACK, opacity=0.5, force_box=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Color</code> <p>The color to use for annotating detections.</p> <code>BLACK</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.5</code> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/background_color.py</code> <pre><code>def __init__(\n    self,\n    color: Color = Color.BLACK,\n    opacity: float = 0.5,\n    force_box: bool = False,\n):\n    \"\"\"\n    Args:\n        color (Color): The color to use for annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n    \"\"\"\n    self.color: Color = color\n    self.opacity = opacity\n    self.force_box = force_box\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/background_color/#inference.core.workflows.core_steps.visualizations.common.annotators.background_color.BackgroundColorAnnotator.annotate","title":"<code>annotate(scene, detections)</code>","text":"<p>Annotates the given scene with masks based on the provided detections. Args:     scene (ImageType): The image where masks will be drawn.         <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code>         or <code>PIL.Image.Image</code>.     detections (Detections): Object detections to annotate. Returns:     The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code>         or <code>PIL.Image.Image</code>) Example:     <pre><code>import supervision as sv\nimage = ...\ndetections = sv.Detections(...)\nbackground_color_annotator = sv.BackgroundColorAnnotator()\nannotated_frame = background_color_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> </p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/background_color.py</code> <pre><code>def annotate(self, scene: np.ndarray, detections: Detections) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with masks based on the provided detections.\n    Args:\n        scene (ImageType): The image where masks will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n    Example:\n        ```python\n        import supervision as sv\n        image = ...\n        detections = sv.Detections(...)\n        background_color_annotator = sv.BackgroundColorAnnotator()\n        annotated_frame = background_color_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n    ![background-color-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/background-color-annotator-example-purple.png)\n    \"\"\"\n\n    colored_mask = np.full_like(scene, self.color.as_bgr(), dtype=np.uint8)\n\n    cv2.addWeighted(\n        scene, 1 - self.opacity, colored_mask, self.opacity, 0, dst=colored_mask\n    )\n\n    if detections.mask is None or self.force_box:\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            colored_mask[y1:y2, x1:x2] = scene[y1:y2, x1:x2]\n    else:\n        for mask in detections.mask:\n            colored_mask[mask] = scene[mask]\n\n    return colored_mask\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/halo/","title":"halo","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/halo/#inference.core.workflows.core_steps.visualizations.common.annotators.halo.HaloAnnotator","title":"<code>HaloAnnotator</code>","text":"<p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing Halos on an image using provided detections.</p> <p>Warning</p> <p>This annotator uses <code>sv.Detections.mask</code>.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/halo.py</code> <pre><code>class HaloAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing Halos on an image using provided detections.\n\n    !!! warning\n\n        This annotator uses `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        opacity: float = 0.8,\n        kernel_size: int = 40,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n            kernel_size (int): The size of the average pooling kernel used for creating\n                the halo.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.opacity = opacity\n        self.color_lookup: ColorLookup = color_lookup\n        self.kernel_size: int = kernel_size\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with halos based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where masks will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            halo_annotator = sv.HaloAnnotator()\n            annotated_frame = halo_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![halo-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/halo-annotator-example-purple.png)\n        \"\"\"\n        assert isinstance(scene, np.ndarray)\n        colored_mask = np.zeros_like(scene, dtype=np.uint8)\n        fmask = np.array([False] * scene.shape[0] * scene.shape[1]).reshape(\n            scene.shape[0], scene.shape[1]\n        )\n\n        for detection_idx in np.flip(np.argsort(detections.area)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=(\n                    self.color_lookup\n                    if custom_color_lookup is None\n                    else custom_color_lookup\n                ),\n            )\n            if detections.mask is None:\n                x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n                mask = np.zeros(scene.shape[:2], dtype=bool)\n                mask[y1:y2, x1:x2] = True\n            else:\n                mask = detections.mask[detection_idx]\n            fmask = np.logical_or(fmask, mask)\n            color_bgr = color.as_bgr()\n            colored_mask[mask] = color_bgr\n\n        colored_mask = cv2.blur(colored_mask, (self.kernel_size, self.kernel_size))\n        colored_mask[fmask] = [0, 0, 0]\n        gray = cv2.cvtColor(colored_mask, cv2.COLOR_BGR2GRAY)\n        alpha = self.opacity * gray / gray.max()\n        alpha_mask = alpha[:, :, np.newaxis]\n        blended_scene = np.uint8(scene * (1 - alpha_mask) + colored_mask * self.opacity)\n        np.copyto(scene, blended_scene)\n        return scene\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/halo/#inference.core.workflows.core_steps.visualizations.common.annotators.halo.HaloAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, opacity=0.8, kernel_size=40, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.8</code> <code>kernel_size</code> <code>int</code> <p>The size of the average pooling kernel used for creating the halo.</p> <code>40</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/halo.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    opacity: float = 0.8,\n    kernel_size: int = 40,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        kernel_size (int): The size of the average pooling kernel used for creating\n            the halo.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.opacity = opacity\n    self.color_lookup: ColorLookup = color_lookup\n    self.kernel_size: int = kernel_size\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/halo/#inference.core.workflows.core_steps.visualizations.common.annotators.halo.HaloAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with halos based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where masks will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nhalo_annotator = sv.HaloAnnotator()\nannotated_frame = halo_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/halo.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with halos based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where masks will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        halo_annotator = sv.HaloAnnotator()\n        annotated_frame = halo_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![halo-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/halo-annotator-example-purple.png)\n    \"\"\"\n    assert isinstance(scene, np.ndarray)\n    colored_mask = np.zeros_like(scene, dtype=np.uint8)\n    fmask = np.array([False] * scene.shape[0] * scene.shape[1]).reshape(\n        scene.shape[0], scene.shape[1]\n    )\n\n    for detection_idx in np.flip(np.argsort(detections.area)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=(\n                self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup\n            ),\n        )\n        if detections.mask is None:\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            mask = np.zeros(scene.shape[:2], dtype=bool)\n            mask[y1:y2, x1:x2] = True\n        else:\n            mask = detections.mask[detection_idx]\n        fmask = np.logical_or(fmask, mask)\n        color_bgr = color.as_bgr()\n        colored_mask[mask] = color_bgr\n\n    colored_mask = cv2.blur(colored_mask, (self.kernel_size, self.kernel_size))\n    colored_mask[fmask] = [0, 0, 0]\n    gray = cv2.cvtColor(colored_mask, cv2.COLOR_BGR2GRAY)\n    alpha = self.opacity * gray / gray.max()\n    alpha_mask = alpha[:, :, np.newaxis]\n    blended_scene = np.uint8(scene * (1 - alpha_mask) + colored_mask * self.opacity)\n    np.copyto(scene, blended_scene)\n    return scene\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison/","title":"model_comparison","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison/#inference.core.workflows.core_steps.visualizations.common.annotators.model_comparison.ModelComparisonAnnotator","title":"<code>ModelComparisonAnnotator</code>","text":"<p>               Bases: <code>BaseAnnotator</code></p> <p>A class for annotating images by highlighting regions predicted by two different models. This annotator visually distinguishes areas uniquely predicted by each model as well as the background where neither model made a prediction.</p> <p>Attributes:</p> Name Type Description <code>color_a</code> <code>Color</code> <p>Color used to highlight predictions made only by Model A.</p> <code>color_b</code> <code>Color</code> <p>Color used to highlight predictions made only by Model B.</p> <code>background_color</code> <code>Color</code> <p>Color used for parts of the image where neither model made a prediction.</p> <code>opacity</code> <code>float</code> <p>Opacity level of the overlays, ranging between 0 and 1.</p> <code>force_box</code> <code>bool</code> <p>If True, forces the use of bounding boxes for predictions even if masks are available.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison.py</code> <pre><code>class ModelComparisonAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for annotating images by highlighting regions predicted by two different models.\n    This annotator visually distinguishes areas uniquely predicted by each model as well as\n    the background where neither model made a prediction.\n\n    Attributes:\n        color_a (Color): Color used to highlight predictions made only by Model A.\n        color_b (Color): Color used to highlight predictions made only by Model B.\n        background_color (Color): Color used for parts of the image where neither model made a prediction.\n        opacity (float): Opacity level of the overlays, ranging between 0 and 1.\n        force_box (bool): If True, forces the use of bounding boxes for predictions even if masks are available.\n    \"\"\"\n\n    def __init__(\n        self,\n        color_a: Color = Color.GREEN,\n        color_b: Color = Color.RED,\n        background_color: Color = Color.BLACK,\n        opacity: float = 0.7,\n        force_box: bool = False,\n    ):\n        \"\"\"\n        Initializes the ModelComparisonAnnotator with the specified colors, opacity, and behavior.\n\n        Args:\n            color_a (Color): Color used to highlight predictions made only by Model A.\n            color_b (Color): Color used to highlight predictions made only by Model B.\n            background_color (Color): Color for parts of the image not covered by any prediction.\n            opacity (float): Opacity of the overlay mask, must be between 0 and 1.\n            force_box (bool): Whether to use bounding boxes instead of masks if masks are available.\n        \"\"\"\n        self.color_a: Color = color_a\n        self.color_b: Color = color_b\n        self.background_color: Color = background_color\n        self.opacity = opacity\n        self.force_box = force_box\n\n    def annotate(\n        self, scene: np.ndarray, detections_a: Detections, detections_b: Detections\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with highlights representing predictions from two models.\n\n        Args:\n            scene (np.ndarray): Original image as a NumPy array (H x W x C).\n            detections_a (Detections): Predictions from Model A.\n            detections_b (Detections): Predictions from Model B.\n\n        Returns:\n            np.ndarray: Annotated image as a NumPy array.\n        \"\"\"\n\n        # Initialize single-channel masks\n        neither_predicted = np.ones(\n            scene.shape[:2], dtype=np.uint8\n        )  # 1 where neither model predicts\n        a_predicted = np.zeros(scene.shape[:2], dtype=np.uint8)\n        b_predicted = np.zeros(scene.shape[:2], dtype=np.uint8)\n\n        # Populate masks based on detections from Model A\n        if detections_a.mask is None or self.force_box:\n            for detection_idx in range(len(detections_a)):\n                x1, y1, x2, y2 = detections_a.xyxy[detection_idx].astype(int)\n                a_predicted[y1:y2, x1:x2] = 1\n                neither_predicted[y1:y2, x1:x2] = 0\n        else:\n            for mask in detections_a.mask:\n                a_predicted[mask.astype(bool)] = 1\n                neither_predicted[mask.astype(bool)] = 0\n\n        # Populate masks based on detections from Model B\n        if detections_b.mask is None or self.force_box:\n            for detection_idx in range(len(detections_b)):\n                x1, y1, x2, y2 = detections_b.xyxy[detection_idx].astype(int)\n                b_predicted[y1:y2, x1:x2] = 1\n                neither_predicted[y1:y2, x1:x2] = 0\n        else:\n            for mask in detections_b.mask:\n                b_predicted[mask.astype(bool)] = 1\n                neither_predicted[mask.astype(bool)] = 0\n\n        # Define combined masks\n        only_a_predicted = a_predicted &amp; (a_predicted ^ b_predicted)\n        only_b_predicted = b_predicted &amp; (b_predicted ^ a_predicted)\n\n        # Prepare overlay colors\n        background_color_bgr = self.background_color.as_bgr()  # Tuple like (B, G, R)\n        color_a_bgr = self.color_a.as_bgr()\n        color_b_bgr = self.color_b.as_bgr()\n\n        # Create full-color overlay images\n        overlay_background = np.full_like(scene, background_color_bgr, dtype=np.uint8)\n        overlay_a = np.full_like(scene, color_a_bgr, dtype=np.uint8)\n        overlay_b = np.full_like(scene, color_b_bgr, dtype=np.uint8)\n\n        # Function to blend and apply overlay based on mask\n        def apply_overlay(base_img, overlay_img, mask, opacity):\n            \"\"\"\n            Blends the overlay with the base image where the mask is set.\n\n            Args:\n                base_img (np.ndarray): Original image.\n                overlay_img (np.ndarray): Overlay color image.\n                mask (np.ndarray): Single-channel mask where to apply the overlay.\n                opacity (float): Opacity of the overlay (0 to 1).\n\n            Returns:\n                np.ndarray: Image with overlay applied.\n            \"\"\"\n            # Blend the entire images\n            blended = cv2.addWeighted(base_img, 1 - opacity, overlay_img, opacity, 0)\n            # Expand mask to three channels\n            mask_3ch = np.stack([mask] * 3, axis=-1)  # Shape: H x W x 3\n            # Ensure mask is boolean\n            mask_bool = mask_3ch.astype(bool)\n            # Apply blended regions where mask is True\n            base_img[mask_bool] = blended[mask_bool]\n            return base_img\n\n        # Apply background overlay where neither model predicted\n        scene = apply_overlay(\n            scene, overlay_background, neither_predicted, self.opacity\n        )\n\n        # Apply overlay for only Model A predictions\n        scene = apply_overlay(scene, overlay_a, only_a_predicted, self.opacity)\n\n        # Apply overlay for only Model B predictions\n        scene = apply_overlay(scene, overlay_b, only_b_predicted, self.opacity)\n\n        # Areas where both models predicted remain unchanged (no overlay)\n\n        return scene\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison/#inference.core.workflows.core_steps.visualizations.common.annotators.model_comparison.ModelComparisonAnnotator.__init__","title":"<code>__init__(color_a=Color.GREEN, color_b=Color.RED, background_color=Color.BLACK, opacity=0.7, force_box=False)</code>","text":"<p>Initializes the ModelComparisonAnnotator with the specified colors, opacity, and behavior.</p> <p>Parameters:</p> Name Type Description Default <code>color_a</code> <code>Color</code> <p>Color used to highlight predictions made only by Model A.</p> <code>GREEN</code> <code>color_b</code> <code>Color</code> <p>Color used to highlight predictions made only by Model B.</p> <code>RED</code> <code>background_color</code> <code>Color</code> <p>Color for parts of the image not covered by any prediction.</p> <code>BLACK</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask, must be between 0 and 1.</p> <code>0.7</code> <code>force_box</code> <code>bool</code> <p>Whether to use bounding boxes instead of masks if masks are available.</p> <code>False</code> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison.py</code> <pre><code>def __init__(\n    self,\n    color_a: Color = Color.GREEN,\n    color_b: Color = Color.RED,\n    background_color: Color = Color.BLACK,\n    opacity: float = 0.7,\n    force_box: bool = False,\n):\n    \"\"\"\n    Initializes the ModelComparisonAnnotator with the specified colors, opacity, and behavior.\n\n    Args:\n        color_a (Color): Color used to highlight predictions made only by Model A.\n        color_b (Color): Color used to highlight predictions made only by Model B.\n        background_color (Color): Color for parts of the image not covered by any prediction.\n        opacity (float): Opacity of the overlay mask, must be between 0 and 1.\n        force_box (bool): Whether to use bounding boxes instead of masks if masks are available.\n    \"\"\"\n    self.color_a: Color = color_a\n    self.color_b: Color = color_b\n    self.background_color: Color = background_color\n    self.opacity = opacity\n    self.force_box = force_box\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison/#inference.core.workflows.core_steps.visualizations.common.annotators.model_comparison.ModelComparisonAnnotator.annotate","title":"<code>annotate(scene, detections_a, detections_b)</code>","text":"<p>Annotates the given scene with highlights representing predictions from two models.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>Original image as a NumPy array (H x W x C).</p> required <code>detections_a</code> <code>Detections</code> <p>Predictions from Model A.</p> required <code>detections_b</code> <code>Detections</code> <p>Predictions from Model B.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Annotated image as a NumPy array.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/model_comparison.py</code> <pre><code>def annotate(\n    self, scene: np.ndarray, detections_a: Detections, detections_b: Detections\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with highlights representing predictions from two models.\n\n    Args:\n        scene (np.ndarray): Original image as a NumPy array (H x W x C).\n        detections_a (Detections): Predictions from Model A.\n        detections_b (Detections): Predictions from Model B.\n\n    Returns:\n        np.ndarray: Annotated image as a NumPy array.\n    \"\"\"\n\n    # Initialize single-channel masks\n    neither_predicted = np.ones(\n        scene.shape[:2], dtype=np.uint8\n    )  # 1 where neither model predicts\n    a_predicted = np.zeros(scene.shape[:2], dtype=np.uint8)\n    b_predicted = np.zeros(scene.shape[:2], dtype=np.uint8)\n\n    # Populate masks based on detections from Model A\n    if detections_a.mask is None or self.force_box:\n        for detection_idx in range(len(detections_a)):\n            x1, y1, x2, y2 = detections_a.xyxy[detection_idx].astype(int)\n            a_predicted[y1:y2, x1:x2] = 1\n            neither_predicted[y1:y2, x1:x2] = 0\n    else:\n        for mask in detections_a.mask:\n            a_predicted[mask.astype(bool)] = 1\n            neither_predicted[mask.astype(bool)] = 0\n\n    # Populate masks based on detections from Model B\n    if detections_b.mask is None or self.force_box:\n        for detection_idx in range(len(detections_b)):\n            x1, y1, x2, y2 = detections_b.xyxy[detection_idx].astype(int)\n            b_predicted[y1:y2, x1:x2] = 1\n            neither_predicted[y1:y2, x1:x2] = 0\n    else:\n        for mask in detections_b.mask:\n            b_predicted[mask.astype(bool)] = 1\n            neither_predicted[mask.astype(bool)] = 0\n\n    # Define combined masks\n    only_a_predicted = a_predicted &amp; (a_predicted ^ b_predicted)\n    only_b_predicted = b_predicted &amp; (b_predicted ^ a_predicted)\n\n    # Prepare overlay colors\n    background_color_bgr = self.background_color.as_bgr()  # Tuple like (B, G, R)\n    color_a_bgr = self.color_a.as_bgr()\n    color_b_bgr = self.color_b.as_bgr()\n\n    # Create full-color overlay images\n    overlay_background = np.full_like(scene, background_color_bgr, dtype=np.uint8)\n    overlay_a = np.full_like(scene, color_a_bgr, dtype=np.uint8)\n    overlay_b = np.full_like(scene, color_b_bgr, dtype=np.uint8)\n\n    # Function to blend and apply overlay based on mask\n    def apply_overlay(base_img, overlay_img, mask, opacity):\n        \"\"\"\n        Blends the overlay with the base image where the mask is set.\n\n        Args:\n            base_img (np.ndarray): Original image.\n            overlay_img (np.ndarray): Overlay color image.\n            mask (np.ndarray): Single-channel mask where to apply the overlay.\n            opacity (float): Opacity of the overlay (0 to 1).\n\n        Returns:\n            np.ndarray: Image with overlay applied.\n        \"\"\"\n        # Blend the entire images\n        blended = cv2.addWeighted(base_img, 1 - opacity, overlay_img, opacity, 0)\n        # Expand mask to three channels\n        mask_3ch = np.stack([mask] * 3, axis=-1)  # Shape: H x W x 3\n        # Ensure mask is boolean\n        mask_bool = mask_3ch.astype(bool)\n        # Apply blended regions where mask is True\n        base_img[mask_bool] = blended[mask_bool]\n        return base_img\n\n    # Apply background overlay where neither model predicted\n    scene = apply_overlay(\n        scene, overlay_background, neither_predicted, self.opacity\n    )\n\n    # Apply overlay for only Model A predictions\n    scene = apply_overlay(scene, overlay_a, only_a_predicted, self.opacity)\n\n    # Apply overlay for only Model B predictions\n    scene = apply_overlay(scene, overlay_b, only_b_predicted, self.opacity)\n\n    # Areas where both models predicted remain unchanged (no overlay)\n\n    return scene\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/polygon/","title":"polygon","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/polygon/#inference.core.workflows.core_steps.visualizations.common.annotators.polygon.PolygonAnnotator","title":"<code>PolygonAnnotator</code>","text":"<p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing polygons on an image using provided detections.</p> <p>Warning</p> <p>This annotator uses <code>sv.Detections.mask</code>.</p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/polygon.py</code> <pre><code>class PolygonAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing polygons on an image using provided detections.\n\n    !!! warning\n\n        This annotator uses `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the polygon lines.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with polygons based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where polygons will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            polygon_annotator = sv.PolygonAnnotator()\n            annotated_frame = polygon_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![polygon-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/polygon-annotator-example-purple.png)\n        \"\"\"\n        assert isinstance(scene, np.ndarray)\n\n        for detection_idx in range(len(detections)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=(\n                    self.color_lookup\n                    if custom_color_lookup is None\n                    else custom_color_lookup\n                ),\n            )\n\n            if detections.mask is None:\n                x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n                cv2.rectangle(\n                    img=scene,\n                    pt1=(x1, y1),\n                    pt2=(x2, y2),\n                    color=color.as_bgr(),\n                    thickness=self.thickness,\n                )\n            else:\n                mask = detections.mask[detection_idx]\n                for polygon in mask_to_polygons(mask=mask):\n                    scene = draw_polygon(\n                        scene=scene,\n                        polygon=polygon,\n                        color=color,\n                        thickness=self.thickness,\n                    )\n\n        return scene\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/polygon/#inference.core.workflows.core_steps.visualizations.common.annotators.polygon.PolygonAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>thickness</code> <code>int</code> <p>Thickness of the polygon lines.</p> <code>2</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/polygon.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the polygon lines.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/common/annotators/polygon/#inference.core.workflows.core_steps.visualizations.common.annotators.polygon.PolygonAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with polygons based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where polygons will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npolygon_annotator = sv.PolygonAnnotator()\nannotated_frame = polygon_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>inference/core/workflows/core_steps/visualizations/common/annotators/polygon.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with polygons based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where polygons will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        polygon_annotator = sv.PolygonAnnotator()\n        annotated_frame = polygon_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![polygon-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/polygon-annotator-example-purple.png)\n    \"\"\"\n    assert isinstance(scene, np.ndarray)\n\n    for detection_idx in range(len(detections)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=(\n                self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup\n            ),\n        )\n\n        if detections.mask is None:\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            cv2.rectangle(\n                img=scene,\n                pt1=(x1, y1),\n                pt2=(x2, y2),\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n        else:\n            mask = detections.mask[detection_idx]\n            for polygon in mask_to_polygons(mask=mask):\n                scene = draw_polygon(\n                    scene=scene,\n                    polygon=polygon,\n                    color=color,\n                    thickness=self.thickness,\n                )\n\n    return scene\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/corner/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/crop/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/dot/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/ellipse/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/halo/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/keypoint/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/label/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/line_zone/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/mask/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/model_comparison/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/pixelate/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/polygon/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/polygon_zone/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/reference_path/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/trace/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/core_steps/visualizations/triangle/v1/","title":"v1","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/constants/","title":"constants","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/core/","title":"core","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/entities/base/","title":"base","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/entities/base/#inference.core.workflows.execution_engine.entities.base.WorkflowImageData","title":"<code>WorkflowImageData</code>","text":"Source code in <code>inference/core/workflows/execution_engine/entities/base.py</code> <pre><code>class WorkflowImageData:\n\n    def __init__(\n        self,\n        parent_metadata: ImageParentMetadata,\n        workflow_root_ancestor_metadata: Optional[ImageParentMetadata] = None,\n        image_reference: Optional[str] = None,\n        base64_image: Optional[str] = None,\n        numpy_image: Optional[np.ndarray] = None,\n        video_metadata: Optional[VideoMetadata] = None,\n    ):\n        if not base64_image and numpy_image is None and not image_reference:\n            raise ValueError(\"Could not initialise empty `WorkflowImageData`.\")\n        self._parent_metadata = parent_metadata\n        self._workflow_root_ancestor_metadata = (\n            workflow_root_ancestor_metadata\n            if workflow_root_ancestor_metadata\n            else self._parent_metadata\n        )\n        self._image_reference = image_reference\n        self._base64_image = base64_image\n        self._numpy_image = numpy_image\n        self._video_metadata = video_metadata\n\n    @classmethod\n    def copy_and_replace(\n        cls, origin_image_data: \"WorkflowImageData\", **kwargs\n    ) -&gt; \"WorkflowImageData\":\n        \"\"\"\n        Creates new instance of `WorkflowImageData` with updated property.\n\n        Properties are passed by kwargs, supported properties are:\n        * parent_metadata\n        * workflow_root_ancestor_metadata\n        * image_reference\n        * base64_image\n        * numpy_image\n        * video_metadata\n\n        When more than one from [\"numpy_image\", \"base64_image\", \"image_reference\"] args are\n        given, they MUST be compliant.\n        \"\"\"\n        parent_metadata = origin_image_data._parent_metadata\n        workflow_root_ancestor_metadata = (\n            origin_image_data._workflow_root_ancestor_metadata\n        )\n        image_reference = origin_image_data._image_reference\n        base64_image = origin_image_data._base64_image\n        numpy_image = origin_image_data._numpy_image\n        video_metadata = origin_image_data._video_metadata\n        if any(k in kwargs for k in [\"numpy_image\", \"base64_image\", \"image_reference\"]):\n            numpy_image = kwargs.get(\"numpy_image\")\n            base64_image = kwargs.get(\"base64_image\")\n            image_reference = kwargs.get(\"image_reference\")\n        if \"parent_metadata\" in kwargs:\n            if workflow_root_ancestor_metadata is parent_metadata:\n                workflow_root_ancestor_metadata = kwargs[\"parent_metadata\"]\n            parent_metadata = kwargs[\"parent_metadata\"]\n        if \"workflow_root_ancestor_metadata\" in kwargs:\n            if parent_metadata is workflow_root_ancestor_metadata:\n                parent_metadata = kwargs[\"workflow_root_ancestor_metadata\"]\n            workflow_root_ancestor_metadata = kwargs[\"workflow_root_ancestor_metadata\"]\n        if \"video_metadata\" in kwargs:\n            video_metadata = kwargs[\"video_metadata\"]\n        return cls(\n            parent_metadata=parent_metadata,\n            workflow_root_ancestor_metadata=workflow_root_ancestor_metadata,\n            image_reference=image_reference,\n            base64_image=base64_image,\n            numpy_image=numpy_image,\n            video_metadata=video_metadata,\n        )\n\n    @classmethod\n    def create_crop(\n        cls,\n        origin_image_data: \"WorkflowImageData\",\n        crop_identifier: str,\n        cropped_image: np.ndarray,\n        offset_x: int,\n        offset_y: int,\n        preserve_video_metadata: bool = False,\n    ) -&gt; \"WorkflowImageData\":\n        \"\"\"\n        Creates new instance of `WorkflowImageData` being a crop of original image,\n        making adjustment to all metadata.\n        \"\"\"\n        parent_metadata = ImageParentMetadata(\n            parent_id=crop_identifier,\n            origin_coordinates=OriginCoordinatesSystem(\n                left_top_x=offset_x,\n                left_top_y=offset_y,\n                origin_width=origin_image_data.numpy_image.shape[1],\n                origin_height=origin_image_data.numpy_image.shape[0],\n            ),\n        )\n        workflow_root_ancestor_coordinates = replace(\n            origin_image_data.workflow_root_ancestor_metadata.origin_coordinates,\n            left_top_x=origin_image_data.workflow_root_ancestor_metadata.origin_coordinates.left_top_x\n            + offset_x,\n            left_top_y=origin_image_data.workflow_root_ancestor_metadata.origin_coordinates.left_top_y\n            + offset_y,\n        )\n        workflow_root_ancestor_metadata = ImageParentMetadata(\n            parent_id=origin_image_data.workflow_root_ancestor_metadata.parent_id,\n            origin_coordinates=workflow_root_ancestor_coordinates,\n        )\n        video_metadata = None\n        if preserve_video_metadata and origin_image_data._video_metadata is not None:\n            video_metadata = copy(origin_image_data._video_metadata)\n            video_metadata.video_identifier = (\n                f\"{video_metadata.video_identifier} | crop: {crop_identifier}\"\n            )\n        return WorkflowImageData(\n            parent_metadata=parent_metadata,\n            workflow_root_ancestor_metadata=workflow_root_ancestor_metadata,\n            numpy_image=cropped_image,\n            video_metadata=video_metadata,\n        )\n\n    @property\n    def parent_metadata(self) -&gt; ImageParentMetadata:\n        if self._parent_metadata.origin_coordinates is None:\n            numpy_image = self.numpy_image\n            origin_coordinates = OriginCoordinatesSystem(\n                left_top_y=0,\n                left_top_x=0,\n                origin_width=numpy_image.shape[1],\n                origin_height=numpy_image.shape[0],\n            )\n            self._parent_metadata = replace(\n                self._parent_metadata, origin_coordinates=origin_coordinates\n            )\n        return self._parent_metadata\n\n    @property\n    def workflow_root_ancestor_metadata(self) -&gt; ImageParentMetadata:\n        if self._workflow_root_ancestor_metadata.origin_coordinates is None:\n            numpy_image = self.numpy_image\n            origin_coordinates = OriginCoordinatesSystem(\n                left_top_y=0,\n                left_top_x=0,\n                origin_width=numpy_image.shape[1],\n                origin_height=numpy_image.shape[0],\n            )\n            self._workflow_root_ancestor_metadata = replace(\n                self._workflow_root_ancestor_metadata,\n                origin_coordinates=origin_coordinates,\n            )\n        return self._workflow_root_ancestor_metadata\n\n    @property\n    def numpy_image(self) -&gt; np.ndarray:\n        if self._numpy_image is not None:\n            return self._numpy_image\n        if self._base64_image:\n            self._numpy_image = attempt_loading_image_from_string(self._base64_image)[0]\n            return self._numpy_image\n        if self._image_reference.startswith(\n            \"http://\"\n        ) or self._image_reference.startswith(\"https://\"):\n            self._numpy_image = load_image_from_url(value=self._image_reference)\n        else:\n            self._numpy_image = cv2.imread(self._image_reference)\n        return self._numpy_image\n\n    @property\n    def base64_image(self) -&gt; str:\n        if self._base64_image is not None:\n            return self._base64_image\n        numpy_image = self.numpy_image\n        self._base64_image = base64.b64encode(\n            encode_image_to_jpeg_bytes(numpy_image)\n        ).decode(\"ascii\")\n        return self._base64_image\n\n    @property\n    def video_metadata(self) -&gt; VideoMetadata:\n        if self._video_metadata is not None:\n            return self._video_metadata\n        return VideoMetadata(\n            video_identifier=self.parent_metadata.parent_id,\n            frame_number=0,\n            frame_timestamp=datetime.now(),\n            fps=30,\n            comes_from_video_file=None,\n        )\n\n    def to_inference_format(self, numpy_preferred: bool = False) -&gt; Dict[str, Any]:\n        if numpy_preferred:\n            return {\"type\": \"numpy_object\", \"value\": self.numpy_image}\n        if self._image_reference:\n            if self._image_reference.startswith(\n                \"http://\"\n            ) or self._image_reference.startswith(\"https://\"):\n                return {\"type\": \"url\", \"value\": self._image_reference}\n            return {\"type\": \"file\", \"value\": self._image_reference}\n        if self._base64_image:\n            return {\"type\": \"base64\", \"value\": self.base64_image}\n        return {\"type\": \"numpy_object\", \"value\": self.numpy_image}\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/execution_engine/entities/base/#inference.core.workflows.execution_engine.entities.base.WorkflowImageData.copy_and_replace","title":"<code>copy_and_replace(origin_image_data, **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates new instance of <code>WorkflowImageData</code> with updated property.</p> <p>Properties are passed by kwargs, supported properties are: * parent_metadata * workflow_root_ancestor_metadata * image_reference * base64_image * numpy_image * video_metadata</p> <p>When more than one from [\"numpy_image\", \"base64_image\", \"image_reference\"] args are given, they MUST be compliant.</p> Source code in <code>inference/core/workflows/execution_engine/entities/base.py</code> <pre><code>@classmethod\ndef copy_and_replace(\n    cls, origin_image_data: \"WorkflowImageData\", **kwargs\n) -&gt; \"WorkflowImageData\":\n    \"\"\"\n    Creates new instance of `WorkflowImageData` with updated property.\n\n    Properties are passed by kwargs, supported properties are:\n    * parent_metadata\n    * workflow_root_ancestor_metadata\n    * image_reference\n    * base64_image\n    * numpy_image\n    * video_metadata\n\n    When more than one from [\"numpy_image\", \"base64_image\", \"image_reference\"] args are\n    given, they MUST be compliant.\n    \"\"\"\n    parent_metadata = origin_image_data._parent_metadata\n    workflow_root_ancestor_metadata = (\n        origin_image_data._workflow_root_ancestor_metadata\n    )\n    image_reference = origin_image_data._image_reference\n    base64_image = origin_image_data._base64_image\n    numpy_image = origin_image_data._numpy_image\n    video_metadata = origin_image_data._video_metadata\n    if any(k in kwargs for k in [\"numpy_image\", \"base64_image\", \"image_reference\"]):\n        numpy_image = kwargs.get(\"numpy_image\")\n        base64_image = kwargs.get(\"base64_image\")\n        image_reference = kwargs.get(\"image_reference\")\n    if \"parent_metadata\" in kwargs:\n        if workflow_root_ancestor_metadata is parent_metadata:\n            workflow_root_ancestor_metadata = kwargs[\"parent_metadata\"]\n        parent_metadata = kwargs[\"parent_metadata\"]\n    if \"workflow_root_ancestor_metadata\" in kwargs:\n        if parent_metadata is workflow_root_ancestor_metadata:\n            parent_metadata = kwargs[\"workflow_root_ancestor_metadata\"]\n        workflow_root_ancestor_metadata = kwargs[\"workflow_root_ancestor_metadata\"]\n    if \"video_metadata\" in kwargs:\n        video_metadata = kwargs[\"video_metadata\"]\n    return cls(\n        parent_metadata=parent_metadata,\n        workflow_root_ancestor_metadata=workflow_root_ancestor_metadata,\n        image_reference=image_reference,\n        base64_image=base64_image,\n        numpy_image=numpy_image,\n        video_metadata=video_metadata,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/execution_engine/entities/base/#inference.core.workflows.execution_engine.entities.base.WorkflowImageData.create_crop","title":"<code>create_crop(origin_image_data, crop_identifier, cropped_image, offset_x, offset_y, preserve_video_metadata=False)</code>  <code>classmethod</code>","text":"<p>Creates new instance of <code>WorkflowImageData</code> being a crop of original image, making adjustment to all metadata.</p> Source code in <code>inference/core/workflows/execution_engine/entities/base.py</code> <pre><code>@classmethod\ndef create_crop(\n    cls,\n    origin_image_data: \"WorkflowImageData\",\n    crop_identifier: str,\n    cropped_image: np.ndarray,\n    offset_x: int,\n    offset_y: int,\n    preserve_video_metadata: bool = False,\n) -&gt; \"WorkflowImageData\":\n    \"\"\"\n    Creates new instance of `WorkflowImageData` being a crop of original image,\n    making adjustment to all metadata.\n    \"\"\"\n    parent_metadata = ImageParentMetadata(\n        parent_id=crop_identifier,\n        origin_coordinates=OriginCoordinatesSystem(\n            left_top_x=offset_x,\n            left_top_y=offset_y,\n            origin_width=origin_image_data.numpy_image.shape[1],\n            origin_height=origin_image_data.numpy_image.shape[0],\n        ),\n    )\n    workflow_root_ancestor_coordinates = replace(\n        origin_image_data.workflow_root_ancestor_metadata.origin_coordinates,\n        left_top_x=origin_image_data.workflow_root_ancestor_metadata.origin_coordinates.left_top_x\n        + offset_x,\n        left_top_y=origin_image_data.workflow_root_ancestor_metadata.origin_coordinates.left_top_y\n        + offset_y,\n    )\n    workflow_root_ancestor_metadata = ImageParentMetadata(\n        parent_id=origin_image_data.workflow_root_ancestor_metadata.parent_id,\n        origin_coordinates=workflow_root_ancestor_coordinates,\n    )\n    video_metadata = None\n    if preserve_video_metadata and origin_image_data._video_metadata is not None:\n        video_metadata = copy(origin_image_data._video_metadata)\n        video_metadata.video_identifier = (\n            f\"{video_metadata.video_identifier} | crop: {crop_identifier}\"\n        )\n    return WorkflowImageData(\n        parent_metadata=parent_metadata,\n        workflow_root_ancestor_metadata=workflow_root_ancestor_metadata,\n        numpy_image=cropped_image,\n        video_metadata=video_metadata,\n    )\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/execution_engine/entities/engine/","title":"engine","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/entities/types/","title":"types","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/introspection/blocks_loader/","title":"blocks_loader","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/introspection/connections_discovery/","title":"connections_discovery","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/introspection/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/introspection/schema_parser/","title":"schema_parser","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/introspection/selectors_parser/","title":"selectors_parser","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/introspection/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/profiling/core/","title":"core","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/core/","title":"core","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/cache/","title":"cache","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/cache/#inference.core.workflows.execution_engine.v1.compiler.cache.BasicWorkflowsCache","title":"<code>BasicWorkflowsCache</code>","text":"<p>               Bases: <code>Generic[V]</code></p> <p>Base cache which is capable of hashing compound payloads based on list of injected hash functions. Hash functions are to produce stable hashing strings. Each function is invoked on <code>get_hash_key(...)</code> kwarg (use named args only!), output string is concatenated and md5 value is calculated.</p> <p>Cache is size bounded, each entry lives until <code>cache_size</code> new entries appear.</p> <p>Raises <code>WorkflowEnvironmentConfigurationError</code> when <code>get_hash_key(...)</code> is not provided with params corresponding to all hash functions.</p> <p>Thread safe thanks to thread lock on <code>get(...)</code> and <code>cache(...)</code>.</p> Source code in <code>inference/core/workflows/execution_engine/v1/compiler/cache.py</code> <pre><code>class BasicWorkflowsCache(Generic[V]):\n    \"\"\"\n    Base cache which is capable of hashing compound payloads based on\n    list of injected hash functions. Hash functions are to produce stable hashing strings.\n    Each function is invoked on `get_hash_key(...)` kwarg (use named args only!),\n    output string is concatenated and md5 value is calculated.\n\n    Cache is size bounded, each entry lives until `cache_size` new entries appear.\n\n    Raises `WorkflowEnvironmentConfigurationError` when `get_hash_key(...)` is not\n    provided with params corresponding to all hash functions.\n\n    Thread safe thanks to thread lock on `get(...)` and `cache(...)`.\n    \"\"\"\n\n    def __init__(\n        self,\n        cache_size: int,\n        hash_functions: List[Tuple[str, Callable[[Any], str]]],\n    ):\n        self._keys_buffer = deque(maxlen=max(cache_size, 1))\n        self._cache: Dict[str, V] = {}\n        self._hash_functions = hash_functions\n        self._cache_lock = Lock()\n\n    def get_hash_key(self, **kwargs) -&gt; str:\n        hash_chunks = []\n        for key_name, hashing_function in self._hash_functions:\n            if key_name not in kwargs:\n                raise WorkflowEnvironmentConfigurationError(\n                    public_message=f\"Cache is miss configured.\",\n                    context=\"workflows_cache | hash_key_generation\",\n                )\n            hash_value = hashing_function(kwargs[key_name])\n            hash_chunks.append(hash_value)\n        return hashlib.md5(\"&lt;|&gt;\".join(hash_chunks).encode(\"utf-8\")).hexdigest()\n\n    def get(self, key: str) -&gt; Optional[V]:\n        with self._cache_lock:\n            return self._cache.get(key)\n\n    def cache(self, key: str, value: V) -&gt; None:\n        with self._cache_lock:\n            if len(self._keys_buffer) == self._keys_buffer.maxlen:\n                to_pop = self._keys_buffer.popleft()\n                del self._cache[to_pop]\n            self._keys_buffer.append(key)\n            self._cache[key] = value\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/core/","title":"core","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/graph_constructor/","title":"graph_constructor","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/graph_traversal/","title":"graph_traversal","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/graph_traversal/#inference.core.workflows.execution_engine.v1.compiler.graph_traversal.traverse_graph_ensuring_parents_are_reached_first","title":"<code>traverse_graph_ensuring_parents_are_reached_first(graph, start_node)</code>","text":"<p>This function works under assumption of common super-input node in the graph - otherwise, there is no common entry point to put as <code>start_node</code>.</p> Source code in <code>inference/core/workflows/execution_engine/v1/compiler/graph_traversal.py</code> <pre><code>def traverse_graph_ensuring_parents_are_reached_first(\n    graph: DiGraph,\n    start_node: str,\n) -&gt; List[str]:\n    \"\"\"\n    This function works under assumption of common super-input node in the graph - otherwise,\n    there is no common entry point to put as `start_node`.\n    \"\"\"\n    graph_copy = graph.copy()\n    distance_key = \"distance\"\n    graph_copy = assign_max_distances_from_start(\n        graph=graph_copy,\n        start_node=start_node,\n        distance_key=distance_key,\n    )\n    nodes_groups = group_nodes_by_sorted_key_value(graph=graph_copy, key=distance_key)\n    return [node for node_group in nodes_groups for node in node_group]\n</code></pre>"},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/reference_type_checker/","title":"reference_type_checker","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/steps_initialiser/","title":"steps_initialiser","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/syntactic_parser/","title":"syntactic_parser","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/compiler/validator/","title":"validator","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/debugger/core/","title":"core","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/dynamic_blocks/block_assembler/","title":"block_assembler","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/dynamic_blocks/block_scaffolding/","title":"block_scaffolding","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/dynamic_blocks/entities/","title":"entities","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/core/","title":"core","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/flow_coordinator/","title":"flow_coordinator","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/output_constructor/","title":"output_constructor","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/runtime_input_assembler/","title":"runtime_input_assembler","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/runtime_input_validator/","title":"runtime_input_validator","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/utils/","title":"utils","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/execution_data_manager/branching_manager/","title":"branching_manager","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/execution_data_manager/dynamic_batches_manager/","title":"dynamic_batches_manager","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/execution_data_manager/execution_cache/","title":"execution_cache","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/execution_data_manager/manager/","title":"manager","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/executor/execution_data_manager/step_input_assembler/","title":"step_input_assembler","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/introspection/inputs_discovery/","title":"inputs_discovery","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/introspection/kinds_schemas/","title":"kinds_schemas","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/introspection/kinds_schemas_register/","title":"kinds_schemas_register","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/introspection/outputs_discovery/","title":"outputs_discovery","text":""},{"location":"docs/reference/inference/core/workflows/execution_engine/v1/introspection/types_discovery/","title":"types_discovery","text":""},{"location":"docs/reference/inference/core/workflows/prototypes/block/","title":"block","text":""},{"location":"docs/reference/inference/enterprise/parallel/dispatch_manager/","title":"dispatch_manager","text":""},{"location":"docs/reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker","title":"<code>ResultsChecker</code>","text":"<p>Class responsible for queuing asyncronous inference runs, keeping track of running requests, and awaiting their results.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>class ResultsChecker:\n    \"\"\"\n    Class responsible for queuing asyncronous inference runs,\n    keeping track of running requests, and awaiting their results.\n    \"\"\"\n\n    def __init__(self, redis: Redis):\n        self.tasks: Dict[str, asyncio.Event] = {}\n        self.dones = dict()\n        self.errors = dict()\n        self.running = True\n        self.redis = redis\n        self.semaphore: BoundedSemaphore = BoundedSemaphore(NUM_PARALLEL_TASKS)\n\n    async def add_task(self, task_id: str, request: InferenceRequest):\n        \"\"\"\n        Wait until there's available cylce to queue a task.\n        When there are cycles, add the task's id to a list to keep track of its results,\n        launch the preprocess celeryt task, set the task's status to in progress in redis.\n        \"\"\"\n        await self.semaphore.acquire()\n        self.tasks[task_id] = asyncio.Event()\n        preprocess.s(request.dict()).delay()\n\n    def get_result(self, task_id: str) -&gt; Any:\n        \"\"\"\n        Check the done tasks and errored tasks for this task id.\n        \"\"\"\n        if task_id in self.dones:\n            return self.dones.pop(task_id)\n        elif task_id in self.errors:\n            message = self.errors.pop(task_id)\n            raise Exception(message)\n        else:\n            raise RuntimeError(\n                \"Task result not found in either success or error dict. Unreachable\"\n            )\n\n    async def loop(self):\n        \"\"\"\n        Main loop. Check all in progress tasks for their status, and if their status is final,\n        (either failure or success) then add their results to the appropriate results dictionary.\n        \"\"\"\n        async with self.redis.pubsub() as pubsub:\n            await pubsub.subscribe(\"results\")\n            async for message in pubsub.listen():\n                if message[\"type\"] != \"message\":\n                    continue\n                message = orjson.loads(message[\"data\"])\n                task_id = message.pop(\"task_id\")\n                if task_id not in self.tasks:\n                    continue\n                self.semaphore.release()\n                status = message.pop(\"status\")\n                if status == FAILURE_STATE:\n                    self.errors[task_id] = message[\"payload\"]\n                elif status == SUCCESS_STATE:\n                    self.dones[task_id] = message[\"payload\"]\n                else:\n                    raise RuntimeError(\n                        \"Task result not found in possible states. Unreachable\"\n                    )\n                self.tasks[task_id].set()\n                await asyncio.sleep(0)\n\n    async def wait_for_response(self, key: str):\n        event = self.tasks[key]\n        await event.wait()\n        del self.tasks[key]\n        return self.get_result(key)\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker.add_task","title":"<code>add_task(task_id, request)</code>  <code>async</code>","text":"<p>Wait until there's available cylce to queue a task. When there are cycles, add the task's id to a list to keep track of its results, launch the preprocess celeryt task, set the task's status to in progress in redis.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>async def add_task(self, task_id: str, request: InferenceRequest):\n    \"\"\"\n    Wait until there's available cylce to queue a task.\n    When there are cycles, add the task's id to a list to keep track of its results,\n    launch the preprocess celeryt task, set the task's status to in progress in redis.\n    \"\"\"\n    await self.semaphore.acquire()\n    self.tasks[task_id] = asyncio.Event()\n    preprocess.s(request.dict()).delay()\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker.get_result","title":"<code>get_result(task_id)</code>","text":"<p>Check the done tasks and errored tasks for this task id.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>def get_result(self, task_id: str) -&gt; Any:\n    \"\"\"\n    Check the done tasks and errored tasks for this task id.\n    \"\"\"\n    if task_id in self.dones:\n        return self.dones.pop(task_id)\n    elif task_id in self.errors:\n        message = self.errors.pop(task_id)\n        raise Exception(message)\n    else:\n        raise RuntimeError(\n            \"Task result not found in either success or error dict. Unreachable\"\n        )\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/dispatch_manager/#inference.enterprise.parallel.dispatch_manager.ResultsChecker.loop","title":"<code>loop()</code>  <code>async</code>","text":"<p>Main loop. Check all in progress tasks for their status, and if their status is final, (either failure or success) then add their results to the appropriate results dictionary.</p> Source code in <code>inference/enterprise/parallel/dispatch_manager.py</code> <pre><code>async def loop(self):\n    \"\"\"\n    Main loop. Check all in progress tasks for their status, and if their status is final,\n    (either failure or success) then add their results to the appropriate results dictionary.\n    \"\"\"\n    async with self.redis.pubsub() as pubsub:\n        await pubsub.subscribe(\"results\")\n        async for message in pubsub.listen():\n            if message[\"type\"] != \"message\":\n                continue\n            message = orjson.loads(message[\"data\"])\n            task_id = message.pop(\"task_id\")\n            if task_id not in self.tasks:\n                continue\n            self.semaphore.release()\n            status = message.pop(\"status\")\n            if status == FAILURE_STATE:\n                self.errors[task_id] = message[\"payload\"]\n            elif status == SUCCESS_STATE:\n                self.dones[task_id] = message[\"payload\"]\n            else:\n                raise RuntimeError(\n                    \"Task result not found in possible states. Unreachable\"\n                )\n            self.tasks[task_id].set()\n            await asyncio.sleep(0)\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/entrypoint/","title":"entrypoint","text":""},{"location":"docs/reference/inference/enterprise/parallel/infer/","title":"infer","text":""},{"location":"docs/reference/inference/enterprise/parallel/infer/#inference.enterprise.parallel.infer.get_batch","title":"<code>get_batch(redis, model_names)</code>","text":"<p>Run a heuristic to select the best batch to infer on redis[Redis]: redis client model_names[List[str]]: list of models with nonzero number of requests returns:     Tuple[List[Dict], str]     List[Dict] represents a batch of request dicts     str is the model id</p> Source code in <code>inference/enterprise/parallel/infer.py</code> <pre><code>def get_batch(redis: Redis, model_names: List[str]) -&gt; Tuple[List[Dict], str]:\n    \"\"\"\n    Run a heuristic to select the best batch to infer on\n    redis[Redis]: redis client\n    model_names[List[str]]: list of models with nonzero number of requests\n    returns:\n        Tuple[List[Dict], str]\n        List[Dict] represents a batch of request dicts\n        str is the model id\n    \"\"\"\n    batch_sizes = [\n        RoboflowInferenceModel.model_metadata_from_memcache_endpoint(m)[\"batch_size\"]\n        for m in model_names\n    ]\n    batch_sizes = [b if not isinstance(b, str) else BATCH_SIZE for b in batch_sizes]\n    batches = [\n        redis.zrange(f\"infer:{m}\", 0, b - 1, withscores=True)\n        for m, b in zip(model_names, batch_sizes)\n    ]\n    model_index = select_best_inference_batch(batches, batch_sizes)\n    batch = batches[model_index]\n    selected_model = model_names[model_index]\n    redis.zrem(f\"infer:{selected_model}\", *[b[0] for b in batch])\n    redis.hincrby(f\"requests\", selected_model, -len(batch))\n    batch = [orjson.loads(b[0]) for b in batch]\n    return batch, selected_model\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/infer/#inference.enterprise.parallel.infer.write_infer_arrays_and_launch_postprocess","title":"<code>write_infer_arrays_and_launch_postprocess(arrs, request, preproc_return_metadata)</code>","text":"<p>Write inference results to shared memory and launch the postprocessing task</p> Source code in <code>inference/enterprise/parallel/infer.py</code> <pre><code>def write_infer_arrays_and_launch_postprocess(\n    arrs: Tuple[np.ndarray, ...],\n    request: InferenceRequest,\n    preproc_return_metadata: Dict,\n):\n    \"\"\"Write inference results to shared memory and launch the postprocessing task\"\"\"\n    shms = [shared_memory.SharedMemory(create=True, size=arr.nbytes) for arr in arrs]\n    with shm_manager(*shms):\n        shm_metadatas = []\n        for arr, shm in zip(arrs, shms):\n            shared = np.ndarray(arr.shape, dtype=arr.dtype, buffer=shm.buf)\n            shared[:] = arr[:]\n            shm_metadata = SharedMemoryMetadata(\n                shm_name=shm.name, array_shape=arr.shape, array_dtype=arr.dtype.name\n            )\n            shm_metadatas.append(asdict(shm_metadata))\n\n        postprocess.s(\n            tuple(shm_metadatas), request.dict(), preproc_return_metadata\n        ).delay()\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/parallel_http_api/","title":"parallel_http_api","text":""},{"location":"docs/reference/inference/enterprise/parallel/parallel_http_config/","title":"parallel_http_config","text":""},{"location":"docs/reference/inference/enterprise/parallel/tasks/","title":"tasks","text":""},{"location":"docs/reference/inference/enterprise/parallel/utils/","title":"utils","text":""},{"location":"docs/reference/inference/enterprise/parallel/utils/#inference.enterprise.parallel.utils.SharedMemoryMetadata","title":"<code>SharedMemoryMetadata</code>  <code>dataclass</code>","text":"<p>Info needed to load array from shared memory</p> Source code in <code>inference/enterprise/parallel/utils.py</code> <pre><code>@dataclass\nclass SharedMemoryMetadata:\n    \"\"\"Info needed to load array from shared memory\"\"\"\n\n    shm_name: str\n    array_shape: List[int]\n    array_dtype: str\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/utils/#inference.enterprise.parallel.utils.failure_handler","title":"<code>failure_handler(redis, *request_ids)</code>","text":"<p>Context manager that updates the status/results key in redis with exception info on failure.</p> Source code in <code>inference/enterprise/parallel/utils.py</code> <pre><code>@contextmanager\ndef failure_handler(redis: Redis, *request_ids: str):\n    \"\"\"\n    Context manager that updates the status/results key in redis with exception\n    info on failure.\n    \"\"\"\n    try:\n        yield\n    except Exception as error:\n        message = type(error).__name__ + \": \" + str(error)\n        for request_id in request_ids:\n            redis.publish(\n                \"results\",\n                json.dumps(\n                    {\"task_id\": request_id, \"status\": FAILURE_STATE, \"payload\": message}\n                ),\n            )\n        raise\n</code></pre>"},{"location":"docs/reference/inference/enterprise/parallel/utils/#inference.enterprise.parallel.utils.shm_manager","title":"<code>shm_manager(*shms, unlink_on_success=False)</code>","text":"<p>Context manager that closes and frees shared memory objects.</p> Source code in <code>inference/enterprise/parallel/utils.py</code> <pre><code>@contextmanager\ndef shm_manager(\n    *shms: Union[str, shared_memory.SharedMemory], unlink_on_success: bool = False\n):\n    \"\"\"Context manager that closes and frees shared memory objects.\"\"\"\n    try:\n        loaded_shms = []\n        for shm in shms:\n            errors = []\n            try:\n                if isinstance(shm, str):\n                    shm = shared_memory.SharedMemory(name=shm)\n                loaded_shms.append(shm)\n            except BaseException as error:\n                errors.append(error)\n            if errors:\n                raise Exception(errors)\n\n        yield loaded_shms\n    except:\n        for shm in loaded_shms:\n            shm.close()\n            shm.unlink()\n        raise\n    else:\n        for shm in loaded_shms:\n            shm.close()\n            if unlink_on_success:\n                shm.unlink()\n</code></pre>"},{"location":"docs/reference/inference/enterprise/stream_management/api/app/","title":"app","text":""},{"location":"docs/reference/inference/enterprise/stream_management/api/entities/","title":"entities","text":""},{"location":"docs/reference/inference/enterprise/stream_management/api/errors/","title":"errors","text":""},{"location":"docs/reference/inference/enterprise/stream_management/api/stream_manager_client/","title":"stream_manager_client","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/app/","title":"app","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/communication/","title":"communication","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/entities/","title":"entities","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/errors/","title":"errors","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/inference_pipeline_manager/","title":"inference_pipeline_manager","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/serialisation/","title":"serialisation","text":""},{"location":"docs/reference/inference/enterprise/stream_management/manager/tcp_server/","title":"tcp_server","text":""},{"location":"docs/reference/inference/models/aliases/","title":"aliases","text":""},{"location":"docs/reference/inference/models/utils/","title":"utils","text":""},{"location":"docs/reference/inference/models/clip/clip_model/","title":"clip_model","text":""},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip","title":"<code>Clip</code>","text":"<p>               Bases: <code>OnnxRoboflowCoreModel</code></p> <p>Roboflow ONNX ClipModel model.</p> <p>This class is responsible for handling the ONNX ClipModel model, including loading the model, preprocessing the input, and performing inference.</p> <p>Attributes:</p> Name Type Description <code>visual_onnx_session</code> <code>InferenceSession</code> <p>ONNX Runtime session for visual inference.</p> <code>textual_onnx_session</code> <code>InferenceSession</code> <p>ONNX Runtime session for textual inference.</p> <code>resolution</code> <code>int</code> <p>The resolution of the input image.</p> <code>clip_preprocess</code> <code>function</code> <p>Function to preprocess the image.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>class Clip(OnnxRoboflowCoreModel):\n    \"\"\"Roboflow ONNX ClipModel model.\n\n    This class is responsible for handling the ONNX ClipModel model, including\n    loading the model, preprocessing the input, and performing inference.\n\n    Attributes:\n        visual_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for visual inference.\n        textual_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for textual inference.\n        resolution (int): The resolution of the input image.\n        clip_preprocess (function): Function to preprocess the image.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        model_id: str = CLIP_MODEL_ID,\n        onnxruntime_execution_providers: List[\n            str\n        ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n        **kwargs,\n    ):\n        \"\"\"Initializes the Clip with the given arguments and keyword arguments.\"\"\"\n        self.onnxruntime_execution_providers = onnxruntime_execution_providers\n        t1 = perf_counter()\n        super().__init__(*args, model_id=model_id, **kwargs)\n        # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n        self.log(\"Creating inference sessions\")\n        self.visual_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"visual.onnx\"),\n            providers=self.onnxruntime_execution_providers,\n        )\n\n        self.textual_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"textual.onnx\"),\n            providers=self.onnxruntime_execution_providers,\n        )\n\n        if REQUIRED_ONNX_PROVIDERS:\n            available_providers = onnxruntime.get_available_providers()\n            for provider in REQUIRED_ONNX_PROVIDERS:\n                if provider not in available_providers:\n                    raise OnnxProviderNotAvailable(\n                        f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                    )\n\n        self.resolution = self.visual_onnx_session.get_inputs()[0].shape[2]\n\n        self.clip_preprocess = clip.clip._transform(self.resolution)\n        self.log(f\"CLIP model loaded in {perf_counter() - t1:.2f} seconds\")\n        self.task_type = \"embedding\"\n\n    def compare(\n        self,\n        subject: Any,\n        prompt: Any,\n        subject_type: str = \"image\",\n        prompt_type: Union[str, List[str], Dict[str, Any]] = \"text\",\n        **kwargs,\n    ) -&gt; Union[List[float], Dict[str, float]]:\n        \"\"\"\n        Compares the subject with the prompt to calculate similarity scores.\n\n        Args:\n            subject (Any): The subject data to be compared. Can be either an image or text.\n            prompt (Any): The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.\n            subject_type (str, optional): Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".\n            prompt_type (Union[str, List[str], Dict[str, Any]], optional): Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s). If prompt is a dictionary, returns a dictionary with keys corresponding to the original prompt dictionary's keys.\n\n        Raises:\n            ValueError: If subject_type or prompt_type is neither \"image\" nor \"text\".\n            ValueError: If the number of prompts exceeds the maximum batch size.\n        \"\"\"\n\n        if subject_type == \"image\":\n            subject_embeddings = self.embed_image(subject)\n        elif subject_type == \"text\":\n            subject_embeddings = self.embed_text(subject)\n        else:\n            raise ValueError(\n                \"subject_type must be either 'image' or 'text', but got {request.subject_type}\"\n            )\n\n        if isinstance(prompt, dict) and not (\"type\" in prompt and \"value\" in prompt):\n            prompt_keys = prompt.keys()\n            prompt = [prompt[k] for k in prompt_keys]\n            prompt_obj = \"dict\"\n        else:\n            prompt = prompt\n            if not isinstance(prompt, list):\n                prompt = [prompt]\n            prompt_obj = \"list\"\n\n        if len(prompt) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of prompts that can be compared at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n\n        if prompt_type == \"image\":\n            prompt_embeddings = self.embed_image(prompt)\n        elif prompt_type == \"text\":\n            prompt_embeddings = self.embed_text(prompt)\n        else:\n            raise ValueError(\n                \"prompt_type must be either 'image' or 'text', but got {request.prompt_type}\"\n            )\n\n        similarities = [\n            cosine_similarity(subject_embeddings, p) for p in prompt_embeddings\n        ]\n\n        if prompt_obj == \"dict\":\n            similarities = dict(zip(prompt_keys, similarities))\n\n        return similarities\n\n    def make_compare_response(\n        self, similarities: Union[List[float], Dict[str, float]]\n    ) -&gt; ClipCompareResponse:\n        \"\"\"\n        Creates a ClipCompareResponse object from the provided similarity data.\n\n        Args:\n            similarities (Union[List[float], Dict[str, float]]): A list or dictionary containing similarity scores.\n\n        Returns:\n            ClipCompareResponse: An instance of the ClipCompareResponse with the given similarity scores.\n\n        Example:\n            Assuming `ClipCompareResponse` expects a dictionary of string-float pairs:\n\n            &gt;&gt;&gt; make_compare_response({\"image1\": 0.98, \"image2\": 0.76})\n            ClipCompareResponse(similarity={\"image1\": 0.98, \"image2\": 0.76})\n        \"\"\"\n        response = ClipCompareResponse(similarity=similarities)\n        return response\n\n    def embed_image(\n        self,\n        image: Any,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Embeds an image or a list of images using the Clip model.\n\n        Args:\n            image (Any): The image or list of images to be embedded. Image can be in any format that is acceptable by the preproc_image method.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            np.ndarray: The embeddings of the image(s) as a numpy array.\n\n        Raises:\n            ValueError: If the number of images in the list exceeds the maximum batch size.\n\n        Notes:\n            The function measures performance using perf_counter and also has support for ONNX session to get embeddings.\n        \"\"\"\n        t1 = perf_counter()\n\n        if isinstance(image, list):\n            if len(image) &gt; CLIP_MAX_BATCH_SIZE:\n                raise ValueError(\n                    f\"The maximum number of images that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n                )\n            imgs = [self.preproc_image(i) for i in image]\n            img_in = np.concatenate(imgs, axis=0)\n        else:\n            img_in = self.preproc_image(image)\n\n        onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n        embeddings = self.visual_onnx_session.run(None, onnx_input_image)[0]\n\n        return embeddings\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n        embeddings = self.visual_onnx_session.run(None, onnx_input_image)[0]\n        return (embeddings,)\n\n    def make_embed_image_response(\n        self, embeddings: np.ndarray\n    ) -&gt; ClipEmbeddingResponse:\n        \"\"\"\n        Converts the given embeddings into a ClipEmbeddingResponse object.\n\n        Args:\n            embeddings (np.ndarray): A numpy array containing the embeddings for an image or images.\n\n        Returns:\n            ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n        Example:\n            &gt;&gt;&gt; embeddings_array = np.array([[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n            &gt;&gt;&gt; make_embed_image_response(embeddings_array)\n            ClipEmbeddingResponse(embeddings=[[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n        \"\"\"\n        response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n\n        return response\n\n    def embed_text(\n        self,\n        text: Union[str, List[str]],\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Embeds a text or a list of texts using the Clip model.\n\n        Args:\n            text (Union[str, List[str]]): The text string or list of text strings to be embedded.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            np.ndarray: The embeddings of the text or texts as a numpy array.\n\n        Raises:\n            ValueError: If the number of text strings in the list exceeds the maximum batch size.\n\n        Notes:\n            The function utilizes an ONNX session to compute embeddings and measures the embedding time with perf_counter.\n        \"\"\"\n        if isinstance(text, list):\n            texts = text\n        else:\n            texts = [text]\n        results = []\n        for texts_batch in create_batches(\n            sequence=texts, batch_size=CLIP_MAX_BATCH_SIZE\n        ):\n            tokenized_batch = clip.tokenize(texts_batch).numpy().astype(np.int32)\n            onnx_input_text = {\n                self.textual_onnx_session.get_inputs()[0].name: tokenized_batch\n            }\n            embeddings = self.textual_onnx_session.run(None, onnx_input_text)[0]\n            results.append(embeddings)\n        return np.concatenate(results, axis=0)\n\n    def make_embed_text_response(self, embeddings: np.ndarray) -&gt; ClipEmbeddingResponse:\n        \"\"\"\n        Converts the given text embeddings into a ClipEmbeddingResponse object.\n\n        Args:\n            embeddings (np.ndarray): A numpy array containing the embeddings for a text or texts.\n\n        Returns:\n            ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n        Example:\n            &gt;&gt;&gt; embeddings_array = np.array([[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n            &gt;&gt;&gt; make_embed_text_response(embeddings_array)\n            ClipEmbeddingResponse(embeddings=[[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n        \"\"\"\n        response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n        return response\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: The list of file names.\n        \"\"\"\n        return [\"textual.onnx\", \"visual.onnx\"]\n\n    def infer_from_request(\n        self, request: ClipInferenceRequest\n    ) -&gt; ClipEmbeddingResponse:\n        \"\"\"Routes the request to the appropriate inference function.\n\n        Args:\n            request (ClipInferenceRequest): The request object containing the inference details.\n\n        Returns:\n            ClipEmbeddingResponse: The response object containing the embeddings.\n        \"\"\"\n        t1 = perf_counter()\n        if isinstance(request, ClipImageEmbeddingRequest):\n            infer_func = self.embed_image\n            make_response_func = self.make_embed_image_response\n        elif isinstance(request, ClipTextEmbeddingRequest):\n            infer_func = self.embed_text\n            make_response_func = self.make_embed_text_response\n        elif isinstance(request, ClipCompareRequest):\n            infer_func = self.compare\n            make_response_func = self.make_compare_response\n        else:\n            raise ValueError(\n                f\"Request type {type(request)} is not a valid ClipInferenceRequest\"\n            )\n        data = infer_func(**request.dict())\n        response = make_response_func(data)\n        response.time = perf_counter() - t1\n        return response\n\n    def make_response(self, embeddings, *args, **kwargs) -&gt; InferenceResponse:\n        return [self.make_embed_image_response(embeddings)]\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; Any:\n        return [self.make_embed_image_response(predictions[0])]\n\n    def infer(self, image: Any, **kwargs) -&gt; Any:\n        \"\"\"Embeds an image\n        - image:\n            can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        \"\"\"\n        return super().infer(image, **kwargs)\n\n    def preproc_image(self, image: InferenceRequestImage) -&gt; np.ndarray:\n        \"\"\"Preprocesses an inference request image.\n\n        Args:\n            image (InferenceRequestImage): The object containing information necessary to load the image for inference.\n\n        Returns:\n            np.ndarray: A numpy array of the preprocessed image pixel data.\n        \"\"\"\n        pil_image = Image.fromarray(load_image_rgb(image))\n        preprocessed_image = self.clip_preprocess(pil_image)\n\n        img_in = np.expand_dims(preprocessed_image, axis=0)\n\n        return img_in.astype(np.float32)\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        return self.preproc_image(image), PreprocessReturnMetadata({})\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.__init__","title":"<code>__init__(*args, model_id=CLIP_MODEL_ID, onnxruntime_execution_providers=get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS), **kwargs)</code>","text":"<p>Initializes the Clip with the given arguments and keyword arguments.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    model_id: str = CLIP_MODEL_ID,\n    onnxruntime_execution_providers: List[\n        str\n    ] = get_onnxruntime_execution_providers(ONNXRUNTIME_EXECUTION_PROVIDERS),\n    **kwargs,\n):\n    \"\"\"Initializes the Clip with the given arguments and keyword arguments.\"\"\"\n    self.onnxruntime_execution_providers = onnxruntime_execution_providers\n    t1 = perf_counter()\n    super().__init__(*args, model_id=model_id, **kwargs)\n    # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n    self.log(\"Creating inference sessions\")\n    self.visual_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"visual.onnx\"),\n        providers=self.onnxruntime_execution_providers,\n    )\n\n    self.textual_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"textual.onnx\"),\n        providers=self.onnxruntime_execution_providers,\n    )\n\n    if REQUIRED_ONNX_PROVIDERS:\n        available_providers = onnxruntime.get_available_providers()\n        for provider in REQUIRED_ONNX_PROVIDERS:\n            if provider not in available_providers:\n                raise OnnxProviderNotAvailable(\n                    f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                )\n\n    self.resolution = self.visual_onnx_session.get_inputs()[0].shape[2]\n\n    self.clip_preprocess = clip.clip._transform(self.resolution)\n    self.log(f\"CLIP model loaded in {perf_counter() - t1:.2f} seconds\")\n    self.task_type = \"embedding\"\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.compare","title":"<code>compare(subject, prompt, subject_type='image', prompt_type='text', **kwargs)</code>","text":"<p>Compares the subject with the prompt to calculate similarity scores.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>Any</code> <p>The subject data to be compared. Can be either an image or text.</p> required <code>prompt</code> <code>Any</code> <p>The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.</p> required <code>subject_type</code> <code>str</code> <p>Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".</p> <code>'image'</code> <code>prompt_type</code> <code>Union[str, List[str], Dict[str, Any]]</code> <p>Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".</p> <code>'text'</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[float], Dict[str, float]]</code> <p>Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s). If prompt is a dictionary, returns a dictionary with keys corresponding to the original prompt dictionary's keys.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If subject_type or prompt_type is neither \"image\" nor \"text\".</p> <code>ValueError</code> <p>If the number of prompts exceeds the maximum batch size.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def compare(\n    self,\n    subject: Any,\n    prompt: Any,\n    subject_type: str = \"image\",\n    prompt_type: Union[str, List[str], Dict[str, Any]] = \"text\",\n    **kwargs,\n) -&gt; Union[List[float], Dict[str, float]]:\n    \"\"\"\n    Compares the subject with the prompt to calculate similarity scores.\n\n    Args:\n        subject (Any): The subject data to be compared. Can be either an image or text.\n        prompt (Any): The prompt data to be compared against the subject. Can be a single value (image/text), list of values, or dictionary of values.\n        subject_type (str, optional): Specifies the type of the subject data. Must be either \"image\" or \"text\". Defaults to \"image\".\n        prompt_type (Union[str, List[str], Dict[str, Any]], optional): Specifies the type of the prompt data. Can be \"image\", \"text\", list of these types, or a dictionary containing these types. Defaults to \"text\".\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Union[List[float], Dict[str, float]]: A list or dictionary containing cosine similarity scores between the subject and prompt(s). If prompt is a dictionary, returns a dictionary with keys corresponding to the original prompt dictionary's keys.\n\n    Raises:\n        ValueError: If subject_type or prompt_type is neither \"image\" nor \"text\".\n        ValueError: If the number of prompts exceeds the maximum batch size.\n    \"\"\"\n\n    if subject_type == \"image\":\n        subject_embeddings = self.embed_image(subject)\n    elif subject_type == \"text\":\n        subject_embeddings = self.embed_text(subject)\n    else:\n        raise ValueError(\n            \"subject_type must be either 'image' or 'text', but got {request.subject_type}\"\n        )\n\n    if isinstance(prompt, dict) and not (\"type\" in prompt and \"value\" in prompt):\n        prompt_keys = prompt.keys()\n        prompt = [prompt[k] for k in prompt_keys]\n        prompt_obj = \"dict\"\n    else:\n        prompt = prompt\n        if not isinstance(prompt, list):\n            prompt = [prompt]\n        prompt_obj = \"list\"\n\n    if len(prompt) &gt; CLIP_MAX_BATCH_SIZE:\n        raise ValueError(\n            f\"The maximum number of prompts that can be compared at once is {CLIP_MAX_BATCH_SIZE}\"\n        )\n\n    if prompt_type == \"image\":\n        prompt_embeddings = self.embed_image(prompt)\n    elif prompt_type == \"text\":\n        prompt_embeddings = self.embed_text(prompt)\n    else:\n        raise ValueError(\n            \"prompt_type must be either 'image' or 'text', but got {request.prompt_type}\"\n        )\n\n    similarities = [\n        cosine_similarity(subject_embeddings, p) for p in prompt_embeddings\n    ]\n\n    if prompt_obj == \"dict\":\n        similarities = dict(zip(prompt_keys, similarities))\n\n    return similarities\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.embed_image","title":"<code>embed_image(image, **kwargs)</code>","text":"<p>Embeds an image or a list of images using the Clip model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image or list of images to be embedded. Image can be in any format that is acceptable by the preproc_image method.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The embeddings of the image(s) as a numpy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of images in the list exceeds the maximum batch size.</p> Notes <p>The function measures performance using perf_counter and also has support for ONNX session to get embeddings.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def embed_image(\n    self,\n    image: Any,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Embeds an image or a list of images using the Clip model.\n\n    Args:\n        image (Any): The image or list of images to be embedded. Image can be in any format that is acceptable by the preproc_image method.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        np.ndarray: The embeddings of the image(s) as a numpy array.\n\n    Raises:\n        ValueError: If the number of images in the list exceeds the maximum batch size.\n\n    Notes:\n        The function measures performance using perf_counter and also has support for ONNX session to get embeddings.\n    \"\"\"\n    t1 = perf_counter()\n\n    if isinstance(image, list):\n        if len(image) &gt; CLIP_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of images that can be embedded at once is {CLIP_MAX_BATCH_SIZE}\"\n            )\n        imgs = [self.preproc_image(i) for i in image]\n        img_in = np.concatenate(imgs, axis=0)\n    else:\n        img_in = self.preproc_image(image)\n\n    onnx_input_image = {self.visual_onnx_session.get_inputs()[0].name: img_in}\n    embeddings = self.visual_onnx_session.run(None, onnx_input_image)[0]\n\n    return embeddings\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.embed_text","title":"<code>embed_text(text, **kwargs)</code>","text":"<p>Embeds a text or a list of texts using the Clip model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Union[str, List[str]]</code> <p>The text string or list of text strings to be embedded.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The embeddings of the text or texts as a numpy array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of text strings in the list exceeds the maximum batch size.</p> Notes <p>The function utilizes an ONNX session to compute embeddings and measures the embedding time with perf_counter.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def embed_text(\n    self,\n    text: Union[str, List[str]],\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    Embeds a text or a list of texts using the Clip model.\n\n    Args:\n        text (Union[str, List[str]]): The text string or list of text strings to be embedded.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        np.ndarray: The embeddings of the text or texts as a numpy array.\n\n    Raises:\n        ValueError: If the number of text strings in the list exceeds the maximum batch size.\n\n    Notes:\n        The function utilizes an ONNX session to compute embeddings and measures the embedding time with perf_counter.\n    \"\"\"\n    if isinstance(text, list):\n        texts = text\n    else:\n        texts = [text]\n    results = []\n    for texts_batch in create_batches(\n        sequence=texts, batch_size=CLIP_MAX_BATCH_SIZE\n    ):\n        tokenized_batch = clip.tokenize(texts_batch).numpy().astype(np.int32)\n        onnx_input_text = {\n            self.textual_onnx_session.get_inputs()[0].name: tokenized_batch\n        }\n        embeddings = self.textual_onnx_session.run(None, onnx_input_text)[0]\n        results.append(embeddings)\n    return np.concatenate(results, axis=0)\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of file names.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: The list of file names.\n    \"\"\"\n    return [\"textual.onnx\", \"visual.onnx\"]\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Embeds an image - image:     can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def infer(self, image: Any, **kwargs) -&gt; Any:\n    \"\"\"Embeds an image\n    - image:\n        can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n    \"\"\"\n    return super().infer(image, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Routes the request to the appropriate inference function.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ClipInferenceRequest</code> <p>The request object containing the inference details.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>The response object containing the embeddings.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def infer_from_request(\n    self, request: ClipInferenceRequest\n) -&gt; ClipEmbeddingResponse:\n    \"\"\"Routes the request to the appropriate inference function.\n\n    Args:\n        request (ClipInferenceRequest): The request object containing the inference details.\n\n    Returns:\n        ClipEmbeddingResponse: The response object containing the embeddings.\n    \"\"\"\n    t1 = perf_counter()\n    if isinstance(request, ClipImageEmbeddingRequest):\n        infer_func = self.embed_image\n        make_response_func = self.make_embed_image_response\n    elif isinstance(request, ClipTextEmbeddingRequest):\n        infer_func = self.embed_text\n        make_response_func = self.make_embed_text_response\n    elif isinstance(request, ClipCompareRequest):\n        infer_func = self.compare\n        make_response_func = self.make_compare_response\n    else:\n        raise ValueError(\n            f\"Request type {type(request)} is not a valid ClipInferenceRequest\"\n        )\n    data = infer_func(**request.dict())\n    response = make_response_func(data)\n    response.time = perf_counter() - t1\n    return response\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.make_compare_response","title":"<code>make_compare_response(similarities)</code>","text":"<p>Creates a ClipCompareResponse object from the provided similarity data.</p> <p>Parameters:</p> Name Type Description Default <code>similarities</code> <code>Union[List[float], Dict[str, float]]</code> <p>A list or dictionary containing similarity scores.</p> required <p>Returns:</p> Name Type Description <code>ClipCompareResponse</code> <code>ClipCompareResponse</code> <p>An instance of the ClipCompareResponse with the given similarity scores.</p> Example <p>Assuming <code>ClipCompareResponse</code> expects a dictionary of string-float pairs:</p> <p>make_compare_response({\"image1\": 0.98, \"image2\": 0.76}) ClipCompareResponse(similarity={\"image1\": 0.98, \"image2\": 0.76})</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def make_compare_response(\n    self, similarities: Union[List[float], Dict[str, float]]\n) -&gt; ClipCompareResponse:\n    \"\"\"\n    Creates a ClipCompareResponse object from the provided similarity data.\n\n    Args:\n        similarities (Union[List[float], Dict[str, float]]): A list or dictionary containing similarity scores.\n\n    Returns:\n        ClipCompareResponse: An instance of the ClipCompareResponse with the given similarity scores.\n\n    Example:\n        Assuming `ClipCompareResponse` expects a dictionary of string-float pairs:\n\n        &gt;&gt;&gt; make_compare_response({\"image1\": 0.98, \"image2\": 0.76})\n        ClipCompareResponse(similarity={\"image1\": 0.98, \"image2\": 0.76})\n    \"\"\"\n    response = ClipCompareResponse(similarity=similarities)\n    return response\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.make_embed_image_response","title":"<code>make_embed_image_response(embeddings)</code>","text":"<p>Converts the given embeddings into a ClipEmbeddingResponse object.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>ndarray</code> <p>A numpy array containing the embeddings for an image or images.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.</p> Example <p>embeddings_array = np.array([[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]]) make_embed_image_response(embeddings_array) ClipEmbeddingResponse(embeddings=[[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def make_embed_image_response(\n    self, embeddings: np.ndarray\n) -&gt; ClipEmbeddingResponse:\n    \"\"\"\n    Converts the given embeddings into a ClipEmbeddingResponse object.\n\n    Args:\n        embeddings (np.ndarray): A numpy array containing the embeddings for an image or images.\n\n    Returns:\n        ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n    Example:\n        &gt;&gt;&gt; embeddings_array = np.array([[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n        &gt;&gt;&gt; make_embed_image_response(embeddings_array)\n        ClipEmbeddingResponse(embeddings=[[0.5, 0.3, 0.2], [0.1, 0.9, 0.0]])\n    \"\"\"\n    response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n\n    return response\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.make_embed_text_response","title":"<code>make_embed_text_response(embeddings)</code>","text":"<p>Converts the given text embeddings into a ClipEmbeddingResponse object.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>ndarray</code> <p>A numpy array containing the embeddings for a text or texts.</p> required <p>Returns:</p> Name Type Description <code>ClipEmbeddingResponse</code> <code>ClipEmbeddingResponse</code> <p>An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.</p> Example <p>embeddings_array = np.array([[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]]) make_embed_text_response(embeddings_array) ClipEmbeddingResponse(embeddings=[[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def make_embed_text_response(self, embeddings: np.ndarray) -&gt; ClipEmbeddingResponse:\n    \"\"\"\n    Converts the given text embeddings into a ClipEmbeddingResponse object.\n\n    Args:\n        embeddings (np.ndarray): A numpy array containing the embeddings for a text or texts.\n\n    Returns:\n        ClipEmbeddingResponse: An instance of the ClipEmbeddingResponse with the provided embeddings converted to a list.\n\n    Example:\n        &gt;&gt;&gt; embeddings_array = np.array([[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n        &gt;&gt;&gt; make_embed_text_response(embeddings_array)\n        ClipEmbeddingResponse(embeddings=[[0.8, 0.1, 0.1], [0.4, 0.5, 0.1]])\n    \"\"\"\n    response = ClipEmbeddingResponse(embeddings=embeddings.tolist())\n    return response\n</code></pre>"},{"location":"docs/reference/inference/models/clip/clip_model/#inference.models.clip.clip_model.Clip.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an inference request image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The object containing information necessary to load the image for inference.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A numpy array of the preprocessed image pixel data.</p> Source code in <code>inference/models/clip/clip_model.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage) -&gt; np.ndarray:\n    \"\"\"Preprocesses an inference request image.\n\n    Args:\n        image (InferenceRequestImage): The object containing information necessary to load the image for inference.\n\n    Returns:\n        np.ndarray: A numpy array of the preprocessed image pixel data.\n    \"\"\"\n    pil_image = Image.fromarray(load_image_rgb(image))\n    preprocessed_image = self.clip_preprocess(pil_image)\n\n    img_in = np.expand_dims(preprocessed_image, axis=0)\n\n    return img_in.astype(np.float32)\n</code></pre>"},{"location":"docs/reference/inference/models/cogvlm/cogvlm/","title":"cogvlm","text":""},{"location":"docs/reference/inference/models/doctr/doctr_model/","title":"doctr_model","text":""},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR","title":"<code>DocTR</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>class DocTR(RoboflowCoreModel):\n    def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n        \"\"\"Initializes the DocTR model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        self.api_key = kwargs.get(\"api_key\")\n        self.dataset_id = \"doctr\"\n        self.version_id = \"default\"\n        self.endpoint = model_id\n        model_id = model_id.lower()\n\n        os.environ[\"DOCTR_CACHE_DIR\"] = os.path.join(MODEL_CACHE_DIR, \"doctr_rec\")\n\n        self.det_model = DocTRDet(api_key=kwargs.get(\"api_key\"))\n        self.rec_model = DocTRRec(api_key=kwargs.get(\"api_key\"))\n\n        os.makedirs(f\"{MODEL_CACHE_DIR}/doctr_rec/models/\", exist_ok=True)\n        os.makedirs(f\"{MODEL_CACHE_DIR}/doctr_det/models/\", exist_ok=True)\n\n        shutil.copyfile(\n            f\"{MODEL_CACHE_DIR}/doctr_det/db_resnet50/model.pt\",\n            f\"{MODEL_CACHE_DIR}/doctr_det/models/db_resnet50-ac60cadc.pt\",\n        )\n        shutil.copyfile(\n            f\"{MODEL_CACHE_DIR}/doctr_rec/crnn_vgg16_bn/model.pt\",\n            f\"{MODEL_CACHE_DIR}/doctr_rec/models/crnn_vgg16_bn-9762b0b0.pt\",\n        )\n\n        self.model = ocr_predictor(\n            det_arch=self.det_model.version_id,\n            reco_arch=self.rec_model.version_id,\n            pretrained=True,\n        )\n        self.task_type = \"ocr\"\n\n    def clear_cache(self) -&gt; None:\n        self.det_model.clear_cache()\n        self.rec_model.clear_cache()\n\n    def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n        \"\"\"\n        DocTR pre-processes images as part of its inference pipeline.\n\n        Thus, no preprocessing is required here.\n        \"\"\"\n        pass\n\n    def infer_from_request(\n        self, request: DoctrOCRInferenceRequest\n    ) -&gt; OCRInferenceResponse:\n        t1 = perf_counter()\n        result = self.infer(**request.dict())\n        return OCRInferenceResponse(\n            result=result,\n            time=perf_counter() - t1,\n        )\n\n    def infer(self, image: Any, **kwargs):\n        \"\"\"\n        Run inference on a provided image.\n            - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n        Args:\n            request (DoctrOCRInferenceRequest): The inference request.\n\n        Returns:\n            OCRInferenceResponse: The inference response.\n        \"\"\"\n\n        img = load_image(image)\n\n        with tempfile.NamedTemporaryFile(suffix=\".jpg\") as f:\n            image = Image.fromarray(img[0])\n\n            image.save(f.name)\n\n            doc = DocumentFile.from_images([f.name])\n\n            result = self.model(doc).export()\n\n            result = result[\"pages\"][0][\"blocks\"]\n\n            result = [\n                \" \".join([word[\"value\"] for word in line[\"words\"]])\n                for block in result\n                for line in block[\"lines\"]\n            ]\n\n            result = \" \".join(result)\n\n            return result\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.__init__","title":"<code>__init__(*args, model_id='doctr_rec/crnn_vgg16_bn', **kwargs)</code>","text":"<p>Initializes the DocTR model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n    \"\"\"Initializes the DocTR model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    self.api_key = kwargs.get(\"api_key\")\n    self.dataset_id = \"doctr\"\n    self.version_id = \"default\"\n    self.endpoint = model_id\n    model_id = model_id.lower()\n\n    os.environ[\"DOCTR_CACHE_DIR\"] = os.path.join(MODEL_CACHE_DIR, \"doctr_rec\")\n\n    self.det_model = DocTRDet(api_key=kwargs.get(\"api_key\"))\n    self.rec_model = DocTRRec(api_key=kwargs.get(\"api_key\"))\n\n    os.makedirs(f\"{MODEL_CACHE_DIR}/doctr_rec/models/\", exist_ok=True)\n    os.makedirs(f\"{MODEL_CACHE_DIR}/doctr_det/models/\", exist_ok=True)\n\n    shutil.copyfile(\n        f\"{MODEL_CACHE_DIR}/doctr_det/db_resnet50/model.pt\",\n        f\"{MODEL_CACHE_DIR}/doctr_det/models/db_resnet50-ac60cadc.pt\",\n    )\n    shutil.copyfile(\n        f\"{MODEL_CACHE_DIR}/doctr_rec/crnn_vgg16_bn/model.pt\",\n        f\"{MODEL_CACHE_DIR}/doctr_rec/models/crnn_vgg16_bn-9762b0b0.pt\",\n    )\n\n    self.model = ocr_predictor(\n        det_arch=self.det_model.version_id,\n        reco_arch=self.rec_model.version_id,\n        pretrained=True,\n    )\n    self.task_type = \"ocr\"\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.infer","title":"<code>infer(image, **kwargs)</code>","text":"<p>Run inference on a provided image.     - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>DoctrOCRInferenceRequest</code> <p>The inference request.</p> required <p>Returns:</p> Name Type Description <code>OCRInferenceResponse</code> <p>The inference response.</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def infer(self, image: Any, **kwargs):\n    \"\"\"\n    Run inference on a provided image.\n        - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n    Args:\n        request (DoctrOCRInferenceRequest): The inference request.\n\n    Returns:\n        OCRInferenceResponse: The inference response.\n    \"\"\"\n\n    img = load_image(image)\n\n    with tempfile.NamedTemporaryFile(suffix=\".jpg\") as f:\n        image = Image.fromarray(img[0])\n\n        image.save(f.name)\n\n        doc = DocumentFile.from_images([f.name])\n\n        result = self.model(doc).export()\n\n        result = result[\"pages\"][0][\"blocks\"]\n\n        result = [\n            \" \".join([word[\"value\"] for word in line[\"words\"]])\n            for block in result\n            for line in block[\"lines\"]\n        ]\n\n        result = \" \".join(result)\n\n        return result\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTR.preprocess_image","title":"<code>preprocess_image(image)</code>","text":"<p>DocTR pre-processes images as part of its inference pipeline.</p> <p>Thus, no preprocessing is required here.</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; Image.Image:\n    \"\"\"\n    DocTR pre-processes images as part of its inference pipeline.\n\n    Thus, no preprocessing is required here.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRDet","title":"<code>DocTRDet</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>DocTR class for document Optical Character Recognition (OCR).</p> <p>Attributes:</p> Name Type Description <code>doctr</code> <p>The DocTR model.</p> <code>ort_session</code> <p>ONNX runtime inference session.</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>class DocTRDet(RoboflowCoreModel):\n    \"\"\"DocTR class for document Optical Character Recognition (OCR).\n\n    Attributes:\n        doctr: The DocTR model.\n        ort_session: ONNX runtime inference session.\n    \"\"\"\n\n    def __init__(self, *args, model_id: str = \"doctr_det/db_resnet50\", **kwargs):\n        \"\"\"Initializes the DocTR model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n\n        self.get_infer_bucket_file_list()\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRDet.__init__","title":"<code>__init__(*args, model_id='doctr_det/db_resnet50', **kwargs)</code>","text":"<p>Initializes the DocTR model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def __init__(self, *args, model_id: str = \"doctr_det/db_resnet50\", **kwargs):\n    \"\"\"Initializes the DocTR model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    self.get_infer_bucket_file_list()\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRDet.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRRec","title":"<code>DocTRRec</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>class DocTRRec(RoboflowCoreModel):\n    def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n        \"\"\"Initializes the DocTR model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        pass\n\n        self.get_infer_bucket_file_list()\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRRec.__init__","title":"<code>__init__(*args, model_id='doctr_rec/crnn_vgg16_bn', **kwargs)</code>","text":"<p>Initializes the DocTR model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def __init__(self, *args, model_id: str = \"doctr_rec/crnn_vgg16_bn\", **kwargs):\n    \"\"\"Initializes the DocTR model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    pass\n\n    self.get_infer_bucket_file_list()\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/models/doctr/doctr_model/#inference.models.doctr.doctr_model.DocTRRec.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/doctr/doctr_model.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"model.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/florence2/florence2/","title":"florence2","text":""},{"location":"docs/reference/inference/models/florence2/utils/","title":"utils","text":""},{"location":"docs/reference/inference/models/florence2/utils/#inference.models.florence2.utils.import_class_from_file","title":"<code>import_class_from_file(file_path, class_name)</code>","text":"<p>Emulates what huggingface transformers does to load remote code with trust_remote_code=True, but allows us to use the class directly so that we don't have to load untrusted code.</p> Source code in <code>inference/models/florence2/utils.py</code> <pre><code>def import_class_from_file(file_path, class_name):\n    \"\"\"\n    Emulates what huggingface transformers does to load remote code with trust_remote_code=True,\n    but allows us to use the class directly so that we don't have to load untrusted code.\n    \"\"\"\n    file_path = os.path.abspath(file_path)\n    module_name = os.path.splitext(os.path.basename(file_path))[0]\n    module_dir = os.path.dirname(file_path)\n    parent_dir = os.path.dirname(module_dir)\n\n    sys.path.insert(0, parent_dir)\n\n    try:\n        spec = importlib.util.spec_from_file_location(module_name, file_path)\n        module = importlib.util.module_from_spec(spec)\n\n        # Manually set the __package__ attribute to the parent package\n        module.__package__ = os.path.basename(module_dir)\n\n        spec.loader.exec_module(module)\n        return getattr(module, class_name)\n    finally:\n        sys.path.pop(0)\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/gaze/","title":"gaze","text":""},{"location":"docs/reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze","title":"<code>Gaze</code>","text":"<p>               Bases: <code>OnnxRoboflowCoreModel</code></p> <p>Roboflow ONNX Gaze model.</p> <p>This class is responsible for handling the ONNX Gaze model, including loading the model, preprocessing the input, and performing inference.</p> <p>Attributes:</p> Name Type Description <code>gaze_onnx_session</code> <code>InferenceSession</code> <p>ONNX Runtime session for gaze detection inference.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>class Gaze(OnnxRoboflowCoreModel):\n    \"\"\"Roboflow ONNX Gaze model.\n\n    This class is responsible for handling the ONNX Gaze model, including\n    loading the model, preprocessing the input, and performing inference.\n\n    Attributes:\n        gaze_onnx_session (onnxruntime.InferenceSession): ONNX Runtime session for gaze detection inference.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initializes the Gaze with the given arguments and keyword arguments.\"\"\"\n\n        t1 = perf_counter()\n        super().__init__(*args, **kwargs)\n        # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n        self.log(\"Creating inference sessions\")\n\n        # TODO: convert face detector (TensorflowLite) to ONNX model\n\n        self.gaze_onnx_session = onnxruntime.InferenceSession(\n            self.cache_file(\"L2CSNet_gaze360_resnet50_90bins.onnx\"),\n            providers=[\n                (\n                    \"TensorrtExecutionProvider\",\n                    {\n                        \"trt_engine_cache_enable\": True,\n                        \"trt_engine_cache_path\": TENSORRT_CACHE_PATH,\n                    },\n                ),\n                \"CUDAExecutionProvider\",\n                \"OpenVINOExecutionProvider\",\n                \"CPUExecutionProvider\",\n            ],\n        )\n\n        if REQUIRED_ONNX_PROVIDERS:\n            available_providers = onnxruntime.get_available_providers()\n            for provider in REQUIRED_ONNX_PROVIDERS:\n                if provider not in available_providers:\n                    raise OnnxProviderNotAvailable(\n                        f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                    )\n\n        # init face detector\n        self.face_detector = mp.tasks.vision.FaceDetector.create_from_options(\n            mp.tasks.vision.FaceDetectorOptions(\n                base_options=mp.tasks.BaseOptions(\n                    model_asset_path=self.cache_file(\"mediapipe_face_detector.tflite\")\n                ),\n                running_mode=mp.tasks.vision.RunningMode.IMAGE,\n            )\n        )\n\n        # additional settings for gaze detection\n        self._gaze_transformations = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Resize(448),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                ),\n            ]\n        )\n        self.task_type = \"gaze-detection\"\n        self.log(f\"GAZE model loaded in {perf_counter() - t1:.2f} seconds\")\n\n    def _crop_face_img(self, np_img: np.ndarray, face: Detection) -&gt; np.ndarray:\n        \"\"\"Extract facial area in an image.\n\n        Args:\n            np_img (np.ndarray): The numpy image.\n            face (mediapipe.tasks.python.components.containers.detections.Detection): The detected face.\n\n        Returns:\n            np.ndarray: Cropped face image.\n        \"\"\"\n        # extract face area\n        bbox = face.bounding_box\n        x_min = bbox.origin_x\n        y_min = bbox.origin_y\n        x_max = bbox.origin_x + bbox.width\n        y_max = bbox.origin_y + bbox.height\n        face_img = np_img[y_min:y_max, x_min:x_max, :]\n        face_img = cv2.resize(face_img, (224, 224))\n        return face_img\n\n    def _detect_gaze(self, np_imgs: List[np.ndarray]) -&gt; List[Tuple[float, float]]:\n        \"\"\"Detect faces and gazes in an image.\n\n        Args:\n            pil_imgs (List[np.ndarray]): The numpy image list, each image is a cropped facial image.\n\n        Returns:\n            List[Tuple[float, float]]: Yaw (radian) and Pitch (radian).\n        \"\"\"\n        ret = []\n        for i in range(0, len(np_imgs), GAZE_MAX_BATCH_SIZE):\n            img_batch = []\n            for j in range(i, min(len(np_imgs), i + GAZE_MAX_BATCH_SIZE)):\n                img = self._gaze_transformations(np_imgs[j])\n                img = np.expand_dims(img, axis=0).astype(np.float32)\n                img_batch.append(img)\n\n            img_batch = np.concatenate(img_batch, axis=0)\n            onnx_input_image = {self.gaze_onnx_session.get_inputs()[0].name: img_batch}\n            yaw, pitch = self.gaze_onnx_session.run(None, onnx_input_image)\n\n            for j in range(len(img_batch)):\n                ret.append((yaw[j], pitch[j]))\n\n        return ret\n\n    def _make_response(\n        self,\n        faces: List[Detection],\n        gazes: List[Tuple[float, float]],\n        imgW: int,\n        imgH: int,\n        time_total: float,\n        time_face_det: float = None,\n        time_gaze_det: float = None,\n    ) -&gt; GazeDetectionInferenceResponse:\n        \"\"\"Prepare response object from detected faces and corresponding gazes.\n\n        Args:\n            faces (List[Detection]): The detected faces.\n            gazes (List[tuple(float, float)]): The detected gazes (yaw, pitch).\n            imgW (int): The width (px) of original image.\n            imgH (int): The height (px) of original image.\n            time_total (float): The processing time.\n            time_face_det (float): The processing time.\n            time_gaze_det (float): The processing time.\n\n        Returns:\n            GazeDetectionInferenceResponse: The response object including the detected faces and gazes info.\n        \"\"\"\n        predictions = []\n        for face, gaze in zip(faces, gazes):\n            landmarks = []\n            for keypoint in face.keypoints:\n                x = min(max(int(keypoint.x * imgW), 0), imgW - 1)\n                y = min(max(int(keypoint.y * imgH), 0), imgH - 1)\n                landmarks.append(Point(x=x, y=y))\n\n            bbox = face.bounding_box\n            x_center = bbox.origin_x + bbox.width / 2\n            y_center = bbox.origin_y + bbox.height / 2\n            score = face.categories[0].score\n\n            prediction = GazeDetectionPrediction(\n                face=FaceDetectionPrediction(\n                    x=x_center,\n                    y=y_center,\n                    width=bbox.width,\n                    height=bbox.height,\n                    confidence=score,\n                    class_name=\"face\",\n                    landmarks=landmarks,\n                ),\n                yaw=gaze[0],\n                pitch=gaze[1],\n            )\n            predictions.append(prediction)\n\n        response = GazeDetectionInferenceResponse(\n            predictions=predictions,\n            time=time_total,\n            time_face_det=time_face_det,\n            time_gaze_det=time_gaze_det,\n        )\n        return response\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: The list of file names.\n        \"\"\"\n        return [\n            \"mediapipe_face_detector.tflite\",\n            \"L2CSNet_gaze360_resnet50_90bins.onnx\",\n        ]\n\n    def infer_from_request(\n        self, request: GazeDetectionInferenceRequest\n    ) -&gt; List[GazeDetectionInferenceResponse]:\n        \"\"\"Detect faces and gazes in image(s).\n\n        Args:\n            request (GazeDetectionInferenceRequest): The request object containing the image.\n\n        Returns:\n            List[GazeDetectionInferenceResponse]: The list of response objects containing the faces and corresponding gazes.\n        \"\"\"\n        if isinstance(request.image, list):\n            if len(request.image) &gt; GAZE_MAX_BATCH_SIZE:\n                raise ValueError(\n                    f\"The maximum number of images that can be inferred with gaze detection at one time is {GAZE_MAX_BATCH_SIZE}\"\n                )\n            imgs = request.image\n        else:\n            imgs = [request.image]\n\n        time_total = perf_counter()\n\n        # load pil images\n        num_img = len(imgs)\n        np_imgs = [load_image_rgb(img) for img in imgs]\n\n        # face detection\n        # TODO: face detection for batch\n        time_face_det = perf_counter()\n        faces = []\n        for np_img in np_imgs:\n            if request.do_run_face_detection:\n                mp_img = mp.Image(\n                    image_format=mp.ImageFormat.SRGB, data=np_img.astype(np.uint8)\n                )\n                faces_per_img = self.face_detector.detect(mp_img).detections\n            else:\n                faces_per_img = [\n                    Detection(\n                        bounding_box=BoundingBox(\n                            origin_x=0,\n                            origin_y=0,\n                            width=np_img.shape[1],\n                            height=np_img.shape[0],\n                        ),\n                        categories=[Category(score=1.0, category_name=\"face\")],\n                        keypoints=[],\n                    )\n                ]\n            faces.append(faces_per_img)\n        time_face_det = (perf_counter() - time_face_det) / num_img\n\n        # gaze detection\n        time_gaze_det = perf_counter()\n        face_imgs = []\n        for i, np_img in enumerate(np_imgs):\n            if request.do_run_face_detection:\n                face_imgs.extend(\n                    [self._crop_face_img(np_img, face) for face in faces[i]]\n                )\n            else:\n                face_imgs.append(cv2.resize(np_img, (224, 224)))\n        gazes = self._detect_gaze(face_imgs)\n        time_gaze_det = (perf_counter() - time_gaze_det) / num_img\n\n        time_total = (perf_counter() - time_total) / num_img\n\n        # prepare response\n        response = []\n        idx_gaze = 0\n        for i in range(len(np_imgs)):\n            imgH, imgW, _ = np_imgs[i].shape\n            faces_per_img = faces[i]\n            gazes_per_img = gazes[idx_gaze : idx_gaze + len(faces_per_img)]\n            response.append(\n                self._make_response(\n                    faces_per_img, gazes_per_img, imgW, imgH, time_total\n                )\n            )\n            idx_gaze += len(faces_per_img)\n\n        return response\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the Gaze with the given arguments and keyword arguments.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initializes the Gaze with the given arguments and keyword arguments.\"\"\"\n\n    t1 = perf_counter()\n    super().__init__(*args, **kwargs)\n    # Create an ONNX Runtime Session with a list of execution providers in priority order. ORT attempts to load providers until one is successful. This keeps the code across devices identical.\n    self.log(\"Creating inference sessions\")\n\n    # TODO: convert face detector (TensorflowLite) to ONNX model\n\n    self.gaze_onnx_session = onnxruntime.InferenceSession(\n        self.cache_file(\"L2CSNet_gaze360_resnet50_90bins.onnx\"),\n        providers=[\n            (\n                \"TensorrtExecutionProvider\",\n                {\n                    \"trt_engine_cache_enable\": True,\n                    \"trt_engine_cache_path\": TENSORRT_CACHE_PATH,\n                },\n            ),\n            \"CUDAExecutionProvider\",\n            \"OpenVINOExecutionProvider\",\n            \"CPUExecutionProvider\",\n        ],\n    )\n\n    if REQUIRED_ONNX_PROVIDERS:\n        available_providers = onnxruntime.get_available_providers()\n        for provider in REQUIRED_ONNX_PROVIDERS:\n            if provider not in available_providers:\n                raise OnnxProviderNotAvailable(\n                    f\"Required ONNX Execution Provider {provider} is not availble. Check that you are using the correct docker image on a supported device.\"\n                )\n\n    # init face detector\n    self.face_detector = mp.tasks.vision.FaceDetector.create_from_options(\n        mp.tasks.vision.FaceDetectorOptions(\n            base_options=mp.tasks.BaseOptions(\n                model_asset_path=self.cache_file(\"mediapipe_face_detector.tflite\")\n            ),\n            running_mode=mp.tasks.vision.RunningMode.IMAGE,\n        )\n    )\n\n    # additional settings for gaze detection\n    self._gaze_transformations = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Resize(448),\n            transforms.Normalize(\n                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n            ),\n        ]\n    )\n    self.task_type = \"gaze-detection\"\n    self.log(f\"GAZE model loaded in {perf_counter() - t1:.2f} seconds\")\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The list of file names.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: The list of file names.\n    \"\"\"\n    return [\n        \"mediapipe_face_detector.tflite\",\n        \"L2CSNet_gaze360_resnet50_90bins.onnx\",\n    ]\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.Gaze.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Detect faces and gazes in image(s).</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>GazeDetectionInferenceRequest</code> <p>The request object containing the image.</p> required <p>Returns:</p> Type Description <code>List[GazeDetectionInferenceResponse]</code> <p>List[GazeDetectionInferenceResponse]: The list of response objects containing the faces and corresponding gazes.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>def infer_from_request(\n    self, request: GazeDetectionInferenceRequest\n) -&gt; List[GazeDetectionInferenceResponse]:\n    \"\"\"Detect faces and gazes in image(s).\n\n    Args:\n        request (GazeDetectionInferenceRequest): The request object containing the image.\n\n    Returns:\n        List[GazeDetectionInferenceResponse]: The list of response objects containing the faces and corresponding gazes.\n    \"\"\"\n    if isinstance(request.image, list):\n        if len(request.image) &gt; GAZE_MAX_BATCH_SIZE:\n            raise ValueError(\n                f\"The maximum number of images that can be inferred with gaze detection at one time is {GAZE_MAX_BATCH_SIZE}\"\n            )\n        imgs = request.image\n    else:\n        imgs = [request.image]\n\n    time_total = perf_counter()\n\n    # load pil images\n    num_img = len(imgs)\n    np_imgs = [load_image_rgb(img) for img in imgs]\n\n    # face detection\n    # TODO: face detection for batch\n    time_face_det = perf_counter()\n    faces = []\n    for np_img in np_imgs:\n        if request.do_run_face_detection:\n            mp_img = mp.Image(\n                image_format=mp.ImageFormat.SRGB, data=np_img.astype(np.uint8)\n            )\n            faces_per_img = self.face_detector.detect(mp_img).detections\n        else:\n            faces_per_img = [\n                Detection(\n                    bounding_box=BoundingBox(\n                        origin_x=0,\n                        origin_y=0,\n                        width=np_img.shape[1],\n                        height=np_img.shape[0],\n                    ),\n                    categories=[Category(score=1.0, category_name=\"face\")],\n                    keypoints=[],\n                )\n            ]\n        faces.append(faces_per_img)\n    time_face_det = (perf_counter() - time_face_det) / num_img\n\n    # gaze detection\n    time_gaze_det = perf_counter()\n    face_imgs = []\n    for i, np_img in enumerate(np_imgs):\n        if request.do_run_face_detection:\n            face_imgs.extend(\n                [self._crop_face_img(np_img, face) for face in faces[i]]\n            )\n        else:\n            face_imgs.append(cv2.resize(np_img, (224, 224)))\n    gazes = self._detect_gaze(face_imgs)\n    time_gaze_det = (perf_counter() - time_gaze_det) / num_img\n\n    time_total = (perf_counter() - time_total) / num_img\n\n    # prepare response\n    response = []\n    idx_gaze = 0\n    for i in range(len(np_imgs)):\n        imgH, imgW, _ = np_imgs[i].shape\n        faces_per_img = faces[i]\n        gazes_per_img = gazes[idx_gaze : idx_gaze + len(faces_per_img)]\n        response.append(\n            self._make_response(\n                faces_per_img, gazes_per_img, imgW, imgH, time_total\n            )\n        )\n        idx_gaze += len(faces_per_img)\n\n    return response\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/gaze/#inference.models.gaze.gaze.L2C2Wrapper","title":"<code>L2C2Wrapper</code>","text":"<p>               Bases: <code>L2CS</code></p> <p>Roboflow L2CS Gaze detection model.</p> <p>This class is responsible for converting L2CS model to ONNX model. It is ONLY intended for internal usage.</p> Workflow <p>After training a L2CS model, create an instance of this wrapper class. Load the trained weights file, and save it as ONNX model.</p> Source code in <code>inference/models/gaze/gaze.py</code> <pre><code>class L2C2Wrapper(L2CS):\n    \"\"\"Roboflow L2CS Gaze detection model.\n\n    This class is responsible for converting L2CS model to ONNX model.\n    It is ONLY intended for internal usage.\n\n    Workflow:\n        After training a L2CS model, create an instance of this wrapper class.\n        Load the trained weights file, and save it as ONNX model.\n    \"\"\"\n\n    def __init__(self):\n        self.device = torch.device(\"cpu\")\n        self.num_bins = 90\n        super().__init__(\n            torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], self.num_bins\n        )\n        self._gaze_softmax = nn.Softmax(dim=1)\n        self._gaze_idx_tensor = torch.FloatTensor([i for i in range(90)]).to(\n            self.device\n        )\n\n    def forward(self, x):\n        idx_tensor = torch.stack(\n            [self._gaze_idx_tensor for i in range(x.shape[0])], dim=0\n        )\n        gaze_yaw, gaze_pitch = super().forward(x)\n\n        yaw_predicted = self._gaze_softmax(gaze_yaw)\n        yaw_radian = (\n            (torch.sum(yaw_predicted * idx_tensor, dim=1) * 4 - 180) * np.pi / 180\n        )\n\n        pitch_predicted = self._gaze_softmax(gaze_pitch)\n        pitch_radian = (\n            (torch.sum(pitch_predicted * idx_tensor, dim=1) * 4 - 180) * np.pi / 180\n        )\n\n        return yaw_radian, pitch_radian\n\n    def load_L2CS_model(\n        self,\n        file_path=f\"{MODEL_CACHE_DIR}/gaze/L2CS/L2CSNet_gaze360_resnet50_90bins.pkl\",\n    ):\n        super().load_state_dict(torch.load(file_path, map_location=self.device))\n        super().to(self.device)\n\n    def saveas_ONNX_model(\n        self,\n        file_path=f\"{MODEL_CACHE_DIR}/gaze/L2CS/L2CSNet_gaze360_resnet50_90bins.onnx\",\n    ):\n        dummy_input = torch.randn(1, 3, 448, 448)\n        dynamic_axes = {\n            \"input\": {0: \"batch_size\"},\n            \"output_yaw\": {0: \"batch_size\"},\n            \"output_pitch\": {0: \"batch_size\"},\n        }\n        torch.onnx.export(\n            self,\n            dummy_input,\n            file_path,\n            input_names=[\"input\"],\n            output_names=[\"output_yaw\", \"output_pitch\"],\n            dynamic_axes=dynamic_axes,\n            verbose=False,\n        )\n</code></pre>"},{"location":"docs/reference/inference/models/gaze/l2cs/","title":"l2cs","text":""},{"location":"docs/reference/inference/models/gaze/l2cs/#inference.models.gaze.l2cs.L2CS","title":"<code>L2CS</code>","text":"<p>               Bases: <code>Module</code></p> <p>L2CS Gaze Detection Model.</p> <p>This class is responsible for performing gaze detection using the L2CS-Net model. Ref: https://github.com/Ahmednull/L2CS-Net</p> <p>Methods:</p> Name Description <code>forward</code> <p>Performs inference on the given image.</p> Source code in <code>inference/models/gaze/l2cs.py</code> <pre><code>class L2CS(nn.Module):\n    \"\"\"L2CS Gaze Detection Model.\n\n    This class is responsible for performing gaze detection using the L2CS-Net model.\n    Ref: https://github.com/Ahmednull/L2CS-Net\n\n    Methods:\n        forward: Performs inference on the given image.\n    \"\"\"\n\n    def __init__(self, block, layers, num_bins):\n        self.inplanes = 64\n        super(L2CS, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n        self.fc_yaw_gaze = nn.Linear(512 * block.expansion, num_bins)\n        self.fc_pitch_gaze = nn.Linear(512 * block.expansion, num_bins)\n\n        # Vestigial layer from previous experiments\n        self.fc_finetune = nn.Linear(512 * block.expansion + 3, 3)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n\n        # gaze\n        pre_yaw_gaze = self.fc_yaw_gaze(x)\n        pre_pitch_gaze = self.fc_pitch_gaze(x)\n        return pre_yaw_gaze, pre_pitch_gaze\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/","title":"grounding_dino","text":""},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO","title":"<code>GroundingDINO</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>GroundingDINO class for zero-shot object detection.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The GroundingDINO model.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>class GroundingDINO(RoboflowCoreModel):\n    \"\"\"GroundingDINO class for zero-shot object detection.\n\n    Attributes:\n        model: The GroundingDINO model.\n    \"\"\"\n\n    def __init__(\n        self, *args, model_id=\"grounding_dino/groundingdino_swint_ogc\", **kwargs\n    ):\n        \"\"\"Initializes the GroundingDINO model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n        GROUNDING_DINO_CACHE_DIR = os.path.join(MODEL_CACHE_DIR, model_id)\n\n        GROUNDING_DINO_CONFIG_PATH = os.path.join(\n            GROUNDING_DINO_CACHE_DIR, \"GroundingDINO_SwinT_OGC.py\"\n        )\n\n        if not os.path.exists(GROUNDING_DINO_CACHE_DIR):\n            os.makedirs(GROUNDING_DINO_CACHE_DIR)\n\n        if not os.path.exists(GROUNDING_DINO_CONFIG_PATH):\n            url = \"https://raw.githubusercontent.com/roboflow/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n            urllib.request.urlretrieve(url, GROUNDING_DINO_CONFIG_PATH)\n\n        self.model = Model(\n            model_config_path=GROUNDING_DINO_CONFIG_PATH,\n            model_checkpoint_path=os.path.join(\n                GROUNDING_DINO_CACHE_DIR, \"groundingdino_swint_ogc.pth\"\n            ),\n            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        )\n        self.task_type = \"object-detection\"\n\n    def preproc_image(self, image: Any):\n        \"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_bgr(image)\n        return np_image\n\n    def infer_from_request(\n        self,\n        request: GroundingDINOInferenceRequest,\n    ) -&gt; ObjectDetectionInferenceResponse:\n        \"\"\"\n        Perform inference based on the details provided in the request, and return the associated responses.\n        \"\"\"\n        result = self.infer(**request.dict())\n        return result\n\n    def infer(\n        self,\n        image: InferenceRequestImage,\n        text: List[str] = None,\n        class_filter: list = None,\n        box_threshold=0.5,\n        text_threshold=0.5,\n        class_agnostic_nms=CLASS_AGNOSTIC_NMS,\n        **kwargs\n    ):\n        \"\"\"\n        Run inference on a provided image.\n            - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n        Args:\n            request (CVInferenceRequest): The inference request.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            GroundingDINOInferenceRequest: The inference response.\n        \"\"\"\n        t1 = perf_counter()\n        image = self.preproc_image(image)\n        img_dims = image.shape\n\n        detections = self.model.predict_with_classes(\n            image=image,\n            classes=text,\n            box_threshold=box_threshold,\n            text_threshold=text_threshold,\n        )\n\n        self.class_names = text\n\n        if class_agnostic_nms:\n            detections = detections.with_nms(class_agnostic=True)\n        else:\n            detections = detections.with_nms()\n\n        xywh_bboxes = [xyxy_to_xywh(detection) for detection in detections.xyxy]\n\n        t2 = perf_counter() - t1\n\n        responses = ObjectDetectionInferenceResponse(\n            predictions=[\n                ObjectDetectionPrediction(\n                    **{\n                        \"x\": xywh_bboxes[i][0],\n                        \"y\": xywh_bboxes[i][1],\n                        \"width\": xywh_bboxes[i][2],\n                        \"height\": xywh_bboxes[i][3],\n                        \"confidence\": detections.confidence[i],\n                        \"class\": self.class_names[int(detections.class_id[i])],\n                        \"class_id\": int(detections.class_id[i]),\n                    }\n                )\n                for i, pred in enumerate(detections.xyxy)\n                if not class_filter\n                or self.class_names[int(pred[6])] in class_filter\n                and detections.class_id[i] is not None\n            ],\n            image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n            time=t2,\n        )\n        return responses\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"groundingdino_swint_ogc.pth\"]\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.__init__","title":"<code>__init__(*args, model_id='grounding_dino/groundingdino_swint_ogc', **kwargs)</code>","text":"<p>Initializes the GroundingDINO model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def __init__(\n    self, *args, model_id=\"grounding_dino/groundingdino_swint_ogc\", **kwargs\n):\n    \"\"\"Initializes the GroundingDINO model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n\n    GROUNDING_DINO_CACHE_DIR = os.path.join(MODEL_CACHE_DIR, model_id)\n\n    GROUNDING_DINO_CONFIG_PATH = os.path.join(\n        GROUNDING_DINO_CACHE_DIR, \"GroundingDINO_SwinT_OGC.py\"\n    )\n\n    if not os.path.exists(GROUNDING_DINO_CACHE_DIR):\n        os.makedirs(GROUNDING_DINO_CACHE_DIR)\n\n    if not os.path.exists(GROUNDING_DINO_CONFIG_PATH):\n        url = \"https://raw.githubusercontent.com/roboflow/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n        urllib.request.urlretrieve(url, GROUNDING_DINO_CONFIG_PATH)\n\n    self.model = Model(\n        model_config_path=GROUNDING_DINO_CONFIG_PATH,\n        model_checkpoint_path=os.path.join(\n            GROUNDING_DINO_CACHE_DIR, \"groundingdino_swint_ogc.pth\"\n        ),\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    )\n    self.task_type = \"object-detection\"\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"groundingdino_swint_ogc.pth\"]\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.infer","title":"<code>infer(image, text=None, class_filter=None, box_threshold=0.5, text_threshold=0.5, class_agnostic_nms=CLASS_AGNOSTIC_NMS, **kwargs)</code>","text":"<p>Run inference on a provided image.     - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>CVInferenceRequest</code> <p>The inference request.</p> required <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GroundingDINOInferenceRequest</code> <p>The inference response.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def infer(\n    self,\n    image: InferenceRequestImage,\n    text: List[str] = None,\n    class_filter: list = None,\n    box_threshold=0.5,\n    text_threshold=0.5,\n    class_agnostic_nms=CLASS_AGNOSTIC_NMS,\n    **kwargs\n):\n    \"\"\"\n    Run inference on a provided image.\n        - image: can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n\n    Args:\n        request (CVInferenceRequest): The inference request.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        GroundingDINOInferenceRequest: The inference response.\n    \"\"\"\n    t1 = perf_counter()\n    image = self.preproc_image(image)\n    img_dims = image.shape\n\n    detections = self.model.predict_with_classes(\n        image=image,\n        classes=text,\n        box_threshold=box_threshold,\n        text_threshold=text_threshold,\n    )\n\n    self.class_names = text\n\n    if class_agnostic_nms:\n        detections = detections.with_nms(class_agnostic=True)\n    else:\n        detections = detections.with_nms()\n\n    xywh_bboxes = [xyxy_to_xywh(detection) for detection in detections.xyxy]\n\n    t2 = perf_counter() - t1\n\n    responses = ObjectDetectionInferenceResponse(\n        predictions=[\n            ObjectDetectionPrediction(\n                **{\n                    \"x\": xywh_bboxes[i][0],\n                    \"y\": xywh_bboxes[i][1],\n                    \"width\": xywh_bboxes[i][2],\n                    \"height\": xywh_bboxes[i][3],\n                    \"confidence\": detections.confidence[i],\n                    \"class\": self.class_names[int(detections.class_id[i])],\n                    \"class_id\": int(detections.class_id[i]),\n                }\n            )\n            for i, pred in enumerate(detections.xyxy)\n            if not class_filter\n            or self.class_names[int(pred[6])] in class_filter\n            and detections.class_id[i] is not None\n        ],\n        image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n        time=t2,\n    )\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Perform inference based on the details provided in the request, and return the associated responses.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def infer_from_request(\n    self,\n    request: GroundingDINOInferenceRequest,\n) -&gt; ObjectDetectionInferenceResponse:\n    \"\"\"\n    Perform inference based on the details provided in the request, and return the associated responses.\n    \"\"\"\n    result = self.infer(**request.dict())\n    return result\n</code></pre>"},{"location":"docs/reference/inference/models/grounding_dino/grounding_dino/#inference.models.grounding_dino.grounding_dino.GroundingDINO.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/grounding_dino/grounding_dino.py</code> <pre><code>def preproc_image(self, image: Any):\n    \"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_bgr(image)\n    return np_image\n</code></pre>"},{"location":"docs/reference/inference/models/owlv2/owlv2/","title":"owlv2","text":""},{"location":"docs/reference/inference/models/owlv2/owlv2/#inference.models.owlv2.owlv2.OwlV2","title":"<code>OwlV2</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> Source code in <code>inference/models/owlv2/owlv2.py</code> <pre><code>class OwlV2(RoboflowCoreModel):\n    task_type = \"object-detection\"\n    box_format = \"xywh\"\n\n    def __init__(self, *args, model_id=\"owlv2/owlv2-base-patch16-ensemble\", **kwargs):\n        super().__init__(*args, model_id=model_id, **kwargs)\n        hf_id = os.path.join(\"google\", self.version_id)\n        processor = Owlv2Processor.from_pretrained(hf_id)\n        self.image_size = tuple(processor.image_processor.size.values())\n        self.image_mean = torch.tensor(\n            processor.image_processor.image_mean, device=DEVICE\n        ).view(1, 3, 1, 1)\n        self.image_std = torch.tensor(\n            processor.image_processor.image_std, device=DEVICE\n        ).view(1, 3, 1, 1)\n        self.model = Owlv2ForObjectDetection.from_pretrained(hf_id).eval().to(DEVICE)\n        self.reset_cache()\n\n        # compile forward pass of the visual backbone of the model\n        # NOTE that this is able to fix the manual attention implementation used in OWLv2\n        # so we don't have to force in flash attention by ourselves\n        # however that is only true if torch version 2.4 or later is used\n        # for torch &lt; 2.4, this is a LOT slower and using flash attention by ourselves is faster\n        # this also breaks in torch &lt; 2.1 so we supress torch._dynamo errors\n        torch._dynamo.config.suppress_errors = True\n        self.model.owlv2.vision_model = torch.compile(self.model.owlv2.vision_model)\n\n    def reset_cache(self):\n        # each entry should be on the order of 300*4KB, so 1000 is 400MB of CUDA memory\n        self.image_embed_cache = LimitedSizeDict(size_limit=1000)\n        # each entry should be on the order of 10 bytes, so 1000 is 10KB\n        self.image_size_cache = LimitedSizeDict(size_limit=1000)\n        # entry size will vary depending on the number of samples, but 100 should be safe\n        self.class_embeddings_cache = LimitedSizeDict(size_limit=100)\n\n    def draw_predictions(\n        self,\n        inference_request,\n        inference_response,\n    ) -&gt; bytes:\n        \"\"\"Draw predictions from an inference response onto the original image provided by an inference request\n\n        Args:\n            inference_request (ObjectDetectionInferenceRequest): The inference request containing the image on which to draw predictions\n            inference_response (ObjectDetectionInferenceResponse): The inference response containing predictions to be drawn\n\n        Returns:\n            str: A base64 encoded image string\n        \"\"\"\n        all_class_names = [x.class_name for x in inference_response.predictions]\n        all_class_names = sorted(list(set(all_class_names)))\n\n        return draw_detection_predictions(\n            inference_request=inference_request,\n            inference_response=inference_response,\n            colors={\n                class_name: DEFAULT_COLOR_PALETTE[i % len(DEFAULT_COLOR_PALETTE)]\n                for (i, class_name) in enumerate(all_class_names)\n            },\n        )\n\n    def download_weights(self) -&gt; None:\n        # Download from huggingface\n        pass\n\n    def compute_image_size(\n        self, image: Union[np.ndarray, LazyImageRetrievalWrapper]\n    ) -&gt; Tuple[int, int]:\n        # we build this in hopes of avoiding having to load the image solely for the purpose of getting its size\n        if isinstance(image, LazyImageRetrievalWrapper):\n            if (image_size := self.image_size_cache.get(image.image_hash)) is None:\n                image_size = image.image_as_numpy.shape[:2][::-1]\n                self.image_size_cache[image.image_hash] = image_size\n        else:\n            image_size = image.shape[:2][::-1]\n        return image_size\n\n    @torch.no_grad()\n    def embed_image(self, image: Union[np.ndarray, LazyImageRetrievalWrapper]) -&gt; Hash:\n        if isinstance(image, LazyImageRetrievalWrapper):\n            image_hash = image.image_hash\n        else:\n            image_hash = hash_function(image.tobytes())\n\n        if image_hash in self.image_embed_cache:\n            return image_hash\n\n        np_image = (\n            image.image_as_numpy\n            if isinstance(image, LazyImageRetrievalWrapper)\n            else image\n        )\n        pixel_values = preprocess_image(\n            np_image, self.image_size, self.image_mean, self.image_std\n        )\n\n        # torch 2.4 lets you use \"cuda:0\" as device_type\n        # but this crashes in 2.3\n        # so we parse DEVICE as a string to make it work in both 2.3 and 2.4\n        # as we don't know a priori our torch version\n        device_str = \"cuda\" if str(DEVICE).startswith(\"cuda\") else \"cpu\"\n        # we disable autocast on CPU for stability, although it's possible using bfloat16 would work\n        with torch.autocast(\n            device_type=device_str, dtype=torch.float16, enabled=device_str == \"cuda\"\n        ):\n            image_embeds, _ = self.model.image_embedder(pixel_values=pixel_values)\n            batch_size, h, w, dim = image_embeds.shape\n            image_features = image_embeds.reshape(batch_size, h * w, dim)\n            objectness = self.model.objectness_predictor(image_features)\n            boxes = self.model.box_predictor(image_features, feature_map=image_embeds)\n\n        image_class_embeds = self.model.class_head.dense0(image_features)\n        image_class_embeds /= (\n            torch.linalg.norm(image_class_embeds, ord=2, dim=-1, keepdim=True) + 1e-6\n        )\n        logit_shift = self.model.class_head.logit_shift(image_features)\n        logit_scale = (\n            self.model.class_head.elu(self.model.class_head.logit_scale(image_features))\n            + 1\n        )\n        objectness = objectness.sigmoid()\n\n        objectness, boxes, image_class_embeds, logit_shift, logit_scale = (\n            filter_tensors_by_objectness(\n                objectness, boxes, image_class_embeds, logit_shift, logit_scale\n            )\n        )\n\n        self.image_embed_cache[image_hash] = (\n            objectness,\n            boxes,\n            image_class_embeds,\n            logit_shift,\n            logit_scale,\n        )\n\n        return image_hash\n\n    def get_query_embedding(\n        self, query_spec: QuerySpecType, iou_threshold: float\n    ) -&gt; torch.Tensor:\n        # NOTE: for now we're handling each image seperately\n        query_embeds = []\n        for image_hash, query_boxes in query_spec.items():\n            try:\n                _objectness, image_boxes, image_class_embeds, _, _ = (\n                    self.image_embed_cache[image_hash]\n                )\n            except KeyError as error:\n                raise KeyError(\"We didn't embed the image first!\") from error\n\n            query_boxes_tensor = torch.tensor(\n                query_boxes, dtype=image_boxes.dtype, device=image_boxes.device\n            )\n            if image_boxes.numel() == 0 or query_boxes_tensor.numel() == 0:\n                continue\n            iou, _ = box_iou(\n                to_corners(image_boxes), to_corners(query_boxes_tensor)\n            )  # 3000, k\n            ious, indices = torch.max(iou, dim=0)\n            # filter for only iou &gt; 0.4\n            iou_mask = ious &gt; iou_threshold\n            indices = indices[iou_mask]\n            if not indices.numel() &gt; 0:\n                continue\n\n            embeds = image_class_embeds[indices]\n            query_embeds.append(embeds)\n        if not query_embeds:\n            return None\n        query = torch.cat(query_embeds, dim=0)\n        return query\n\n    def infer_from_embed(\n        self,\n        image_hash: Hash,\n        query_embeddings: Dict[str, PosNegDictType],\n        confidence: float,\n        iou_threshold: float,\n    ) -&gt; List[Dict]:\n        _, image_boxes, image_class_embeds, _, _ = self.image_embed_cache[image_hash]\n        class_map, class_names = make_class_map(query_embeddings)\n        all_predicted_boxes, all_predicted_classes, all_predicted_scores = [], [], []\n        for class_name, pos_neg_embedding_dict in query_embeddings.items():\n            boxes, classes, scores = get_class_preds_from_embeds(\n                pos_neg_embedding_dict,\n                image_class_embeds,\n                confidence,\n                image_boxes,\n                class_map,\n                class_name,\n                iou_threshold,\n            )\n\n            all_predicted_boxes.append(boxes)\n            all_predicted_classes.append(classes)\n            all_predicted_scores.append(scores)\n\n        if not all_predicted_boxes:\n            return []\n\n        all_predicted_boxes = torch.cat(all_predicted_boxes, dim=0)\n        all_predicted_classes = torch.cat(all_predicted_classes, dim=0)\n        all_predicted_scores = torch.cat(all_predicted_scores, dim=0)\n\n        # run nms on all predictions\n        survival_indices = torchvision.ops.nms(\n            to_corners(all_predicted_boxes), all_predicted_scores, iou_threshold\n        )\n        all_predicted_boxes = all_predicted_boxes[survival_indices]\n        all_predicted_classes = all_predicted_classes[survival_indices]\n        all_predicted_scores = all_predicted_scores[survival_indices]\n\n        # move tensors to numpy before returning\n        all_predicted_boxes = all_predicted_boxes.cpu().numpy()\n        all_predicted_classes = all_predicted_classes.cpu().numpy()\n        all_predicted_scores = all_predicted_scores.cpu().numpy()\n\n        return [\n            {\n                \"class_name\": class_names[int(c)],\n                \"x\": float(x),\n                \"y\": float(y),\n                \"w\": float(w),\n                \"h\": float(h),\n                \"confidence\": float(score),\n            }\n            for c, (x, y, w, h), score in zip(\n                all_predicted_classes, all_predicted_boxes, all_predicted_scores\n            )\n        ]\n\n    def infer(\n        self,\n        image: Any,\n        training_data: Dict,\n        confidence=0.99,\n        iou_threshold=0.3,\n        **kwargs,\n    ):\n        class_embeddings_dict = self.make_class_embeddings_dict(\n            training_data, iou_threshold\n        )\n\n        if not isinstance(image, list):\n            images = [image]\n        else:\n            images = image\n\n        images = [LazyImageRetrievalWrapper(image) for image in images]\n\n        results = []\n        image_sizes = []\n        for image_wrapper in images:\n            # happy path here is that both image size and image embeddings are cached\n            # in which case we avoid loading the image at all\n            image_size = self.compute_image_size(image_wrapper)\n            image_sizes.append(image_size)\n            image_hash = self.embed_image(image_wrapper)\n            result = self.infer_from_embed(\n                image_hash, class_embeddings_dict, confidence, iou_threshold\n            )\n            results.append(result)\n        return self.make_response(\n            results, image_sizes, sorted(list(class_embeddings_dict.keys()))\n        )\n\n    def make_class_embeddings_dict(\n        self, training_data: List[Any], iou_threshold: float\n    ) -&gt; Dict[str, PosNegDictType]:\n        wrapped_training_data = [\n            {\n                \"image\": LazyImageRetrievalWrapper(train_image[\"image\"]),\n                \"boxes\": train_image[\"boxes\"],\n            }\n            for train_image in training_data\n        ]\n\n        wrapped_training_data_hash = hash_wrapped_training_data(wrapped_training_data)\n        if (\n            class_embeddings_dict := self.class_embeddings_cache.get(\n                wrapped_training_data_hash\n            )\n        ) is not None:\n            return class_embeddings_dict\n\n        class_embeddings_dict = defaultdict(lambda: {\"positive\": [], \"negative\": []})\n\n        bool_to_literal = {True: \"positive\", False: \"negative\"}\n        for train_image in wrapped_training_data:\n            # grab and embed image\n            image_hash = self.embed_image(train_image[\"image\"])\n\n            # grab and normalize box prompts for this image\n            image_size = self.compute_image_size(train_image[\"image\"])\n            boxes = train_image[\"boxes\"]\n            print(f\"boxes: {boxes}\")\n            coords = [[box[\"x\"], box[\"y\"], box[\"w\"], box[\"h\"]] for box in boxes]\n            coords = [tuple([c / max(image_size) for c in coord]) for coord in coords]\n            classes = [box[\"cls\"] for box in boxes]\n            is_positive = [not box[\"negative\"] for box in boxes]\n\n            # compute the embeddings for the box prompts\n            query_spec = {image_hash: coords}\n            # NOTE: because we just computed the embedding for this image, this should never result in a KeyError\n            embeddings = self.get_query_embedding(query_spec, iou_threshold)\n\n            if embeddings is None:\n                continue\n\n            # add the embeddings to their appropriate class and positive/negative list\n            for embedding, class_name, is_positive in zip(\n                embeddings, classes, is_positive\n            ):\n                class_embeddings_dict[class_name][bool_to_literal[is_positive]].append(\n                    embedding\n                )\n\n        # convert the lists of embeddings to tensors\n\n        class_embeddings_dict = {\n            k: {\n                \"positive\": torch.stack(v[\"positive\"]) if v[\"positive\"] else None,\n                \"negative\": torch.stack(v[\"negative\"]) if v[\"negative\"] else None,\n            }\n            for k, v in class_embeddings_dict.items()\n        }\n\n        self.class_embeddings_cache[wrapped_training_data_hash] = class_embeddings_dict\n\n        return class_embeddings_dict\n\n    def make_response(self, predictions, image_sizes, class_names):\n        responses = [\n            ObjectDetectionInferenceResponse(\n                predictions=[\n                    ObjectDetectionPrediction(\n                        # Passing args as a dictionary here since one of the args is 'class' (a protected term in Python)\n                        **{\n                            \"x\": pred[\"x\"] * max(image_sizes[ind]),\n                            \"y\": pred[\"y\"] * max(image_sizes[ind]),\n                            \"width\": pred[\"w\"] * max(image_sizes[ind]),\n                            \"height\": pred[\"h\"] * max(image_sizes[ind]),\n                            \"confidence\": pred[\"confidence\"],\n                            \"class\": pred[\"class_name\"],\n                            \"class_id\": class_names.index(pred[\"class_name\"]),\n                        }\n                    )\n                    for pred in batch_predictions\n                ],\n                image=InferenceResponseImage(\n                    width=image_sizes[ind][0], height=image_sizes[ind][1]\n                ),\n            )\n            for ind, batch_predictions in enumerate(predictions)\n        ]\n        return responses\n</code></pre>"},{"location":"docs/reference/inference/models/owlv2/owlv2/#inference.models.owlv2.owlv2.OwlV2.draw_predictions","title":"<code>draw_predictions(inference_request, inference_response)</code>","text":"<p>Draw predictions from an inference response onto the original image provided by an inference request</p> <p>Parameters:</p> Name Type Description Default <code>inference_request</code> <code>ObjectDetectionInferenceRequest</code> <p>The inference request containing the image on which to draw predictions</p> required <code>inference_response</code> <code>ObjectDetectionInferenceResponse</code> <p>The inference response containing predictions to be drawn</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>bytes</code> <p>A base64 encoded image string</p> Source code in <code>inference/models/owlv2/owlv2.py</code> <pre><code>def draw_predictions(\n    self,\n    inference_request,\n    inference_response,\n) -&gt; bytes:\n    \"\"\"Draw predictions from an inference response onto the original image provided by an inference request\n\n    Args:\n        inference_request (ObjectDetectionInferenceRequest): The inference request containing the image on which to draw predictions\n        inference_response (ObjectDetectionInferenceResponse): The inference response containing predictions to be drawn\n\n    Returns:\n        str: A base64 encoded image string\n    \"\"\"\n    all_class_names = [x.class_name for x in inference_response.predictions]\n    all_class_names = sorted(list(set(all_class_names)))\n\n    return draw_detection_predictions(\n        inference_request=inference_request,\n        inference_response=inference_response,\n        colors={\n            class_name: DEFAULT_COLOR_PALETTE[i % len(DEFAULT_COLOR_PALETTE)]\n            for (i, class_name) in enumerate(all_class_names)\n        },\n    )\n</code></pre>"},{"location":"docs/reference/inference/models/owlv2/owlv2/#inference.models.owlv2.owlv2.preprocess_image","title":"<code>preprocess_image(np_image, image_size, image_mean, image_std)</code>","text":"<p>Preprocess an image for OWLv2 by resizing, normalizing, and padding it. This is much faster than using the Owlv2Processor directly, as we ensure we use GPU if available.</p> <p>Parameters:</p> Name Type Description Default <code>np_image</code> <code>ndarray</code> <p>The image to preprocess, with shape (H, W, 3)</p> required <code>image_size</code> <code>tuple[int, int]</code> <p>The target size of the image</p> required <code>image_mean</code> <code>Tensor</code> <p>The mean of the image, on DEVICE, with shape (1, 3, 1, 1)</p> required <code>image_std</code> <code>Tensor</code> <p>The standard deviation of the image, on DEVICE, with shape (1, 3, 1, 1)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The preprocessed image, on DEVICE, with shape (1, 3, H, W)</p> Source code in <code>inference/models/owlv2/owlv2.py</code> <pre><code>def preprocess_image(\n    np_image: np.ndarray,\n    image_size: Tuple[int, int],\n    image_mean: torch.Tensor,\n    image_std: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Preprocess an image for OWLv2 by resizing, normalizing, and padding it.\n    This is much faster than using the Owlv2Processor directly, as we ensure we use GPU if available.\n\n    Args:\n        np_image (np.ndarray): The image to preprocess, with shape (H, W, 3)\n        image_size (tuple[int, int]): The target size of the image\n        image_mean (torch.Tensor): The mean of the image, on DEVICE, with shape (1, 3, 1, 1)\n        image_std (torch.Tensor): The standard deviation of the image, on DEVICE, with shape (1, 3, 1, 1)\n\n    Returns:\n        torch.Tensor: The preprocessed image, on DEVICE, with shape (1, 3, H, W)\n    \"\"\"\n    current_size = np_image.shape[:2]\n\n    r = min(image_size[0] / current_size[0], image_size[1] / current_size[1])\n    target_size = (int(r * current_size[0]), int(r * current_size[1]))\n\n    torch_image = (\n        torch.tensor(np_image)\n        .permute(2, 0, 1)\n        .unsqueeze(0)\n        .to(DEVICE)\n        .to(dtype=torch.float32)\n        / 255.0\n    )\n    torch_image = F.interpolate(\n        torch_image, size=target_size, mode=\"bilinear\", align_corners=False\n    )\n\n    padded_image_tensor = torch.ones((1, 3, *image_size), device=DEVICE) * 0.5\n    padded_image_tensor[:, :, : torch_image.shape[2], : torch_image.shape[3]] = (\n        torch_image\n    )\n\n    padded_image_tensor = (padded_image_tensor - image_mean) / image_std\n\n    return padded_image_tensor\n</code></pre>"},{"location":"docs/reference/inference/models/paligemma/paligemma/","title":"paligemma","text":""},{"location":"docs/reference/inference/models/paligemma/paligemma/#inference.models.paligemma.paligemma.LoRAPaliGemma","title":"<code>LoRAPaliGemma</code>","text":"<p>               Bases: <code>LoRATransformerModel</code></p> <p>By using you agree to the terms listed at https://ai.google.dev/gemma/terms</p> Source code in <code>inference/models/paligemma/paligemma.py</code> <pre><code>class LoRAPaliGemma(LoRATransformerModel):\n    \"\"\"By using you agree to the terms listed at https://ai.google.dev/gemma/terms\"\"\"\n\n    generation_includes_input = True\n    transformers_class = PaliGemmaForConditionalGeneration\n    load_base_from_roboflow = True\n</code></pre>"},{"location":"docs/reference/inference/models/paligemma/paligemma/#inference.models.paligemma.paligemma.PaliGemma","title":"<code>PaliGemma</code>","text":"<p>               Bases: <code>TransformerModel</code></p> <p>By using you agree to the terms listed at https://ai.google.dev/gemma/terms</p> Source code in <code>inference/models/paligemma/paligemma.py</code> <pre><code>class PaliGemma(TransformerModel):\n    \"\"\"By using you agree to the terms listed at https://ai.google.dev/gemma/terms\"\"\"\n\n    generation_includes_input = True\n    transformers_class = PaliGemmaForConditionalGeneration\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/","title":"segment_anything","text":""},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything","title":"<code>SegmentAnything</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>SegmentAnything class for handling segmentation tasks.</p> <p>Attributes:</p> Name Type Description <code>sam</code> <p>The segmentation model.</p> <code>predictor</code> <p>The predictor for the segmentation model.</p> <code>ort_session</code> <p>ONNX runtime inference session.</p> <code>embedding_cache</code> <p>Cache for embeddings.</p> <code>image_size_cache</code> <p>Cache for image sizes.</p> <code>embedding_cache_keys</code> <p>Keys for the embedding cache.</p> <code>low_res_logits_cache</code> <p>Cache for low resolution logits.</p> <code>segmentation_cache_keys</code> <p>Keys for the segmentation cache.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>class SegmentAnything(RoboflowCoreModel):\n    \"\"\"SegmentAnything class for handling segmentation tasks.\n\n    Attributes:\n        sam: The segmentation model.\n        predictor: The predictor for the segmentation model.\n        ort_session: ONNX runtime inference session.\n        embedding_cache: Cache for embeddings.\n        image_size_cache: Cache for image sizes.\n        embedding_cache_keys: Keys for the embedding cache.\n        low_res_logits_cache: Cache for low resolution logits.\n        segmentation_cache_keys: Keys for the segmentation cache.\n    \"\"\"\n\n    def __init__(self, *args, model_id: str = f\"sam/{SAM_VERSION_ID}\", **kwargs):\n        \"\"\"Initializes the SegmentAnything.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, model_id=model_id, **kwargs)\n        self.sam = sam_model_registry[self.version_id](\n            checkpoint=self.cache_file(\"encoder.pth\")\n        )\n        self.sam.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.predictor = SamPredictor(self.sam)\n        self.ort_session = onnxruntime.InferenceSession(\n            self.cache_file(\"decoder.onnx\"),\n            providers=[\n                \"CUDAExecutionProvider\",\n                \"OpenVINOExecutionProvider\",\n                \"CPUExecutionProvider\",\n            ],\n        )\n        self.embedding_cache = {}\n        self.image_size_cache = {}\n        self.embedding_cache_keys = []\n\n        self.low_res_logits_cache = {}\n        self.segmentation_cache_keys = []\n        self.task_type = \"unsupervised-segmentation\"\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: List of file names.\n        \"\"\"\n        return [\"encoder.pth\", \"decoder.onnx\"]\n\n    def embed_image(self, image: Any, image_id: Optional[str] = None, **kwargs):\n        \"\"\"\n        Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached,\n        the cached result will be returned.\n\n        Args:\n            image (Any): The image to be embedded. The format should be compatible with the preproc_image method.\n            image_id (Optional[str]): An identifier for the image. If provided, the embedding result will be cached\n                                      with this ID. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image\n                                               and the second element is the shape (height, width) of the processed image.\n\n        Notes:\n            - Embeddings and image sizes are cached to improve performance on repeated requests for the same image.\n            - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n              the oldest entries are removed.\n\n        Example:\n            &gt;&gt;&gt; img_array = ... # some image array\n            &gt;&gt;&gt; embed_image(img_array, image_id=\"sample123\")\n            (array([...]), (224, 224))\n        \"\"\"\n        if image_id and image_id in self.embedding_cache:\n            return (\n                self.embedding_cache[image_id],\n                self.image_size_cache[image_id],\n            )\n        img_in = self.preproc_image(image)\n        self.predictor.set_image(img_in)\n        embedding = self.predictor.get_image_embedding().cpu().numpy()\n        if image_id:\n            self.embedding_cache[image_id] = embedding\n            self.image_size_cache[image_id] = img_in.shape[:2]\n            self.embedding_cache_keys.append(image_id)\n            if len(self.embedding_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n                cache_key = self.embedding_cache_keys.pop(0)\n                del self.embedding_cache[cache_key]\n                del self.image_size_cache[cache_key]\n        return (embedding, img_in.shape[:2])\n\n    def infer_from_request(self, request: SamInferenceRequest):\n        \"\"\"Performs inference based on the request type.\n\n        Args:\n            request (SamInferenceRequest): The inference request.\n\n        Returns:\n            Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n        \"\"\"\n        t1 = perf_counter()\n        if isinstance(request, SamEmbeddingRequest):\n            embedding, _ = self.embed_image(**request.dict())\n            inference_time = perf_counter() - t1\n            if request.format == \"json\":\n                return SamEmbeddingResponse(\n                    embeddings=embedding.tolist(), time=inference_time\n                )\n            elif request.format == \"binary\":\n                binary_vector = BytesIO()\n                np.save(binary_vector, embedding)\n                binary_vector.seek(0)\n                return SamEmbeddingResponse(\n                    embeddings=binary_vector.getvalue(), time=inference_time\n                )\n        elif isinstance(request, SamSegmentationRequest):\n            masks, low_res_masks = self.segment_image(**request.dict())\n            if request.format == \"json\":\n                masks = masks &gt; self.predictor.model.mask_threshold\n                masks = masks2poly(masks)\n                low_res_masks = low_res_masks &gt; self.predictor.model.mask_threshold\n                low_res_masks = masks2poly(low_res_masks)\n            elif request.format == \"binary\":\n                binary_vector = BytesIO()\n                np.savez_compressed(\n                    binary_vector, masks=masks, low_res_masks=low_res_masks\n                )\n                binary_vector.seek(0)\n                binary_data = binary_vector.getvalue()\n                return binary_data\n            else:\n                raise ValueError(f\"Invalid format {request.format}\")\n\n            response = SamSegmentationResponse(\n                masks=[m.tolist() for m in masks],\n                low_res_masks=[m.tolist() for m in low_res_masks],\n                time=perf_counter() - t1,\n            )\n            return response\n\n    def preproc_image(self, image: InferenceRequestImage):\n        \"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_rgb(image)\n        return np_image\n\n    def segment_image(\n        self,\n        image: Any,\n        embeddings: Optional[Union[np.ndarray, List[List[float]]]] = None,\n        embeddings_format: Optional[str] = \"json\",\n        has_mask_input: Optional[bool] = False,\n        image_id: Optional[str] = None,\n        mask_input: Optional[Union[np.ndarray, List[List[List[float]]]]] = None,\n        mask_input_format: Optional[str] = \"json\",\n        orig_im_size: Optional[List[int]] = None,\n        point_coords: Optional[List[List[float]]] = [],\n        point_labels: Optional[List[int]] = [],\n        use_mask_input_cache: Optional[bool] = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Segments an image based on provided embeddings, points, masks, or cached results.\n        If embeddings are not directly provided, the function can derive them from the input image or cache.\n\n        Args:\n            image (Any): The image to be segmented.\n            embeddings (Optional[Union[np.ndarray, List[List[float]]]]): The embeddings of the image.\n                Defaults to None, in which case the image is used to compute embeddings.\n            embeddings_format (Optional[str]): Format of the provided embeddings; either 'json' or 'binary'. Defaults to 'json'.\n            has_mask_input (Optional[bool]): Specifies whether mask input is provided. Defaults to False.\n            image_id (Optional[str]): A cached identifier for the image. Useful for accessing cached embeddings or masks.\n            mask_input (Optional[Union[np.ndarray, List[List[List[float]]]]]): Input mask for the image.\n            mask_input_format (Optional[str]): Format of the provided mask input; either 'json' or 'binary'. Defaults to 'json'.\n            orig_im_size (Optional[List[int]]): Original size of the image when providing embeddings directly.\n            point_coords (Optional[List[List[float]]]): Coordinates of points in the image. Defaults to an empty list.\n            point_labels (Optional[List[int]]): Labels associated with the provided points. Defaults to an empty list.\n            use_mask_input_cache (Optional[bool]): Flag to determine if cached mask input should be used. Defaults to True.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: A tuple where the first element is the segmentation masks of the image\n                                          and the second element is the low resolution segmentation masks.\n\n        Raises:\n            ValueError: If necessary inputs are missing or inconsistent.\n\n        Notes:\n            - Embeddings, segmentations, and low-resolution logits can be cached to improve performance\n              on repeated requests for the same image.\n            - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n              the oldest entries are removed.\n        \"\"\"\n        if not embeddings:\n            if not image and not image_id:\n                raise ValueError(\n                    \"Must provide either image, cached image_id, or embeddings\"\n                )\n            elif image_id and not image and image_id not in self.embedding_cache:\n                raise ValueError(\n                    f\"Image ID {image_id} not in embedding cache, must provide the image or embeddings\"\n                )\n            embedding, original_image_size = self.embed_image(\n                image=image, image_id=image_id\n            )\n        else:\n            if not orig_im_size:\n                raise ValueError(\n                    \"Must provide original image size if providing embeddings\"\n                )\n            original_image_size = orig_im_size\n            if embeddings_format == \"json\":\n                embedding = np.array(embeddings)\n            elif embeddings_format == \"binary\":\n                embedding = np.load(BytesIO(embeddings))\n\n        point_coords = point_coords\n        point_coords.append([0, 0])\n        point_coords = np.array(point_coords, dtype=np.float32)\n        point_coords = np.expand_dims(point_coords, axis=0)\n        point_coords = self.predictor.transform.apply_coords(\n            point_coords,\n            original_image_size,\n        )\n\n        point_labels = point_labels\n        point_labels.append(-1)\n        point_labels = np.array(point_labels, dtype=np.float32)\n        point_labels = np.expand_dims(point_labels, axis=0)\n\n        if has_mask_input:\n            if (\n                image_id\n                and image_id in self.low_res_logits_cache\n                and use_mask_input_cache\n            ):\n                mask_input = self.low_res_logits_cache[image_id]\n            elif not mask_input and (\n                not image_id or image_id not in self.low_res_logits_cache\n            ):\n                raise ValueError(\"Must provide either mask_input or cached image_id\")\n            else:\n                if mask_input_format == \"json\":\n                    polys = mask_input\n                    mask_input = np.zeros((1, len(polys), 256, 256), dtype=np.uint8)\n                    for i, poly in enumerate(polys):\n                        poly = ShapelyPolygon(poly)\n                        raster = rasterio.features.rasterize(\n                            [poly], out_shape=(256, 256)\n                        )\n                        mask_input[0, i, :, :] = raster\n                elif mask_input_format == \"binary\":\n                    binary_data = base64.b64decode(mask_input)\n                    mask_input = np.load(BytesIO(binary_data))\n        else:\n            mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n\n        ort_inputs = {\n            \"image_embeddings\": embedding.astype(np.float32),\n            \"point_coords\": point_coords.astype(np.float32),\n            \"point_labels\": point_labels,\n            \"mask_input\": mask_input.astype(np.float32),\n            \"has_mask_input\": (\n                np.zeros(1, dtype=np.float32)\n                if not has_mask_input\n                else np.ones(1, dtype=np.float32)\n            ),\n            \"orig_im_size\": np.array(original_image_size, dtype=np.float32),\n        }\n        masks, _, low_res_logits = self.ort_session.run(None, ort_inputs)\n        if image_id:\n            self.low_res_logits_cache[image_id] = low_res_logits\n            if image_id not in self.segmentation_cache_keys:\n                self.segmentation_cache_keys.append(image_id)\n            if len(self.segmentation_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n                cache_key = self.segmentation_cache_keys.pop(0)\n                del self.low_res_logits_cache[cache_key]\n        masks = masks[0]\n        low_res_masks = low_res_logits[0]\n\n        return masks, low_res_masks\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.__init__","title":"<code>__init__(*args, model_id=f'sam/{SAM_VERSION_ID}', **kwargs)</code>","text":"<p>Initializes the SegmentAnything.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def __init__(self, *args, model_id: str = f\"sam/{SAM_VERSION_ID}\", **kwargs):\n    \"\"\"Initializes the SegmentAnything.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, model_id=model_id, **kwargs)\n    self.sam = sam_model_registry[self.version_id](\n        checkpoint=self.cache_file(\"encoder.pth\")\n    )\n    self.sam.to(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.predictor = SamPredictor(self.sam)\n    self.ort_session = onnxruntime.InferenceSession(\n        self.cache_file(\"decoder.onnx\"),\n        providers=[\n            \"CUDAExecutionProvider\",\n            \"OpenVINOExecutionProvider\",\n            \"CPUExecutionProvider\",\n        ],\n    )\n    self.embedding_cache = {}\n    self.image_size_cache = {}\n    self.embedding_cache_keys = []\n\n    self.low_res_logits_cache = {}\n    self.segmentation_cache_keys = []\n    self.task_type = \"unsupervised-segmentation\"\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.embed_image","title":"<code>embed_image(image, image_id=None, **kwargs)</code>","text":"<p>Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached, the cached result will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to be embedded. The format should be compatible with the preproc_image method.</p> required <code>image_id</code> <code>Optional[str]</code> <p>An identifier for the image. If provided, the embedding result will be cached                       with this ID. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image                                and the second element is the shape (height, width) of the processed image.</p> Notes <ul> <li>Embeddings and image sizes are cached to improve performance on repeated requests for the same image.</li> <li>The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,   the oldest entries are removed.</li> </ul> Example <p>img_array = ... # some image array embed_image(img_array, image_id=\"sample123\") (array([...]), (224, 224))</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def embed_image(self, image: Any, image_id: Optional[str] = None, **kwargs):\n    \"\"\"\n    Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached,\n    the cached result will be returned.\n\n    Args:\n        image (Any): The image to be embedded. The format should be compatible with the preproc_image method.\n        image_id (Optional[str]): An identifier for the image. If provided, the embedding result will be cached\n                                  with this ID. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image\n                                           and the second element is the shape (height, width) of the processed image.\n\n    Notes:\n        - Embeddings and image sizes are cached to improve performance on repeated requests for the same image.\n        - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n          the oldest entries are removed.\n\n    Example:\n        &gt;&gt;&gt; img_array = ... # some image array\n        &gt;&gt;&gt; embed_image(img_array, image_id=\"sample123\")\n        (array([...]), (224, 224))\n    \"\"\"\n    if image_id and image_id in self.embedding_cache:\n        return (\n            self.embedding_cache[image_id],\n            self.image_size_cache[image_id],\n        )\n    img_in = self.preproc_image(image)\n    self.predictor.set_image(img_in)\n    embedding = self.predictor.get_image_embedding().cpu().numpy()\n    if image_id:\n        self.embedding_cache[image_id] = embedding\n        self.image_size_cache[image_id] = img_in.shape[:2]\n        self.embedding_cache_keys.append(image_id)\n        if len(self.embedding_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n            cache_key = self.embedding_cache_keys.pop(0)\n            del self.embedding_cache[cache_key]\n            del self.image_size_cache[cache_key]\n    return (embedding, img_in.shape[:2])\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of file names.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: List of file names.\n    \"\"\"\n    return [\"encoder.pth\", \"decoder.onnx\"]\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Performs inference based on the request type.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>SamInferenceRequest</code> <p>The inference request.</p> required <p>Returns:</p> Type Description <p>Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def infer_from_request(self, request: SamInferenceRequest):\n    \"\"\"Performs inference based on the request type.\n\n    Args:\n        request (SamInferenceRequest): The inference request.\n\n    Returns:\n        Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n    \"\"\"\n    t1 = perf_counter()\n    if isinstance(request, SamEmbeddingRequest):\n        embedding, _ = self.embed_image(**request.dict())\n        inference_time = perf_counter() - t1\n        if request.format == \"json\":\n            return SamEmbeddingResponse(\n                embeddings=embedding.tolist(), time=inference_time\n            )\n        elif request.format == \"binary\":\n            binary_vector = BytesIO()\n            np.save(binary_vector, embedding)\n            binary_vector.seek(0)\n            return SamEmbeddingResponse(\n                embeddings=binary_vector.getvalue(), time=inference_time\n            )\n    elif isinstance(request, SamSegmentationRequest):\n        masks, low_res_masks = self.segment_image(**request.dict())\n        if request.format == \"json\":\n            masks = masks &gt; self.predictor.model.mask_threshold\n            masks = masks2poly(masks)\n            low_res_masks = low_res_masks &gt; self.predictor.model.mask_threshold\n            low_res_masks = masks2poly(low_res_masks)\n        elif request.format == \"binary\":\n            binary_vector = BytesIO()\n            np.savez_compressed(\n                binary_vector, masks=masks, low_res_masks=low_res_masks\n            )\n            binary_vector.seek(0)\n            binary_data = binary_vector.getvalue()\n            return binary_data\n        else:\n            raise ValueError(f\"Invalid format {request.format}\")\n\n        response = SamSegmentationResponse(\n            masks=[m.tolist() for m in masks],\n            low_res_masks=[m.tolist() for m in low_res_masks],\n            time=perf_counter() - t1,\n        )\n        return response\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage):\n    \"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_rgb(image)\n    return np_image\n</code></pre>"},{"location":"docs/reference/inference/models/sam/segment_anything/#inference.models.sam.segment_anything.SegmentAnything.segment_image","title":"<code>segment_image(image, embeddings=None, embeddings_format='json', has_mask_input=False, image_id=None, mask_input=None, mask_input_format='json', orig_im_size=None, point_coords=[], point_labels=[], use_mask_input_cache=True, **kwargs)</code>","text":"<p>Segments an image based on provided embeddings, points, masks, or cached results. If embeddings are not directly provided, the function can derive them from the input image or cache.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to be segmented.</p> required <code>embeddings</code> <code>Optional[Union[ndarray, List[List[float]]]]</code> <p>The embeddings of the image. Defaults to None, in which case the image is used to compute embeddings.</p> <code>None</code> <code>embeddings_format</code> <code>Optional[str]</code> <p>Format of the provided embeddings; either 'json' or 'binary'. Defaults to 'json'.</p> <code>'json'</code> <code>has_mask_input</code> <code>Optional[bool]</code> <p>Specifies whether mask input is provided. Defaults to False.</p> <code>False</code> <code>image_id</code> <code>Optional[str]</code> <p>A cached identifier for the image. Useful for accessing cached embeddings or masks.</p> <code>None</code> <code>mask_input</code> <code>Optional[Union[ndarray, List[List[List[float]]]]]</code> <p>Input mask for the image.</p> <code>None</code> <code>mask_input_format</code> <code>Optional[str]</code> <p>Format of the provided mask input; either 'json' or 'binary'. Defaults to 'json'.</p> <code>'json'</code> <code>orig_im_size</code> <code>Optional[List[int]]</code> <p>Original size of the image when providing embeddings directly.</p> <code>None</code> <code>point_coords</code> <code>Optional[List[List[float]]]</code> <p>Coordinates of points in the image. Defaults to an empty list.</p> <code>[]</code> <code>point_labels</code> <code>Optional[List[int]]</code> <p>Labels associated with the provided points. Defaults to an empty list.</p> <code>[]</code> <code>use_mask_input_cache</code> <code>Optional[bool]</code> <p>Flag to determine if cached mask input should be used. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Tuple[np.ndarray, np.ndarray]: A tuple where the first element is the segmentation masks of the image                           and the second element is the low resolution segmentation masks.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If necessary inputs are missing or inconsistent.</p> Notes <ul> <li>Embeddings, segmentations, and low-resolution logits can be cached to improve performance   on repeated requests for the same image.</li> <li>The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,   the oldest entries are removed.</li> </ul> Source code in <code>inference/models/sam/segment_anything.py</code> <pre><code>def segment_image(\n    self,\n    image: Any,\n    embeddings: Optional[Union[np.ndarray, List[List[float]]]] = None,\n    embeddings_format: Optional[str] = \"json\",\n    has_mask_input: Optional[bool] = False,\n    image_id: Optional[str] = None,\n    mask_input: Optional[Union[np.ndarray, List[List[List[float]]]]] = None,\n    mask_input_format: Optional[str] = \"json\",\n    orig_im_size: Optional[List[int]] = None,\n    point_coords: Optional[List[List[float]]] = [],\n    point_labels: Optional[List[int]] = [],\n    use_mask_input_cache: Optional[bool] = True,\n    **kwargs,\n):\n    \"\"\"\n    Segments an image based on provided embeddings, points, masks, or cached results.\n    If embeddings are not directly provided, the function can derive them from the input image or cache.\n\n    Args:\n        image (Any): The image to be segmented.\n        embeddings (Optional[Union[np.ndarray, List[List[float]]]]): The embeddings of the image.\n            Defaults to None, in which case the image is used to compute embeddings.\n        embeddings_format (Optional[str]): Format of the provided embeddings; either 'json' or 'binary'. Defaults to 'json'.\n        has_mask_input (Optional[bool]): Specifies whether mask input is provided. Defaults to False.\n        image_id (Optional[str]): A cached identifier for the image. Useful for accessing cached embeddings or masks.\n        mask_input (Optional[Union[np.ndarray, List[List[List[float]]]]]): Input mask for the image.\n        mask_input_format (Optional[str]): Format of the provided mask input; either 'json' or 'binary'. Defaults to 'json'.\n        orig_im_size (Optional[List[int]]): Original size of the image when providing embeddings directly.\n        point_coords (Optional[List[List[float]]]): Coordinates of points in the image. Defaults to an empty list.\n        point_labels (Optional[List[int]]): Labels associated with the provided points. Defaults to an empty list.\n        use_mask_input_cache (Optional[bool]): Flag to determine if cached mask input should be used. Defaults to True.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: A tuple where the first element is the segmentation masks of the image\n                                      and the second element is the low resolution segmentation masks.\n\n    Raises:\n        ValueError: If necessary inputs are missing or inconsistent.\n\n    Notes:\n        - Embeddings, segmentations, and low-resolution logits can be cached to improve performance\n          on repeated requests for the same image.\n        - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n          the oldest entries are removed.\n    \"\"\"\n    if not embeddings:\n        if not image and not image_id:\n            raise ValueError(\n                \"Must provide either image, cached image_id, or embeddings\"\n            )\n        elif image_id and not image and image_id not in self.embedding_cache:\n            raise ValueError(\n                f\"Image ID {image_id} not in embedding cache, must provide the image or embeddings\"\n            )\n        embedding, original_image_size = self.embed_image(\n            image=image, image_id=image_id\n        )\n    else:\n        if not orig_im_size:\n            raise ValueError(\n                \"Must provide original image size if providing embeddings\"\n            )\n        original_image_size = orig_im_size\n        if embeddings_format == \"json\":\n            embedding = np.array(embeddings)\n        elif embeddings_format == \"binary\":\n            embedding = np.load(BytesIO(embeddings))\n\n    point_coords = point_coords\n    point_coords.append([0, 0])\n    point_coords = np.array(point_coords, dtype=np.float32)\n    point_coords = np.expand_dims(point_coords, axis=0)\n    point_coords = self.predictor.transform.apply_coords(\n        point_coords,\n        original_image_size,\n    )\n\n    point_labels = point_labels\n    point_labels.append(-1)\n    point_labels = np.array(point_labels, dtype=np.float32)\n    point_labels = np.expand_dims(point_labels, axis=0)\n\n    if has_mask_input:\n        if (\n            image_id\n            and image_id in self.low_res_logits_cache\n            and use_mask_input_cache\n        ):\n            mask_input = self.low_res_logits_cache[image_id]\n        elif not mask_input and (\n            not image_id or image_id not in self.low_res_logits_cache\n        ):\n            raise ValueError(\"Must provide either mask_input or cached image_id\")\n        else:\n            if mask_input_format == \"json\":\n                polys = mask_input\n                mask_input = np.zeros((1, len(polys), 256, 256), dtype=np.uint8)\n                for i, poly in enumerate(polys):\n                    poly = ShapelyPolygon(poly)\n                    raster = rasterio.features.rasterize(\n                        [poly], out_shape=(256, 256)\n                    )\n                    mask_input[0, i, :, :] = raster\n            elif mask_input_format == \"binary\":\n                binary_data = base64.b64decode(mask_input)\n                mask_input = np.load(BytesIO(binary_data))\n    else:\n        mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n\n    ort_inputs = {\n        \"image_embeddings\": embedding.astype(np.float32),\n        \"point_coords\": point_coords.astype(np.float32),\n        \"point_labels\": point_labels,\n        \"mask_input\": mask_input.astype(np.float32),\n        \"has_mask_input\": (\n            np.zeros(1, dtype=np.float32)\n            if not has_mask_input\n            else np.ones(1, dtype=np.float32)\n        ),\n        \"orig_im_size\": np.array(original_image_size, dtype=np.float32),\n    }\n    masks, _, low_res_logits = self.ort_session.run(None, ort_inputs)\n    if image_id:\n        self.low_res_logits_cache[image_id] = low_res_logits\n        if image_id not in self.segmentation_cache_keys:\n            self.segmentation_cache_keys.append(image_id)\n        if len(self.segmentation_cache_keys) &gt; SAM_MAX_EMBEDDING_CACHE_SIZE:\n            cache_key = self.segmentation_cache_keys.pop(0)\n            del self.low_res_logits_cache[cache_key]\n    masks = masks[0]\n    low_res_masks = low_res_logits[0]\n\n    return masks, low_res_masks\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/","title":"segment_anything2","text":""},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2","title":"<code>SegmentAnything2</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>SegmentAnything class for handling segmentation tasks.</p> <p>Attributes:</p> Name Type Description <code>sam</code> <p>The segmentation model.</p> <code>predictor</code> <p>The predictor for the segmentation model.</p> <code>ort_session</code> <p>ONNX runtime inference session.</p> <code>embedding_cache</code> <p>Cache for embeddings.</p> <code>image_size_cache</code> <p>Cache for image sizes.</p> <code>embedding_cache_keys</code> <p>Keys for the embedding cache.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>class SegmentAnything2(RoboflowCoreModel):\n    \"\"\"SegmentAnything class for handling segmentation tasks.\n\n    Attributes:\n        sam: The segmentation model.\n        predictor: The predictor for the segmentation model.\n        ort_session: ONNX runtime inference session.\n        embedding_cache: Cache for embeddings.\n        image_size_cache: Cache for image sizes.\n        embedding_cache_keys: Keys for the embedding cache.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        model_id: str = f\"sam2/{SAM2_VERSION_ID}\",\n        low_res_logits_cache_size: int = SAM2_MAX_LOGITS_CACHE_SIZE,\n        embedding_cache_size: int = SAM2_MAX_EMBEDDING_CACHE_SIZE,\n        **kwargs,\n    ):\n        \"\"\"Initializes the SegmentAnything.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, model_id=model_id, **kwargs)\n        checkpoint = self.cache_file(\"weights.pt\")\n        model_cfg = {\n            \"hiera_large\": \"sam2_hiera_l.yaml\",\n            \"hiera_small\": \"sam2_hiera_s.yaml\",\n            \"hiera_tiny\": \"sam2_hiera_t.yaml\",\n            \"hiera_b_plus\": \"sam2_hiera_b+.yaml\",\n        }[self.version_id]\n\n        self.sam = build_sam2(model_cfg, checkpoint, device=DEVICE)\n        self.low_res_logits_cache_size = low_res_logits_cache_size\n        self.embedding_cache_size = embedding_cache_size\n\n        self.predictor = SAM2ImagePredictor(self.sam)\n\n        self.embedding_cache = {}\n        self.image_size_cache = {}\n        self.embedding_cache_keys = []\n        self.low_res_logits_cache: Dict[Tuple[str, str], LogitsCacheType] = {}\n        self.low_res_logits_cache_keys = []\n\n        self.task_type = \"unsupervised-segmentation\"\n\n    def get_infer_bucket_file_list(self) -&gt; List[str]:\n        \"\"\"Gets the list of files required for inference.\n\n        Returns:\n            List[str]: List of file names.\n        \"\"\"\n        return [\"weights.pt\"]\n\n    def embed_image(\n        self,\n        image: Optional[InferenceRequestImage],\n        image_id: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached,\n        the cached result will be returned.\n\n        Args:\n            image (Any): The image to be embedded. The format should be compatible with the preproc_image method.\n            image_id (Optional[str]): An identifier for the image. If provided, the embedding result will be cached\n                                      with this ID. Defaults to None.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image\n                                               and the second element is the shape (height, width) of the processed image.\n\n        Notes:\n            - Embeddings and image sizes are cached to improve performance on repeated requests for the same image.\n            - The cache has a maximum size defined by SAM2_MAX_CACHE_SIZE. When the cache exceeds this size,\n              the oldest entries are removed.\n\n        Example:\n            &gt;&gt;&gt; img_array = ... # some image array\n            &gt;&gt;&gt; embed_image(img_array, image_id=\"sample123\")\n            (array([...]), (224, 224))\n        \"\"\"\n        if image_id and image_id in self.embedding_cache:\n            return (\n                self.embedding_cache[image_id],\n                self.image_size_cache[image_id],\n                image_id,\n            )\n\n        img_in = self.preproc_image(image)\n        if image_id is None:\n            image_id = hashlib.md5(img_in.tobytes()).hexdigest()[:12]\n\n        if image_id in self.embedding_cache:\n            return (\n                self.embedding_cache[image_id],\n                self.image_size_cache[image_id],\n                image_id,\n            )\n\n        with torch.inference_mode():\n            self.predictor.set_image(img_in)\n            embedding_dict = self.predictor._features\n\n        self.embedding_cache[image_id] = embedding_dict\n        self.image_size_cache[image_id] = img_in.shape[:2]\n        if image_id in self.embedding_cache_keys:\n            self.embedding_cache_keys.remove(image_id)\n        self.embedding_cache_keys.append(image_id)\n        if len(self.embedding_cache_keys) &gt; self.embedding_cache_size:\n            cache_key = self.embedding_cache_keys.pop(0)\n            del self.embedding_cache[cache_key]\n            del self.image_size_cache[cache_key]\n        return (embedding_dict, img_in.shape[:2], image_id)\n\n    def infer_from_request(self, request: Sam2InferenceRequest):\n        \"\"\"Performs inference based on the request type.\n\n        Args:\n            request (SamInferenceRequest): The inference request.\n\n        Returns:\n            Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n        \"\"\"\n        t1 = perf_counter()\n        if isinstance(request, Sam2EmbeddingRequest):\n            _, _, image_id = self.embed_image(**request.dict())\n            inference_time = perf_counter() - t1\n            return Sam2EmbeddingResponse(time=inference_time, image_id=image_id)\n        elif isinstance(request, Sam2SegmentationRequest):\n            masks, scores, low_resolution_logits = self.segment_image(**request.dict())\n            if request.format == \"json\":\n                return turn_segmentation_results_into_api_response(\n                    masks=masks,\n                    scores=scores,\n                    mask_threshold=self.predictor.mask_threshold,\n                    inference_start_timestamp=t1,\n                )\n            elif request.format == \"binary\":\n                binary_vector = BytesIO()\n                np.savez_compressed(\n                    binary_vector, masks=masks, low_res_masks=low_resolution_logits\n                )\n                binary_vector.seek(0)\n                binary_data = binary_vector.getvalue()\n                return binary_data\n            else:\n                raise ValueError(f\"Invalid format {request.format}\")\n\n        else:\n            raise ValueError(f\"Invalid request type {type(request)}\")\n\n    def preproc_image(self, image: InferenceRequestImage):\n        \"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_rgb(image)\n        return np_image\n\n    def segment_image(\n        self,\n        image: Optional[InferenceRequestImage],\n        image_id: Optional[str] = None,\n        prompts: Optional[Union[Sam2PromptSet, dict]] = None,\n        multimask_output: Optional[bool] = True,\n        mask_input: Optional[Union[np.ndarray, List[List[List[float]]]]] = None,\n        save_logits_to_cache: bool = False,\n        load_logits_from_cache: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Segments an image based on provided embeddings, points, masks, or cached results.\n        If embeddings are not directly provided, the function can derive them from the input image or cache.\n\n        Args:\n            image (Any): The image to be segmented.\n            image_id (Optional[str]): A cached identifier for the image. Useful for accessing cached embeddings or masks.\n            prompts (Optional[List[Sam2Prompt]]): List of prompts to use for segmentation. Defaults to None.\n            mask_input (Optional[Union[np.ndarray, List[List[List[float]]]]]): Input low_res_logits for the image.\n            multimask_output: (bool): Flag to decide if multiple masks proposal to be predicted (among which the most\n                promising will be returned\n            )\n            use_logits_cache: (bool): Flag to decide to use cached logits from prior prompting\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray, np.ndarray]: Tuple of np.array, where:\n                - first element is of size (prompt_set_size, h, w) and represent mask with the highest confidence\n                    for each prompt element\n                - second element is of size (prompt_set_size, ) and represents ths score for most confident mask\n                    of each prompt element\n                - third element is of size (prompt_set_size, 256, 256) and represents the low resolution logits\n                    for most confident mask of each prompt element\n\n        Raises:\n            ValueError: If necessary inputs are missing or inconsistent.\n\n        Notes:\n            - Embeddings, segmentations, and low-resolution logits can be cached to improve performance\n              on repeated requests for the same image.\n            - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n              the oldest entries are removed.\n        \"\"\"\n        load_logits_from_cache = (\n            load_logits_from_cache and not DISABLE_SAM2_LOGITS_CACHE\n        )\n        save_logits_to_cache = save_logits_to_cache and not DISABLE_SAM2_LOGITS_CACHE\n        with torch.inference_mode():\n            if image is None and not image_id:\n                raise ValueError(\"Must provide either image or  cached image_id\")\n            elif image_id and image is None and image_id not in self.embedding_cache:\n                raise ValueError(\n                    f\"Image ID {image_id} not in embedding cache, must provide the image or embeddings\"\n                )\n            embedding, original_image_size, image_id = self.embed_image(\n                image=image, image_id=image_id\n            )\n\n            self.predictor._is_image_set = True\n            self.predictor._features = embedding\n            self.predictor._orig_hw = [original_image_size]\n            self.predictor._is_batch = False\n            args = dict()\n            prompt_set: Sam2PromptSet\n            if prompts:\n                if type(prompts) is dict:\n                    prompt_set = Sam2PromptSet(**prompts)\n                    args = prompt_set.to_sam2_inputs()\n                else:\n                    prompt_set = prompts\n                    args = prompts.to_sam2_inputs()\n            else:\n                prompt_set = Sam2PromptSet()\n\n            if mask_input is None and load_logits_from_cache:\n                mask_input = maybe_load_low_res_logits_from_cache(\n                    image_id, prompt_set, self.low_res_logits_cache\n                )\n\n            args = pad_points(args)\n            if not any(args.values()):\n                args = {\"point_coords\": [[0, 0]], \"point_labels\": [-1], \"box\": None}\n            masks, scores, low_resolution_logits = self.predictor.predict(\n                mask_input=mask_input,\n                multimask_output=multimask_output,\n                return_logits=True,\n                normalize_coords=True,\n                **args,\n            )\n            masks, scores, low_resolution_logits = choose_most_confident_sam_prediction(\n                masks=masks,\n                scores=scores,\n                low_resolution_logits=low_resolution_logits,\n            )\n\n            if save_logits_to_cache:\n                self.add_low_res_logits_to_cache(\n                    low_resolution_logits, image_id, prompt_set\n                )\n\n            return masks, scores, low_resolution_logits\n\n    def add_low_res_logits_to_cache(\n        self, logits: np.ndarray, image_id: str, prompt_set: Sam2PromptSet\n    ) -&gt; None:\n        logits = logits[:, None, :, :]\n        prompt_id = hash_prompt_set(image_id, prompt_set)\n        self.low_res_logits_cache[prompt_id] = {\n            \"logits\": logits,\n            \"prompt_set\": prompt_set,\n        }\n        if prompt_id in self.low_res_logits_cache_keys:\n            self.low_res_logits_cache_keys.remove(prompt_id)\n        self.low_res_logits_cache_keys.append(prompt_id)\n        if len(self.low_res_logits_cache_keys) &gt; self.low_res_logits_cache_size:\n            cache_key = self.low_res_logits_cache_keys.pop(0)\n            del self.low_res_logits_cache[cache_key]\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.__init__","title":"<code>__init__(*args, model_id=f'sam2/{SAM2_VERSION_ID}', low_res_logits_cache_size=SAM2_MAX_LOGITS_CACHE_SIZE, embedding_cache_size=SAM2_MAX_EMBEDDING_CACHE_SIZE, **kwargs)</code>","text":"<p>Initializes the SegmentAnything.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    model_id: str = f\"sam2/{SAM2_VERSION_ID}\",\n    low_res_logits_cache_size: int = SAM2_MAX_LOGITS_CACHE_SIZE,\n    embedding_cache_size: int = SAM2_MAX_EMBEDDING_CACHE_SIZE,\n    **kwargs,\n):\n    \"\"\"Initializes the SegmentAnything.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, model_id=model_id, **kwargs)\n    checkpoint = self.cache_file(\"weights.pt\")\n    model_cfg = {\n        \"hiera_large\": \"sam2_hiera_l.yaml\",\n        \"hiera_small\": \"sam2_hiera_s.yaml\",\n        \"hiera_tiny\": \"sam2_hiera_t.yaml\",\n        \"hiera_b_plus\": \"sam2_hiera_b+.yaml\",\n    }[self.version_id]\n\n    self.sam = build_sam2(model_cfg, checkpoint, device=DEVICE)\n    self.low_res_logits_cache_size = low_res_logits_cache_size\n    self.embedding_cache_size = embedding_cache_size\n\n    self.predictor = SAM2ImagePredictor(self.sam)\n\n    self.embedding_cache = {}\n    self.image_size_cache = {}\n    self.embedding_cache_keys = []\n    self.low_res_logits_cache: Dict[Tuple[str, str], LogitsCacheType] = {}\n    self.low_res_logits_cache_keys = []\n\n    self.task_type = \"unsupervised-segmentation\"\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.embed_image","title":"<code>embed_image(image, image_id=None, **kwargs)</code>","text":"<p>Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached, the cached result will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to be embedded. The format should be compatible with the preproc_image method.</p> required <code>image_id</code> <code>Optional[str]</code> <p>An identifier for the image. If provided, the embedding result will be cached                       with this ID. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image                                and the second element is the shape (height, width) of the processed image.</p> Notes <ul> <li>Embeddings and image sizes are cached to improve performance on repeated requests for the same image.</li> <li>The cache has a maximum size defined by SAM2_MAX_CACHE_SIZE. When the cache exceeds this size,   the oldest entries are removed.</li> </ul> Example <p>img_array = ... # some image array embed_image(img_array, image_id=\"sample123\") (array([...]), (224, 224))</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def embed_image(\n    self,\n    image: Optional[InferenceRequestImage],\n    image_id: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Embeds an image and caches the result if an image_id is provided. If the image has been embedded before and cached,\n    the cached result will be returned.\n\n    Args:\n        image (Any): The image to be embedded. The format should be compatible with the preproc_image method.\n        image_id (Optional[str]): An identifier for the image. If provided, the embedding result will be cached\n                                  with this ID. Defaults to None.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray, Tuple[int, int]]: A tuple where the first element is the embedding of the image\n                                           and the second element is the shape (height, width) of the processed image.\n\n    Notes:\n        - Embeddings and image sizes are cached to improve performance on repeated requests for the same image.\n        - The cache has a maximum size defined by SAM2_MAX_CACHE_SIZE. When the cache exceeds this size,\n          the oldest entries are removed.\n\n    Example:\n        &gt;&gt;&gt; img_array = ... # some image array\n        &gt;&gt;&gt; embed_image(img_array, image_id=\"sample123\")\n        (array([...]), (224, 224))\n    \"\"\"\n    if image_id and image_id in self.embedding_cache:\n        return (\n            self.embedding_cache[image_id],\n            self.image_size_cache[image_id],\n            image_id,\n        )\n\n    img_in = self.preproc_image(image)\n    if image_id is None:\n        image_id = hashlib.md5(img_in.tobytes()).hexdigest()[:12]\n\n    if image_id in self.embedding_cache:\n        return (\n            self.embedding_cache[image_id],\n            self.image_size_cache[image_id],\n            image_id,\n        )\n\n    with torch.inference_mode():\n        self.predictor.set_image(img_in)\n        embedding_dict = self.predictor._features\n\n    self.embedding_cache[image_id] = embedding_dict\n    self.image_size_cache[image_id] = img_in.shape[:2]\n    if image_id in self.embedding_cache_keys:\n        self.embedding_cache_keys.remove(image_id)\n    self.embedding_cache_keys.append(image_id)\n    if len(self.embedding_cache_keys) &gt; self.embedding_cache_size:\n        cache_key = self.embedding_cache_keys.pop(0)\n        del self.embedding_cache[cache_key]\n        del self.image_size_cache[cache_key]\n    return (embedding_dict, img_in.shape[:2], image_id)\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Gets the list of files required for inference.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of file names.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; List[str]:\n    \"\"\"Gets the list of files required for inference.\n\n    Returns:\n        List[str]: List of file names.\n    \"\"\"\n    return [\"weights.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Performs inference based on the request type.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>SamInferenceRequest</code> <p>The inference request.</p> required <p>Returns:</p> Type Description <p>Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def infer_from_request(self, request: Sam2InferenceRequest):\n    \"\"\"Performs inference based on the request type.\n\n    Args:\n        request (SamInferenceRequest): The inference request.\n\n    Returns:\n        Union[SamEmbeddingResponse, SamSegmentationResponse]: The inference response.\n    \"\"\"\n    t1 = perf_counter()\n    if isinstance(request, Sam2EmbeddingRequest):\n        _, _, image_id = self.embed_image(**request.dict())\n        inference_time = perf_counter() - t1\n        return Sam2EmbeddingResponse(time=inference_time, image_id=image_id)\n    elif isinstance(request, Sam2SegmentationRequest):\n        masks, scores, low_resolution_logits = self.segment_image(**request.dict())\n        if request.format == \"json\":\n            return turn_segmentation_results_into_api_response(\n                masks=masks,\n                scores=scores,\n                mask_threshold=self.predictor.mask_threshold,\n                inference_start_timestamp=t1,\n            )\n        elif request.format == \"binary\":\n            binary_vector = BytesIO()\n            np.savez_compressed(\n                binary_vector, masks=masks, low_res_masks=low_resolution_logits\n            )\n            binary_vector.seek(0)\n            binary_data = binary_vector.getvalue()\n            return binary_data\n        else:\n            raise ValueError(f\"Invalid format {request.format}\")\n\n    else:\n        raise ValueError(f\"Invalid request type {type(request)}\")\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def preproc_image(self, image: InferenceRequestImage):\n    \"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_rgb(image)\n    return np_image\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.SegmentAnything2.segment_image","title":"<code>segment_image(image, image_id=None, prompts=None, multimask_output=True, mask_input=None, save_logits_to_cache=False, load_logits_from_cache=False, **kwargs)</code>","text":"<p>Segments an image based on provided embeddings, points, masks, or cached results. If embeddings are not directly provided, the function can derive them from the input image or cache.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image to be segmented.</p> required <code>image_id</code> <code>Optional[str]</code> <p>A cached identifier for the image. Useful for accessing cached embeddings or masks.</p> <code>None</code> <code>prompts</code> <code>Optional[List[Sam2Prompt]]</code> <p>List of prompts to use for segmentation. Defaults to None.</p> <code>None</code> <code>mask_input</code> <code>Optional[Union[ndarray, List[List[List[float]]]]]</code> <p>Input low_res_logits for the image.</p> <code>None</code> <code>multimask_output</code> <code>Optional[bool]</code> <p>(bool): Flag to decide if multiple masks proposal to be predicted (among which the most promising will be returned</p> <code>True</code> <code>use_logits_cache</code> <p>(bool): Flag to decide to use cached logits from prior prompting</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>Tuple[np.ndarray, np.ndarray, np.ndarray]: Tuple of np.array, where: - first element is of size (prompt_set_size, h, w) and represent mask with the highest confidence     for each prompt element - second element is of size (prompt_set_size, ) and represents ths score for most confident mask     of each prompt element - third element is of size (prompt_set_size, 256, 256) and represents the low resolution logits     for most confident mask of each prompt element</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If necessary inputs are missing or inconsistent.</p> Notes <ul> <li>Embeddings, segmentations, and low-resolution logits can be cached to improve performance   on repeated requests for the same image.</li> <li>The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,   the oldest entries are removed.</li> </ul> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def segment_image(\n    self,\n    image: Optional[InferenceRequestImage],\n    image_id: Optional[str] = None,\n    prompts: Optional[Union[Sam2PromptSet, dict]] = None,\n    multimask_output: Optional[bool] = True,\n    mask_input: Optional[Union[np.ndarray, List[List[List[float]]]]] = None,\n    save_logits_to_cache: bool = False,\n    load_logits_from_cache: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Segments an image based on provided embeddings, points, masks, or cached results.\n    If embeddings are not directly provided, the function can derive them from the input image or cache.\n\n    Args:\n        image (Any): The image to be segmented.\n        image_id (Optional[str]): A cached identifier for the image. Useful for accessing cached embeddings or masks.\n        prompts (Optional[List[Sam2Prompt]]): List of prompts to use for segmentation. Defaults to None.\n        mask_input (Optional[Union[np.ndarray, List[List[List[float]]]]]): Input low_res_logits for the image.\n        multimask_output: (bool): Flag to decide if multiple masks proposal to be predicted (among which the most\n            promising will be returned\n        )\n        use_logits_cache: (bool): Flag to decide to use cached logits from prior prompting\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray]: Tuple of np.array, where:\n            - first element is of size (prompt_set_size, h, w) and represent mask with the highest confidence\n                for each prompt element\n            - second element is of size (prompt_set_size, ) and represents ths score for most confident mask\n                of each prompt element\n            - third element is of size (prompt_set_size, 256, 256) and represents the low resolution logits\n                for most confident mask of each prompt element\n\n    Raises:\n        ValueError: If necessary inputs are missing or inconsistent.\n\n    Notes:\n        - Embeddings, segmentations, and low-resolution logits can be cached to improve performance\n          on repeated requests for the same image.\n        - The cache has a maximum size defined by SAM_MAX_EMBEDDING_CACHE_SIZE. When the cache exceeds this size,\n          the oldest entries are removed.\n    \"\"\"\n    load_logits_from_cache = (\n        load_logits_from_cache and not DISABLE_SAM2_LOGITS_CACHE\n    )\n    save_logits_to_cache = save_logits_to_cache and not DISABLE_SAM2_LOGITS_CACHE\n    with torch.inference_mode():\n        if image is None and not image_id:\n            raise ValueError(\"Must provide either image or  cached image_id\")\n        elif image_id and image is None and image_id not in self.embedding_cache:\n            raise ValueError(\n                f\"Image ID {image_id} not in embedding cache, must provide the image or embeddings\"\n            )\n        embedding, original_image_size, image_id = self.embed_image(\n            image=image, image_id=image_id\n        )\n\n        self.predictor._is_image_set = True\n        self.predictor._features = embedding\n        self.predictor._orig_hw = [original_image_size]\n        self.predictor._is_batch = False\n        args = dict()\n        prompt_set: Sam2PromptSet\n        if prompts:\n            if type(prompts) is dict:\n                prompt_set = Sam2PromptSet(**prompts)\n                args = prompt_set.to_sam2_inputs()\n            else:\n                prompt_set = prompts\n                args = prompts.to_sam2_inputs()\n        else:\n            prompt_set = Sam2PromptSet()\n\n        if mask_input is None and load_logits_from_cache:\n            mask_input = maybe_load_low_res_logits_from_cache(\n                image_id, prompt_set, self.low_res_logits_cache\n            )\n\n        args = pad_points(args)\n        if not any(args.values()):\n            args = {\"point_coords\": [[0, 0]], \"point_labels\": [-1], \"box\": None}\n        masks, scores, low_resolution_logits = self.predictor.predict(\n            mask_input=mask_input,\n            multimask_output=multimask_output,\n            return_logits=True,\n            normalize_coords=True,\n            **args,\n        )\n        masks, scores, low_resolution_logits = choose_most_confident_sam_prediction(\n            masks=masks,\n            scores=scores,\n            low_resolution_logits=low_resolution_logits,\n        )\n\n        if save_logits_to_cache:\n            self.add_low_res_logits_to_cache(\n                low_resolution_logits, image_id, prompt_set\n            )\n\n        return masks, scores, low_resolution_logits\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.choose_most_confident_sam_prediction","title":"<code>choose_most_confident_sam_prediction(masks, scores, low_resolution_logits)</code>","text":"<p>This function is supposed to post-process SAM2 inference and choose most confident mask regardless of <code>multimask_output</code> parameter value Args:     masks: np array with values 0.0 and 1.0 representing predicted mask of size         (prompt_set_size, proposed_maks, h, w) or (proposed_maks, h, w) - depending on         prompt set size - unfortunately, prompt_set_size=1 causes squeeze operation         in SAM2 library, so to handle inference uniformly, we need to compensate with         this function.     scores: array of size (prompt_set_size, proposed_maks) or (proposed_maks, ) depending         on prompt set size - this array gives confidence score for mask proposal     low_resolution_logits: array of size (prompt_set_size, proposed_maks, 256, 256) or         (proposed_maks, 256, 256) - depending on prompt set size. These low resolution logits          can be passed to a subsequent iteration as mask input. Returns:     Tuple of np.array, where:         - first element is of size (prompt_set_size, h, w) and represent mask with the highest confidence             for each prompt element         - second element is of size (prompt_set_size, ) and represents ths score for most confident mask             of each prompt element         - third element is of size (prompt_set_size, 256, 256) and represents the low resolution logits             for most confident mask of each prompt element</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def choose_most_confident_sam_prediction(\n    masks: np.ndarray,\n    scores: np.ndarray,\n    low_resolution_logits: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    This function is supposed to post-process SAM2 inference and choose most confident\n    mask regardless of `multimask_output` parameter value\n    Args:\n        masks: np array with values 0.0 and 1.0 representing predicted mask of size\n            (prompt_set_size, proposed_maks, h, w) or (proposed_maks, h, w) - depending on\n            prompt set size - unfortunately, prompt_set_size=1 causes squeeze operation\n            in SAM2 library, so to handle inference uniformly, we need to compensate with\n            this function.\n        scores: array of size (prompt_set_size, proposed_maks) or (proposed_maks, ) depending\n            on prompt set size - this array gives confidence score for mask proposal\n        low_resolution_logits: array of size (prompt_set_size, proposed_maks, 256, 256) or\n            (proposed_maks, 256, 256) - depending on prompt set size. These low resolution logits\n             can be passed to a subsequent iteration as mask input.\n    Returns:\n        Tuple of np.array, where:\n            - first element is of size (prompt_set_size, h, w) and represent mask with the highest confidence\n                for each prompt element\n            - second element is of size (prompt_set_size, ) and represents ths score for most confident mask\n                of each prompt element\n            - third element is of size (prompt_set_size, 256, 256) and represents the low resolution logits\n                for most confident mask of each prompt element\n    \"\"\"\n    if len(masks.shape) == 3:\n        masks = np.expand_dims(masks, axis=0)\n        scores = np.expand_dims(scores, axis=0)\n        low_resolution_logits = np.expand_dims(low_resolution_logits, axis=0)\n    selected_masks, selected_scores, selected_low_resolution_logits = [], [], []\n    for mask, score, low_resolution_logit in zip(masks, scores, low_resolution_logits):\n        selected_mask, selected_score, selected_low_resolution_logit = (\n            choose_most_confident_prompt_set_element_prediction(\n                mask=mask,\n                score=score,\n                low_resolution_logit=low_resolution_logit,\n            )\n        )\n        selected_masks.append(selected_mask)\n        selected_scores.append(selected_score)\n        selected_low_resolution_logits.append(selected_low_resolution_logit)\n    return (\n        np.asarray(selected_masks),\n        np.asarray(selected_scores),\n        np.asarray(selected_low_resolution_logits),\n    )\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.find_prior_prompt_in_cache","title":"<code>find_prior_prompt_in_cache(initial_prompt_set, image_id, cache)</code>","text":"<p>Performs search over the cache to see if prior used prompts are subset of this one.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def find_prior_prompt_in_cache(\n    initial_prompt_set: Sam2PromptSet,\n    image_id: str,\n    cache: Dict[Tuple[str, str], LogitsCacheType],\n) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Performs search over the cache to see if prior used prompts are subset of this one.\n    \"\"\"\n\n    logits_for_image = [cache[k] for k in cache if k[0] == image_id]\n    maxed_size = 0\n    best_match: Optional[np.ndarray] = None\n    desired_size = initial_prompt_set.num_points() - 1\n    for cached_dict in logits_for_image[::-1]:\n        logits = cached_dict[\"logits\"]\n        prompt_set: Sam2PromptSet = cached_dict[\"prompt_set\"]\n        is_viable = is_prompt_strict_subset(prompt_set, initial_prompt_set)\n        if not is_viable:\n            continue\n\n        size = prompt_set.num_points()\n        # short circuit search if we find prompt with one less point (most recent possible mask)\n        if size == desired_size:\n            return logits\n        if size &gt;= maxed_size:\n            maxed_size = size\n            best_match = logits\n\n    return best_match\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.hash_prompt_set","title":"<code>hash_prompt_set(image_id, prompt_set)</code>","text":"<p>Computes unique hash from a prompt set.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def hash_prompt_set(image_id: str, prompt_set: Sam2PromptSet) -&gt; Tuple[str, str]:\n    \"\"\"Computes unique hash from a prompt set.\"\"\"\n    md5_hash = hashlib.md5()\n    md5_hash.update(str(prompt_set).encode(\"utf-8\"))\n    return image_id, md5_hash.hexdigest()[:12]\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.maybe_load_low_res_logits_from_cache","title":"<code>maybe_load_low_res_logits_from_cache(image_id, prompt_set, cache)</code>","text":"<p>Loads prior masks from the cache by searching over possibel prior prompts.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def maybe_load_low_res_logits_from_cache(\n    image_id: str,\n    prompt_set: Sam2PromptSet,\n    cache: Dict[Tuple[str, str], LogitsCacheType],\n) -&gt; Optional[np.ndarray]:\n    \"Loads prior masks from the cache by searching over possibel prior prompts.\"\n    prompts = prompt_set.prompts\n    if not prompts:\n        return None\n\n    return find_prior_prompt_in_cache(prompt_set, image_id, cache)\n</code></pre>"},{"location":"docs/reference/inference/models/sam2/segment_anything2/#inference.models.sam2.segment_anything2.pad_points","title":"<code>pad_points(args)</code>","text":"<p>Pad arguments to be passed to sam2 model with not_a_point label (-1). This is necessary when there are multiple prompts per image so that a tensor can be created.</p> <p>Also pads empty point lists with a dummy non-point entry.</p> Source code in <code>inference/models/sam2/segment_anything2.py</code> <pre><code>def pad_points(args: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Pad arguments to be passed to sam2 model with not_a_point label (-1).\n    This is necessary when there are multiple prompts per image so that a tensor can be created.\n\n\n    Also pads empty point lists with a dummy non-point entry.\n    \"\"\"\n    args = copy.deepcopy(args)\n    if args[\"point_coords\"] is not None:\n        max_len = max(max(len(prompt) for prompt in args[\"point_coords\"]), 1)\n        for prompt in args[\"point_coords\"]:\n            for _ in range(max_len - len(prompt)):\n                prompt.append([0, 0])\n        for label in args[\"point_labels\"]:\n            for _ in range(max_len - len(label)):\n                label.append(-1)\n    else:\n        if args[\"point_labels\"] is not None:\n            raise ValueError(\n                \"Can't have point labels without corresponding point coordinates\"\n            )\n    return args\n</code></pre>"},{"location":"docs/reference/inference/models/transformers/transformers/","title":"transformers","text":""},{"location":"docs/reference/inference/models/transformers/transformers/#inference.models.transformers.transformers.LoRATransformerModel","title":"<code>LoRATransformerModel</code>","text":"<p>               Bases: <code>TransformerModel</code></p> Source code in <code>inference/models/transformers/transformers.py</code> <pre><code>class LoRATransformerModel(TransformerModel):\n    load_base_from_roboflow = False\n\n    def initialize_model(self):\n        lora_config = LoraConfig.from_pretrained(self.cache_dir, device_map=DEVICE)\n        model_id = lora_config.base_model_name_or_path\n        revision = lora_config.revision\n        if revision is not None:\n            try:\n                self.dtype = getattr(torch, revision)\n            except AttributeError:\n                pass\n        if not self.load_base_from_roboflow:\n            model_load_id = model_id\n            cache_dir = os.path.join(MODEL_CACHE_DIR, \"huggingface\")\n            revision = revision\n            token = self.huggingface_token\n        else:\n            model_load_id = self.get_lora_base_from_roboflow(model_id, revision)\n            cache_dir = model_load_id\n            revision = None\n            token = None\n        self.base_model = self.transformers_class.from_pretrained(\n            model_load_id,\n            revision=revision,\n            device_map=DEVICE,\n            cache_dir=cache_dir,\n            token=token,\n        ).to(self.dtype)\n        self.model = (\n            PeftModel.from_pretrained(self.base_model, self.cache_dir)\n            .eval()\n            .to(self.dtype)\n        )\n\n        self.processor = self.processor_class.from_pretrained(\n            model_load_id, revision=revision, cache_dir=cache_dir, token=token\n        )\n\n    def get_lora_base_from_roboflow(self, repo, revision) -&gt; str:\n        base_dir = os.path.join(\"lora-bases\", repo, revision)\n        cache_dir = get_cache_dir(base_dir)\n        if os.path.exists(cache_dir):\n            return cache_dir\n        api_data = get_roboflow_base_lora(self.api_key, repo, revision, self.device_id)\n        if \"weights\" not in api_data:\n            raise ModelArtefactError(\n                f\"`weights` key not available in Roboflow API response while downloading model weights.\"\n            )\n\n        weights_url = api_data[\"weights\"][\"model\"]\n        model_weights_response = get_from_url(weights_url, json_response=False)\n        filename = weights_url.split(\"?\")[0].split(\"/\")[-1]\n        assert filename.endswith(\"tar.gz\")\n        save_bytes_in_cache(\n            content=model_weights_response.content,\n            file=filename,\n            model_id=base_dir,\n        )\n        tar_file_path = get_cache_file_path(filename, base_dir)\n        with tarfile.open(tar_file_path, \"r:gz\") as tar:\n            tar.extractall(path=cache_dir)\n\n        return cache_dir\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\n            \"adapter_config.json\",\n            \"special_tokens_map.json\",\n            \"tokenizer.json\",\n            \"tokenizer.model\",\n            \"adapter_model.safetensors\",\n            \"preprocessor_config.json\",\n            \"tokenizer_config.json\",\n        ]\n</code></pre>"},{"location":"docs/reference/inference/models/transformers/transformers/#inference.models.transformers.transformers.LoRATransformerModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/transformers/transformers.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\n        \"adapter_config.json\",\n        \"special_tokens_map.json\",\n        \"tokenizer.json\",\n        \"tokenizer.model\",\n        \"adapter_model.safetensors\",\n        \"preprocessor_config.json\",\n        \"tokenizer_config.json\",\n    ]\n</code></pre>"},{"location":"docs/reference/inference/models/transformers/transformers/#inference.models.transformers.transformers.TransformerModel","title":"<code>TransformerModel</code>","text":"<p>               Bases: <code>RoboflowInferenceModel</code></p> Source code in <code>inference/models/transformers/transformers.py</code> <pre><code>class TransformerModel(RoboflowInferenceModel):\n    task_type = \"lmm\"\n    transformers_class = AutoModel\n    processor_class = AutoProcessor\n    default_dtype = torch.float16\n    generation_includes_input = False\n    needs_hf_token = False\n    skip_special_tokens = True\n\n    def __init__(\n        self, model_id, *args, dtype=None, huggingface_token=HUGGINGFACE_TOKEN, **kwargs\n    ):\n        super().__init__(model_id, *args, **kwargs)\n        self.huggingface_token = huggingface_token\n        if self.needs_hf_token and self.huggingface_token is None:\n            raise RuntimeError(\n                \"Must set environment variable HUGGINGFACE_TOKEN to load LoRA \"\n                \"(or pass huggingface_token to this __init__)\"\n            )\n        self.dtype = dtype\n        if self.dtype is None:\n            self.dtype = self.default_dtype\n        self.cache_model_artefacts()\n\n        self.cache_dir = os.path.join(MODEL_CACHE_DIR, self.endpoint + \"/\")\n        self.initialize_model()\n\n    def initialize_model(self):\n        self.model = (\n            self.transformers_class.from_pretrained(\n                self.cache_dir,\n                device_map=DEVICE,\n                token=self.huggingface_token,\n            )\n            .eval()\n            .to(self.dtype)\n        )\n\n        self.processor = self.processor_class.from_pretrained(\n            self.cache_dir, token=self.huggingface_token\n        )\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[Image.Image, PreprocessReturnMetadata]:\n        pil_image = Image.fromarray(load_image_rgb(image))\n        image_dims = pil_image.size\n\n        return pil_image, PreprocessReturnMetadata({\"image_dims\": image_dims})\n\n    def postprocess(\n        self,\n        predictions: Tuple[str],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; LMMInferenceResponse:\n        text = predictions[0]\n        image_dims = preprocess_return_metadata[\"image_dims\"]\n        response = LMMInferenceResponse(\n            response=text,\n            image=InferenceResponseImage(width=image_dims[0], height=image_dims[1]),\n        )\n        return [response]\n\n    def predict(self, image_in: Image.Image, prompt=\"\", history=None, **kwargs):\n        model_inputs = self.processor(\n            text=prompt, images=image_in, return_tensors=\"pt\"\n        ).to(self.model.device)\n        input_len = model_inputs[\"input_ids\"].shape[-1]\n\n        with torch.inference_mode():\n            prepared_inputs = self.prepare_generation_params(\n                preprocessed_inputs=model_inputs\n            )\n            generation = self.model.generate(\n                **prepared_inputs,\n                max_new_tokens=1000,\n                do_sample=False,\n                early_stopping=False,\n            )\n            generation = generation[0]\n            if self.generation_includes_input:\n                generation = generation[input_len:]\n            decoded = self.processor.decode(\n                generation, skip_special_tokens=self.skip_special_tokens\n            )\n\n        return (decoded,)\n\n    def prepare_generation_params(\n        self, preprocessed_inputs: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        return preprocessed_inputs\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\n            \"config.json\",\n            \"special_tokens_map.json\",\n            \"generation_config.json\",\n            \"model.safetensors.index.json\",\n            \"tokenizer.json\",\n            re.compile(r\"model-\\d{5}-of-\\d{5}\\.safetensors\"),\n            \"preprocessor_config.json\",\n            \"tokenizer_config.json\",\n        ]\n\n    def download_model_artifacts_from_roboflow_api(self) -&gt; None:\n        api_data = get_roboflow_model_data(\n            api_key=self.api_key,\n            model_id=self.endpoint,\n            endpoint_type=ModelEndpointType.ORT,\n            device_id=self.device_id,\n        )\n        if \"weights\" not in api_data[\"ort\"]:\n            raise ModelArtefactError(\n                f\"`weights` key not available in Roboflow API response while downloading model weights.\"\n            )\n        for weights_url in api_data[\"ort\"][\"weights\"].values():\n            t1 = perf_counter()\n            filename = weights_url.split(\"?\")[0].split(\"/\")[-1]\n            if filename.endswith(\".npz\"):\n                continue\n            model_weights_response = get_from_url(weights_url, json_response=False)\n            save_bytes_in_cache(\n                content=model_weights_response.content,\n                file=filename,\n                model_id=self.endpoint,\n            )\n            if filename.endswith(\"tar.gz\"):\n                subprocess.run(\n                    [\n                        \"tar\",\n                        \"-xzf\",\n                        os.path.join(self.cache_dir, filename),\n                        \"-C\",\n                        self.cache_dir,\n                    ],\n                    check=True,\n                )\n\n            if perf_counter() - t1 &gt; 120:\n                logger.debug(\n                    \"Weights download took longer than 120 seconds, refreshing API request\"\n                )\n                api_data = get_roboflow_model_data(\n                    api_key=self.api_key,\n                    model_id=self.endpoint,\n                    endpoint_type=ModelEndpointType.ORT,\n                    device_id=self.device_id,\n                )\n\n    @property\n    def weights_file(self) -&gt; None:\n        return None\n\n    def download_model_artefacts_from_s3(self) -&gt; None:\n        raise NotImplementedError()\n</code></pre>"},{"location":"docs/reference/inference/models/transformers/transformers/#inference.models.transformers.transformers.TransformerModel.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/transformers/transformers.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\n        \"config.json\",\n        \"special_tokens_map.json\",\n        \"generation_config.json\",\n        \"model.safetensors.index.json\",\n        \"tokenizer.json\",\n        re.compile(r\"model-\\d{5}-of-\\d{5}\\.safetensors\"),\n        \"preprocessor_config.json\",\n        \"tokenizer_config.json\",\n    ]\n</code></pre>"},{"location":"docs/reference/inference/models/trocr/trocr/","title":"trocr","text":""},{"location":"docs/reference/inference/models/vit/vit_classification/","title":"vit_classification","text":""},{"location":"docs/reference/inference/models/vit/vit_classification/#inference.models.vit.vit_classification.VitClassification","title":"<code>VitClassification</code>","text":"<p>               Bases: <code>ClassificationBaseOnnxRoboflowInferenceModel</code></p> <p>VitClassification handles classification inference for Vision Transformer (ViT) models using ONNX.</p> Inherits <p>ClassificationBaseOnnxRoboflowInferenceModel: Base class for ONNX Roboflow Inference. ClassificationMixin: Mixin class providing classification-specific methods.</p> <p>Attributes:</p> Name Type Description <code>multiclass</code> <code>bool</code> <p>A flag that specifies if the model should handle multiclass classification.</p> Source code in <code>inference/models/vit/vit_classification.py</code> <pre><code>class VitClassification(ClassificationBaseOnnxRoboflowInferenceModel):\n    \"\"\"VitClassification handles classification inference\n    for Vision Transformer (ViT) models using ONNX.\n\n    Inherits:\n        ClassificationBaseOnnxRoboflowInferenceModel: Base class for ONNX Roboflow Inference.\n        ClassificationMixin: Mixin class providing classification-specific methods.\n\n    Attributes:\n        multiclass (bool): A flag that specifies if the model should handle multiclass classification.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initializes the VitClassification instance.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.multiclass = self.environment.get(\"MULTICLASS\", False)\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Determines the weights file to be used based on the availability of AWS keys.\n\n        If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set, it returns the path to 'weights.onnx'.\n        Otherwise, it returns the path to 'best.onnx'.\n\n        Returns:\n            str: Path to the weights file.\n        \"\"\"\n        if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY and LAMBDA:\n            return \"weights.onnx\"\n        else:\n            return \"best.onnx\"\n</code></pre>"},{"location":"docs/reference/inference/models/vit/vit_classification/#inference.models.vit.vit_classification.VitClassification.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Determines the weights file to be used based on the availability of AWS keys.</p> <p>If AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set, it returns the path to 'weights.onnx'. Otherwise, it returns the path to 'best.onnx'.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the weights file.</p>"},{"location":"docs/reference/inference/models/vit/vit_classification/#inference.models.vit.vit_classification.VitClassification.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initializes the VitClassification instance.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/vit/vit_classification.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    \"\"\"Initializes the VitClassification instance.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.multiclass = self.environment.get(\"MULTICLASS\", False)\n</code></pre>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/","title":"yolact_instance_segmentation","text":""},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT","title":"<code>YOLACT</code>","text":"<p>               Bases: <code>OnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method)</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>class YOLACT(OnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method)\"\"\"\n\n    task_type = \"instance-segmentation\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file.\n\n        Returns:\n            str: Path to the weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def infer(\n        self,\n        image: Any,\n        class_agnostic_nms: bool = False,\n        confidence: float = 0.5,\n        iou_threshold: float = 0.5,\n        max_candidates: int = 3000,\n        max_detections: int = 300,\n        return_image_dims: bool = False,\n        **kwargs,\n    ) -&gt; List[List[dict]]:\n        \"\"\"\n        Performs instance segmentation inference on a given image, post-processes the results,\n        and returns the segmented instances as dictionaries containing their properties.\n\n        Args:\n            image (Any): The image or list of images to segment.\n                - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_agnostic_nms (bool, optional): Whether to perform class-agnostic non-max suppression. Defaults to False.\n            confidence (float, optional): Confidence threshold for filtering weak detections. Defaults to 0.5.\n            iou_threshold (float, optional): Intersection-over-union threshold for non-max suppression. Defaults to 0.5.\n            max_candidates (int, optional): Maximum number of candidate detections to consider. Defaults to 3000.\n            max_detections (int, optional): Maximum number of detections to return after non-max suppression. Defaults to 300.\n            return_image_dims (bool, optional): Whether to return the dimensions of the input image(s). Defaults to False.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            List[List[dict]]: Each list contains dictionaries of segmented instances for a given image. Each dictionary contains:\n                - x, y: Center coordinates of the instance.\n                - width, height: Width and height of the bounding box around the instance.\n                - class: Name of the detected class.\n                - confidence: Confidence score of the detection.\n                - points: List of points describing the segmented mask's boundary.\n                - class_id: ID corresponding to the detected class.\n            If `return_image_dims` is True, the function returns a tuple where the first element is the list of detections and the\n            second element is the list of image dimensions.\n\n        Notes:\n            - The function supports processing multiple images in a batch.\n            - If an input list of images is provided, the function returns a list of lists,\n              where each inner list corresponds to the detections for a specific image.\n            - The function internally uses an ONNX model for inference.\n        \"\"\"\n        return super().infer(\n            image,\n            class_agnostic_nms=class_agnostic_nms,\n            confidence=confidence,\n            iou_threshold=iou_threshold,\n            max_candidates=max_candidates,\n            max_detections=max_detections,\n            return_image_dims=return_image_dims,\n            **kwargs,\n        )\n\n    def preprocess(\n        self, image: Any, **kwargs\n    ) -&gt; Tuple[np.ndarray, PreprocessReturnMetadata]:\n        if isinstance(image, list):\n            imgs_with_dims = [self.preproc_image(i) for i in image]\n            imgs, img_dims = zip(*imgs_with_dims)\n            img_in = np.concatenate(imgs, axis=0)\n            unwrap = False\n        else:\n            img_in, img_dims = self.preproc_image(image)\n            img_dims = [img_dims]\n            unwrap = True\n\n        # IN BGR order (for some reason)\n        mean = (103.94, 116.78, 123.68)\n        std = (57.38, 57.12, 58.40)\n\n        img_in = img_in.astype(np.float32)\n\n        # Our channels are RGB, so apply mean and std accordingly\n        img_in[:, 0, :, :] = (img_in[:, 0, :, :] - mean[2]) / std[2]\n        img_in[:, 1, :, :] = (img_in[:, 1, :, :] - mean[1]) / std[1]\n        img_in[:, 2, :, :] = (img_in[:, 2, :, :] - mean[0]) / std[0]\n\n        return img_in, PreprocessReturnMetadata(\n            {\n                \"img_dims\": img_dims,\n                \"im_shape\": img_in.shape,\n            }\n        )\n\n    def predict(\n        self, img_in: np.ndarray, **kwargs\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        return self.onnx_session.run(None, {self.input_name: img_in})\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n        preprocess_return_metadata: PreprocessReturnMetadata,\n        **kwargs,\n    ) -&gt; List[InstanceSegmentationInferenceResponse]:\n        loc_data = np.float32(predictions[0])\n        conf_data = np.float32(predictions[1])\n        mask_data = np.float32(predictions[2])\n        prior_data = np.float32(predictions[3])\n        proto_data = np.float32(predictions[4])\n\n        batch_size = loc_data.shape[0]\n        num_priors = prior_data.shape[0]\n\n        boxes = np.zeros((batch_size, num_priors, 4))\n        for batch_idx in range(batch_size):\n            boxes[batch_idx, :, :] = self.decode_predicted_bboxes(\n                loc_data[batch_idx], prior_data\n            )\n\n        conf_preds = np.reshape(\n            conf_data, (batch_size, num_priors, self.num_classes + 1)\n        )\n        class_confs = conf_preds[:, :, 1:]  # remove background class\n        box_confs = np.expand_dims(\n            np.max(class_confs, axis=2), 2\n        )  # get max conf for each box\n\n        predictions = np.concatenate((boxes, box_confs, class_confs, mask_data), axis=2)\n\n        img_in_shape = preprocess_return_metadata[\"im_shape\"]\n        predictions[:, :, 0] *= img_in_shape[2]\n        predictions[:, :, 1] *= img_in_shape[3]\n        predictions[:, :, 2] *= img_in_shape[2]\n        predictions[:, :, 3] *= img_in_shape[3]\n        predictions = w_np_non_max_suppression(\n            predictions,\n            conf_thresh=kwargs[\"confidence\"],\n            iou_thresh=kwargs[\"iou_threshold\"],\n            class_agnostic=kwargs[\"class_agnostic_nms\"],\n            max_detections=kwargs[\"max_detections\"],\n            max_candidate_detections=kwargs[\"max_candidates\"],\n            num_masks=32,\n            box_format=\"xyxy\",\n        )\n        predictions = np.array(predictions)\n        batch_preds = []\n        if predictions.shape != (1, 0):\n            for batch_idx, img_dim in enumerate(preprocess_return_metadata[\"img_dims\"]):\n                boxes = predictions[batch_idx, :, :4]\n                scores = predictions[batch_idx, :, 4]\n                classes = predictions[batch_idx, :, 6]\n                masks = predictions[batch_idx, :, 7:]\n                proto = proto_data[batch_idx]\n                decoded_masks = self.decode_masks(boxes, masks, proto, img_in_shape[2:])\n                polys = masks2poly(decoded_masks)\n                infer_shape = (self.img_size_w, self.img_size_h)\n                boxes = post_process_bboxes(\n                    [boxes], infer_shape, [img_dim], self.preproc, self.resize_method\n                )[0]\n                polys = post_process_polygons(\n                    img_in_shape[2:],\n                    polys,\n                    img_dim,\n                    self.preproc,\n                    resize_method=self.resize_method,\n                )\n                preds = []\n                for box, poly, score, cls in zip(boxes, polys, scores, classes):\n                    confidence = float(score)\n                    class_name = self.class_names[int(cls)]\n                    points = [{\"x\": round(x, 1), \"y\": round(y, 1)} for (x, y) in poly]\n                    pred = {\n                        \"x\": round((box[2] + box[0]) / 2, 1),\n                        \"y\": round((box[3] + box[1]) / 2, 1),\n                        \"width\": int(box[2] - box[0]),\n                        \"height\": int(box[3] - box[1]),\n                        \"class\": class_name,\n                        \"confidence\": round(confidence, 3),\n                        \"points\": points,\n                        \"class_id\": int(cls),\n                    }\n                    preds.append(pred)\n                batch_preds.append(preds)\n        else:\n            batch_preds.append([])\n        img_dims = preprocess_return_metadata[\"img_dims\"]\n        responses = self.make_response(batch_preds, img_dims, **kwargs)\n        if kwargs[\"return_image_dims\"]:\n            return responses, preprocess_return_metadata[\"img_dims\"]\n        else:\n            return responses\n\n    def make_response(\n        self,\n        predictions: List[List[dict]],\n        img_dims: List[Tuple[int, int]],\n        class_filter: List[str] = None,\n        **kwargs,\n    ) -&gt; List[InstanceSegmentationInferenceResponse]:\n        \"\"\"\n        Constructs a list of InstanceSegmentationInferenceResponse objects based on the provided predictions\n        and image dimensions, optionally filtering by class name.\n\n        Args:\n            predictions (List[List[dict]]): A list containing batch predictions, where each inner list contains\n                dictionaries of segmented instances for a given image.\n            img_dims (List[Tuple[int, int]]): List of tuples specifying the dimensions of each image in the format\n                (height, width).\n            class_filter (List[str], optional): A list of class names to filter the predictions by. If not provided,\n                all predictions are included.\n\n        Returns:\n            List[InstanceSegmentationInferenceResponse]: A list of response objects, each containing the filtered\n            predictions and corresponding image dimensions for a given image.\n\n        Examples:\n            &gt;&gt;&gt; predictions = [[{\"class_name\": \"cat\", ...}, {\"class_name\": \"dog\", ...}], ...]\n            &gt;&gt;&gt; img_dims = [(300, 400), ...]\n            &gt;&gt;&gt; responses = make_response(predictions, img_dims, class_filter=[\"cat\"])\n            &gt;&gt;&gt; len(responses[0].predictions)  # Only predictions with \"cat\" class are included\n            1\n        \"\"\"\n        responses = [\n            InstanceSegmentationInferenceResponse(\n                predictions=[\n                    InstanceSegmentationPrediction(**p)\n                    for p in batch_pred\n                    if not class_filter or p[\"class_name\"] in class_filter\n                ],\n                image=InferenceResponseImage(\n                    width=img_dims[i][1], height=img_dims[i][0]\n                ),\n            )\n            for i, batch_pred in enumerate(predictions)\n        ]\n        return responses\n\n    def decode_masks(self, boxes, masks, proto, img_dim):\n        \"\"\"Decodes the masks from the given parameters.\n\n        Args:\n            boxes (np.array): Bounding boxes.\n            masks (np.array): Masks.\n            proto (np.array): Proto data.\n            img_dim (tuple): Image dimensions.\n\n        Returns:\n            np.array: Decoded masks.\n        \"\"\"\n        ret_mask = np.matmul(proto, np.transpose(masks))\n        ret_mask = 1 / (1 + np.exp(-ret_mask))\n        w, h, _ = ret_mask.shape\n        gain = min(h / img_dim[0], w / img_dim[1])  # gain  = old / new\n        pad = (w - img_dim[1] * gain) / 2, (h - img_dim[0] * gain) / 2  # wh padding\n        top, left = int(pad[1]), int(pad[0])  # y, x\n        bottom, right = int(h - pad[1]), int(w - pad[0])\n        ret_mask = np.transpose(ret_mask, (2, 0, 1))\n        ret_mask = ret_mask[:, top:bottom, left:right]\n        if len(ret_mask.shape) == 2:\n            ret_mask = np.expand_dims(ret_mask, axis=0)\n        ret_mask = ret_mask.transpose((1, 2, 0))\n        ret_mask = cv2.resize(ret_mask, img_dim, interpolation=cv2.INTER_LINEAR)\n        if len(ret_mask.shape) == 2:\n            ret_mask = np.expand_dims(ret_mask, axis=2)\n        ret_mask = ret_mask.transpose((2, 0, 1))\n        ret_mask = crop_mask(ret_mask, boxes)  # CHW\n        ret_mask[ret_mask &lt; 0.5] = 0\n\n        return ret_mask\n\n    def decode_predicted_bboxes(self, loc, priors):\n        \"\"\"Decode predicted bounding box coordinates using the scheme employed by Yolov2.\n\n        Args:\n            loc (np.array): The predicted bounding boxes of size [num_priors, 4].\n            priors (np.array): The prior box coordinates with size [num_priors, 4].\n\n        Returns:\n            np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].\n        \"\"\"\n\n        variances = [0.1, 0.2]\n\n        boxes = np.concatenate(\n            [\n                priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n                priors[:, 2:] * np.exp(loc[:, 2:] * variances[1]),\n            ],\n            1,\n        )\n        boxes[:, :2] -= boxes[:, 2:] / 2\n        boxes[:, 2:] += boxes[:, :2]\n\n        return boxes\n</code></pre>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the weights file.</p>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.decode_masks","title":"<code>decode_masks(boxes, masks, proto, img_dim)</code>","text":"<p>Decodes the masks from the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>array</code> <p>Bounding boxes.</p> required <code>masks</code> <code>array</code> <p>Masks.</p> required <code>proto</code> <code>array</code> <p>Proto data.</p> required <code>img_dim</code> <code>tuple</code> <p>Image dimensions.</p> required <p>Returns:</p> Type Description <p>np.array: Decoded masks.</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def decode_masks(self, boxes, masks, proto, img_dim):\n    \"\"\"Decodes the masks from the given parameters.\n\n    Args:\n        boxes (np.array): Bounding boxes.\n        masks (np.array): Masks.\n        proto (np.array): Proto data.\n        img_dim (tuple): Image dimensions.\n\n    Returns:\n        np.array: Decoded masks.\n    \"\"\"\n    ret_mask = np.matmul(proto, np.transpose(masks))\n    ret_mask = 1 / (1 + np.exp(-ret_mask))\n    w, h, _ = ret_mask.shape\n    gain = min(h / img_dim[0], w / img_dim[1])  # gain  = old / new\n    pad = (w - img_dim[1] * gain) / 2, (h - img_dim[0] * gain) / 2  # wh padding\n    top, left = int(pad[1]), int(pad[0])  # y, x\n    bottom, right = int(h - pad[1]), int(w - pad[0])\n    ret_mask = np.transpose(ret_mask, (2, 0, 1))\n    ret_mask = ret_mask[:, top:bottom, left:right]\n    if len(ret_mask.shape) == 2:\n        ret_mask = np.expand_dims(ret_mask, axis=0)\n    ret_mask = ret_mask.transpose((1, 2, 0))\n    ret_mask = cv2.resize(ret_mask, img_dim, interpolation=cv2.INTER_LINEAR)\n    if len(ret_mask.shape) == 2:\n        ret_mask = np.expand_dims(ret_mask, axis=2)\n    ret_mask = ret_mask.transpose((2, 0, 1))\n    ret_mask = crop_mask(ret_mask, boxes)  # CHW\n    ret_mask[ret_mask &lt; 0.5] = 0\n\n    return ret_mask\n</code></pre>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.decode_predicted_bboxes","title":"<code>decode_predicted_bboxes(loc, priors)</code>","text":"<p>Decode predicted bounding box coordinates using the scheme employed by Yolov2.</p> <p>Parameters:</p> Name Type Description Default <code>loc</code> <code>array</code> <p>The predicted bounding boxes of size [num_priors, 4].</p> required <code>priors</code> <code>array</code> <p>The prior box coordinates with size [num_priors, 4].</p> required <p>Returns:</p> Type Description <p>np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].</p> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def decode_predicted_bboxes(self, loc, priors):\n    \"\"\"Decode predicted bounding box coordinates using the scheme employed by Yolov2.\n\n    Args:\n        loc (np.array): The predicted bounding boxes of size [num_priors, 4].\n        priors (np.array): The prior box coordinates with size [num_priors, 4].\n\n    Returns:\n        np.array: A tensor of decoded relative coordinates in point form with size [num_priors, 4].\n    \"\"\"\n\n    variances = [0.1, 0.2]\n\n    boxes = np.concatenate(\n        [\n            priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n            priors[:, 2:] * np.exp(loc[:, 2:] * variances[1]),\n        ],\n        1,\n    )\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n\n    return boxes\n</code></pre>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.infer","title":"<code>infer(image, class_agnostic_nms=False, confidence=0.5, iou_threshold=0.5, max_candidates=3000, max_detections=300, return_image_dims=False, **kwargs)</code>","text":"<p>Performs instance segmentation inference on a given image, post-processes the results, and returns the segmented instances as dictionaries containing their properties.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Any</code> <p>The image or list of images to segment. - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.</p> required <code>class_agnostic_nms</code> <code>bool</code> <p>Whether to perform class-agnostic non-max suppression. Defaults to False.</p> <code>False</code> <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering weak detections. Defaults to 0.5.</p> <code>0.5</code> <code>iou_threshold</code> <code>float</code> <p>Intersection-over-union threshold for non-max suppression. Defaults to 0.5.</p> <code>0.5</code> <code>max_candidates</code> <code>int</code> <p>Maximum number of candidate detections to consider. Defaults to 3000.</p> <code>3000</code> <code>max_detections</code> <code>int</code> <p>Maximum number of detections to return after non-max suppression. Defaults to 300.</p> <code>300</code> <code>return_image_dims</code> <code>bool</code> <p>Whether to return the dimensions of the input image(s). Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[dict]]</code> <p>List[List[dict]]: Each list contains dictionaries of segmented instances for a given image. Each dictionary contains: - x, y: Center coordinates of the instance. - width, height: Width and height of the bounding box around the instance. - class: Name of the detected class. - confidence: Confidence score of the detection. - points: List of points describing the segmented mask's boundary. - class_id: ID corresponding to the detected class.</p> <code>List[List[dict]]</code> <p>If <code>return_image_dims</code> is True, the function returns a tuple where the first element is the list of detections and the</p> <code>List[List[dict]]</code> <p>second element is the list of image dimensions.</p> Notes <ul> <li>The function supports processing multiple images in a batch.</li> <li>If an input list of images is provided, the function returns a list of lists,   where each inner list corresponds to the detections for a specific image.</li> <li>The function internally uses an ONNX model for inference.</li> </ul> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def infer(\n    self,\n    image: Any,\n    class_agnostic_nms: bool = False,\n    confidence: float = 0.5,\n    iou_threshold: float = 0.5,\n    max_candidates: int = 3000,\n    max_detections: int = 300,\n    return_image_dims: bool = False,\n    **kwargs,\n) -&gt; List[List[dict]]:\n    \"\"\"\n    Performs instance segmentation inference on a given image, post-processes the results,\n    and returns the segmented instances as dictionaries containing their properties.\n\n    Args:\n        image (Any): The image or list of images to segment.\n            - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_agnostic_nms (bool, optional): Whether to perform class-agnostic non-max suppression. Defaults to False.\n        confidence (float, optional): Confidence threshold for filtering weak detections. Defaults to 0.5.\n        iou_threshold (float, optional): Intersection-over-union threshold for non-max suppression. Defaults to 0.5.\n        max_candidates (int, optional): Maximum number of candidate detections to consider. Defaults to 3000.\n        max_detections (int, optional): Maximum number of detections to return after non-max suppression. Defaults to 300.\n        return_image_dims (bool, optional): Whether to return the dimensions of the input image(s). Defaults to False.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        List[List[dict]]: Each list contains dictionaries of segmented instances for a given image. Each dictionary contains:\n            - x, y: Center coordinates of the instance.\n            - width, height: Width and height of the bounding box around the instance.\n            - class: Name of the detected class.\n            - confidence: Confidence score of the detection.\n            - points: List of points describing the segmented mask's boundary.\n            - class_id: ID corresponding to the detected class.\n        If `return_image_dims` is True, the function returns a tuple where the first element is the list of detections and the\n        second element is the list of image dimensions.\n\n    Notes:\n        - The function supports processing multiple images in a batch.\n        - If an input list of images is provided, the function returns a list of lists,\n          where each inner list corresponds to the detections for a specific image.\n        - The function internally uses an ONNX model for inference.\n    \"\"\"\n    return super().infer(\n        image,\n        class_agnostic_nms=class_agnostic_nms,\n        confidence=confidence,\n        iou_threshold=iou_threshold,\n        max_candidates=max_candidates,\n        max_detections=max_detections,\n        return_image_dims=return_image_dims,\n        **kwargs,\n    )\n</code></pre>"},{"location":"docs/reference/inference/models/yolact/yolact_instance_segmentation/#inference.models.yolact.yolact_instance_segmentation.YOLACT.make_response","title":"<code>make_response(predictions, img_dims, class_filter=None, **kwargs)</code>","text":"<p>Constructs a list of InstanceSegmentationInferenceResponse objects based on the provided predictions and image dimensions, optionally filtering by class name.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[List[dict]]</code> <p>A list containing batch predictions, where each inner list contains dictionaries of segmented instances for a given image.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>List of tuples specifying the dimensions of each image in the format (height, width).</p> required <code>class_filter</code> <code>List[str]</code> <p>A list of class names to filter the predictions by. If not provided, all predictions are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[InstanceSegmentationInferenceResponse]</code> <p>List[InstanceSegmentationInferenceResponse]: A list of response objects, each containing the filtered</p> <code>List[InstanceSegmentationInferenceResponse]</code> <p>predictions and corresponding image dimensions for a given image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; predictions = [[{\"class_name\": \"cat\", ...}, {\"class_name\": \"dog\", ...}], ...]\n&gt;&gt;&gt; img_dims = [(300, 400), ...]\n&gt;&gt;&gt; responses = make_response(predictions, img_dims, class_filter=[\"cat\"])\n&gt;&gt;&gt; len(responses[0].predictions)  # Only predictions with \"cat\" class are included\n1\n</code></pre> Source code in <code>inference/models/yolact/yolact_instance_segmentation.py</code> <pre><code>def make_response(\n    self,\n    predictions: List[List[dict]],\n    img_dims: List[Tuple[int, int]],\n    class_filter: List[str] = None,\n    **kwargs,\n) -&gt; List[InstanceSegmentationInferenceResponse]:\n    \"\"\"\n    Constructs a list of InstanceSegmentationInferenceResponse objects based on the provided predictions\n    and image dimensions, optionally filtering by class name.\n\n    Args:\n        predictions (List[List[dict]]): A list containing batch predictions, where each inner list contains\n            dictionaries of segmented instances for a given image.\n        img_dims (List[Tuple[int, int]]): List of tuples specifying the dimensions of each image in the format\n            (height, width).\n        class_filter (List[str], optional): A list of class names to filter the predictions by. If not provided,\n            all predictions are included.\n\n    Returns:\n        List[InstanceSegmentationInferenceResponse]: A list of response objects, each containing the filtered\n        predictions and corresponding image dimensions for a given image.\n\n    Examples:\n        &gt;&gt;&gt; predictions = [[{\"class_name\": \"cat\", ...}, {\"class_name\": \"dog\", ...}], ...]\n        &gt;&gt;&gt; img_dims = [(300, 400), ...]\n        &gt;&gt;&gt; responses = make_response(predictions, img_dims, class_filter=[\"cat\"])\n        &gt;&gt;&gt; len(responses[0].predictions)  # Only predictions with \"cat\" class are included\n        1\n    \"\"\"\n    responses = [\n        InstanceSegmentationInferenceResponse(\n            predictions=[\n                InstanceSegmentationPrediction(**p)\n                for p in batch_pred\n                if not class_filter or p[\"class_name\"] in class_filter\n            ],\n            image=InferenceResponseImage(\n                width=img_dims[i][1], height=img_dims[i][0]\n            ),\n        )\n        for i, batch_pred in enumerate(predictions)\n    ]\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/","title":"yolo_world","text":""},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld","title":"<code>YOLOWorld</code>","text":"<p>               Bases: <code>RoboflowCoreModel</code></p> <p>YOLO-World class for zero-shot object detection.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>The YOLO-World model.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>class YOLOWorld(RoboflowCoreModel):\n    \"\"\"YOLO-World class for zero-shot object detection.\n\n    Attributes:\n        model: The YOLO-World model.\n    \"\"\"\n\n    task_type = \"object-detection\"\n\n    def __init__(self, *args, model_id=\"yolo_world/l\", **kwargs):\n        \"\"\"Initializes the YOLO-World model.\n\n        Args:\n            *args: Variable length argument list.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n\n        super().__init__(*args, model_id=model_id, **kwargs)\n\n        self.model = YOLO(self.cache_file(\"yolo-world.pt\"))\n        logger.debug(\"Loading CLIP ViT-B/32\")\n        clip_model = Clip(model_id=\"clip/ViT-B-32\")\n        logger.debug(\"CLIP loaded\")\n        self.clip_model = clip_model\n        self.class_names = None\n\n    def preproc_image(self, image: Any):\n        \"\"\"Preprocesses an image.\n\n        Args:\n            image (InferenceRequestImage): The image to preprocess.\n\n        Returns:\n            np.array: The preprocessed image.\n        \"\"\"\n        np_image = load_image_rgb(image)\n        return np_image[:, :, ::-1]\n\n    def infer_from_request(\n        self,\n        request: YOLOWorldInferenceRequest,\n    ) -&gt; ObjectDetectionInferenceResponse:\n        \"\"\"\n        Perform inference based on the details provided in the request, and return the associated responses.\n        \"\"\"\n        result = self.infer(**request.dict())\n        return result\n\n    def infer(\n        self,\n        image: Any = None,\n        text: list = None,\n        confidence: float = DEFAULT_CONFIDENCE,\n        max_detections: Optional[int] = DEFAUlT_MAX_DETECTIONS,\n        iou_threshold: float = DEFAULT_IOU_THRESH,\n        max_candidates: int = DEFAULT_MAX_CANDIDATES,\n        class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n        **kwargs,\n    ):\n        \"\"\"\n        Run inference on a provided image.\n\n        Args:\n            image - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n            class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n        Returns:\n            GroundingDINOInferenceRequest: The inference response.\n        \"\"\"\n        logger.debug(\"YOLOWorld infer() - image preprocessing.\")\n        t1 = perf_counter()\n        image = self.preproc_image(image)\n        logger.debug(\"YOLOWorld infer() - image ready.\")\n        img_dims = image.shape\n\n        if text is not None and text != self.class_names:\n            logger.debug(\"YOLOWorld infer() - classes embeddings are calculated.\")\n            self.set_classes(text)\n            logger.debug(\"YOLOWorld infer() - classes embeddings are ready.\")\n        if self.class_names is None:\n            raise ValueError(\n                \"Class names not set and not provided in the request. Must set class names before inference or provide them via the argument `text`.\"\n            )\n        logger.debug(\"YOLOWorld infer() - prediction starts.\")\n        results = self.model.predict(\n            image,\n            conf=confidence,\n            verbose=False,\n        )[0]\n        logger.debug(\"YOLOWorld infer() - predictions ready.\")\n        t2 = perf_counter() - t1\n\n        logger.debug(\"YOLOWorld infer() - post-processing starting\")\n        if len(results) &gt; 0:\n            bbox_array = np.array([box.xywh.tolist()[0] for box in results.boxes])\n            conf_array = np.array([[float(box.conf)] for box in results.boxes])\n            cls_array = np.array(\n                [\n                    self.get_cls_conf_array(\n                        max_class_id=int(box.cls),\n                        max_class_confidence=float(box.conf),\n                    )\n                    for box in results.boxes\n                ]\n            )\n\n            pred_array = np.concatenate([bbox_array, conf_array, cls_array], axis=1)\n            pred_array = np.expand_dims(pred_array, axis=0)\n            pred_array = w_np_non_max_suppression(\n                pred_array,\n                conf_thresh=confidence,\n                iou_thresh=iou_threshold,\n                class_agnostic=class_agnostic_nms,\n                max_detections=max_detections,\n                max_candidate_detections=max_candidates,\n                box_format=\"xywh\",\n            )[0]\n        else:\n            pred_array = []\n        predictions = []\n        logger.debug(\"YOLOWorld infer() - post-processing done\")\n        for i, pred in enumerate(pred_array):\n            predictions.append(\n                ObjectDetectionPrediction(\n                    **{\n                        \"x\": (pred[0] + pred[2]) / 2,\n                        \"y\": (pred[1] + pred[3]) / 2,\n                        \"width\": pred[2] - pred[0],\n                        \"height\": pred[3] - pred[1],\n                        \"confidence\": pred[4],\n                        \"class\": self.class_names[int(pred[6])],\n                        \"class_id\": int(pred[6]),\n                    }\n                )\n            )\n\n        responses = ObjectDetectionInferenceResponse(\n            predictions=predictions,\n            image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n            time=t2,\n        )\n        return responses\n\n    def set_classes(self, text: list):\n        \"\"\"Set the class names for the model.\n\n        Args:\n            text (list): The class names.\n        \"\"\"\n        class_names_to_calculate_embeddings = []\n        classes_embeddings = {}\n        for class_name in text:\n            class_name_hash = f\"clip-embedding:{get_text_hash(text=class_name)}\"\n            embedding_for_class = cache.get_numpy(class_name_hash)\n            if embedding_for_class is not None:\n                logger.debug(f\"Cache hit for class: {class_name}\")\n                classes_embeddings[class_name] = embedding_for_class\n            else:\n                logger.debug(f\"Cache miss for class: {class_name}\")\n                class_names_to_calculate_embeddings.append(class_name)\n        if len(class_names_to_calculate_embeddings) &gt; 0:\n            logger.debug(\n                f\"Calculating CLIP embeddings for {len(class_names_to_calculate_embeddings)} class names\"\n            )\n            cache_miss_embeddings = self.clip_model.embed_text(\n                text=class_names_to_calculate_embeddings\n            )\n        else:\n            cache_miss_embeddings = []\n        for missing_class_name, calculated_embedding in zip(\n            class_names_to_calculate_embeddings, cache_miss_embeddings\n        ):\n            classes_embeddings[missing_class_name] = calculated_embedding\n            missing_class_name_hash = (\n                f\"clip-embedding:{get_text_hash(text=missing_class_name)}\"\n            )\n            cache.set_numpy(  # caching vectors of shape (512,)\n                missing_class_name_hash,\n                calculated_embedding,\n                expire=EMBEDDINGS_EXPIRE_TIMEOUT,\n            )\n        embeddings_in_order = np.stack(\n            [classes_embeddings[class_name] for class_name in text], axis=0\n        )\n        txt_feats = torch.from_numpy(embeddings_in_order)\n        txt_feats = txt_feats / txt_feats.norm(p=2, dim=-1, keepdim=True)\n        self.model.model.txt_feats = txt_feats.reshape(\n            -1, len(text), txt_feats.shape[-1]\n        ).detach()\n        self.model.model.model[-1].nc = len(text)\n        self.class_names = text\n\n    def get_infer_bucket_file_list(self) -&gt; list:\n        \"\"\"Get the list of required files for inference.\n\n        Returns:\n            list: A list of required files for inference, e.g., [\"model.pt\"].\n        \"\"\"\n        return [\"yolo-world.pt\"]\n\n    def get_cls_conf_array(\n        self, max_class_id: int, max_class_confidence: float\n    ) -&gt; List[float]:\n        arr = [0.0] * len(self.class_names)\n        arr[max_class_id] = max_class_confidence\n        return arr\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.__init__","title":"<code>__init__(*args, model_id='yolo_world/l', **kwargs)</code>","text":"<p>Initializes the YOLO-World model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def __init__(self, *args, model_id=\"yolo_world/l\", **kwargs):\n    \"\"\"Initializes the YOLO-World model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n\n    super().__init__(*args, model_id=model_id, **kwargs)\n\n    self.model = YOLO(self.cache_file(\"yolo-world.pt\"))\n    logger.debug(\"Loading CLIP ViT-B/32\")\n    clip_model = Clip(model_id=\"clip/ViT-B-32\")\n    logger.debug(\"CLIP loaded\")\n    self.clip_model = clip_model\n    self.class_names = None\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.get_infer_bucket_file_list","title":"<code>get_infer_bucket_file_list()</code>","text":"<p>Get the list of required files for inference.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of required files for inference, e.g., [\"model.pt\"].</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def get_infer_bucket_file_list(self) -&gt; list:\n    \"\"\"Get the list of required files for inference.\n\n    Returns:\n        list: A list of required files for inference, e.g., [\"model.pt\"].\n    \"\"\"\n    return [\"yolo-world.pt\"]\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.infer","title":"<code>infer(image=None, text=None, confidence=DEFAULT_CONFIDENCE, max_detections=DEFAUlT_MAX_DETECTIONS, iou_threshold=DEFAULT_IOU_THRESH, max_candidates=DEFAULT_MAX_CANDIDATES, class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS, **kwargs)</code>","text":"<p>Run inference on a provided image.</p> <p>Parameters:</p> Name Type Description Default <code>class_filter</code> <code>Optional[List[str]]</code> <p>A list of class names to filter, if provided.</p> required <p>Returns:</p> Name Type Description <code>GroundingDINOInferenceRequest</code> <p>The inference response.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def infer(\n    self,\n    image: Any = None,\n    text: list = None,\n    confidence: float = DEFAULT_CONFIDENCE,\n    max_detections: Optional[int] = DEFAUlT_MAX_DETECTIONS,\n    iou_threshold: float = DEFAULT_IOU_THRESH,\n    max_candidates: int = DEFAULT_MAX_CANDIDATES,\n    class_agnostic_nms=DEFAULT_CLASS_AGNOSTIC_NMS,\n    **kwargs,\n):\n    \"\"\"\n    Run inference on a provided image.\n\n    Args:\n        image - can be a BGR numpy array, filepath, InferenceRequestImage, PIL Image, byte-string, etc.\n        class_filter (Optional[List[str]]): A list of class names to filter, if provided.\n\n    Returns:\n        GroundingDINOInferenceRequest: The inference response.\n    \"\"\"\n    logger.debug(\"YOLOWorld infer() - image preprocessing.\")\n    t1 = perf_counter()\n    image = self.preproc_image(image)\n    logger.debug(\"YOLOWorld infer() - image ready.\")\n    img_dims = image.shape\n\n    if text is not None and text != self.class_names:\n        logger.debug(\"YOLOWorld infer() - classes embeddings are calculated.\")\n        self.set_classes(text)\n        logger.debug(\"YOLOWorld infer() - classes embeddings are ready.\")\n    if self.class_names is None:\n        raise ValueError(\n            \"Class names not set and not provided in the request. Must set class names before inference or provide them via the argument `text`.\"\n        )\n    logger.debug(\"YOLOWorld infer() - prediction starts.\")\n    results = self.model.predict(\n        image,\n        conf=confidence,\n        verbose=False,\n    )[0]\n    logger.debug(\"YOLOWorld infer() - predictions ready.\")\n    t2 = perf_counter() - t1\n\n    logger.debug(\"YOLOWorld infer() - post-processing starting\")\n    if len(results) &gt; 0:\n        bbox_array = np.array([box.xywh.tolist()[0] for box in results.boxes])\n        conf_array = np.array([[float(box.conf)] for box in results.boxes])\n        cls_array = np.array(\n            [\n                self.get_cls_conf_array(\n                    max_class_id=int(box.cls),\n                    max_class_confidence=float(box.conf),\n                )\n                for box in results.boxes\n            ]\n        )\n\n        pred_array = np.concatenate([bbox_array, conf_array, cls_array], axis=1)\n        pred_array = np.expand_dims(pred_array, axis=0)\n        pred_array = w_np_non_max_suppression(\n            pred_array,\n            conf_thresh=confidence,\n            iou_thresh=iou_threshold,\n            class_agnostic=class_agnostic_nms,\n            max_detections=max_detections,\n            max_candidate_detections=max_candidates,\n            box_format=\"xywh\",\n        )[0]\n    else:\n        pred_array = []\n    predictions = []\n    logger.debug(\"YOLOWorld infer() - post-processing done\")\n    for i, pred in enumerate(pred_array):\n        predictions.append(\n            ObjectDetectionPrediction(\n                **{\n                    \"x\": (pred[0] + pred[2]) / 2,\n                    \"y\": (pred[1] + pred[3]) / 2,\n                    \"width\": pred[2] - pred[0],\n                    \"height\": pred[3] - pred[1],\n                    \"confidence\": pred[4],\n                    \"class\": self.class_names[int(pred[6])],\n                    \"class_id\": int(pred[6]),\n                }\n            )\n        )\n\n    responses = ObjectDetectionInferenceResponse(\n        predictions=predictions,\n        image=InferenceResponseImage(width=img_dims[1], height=img_dims[0]),\n        time=t2,\n    )\n    return responses\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.infer_from_request","title":"<code>infer_from_request(request)</code>","text":"<p>Perform inference based on the details provided in the request, and return the associated responses.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def infer_from_request(\n    self,\n    request: YOLOWorldInferenceRequest,\n) -&gt; ObjectDetectionInferenceResponse:\n    \"\"\"\n    Perform inference based on the details provided in the request, and return the associated responses.\n    \"\"\"\n    result = self.infer(**request.dict())\n    return result\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.preproc_image","title":"<code>preproc_image(image)</code>","text":"<p>Preprocesses an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>InferenceRequestImage</code> <p>The image to preprocess.</p> required <p>Returns:</p> Type Description <p>np.array: The preprocessed image.</p> Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def preproc_image(self, image: Any):\n    \"\"\"Preprocesses an image.\n\n    Args:\n        image (InferenceRequestImage): The image to preprocess.\n\n    Returns:\n        np.array: The preprocessed image.\n    \"\"\"\n    np_image = load_image_rgb(image)\n    return np_image[:, :, ::-1]\n</code></pre>"},{"location":"docs/reference/inference/models/yolo_world/yolo_world/#inference.models.yolo_world.yolo_world.YOLOWorld.set_classes","title":"<code>set_classes(text)</code>","text":"<p>Set the class names for the model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>list</code> <p>The class names.</p> required Source code in <code>inference/models/yolo_world/yolo_world.py</code> <pre><code>def set_classes(self, text: list):\n    \"\"\"Set the class names for the model.\n\n    Args:\n        text (list): The class names.\n    \"\"\"\n    class_names_to_calculate_embeddings = []\n    classes_embeddings = {}\n    for class_name in text:\n        class_name_hash = f\"clip-embedding:{get_text_hash(text=class_name)}\"\n        embedding_for_class = cache.get_numpy(class_name_hash)\n        if embedding_for_class is not None:\n            logger.debug(f\"Cache hit for class: {class_name}\")\n            classes_embeddings[class_name] = embedding_for_class\n        else:\n            logger.debug(f\"Cache miss for class: {class_name}\")\n            class_names_to_calculate_embeddings.append(class_name)\n    if len(class_names_to_calculate_embeddings) &gt; 0:\n        logger.debug(\n            f\"Calculating CLIP embeddings for {len(class_names_to_calculate_embeddings)} class names\"\n        )\n        cache_miss_embeddings = self.clip_model.embed_text(\n            text=class_names_to_calculate_embeddings\n        )\n    else:\n        cache_miss_embeddings = []\n    for missing_class_name, calculated_embedding in zip(\n        class_names_to_calculate_embeddings, cache_miss_embeddings\n    ):\n        classes_embeddings[missing_class_name] = calculated_embedding\n        missing_class_name_hash = (\n            f\"clip-embedding:{get_text_hash(text=missing_class_name)}\"\n        )\n        cache.set_numpy(  # caching vectors of shape (512,)\n            missing_class_name_hash,\n            calculated_embedding,\n            expire=EMBEDDINGS_EXPIRE_TIMEOUT,\n        )\n    embeddings_in_order = np.stack(\n        [classes_embeddings[class_name] for class_name in text], axis=0\n    )\n    txt_feats = torch.from_numpy(embeddings_in_order)\n    txt_feats = txt_feats / txt_feats.norm(p=2, dim=-1, keepdim=True)\n    self.model.model.txt_feats = txt_feats.reshape(\n        -1, len(text), txt_feats.shape[-1]\n    ).detach()\n    self.model.model.model[-1].nc = len(text)\n    self.class_names = text\n</code></pre>"},{"location":"docs/reference/inference/models/yolonas/yolonas_object_detection/","title":"yolonas_object_detection","text":""},{"location":"docs/reference/inference/models/yolonas/yolonas_object_detection/#inference.models.yolonas.yolonas_object_detection.YOLONASObjectDetection","title":"<code>YOLONASObjectDetection</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> Source code in <code>inference/models/yolonas/yolonas_object_detection.py</code> <pre><code>class YOLONASObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    box_format = \"xyxy\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLO-NAS model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        boxes = predictions[0]\n        class_confs = predictions[1]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n        return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolonas/yolonas_object_detection/#inference.models.yolonas.yolonas_object_detection.YOLONASObjectDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLO-NAS model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolonas/yolonas_object_detection/#inference.models.yolonas.yolonas_object_detection.YOLONASObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolonas/yolonas_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    boxes = predictions[0]\n    class_confs = predictions[1]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n    return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov10/yolov10_object_detection/","title":"yolov10_object_detection","text":""},{"location":"docs/reference/inference/models/yolov10/yolov10_object_detection/#inference.models.yolov10.yolov10_object_detection.YOLOv10ObjectDetection","title":"<code>YOLOv10ObjectDetection</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv10 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs object detection on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov10/yolov10_object_detection.py</code> <pre><code>class YOLOv10ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv10 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs object detection on the given image using the ONNX session.\n    \"\"\"\n\n    box_format = \"xyxy\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv10 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n\n        return (predictions,)\n\n    def postprocess(\n        self,\n        predictions: Tuple[np.ndarray, ...],\n        preproc_return_metadata: PreprocessReturnMetadata,\n        confidence: float = DEFAULT_CONFIDENCE,\n        max_detections: int = DEFAUlT_MAX_DETECTIONS,\n        **kwargs,\n    ) -&gt; List[ObjectDetectionInferenceResponse]:\n        \"\"\"Postprocesses the object detection predictions.\n\n        Args:\n            predictions (np.ndarray): Raw predictions from the model.\n            img_dims (List[Tuple[int, int]]): Dimensions of the images.\n            confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n            max_detections (int): Maximum number of final detections. Default is 300.\n\n        Returns:\n            List[ObjectDetectionInferenceResponse]: The post-processed predictions.\n        \"\"\"\n        predictions = predictions[0]\n        predictions = np.append(predictions, predictions[..., 5:], axis=-1)\n        predictions[..., 5] = predictions[..., 4]\n\n        mask = predictions[..., 4] &gt; confidence\n        predictions = [\n            p[mask[idx]][:max_detections] for idx, p in enumerate(predictions)\n        ]\n\n        infer_shape = (self.img_size_h, self.img_size_w)\n        img_dims = preproc_return_metadata[\"img_dims\"]\n        predictions = post_process_bboxes(\n            predictions,\n            infer_shape,\n            img_dims,\n            self.preproc,\n            resize_method=self.resize_method,\n            disable_preproc_static_crop=preproc_return_metadata[\n                \"disable_preproc_static_crop\"\n            ],\n        )\n        return self.make_response(predictions, img_dims, **kwargs)\n\n    def validate_model_classes(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"docs/reference/inference/models/yolov10/yolov10_object_detection/#inference.models.yolov10.yolov10_object_detection.YOLOv10ObjectDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv10 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov10/yolov10_object_detection/#inference.models.yolov10.yolov10_object_detection.YOLOv10ObjectDetection.postprocess","title":"<code>postprocess(predictions, preproc_return_metadata, confidence=DEFAULT_CONFIDENCE, max_detections=DEFAUlT_MAX_DETECTIONS, **kwargs)</code>","text":"<p>Postprocesses the object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Raw predictions from the model.</p> required <code>img_dims</code> <code>List[Tuple[int, int]]</code> <p>Dimensions of the images.</p> required <code>confidence</code> <code>float</code> <p>Confidence threshold for filtering detections. Default is 0.5.</p> <code>DEFAULT_CONFIDENCE</code> <code>max_detections</code> <code>int</code> <p>Maximum number of final detections. Default is 300.</p> <code>DEFAUlT_MAX_DETECTIONS</code> <p>Returns:</p> Type Description <code>List[ObjectDetectionInferenceResponse]</code> <p>List[ObjectDetectionInferenceResponse]: The post-processed predictions.</p> Source code in <code>inference/models/yolov10/yolov10_object_detection.py</code> <pre><code>def postprocess(\n    self,\n    predictions: Tuple[np.ndarray, ...],\n    preproc_return_metadata: PreprocessReturnMetadata,\n    confidence: float = DEFAULT_CONFIDENCE,\n    max_detections: int = DEFAUlT_MAX_DETECTIONS,\n    **kwargs,\n) -&gt; List[ObjectDetectionInferenceResponse]:\n    \"\"\"Postprocesses the object detection predictions.\n\n    Args:\n        predictions (np.ndarray): Raw predictions from the model.\n        img_dims (List[Tuple[int, int]]): Dimensions of the images.\n        confidence (float): Confidence threshold for filtering detections. Default is 0.5.\n        max_detections (int): Maximum number of final detections. Default is 300.\n\n    Returns:\n        List[ObjectDetectionInferenceResponse]: The post-processed predictions.\n    \"\"\"\n    predictions = predictions[0]\n    predictions = np.append(predictions, predictions[..., 5:], axis=-1)\n    predictions[..., 5] = predictions[..., 4]\n\n    mask = predictions[..., 4] &gt; confidence\n    predictions = [\n        p[mask[idx]][:max_detections] for idx, p in enumerate(predictions)\n    ]\n\n    infer_shape = (self.img_size_h, self.img_size_w)\n    img_dims = preproc_return_metadata[\"img_dims\"]\n    predictions = post_process_bboxes(\n        predictions,\n        infer_shape,\n        img_dims,\n        self.preproc,\n        resize_method=self.resize_method,\n        disable_preproc_static_crop=preproc_return_metadata[\n            \"disable_preproc_static_crop\"\n        ],\n    )\n    return self.make_response(predictions, img_dims, **kwargs)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov10/yolov10_object_detection/#inference.models.yolov10.yolov10_object_detection.YOLOv10ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolov10/yolov10_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n\n    return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov11/yolov11_instance_segmentation/","title":"yolov11_instance_segmentation","text":""},{"location":"docs/reference/inference/models/yolov11/yolov11_keypoints_detection/","title":"yolov11_keypoints_detection","text":""},{"location":"docs/reference/inference/models/yolov11/yolov11_object_detection/","title":"yolov11_object_detection","text":""},{"location":"docs/reference/inference/models/yolov5/yolov5_instance_segmentation/","title":"yolov5_instance_segmentation","text":""},{"location":"docs/reference/inference/models/yolov5/yolov5_instance_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentation","title":"<code>YOLOv5InstanceSegmentation</code>","text":"<p>               Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv5 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv5 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov5/yolov5_instance_segmentation.py</code> <pre><code>class YOLOv5InstanceSegmentation(InstanceSegmentationBaseOnnxRoboflowInferenceModel):\n    \"\"\"YOLOv5 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv5 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv5 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"yolov5s_weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        return predictions[0], predictions[1]\n</code></pre>"},{"location":"docs/reference/inference/models/yolov5/yolov5_instance_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentation.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv5 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov5/yolov5_instance_segmentation/#inference.models.yolov5.yolov5_instance_segmentation.YOLOv5InstanceSegmentation.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.</p> Source code in <code>inference/models/yolov5/yolov5_instance_segmentation.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    return predictions[0], predictions[1]\n</code></pre>"},{"location":"docs/reference/inference/models/yolov5/yolov5_object_detection/","title":"yolov5_object_detection","text":""},{"location":"docs/reference/inference/models/yolov5/yolov5_object_detection/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetection","title":"<code>YOLOv5ObjectDetection</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv5 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov5/yolov5_object_detection.py</code> <pre><code>class YOLOv5ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv5 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv5 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"yolov5s_weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n        return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov5/yolov5_object_detection/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv5 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov5/yolov5_object_detection/#inference.models.yolov5.yolov5_object_detection.YOLOv5ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions.</p> Source code in <code>inference/models/yolov5/yolov5_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n    return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov7/yolov7_instance_segmentation/","title":"yolov7_instance_segmentation","text":""},{"location":"docs/reference/inference/models/yolov7/yolov7_instance_segmentation/#inference.models.yolov7.yolov7_instance_segmentation.YOLOv7InstanceSegmentation","title":"<code>YOLOv7InstanceSegmentation</code>","text":"<p>               Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv7 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv7 model with ONNX runtime.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs inference on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov7/yolov7_instance_segmentation.py</code> <pre><code>class YOLOv7InstanceSegmentation(InstanceSegmentationBaseOnnxRoboflowInferenceModel):\n    \"\"\"YOLOv7 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv7 model\n    with ONNX runtime.\n\n    Methods:\n        predict: Performs inference on the given image using the ONNX session.\n    \"\"\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        protos = predictions[4]\n        predictions = predictions[0]\n        return predictions, protos\n</code></pre>"},{"location":"docs/reference/inference/models/yolov7/yolov7_instance_segmentation/#inference.models.yolov7.yolov7_instance_segmentation.YOLOv7InstanceSegmentation.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.</p> Source code in <code>inference/models/yolov7/yolov7_instance_segmentation.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    protos = predictions[4]\n    predictions = predictions[0]\n    return predictions, protos\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_classification/","title":"yolov8_classification","text":""},{"location":"docs/reference/inference/models/yolov8/yolov8_instance_segmentation/","title":"yolov8_instance_segmentation","text":""},{"location":"docs/reference/inference/models/yolov8/yolov8_instance_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentation","title":"<code>YOLOv8InstanceSegmentation</code>","text":"<p>               Bases: <code>InstanceSegmentationBaseOnnxRoboflowInferenceModel</code></p> <p>YOLOv8 Instance Segmentation ONNX Inference Model.</p> <p>This class is responsible for performing instance segmentation using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs inference on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_instance_segmentation.py</code> <pre><code>class YOLOv8InstanceSegmentation(InstanceSegmentationBaseOnnxRoboflowInferenceModel):\n    \"\"\"YOLOv8 Instance Segmentation ONNX Inference Model.\n\n    This class is responsible for performing instance segmentation using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs inference on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Performs inference on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})\n        protos = predictions[1]\n        predictions = predictions[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:-32]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        masks = predictions[:, :, -32:]\n        predictions = np.concatenate([boxes, confs, class_confs, masks], axis=2)\n        return predictions, protos\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_instance_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentation.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov8/yolov8_instance_segmentation/#inference.models.yolov8.yolov8_instance_segmentation.YOLOv8InstanceSegmentation.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs inference on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.</p> Source code in <code>inference/models/yolov8/yolov8_instance_segmentation.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Performs inference on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: Tuple containing two NumPy arrays representing the predictions and protos. The predictions include boxes, confidence scores, class confidence scores, and masks.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})\n    protos = predictions[1]\n    predictions = predictions[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:-32]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    masks = predictions[:, :, -32:]\n    predictions = np.concatenate([boxes, confs, class_confs, masks], axis=2)\n    return predictions, protos\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_keypoints_detection/","title":"yolov8_keypoints_detection","text":""},{"location":"docs/reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection","title":"<code>YOLOv8KeypointsDetection</code>","text":"<p>               Bases: <code>KeypointsDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX keypoints detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing keypoints detection using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs object detection on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_keypoints_detection.py</code> <pre><code>class YOLOv8KeypointsDetection(KeypointsDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX keypoints detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing keypoints detection using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs object detection on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        number_of_classes = len(self.get_class_names)\n        class_confs = predictions[:, :, 4 : 4 + number_of_classes]\n        keypoints_detections = predictions[:, :, 4 + number_of_classes :]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        bboxes_predictions = np.concatenate(\n            [boxes, confs, class_confs, keypoints_detections], axis=2\n        )\n        return (bboxes_predictions,)\n\n    def keypoints_count(self) -&gt; int:\n        \"\"\"Returns the number of keypoints in the model.\"\"\"\n        if self.keypoints_metadata is None:\n            raise ModelArtefactError(\"Keypoints metadata not available.\")\n        return superset_keypoints_count(self.keypoints_metadata)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection.keypoints_count","title":"<code>keypoints_count()</code>","text":"<p>Returns the number of keypoints in the model.</p> Source code in <code>inference/models/yolov8/yolov8_keypoints_detection.py</code> <pre><code>def keypoints_count(self) -&gt; int:\n    \"\"\"Returns the number of keypoints in the model.\"\"\"\n    if self.keypoints_metadata is None:\n        raise ModelArtefactError(\"Keypoints metadata not available.\")\n    return superset_keypoints_count(self.keypoints_metadata)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_keypoints_detection/#inference.models.yolov8.yolov8_keypoints_detection.YOLOv8KeypointsDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ...]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolov8/yolov8_keypoints_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray, ...]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    number_of_classes = len(self.get_class_names)\n    class_confs = predictions[:, :, 4 : 4 + number_of_classes]\n    keypoints_detections = predictions[:, :, 4 + number_of_classes :]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    bboxes_predictions = np.concatenate(\n        [boxes, confs, class_confs, keypoints_detections], axis=2\n    )\n    return (bboxes_predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_object_detection/","title":"yolov8_object_detection","text":""},{"location":"docs/reference/inference/models/yolov8/yolov8_object_detection/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetection","title":"<code>YOLOv8ObjectDetection</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv8 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> <p>Methods:</p> Name Description <code>predict</code> <p>Performs object detection on the given image using the ONNX session.</p> Source code in <code>inference/models/yolov8/yolov8_object_detection.py</code> <pre><code>class YOLOv8ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv8 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n\n    Methods:\n        predict: Performs object detection on the given image using the ONNX session.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv8 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n        \"\"\"\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n        return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov8/yolov8_object_detection/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv8 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov8/yolov8_object_detection/#inference.models.yolov8.yolov8_object_detection.YOLOv8ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.</p> Source code in <code>inference/models/yolov8/yolov8_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions, including boxes, confidence scores, and class confidence scores.\n    \"\"\"\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n    return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov9/yolov9_object_detection/","title":"yolov9_object_detection","text":""},{"location":"docs/reference/inference/models/yolov9/yolov9_object_detection/#inference.models.yolov9.yolov9_object_detection.YOLOv9ObjectDetection","title":"<code>YOLOv9ObjectDetection</code>","text":"<p>               Bases: <code>ObjectDetectionBaseOnnxRoboflowInferenceModel</code></p> <p>Roboflow ONNX Object detection model (Implements an object detection specific infer method).</p> <p>This class is responsible for performing object detection using the YOLOv9 model with ONNX runtime.</p> <p>Attributes:</p> Name Type Description <code>weights_file</code> <code>str</code> <p>Path to the ONNX weights file.</p> Source code in <code>inference/models/yolov9/yolov9_object_detection.py</code> <pre><code>class YOLOv9ObjectDetection(ObjectDetectionBaseOnnxRoboflowInferenceModel):\n    \"\"\"Roboflow ONNX Object detection model (Implements an object detection specific infer method).\n\n    This class is responsible for performing object detection using the YOLOv9 model\n    with ONNX runtime.\n\n    Attributes:\n        weights_file (str): Path to the ONNX weights file.\n    \"\"\"\n\n    @property\n    def weights_file(self) -&gt; str:\n        \"\"\"Gets the weights file for the YOLOv9 model.\n\n        Returns:\n            str: Path to the ONNX weights file.\n        \"\"\"\n        return \"weights.onnx\"\n\n    def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n        \"\"\"Performs object detection on the given image using the ONNX session.\n\n        Args:\n            img_in (np.ndarray): Input image as a NumPy array.\n\n        Returns:\n            Tuple[np.ndarray]: NumPy array representing the predictions.\n        \"\"\"\n        # (b x 8 x 8000)\n        predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n        predictions = predictions.transpose(0, 2, 1)\n        boxes = predictions[:, :, :4]\n        class_confs = predictions[:, :, 4:]\n        confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n        predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n        return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/models/yolov9/yolov9_object_detection/#inference.models.yolov9.yolov9_object_detection.YOLOv9ObjectDetection.weights_file","title":"<code>weights_file: str</code>  <code>property</code>","text":"<p>Gets the weights file for the YOLOv9 model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the ONNX weights file.</p>"},{"location":"docs/reference/inference/models/yolov9/yolov9_object_detection/#inference.models.yolov9.yolov9_object_detection.YOLOv9ObjectDetection.predict","title":"<code>predict(img_in, **kwargs)</code>","text":"<p>Performs object detection on the given image using the ONNX session.</p> <p>Parameters:</p> Name Type Description Default <code>img_in</code> <code>ndarray</code> <p>Input image as a NumPy array.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray]</code> <p>Tuple[np.ndarray]: NumPy array representing the predictions.</p> Source code in <code>inference/models/yolov9/yolov9_object_detection.py</code> <pre><code>def predict(self, img_in: np.ndarray, **kwargs) -&gt; Tuple[np.ndarray]:\n    \"\"\"Performs object detection on the given image using the ONNX session.\n\n    Args:\n        img_in (np.ndarray): Input image as a NumPy array.\n\n    Returns:\n        Tuple[np.ndarray]: NumPy array representing the predictions.\n    \"\"\"\n    # (b x 8 x 8000)\n    predictions = self.onnx_session.run(None, {self.input_name: img_in})[0]\n    predictions = predictions.transpose(0, 2, 1)\n    boxes = predictions[:, :, :4]\n    class_confs = predictions[:, :, 4:]\n    confs = np.expand_dims(np.max(class_confs, axis=2), axis=2)\n    predictions = np.concatenate([boxes, confs, class_confs], axis=2)\n    return (predictions,)\n</code></pre>"},{"location":"docs/reference/inference/usage_tracking/collector/","title":"collector","text":""},{"location":"docs/reference/inference/usage_tracking/config/","title":"config","text":""},{"location":"docs/reference/inference/usage_tracking/payload_helpers/","title":"payload_helpers","text":""},{"location":"docs/reference/inference/usage_tracking/plan_details/","title":"plan_details","text":""},{"location":"docs/reference/inference/usage_tracking/redis_queue/","title":"redis_queue","text":""},{"location":"docs/reference/inference/usage_tracking/redis_queue/#inference.usage_tracking.redis_queue.RedisQueue","title":"<code>RedisQueue</code>","text":"<p>Store and forget, keys with specified hash tag are handled by external service</p> Source code in <code>inference/usage_tracking/redis_queue.py</code> <pre><code>class RedisQueue:\n    \"\"\"\n    Store and forget, keys with specified hash tag are handled by external service\n    \"\"\"\n\n    def __init__(\n        self,\n        hash_tag: str = \"UsageCollector\",\n        redis_cache: Optional[RedisCache] = None,\n    ):\n        # prefix must contain hash-tag to avoid CROSSLOT errors when using mget\n        # hash-tag is common part of the key wrapped within '{}'\n        # removing hash-tag will cause clients utilizing mget to fail\n        self._prefix: str = f\"{{{hash_tag}}}:{time.time()}:{uuid4().hex[:5]}\"\n        self._redis_cache: RedisCache = redis_cache or cache\n        self._increment: int = 0\n        self._lock: Lock = Lock()\n\n    def put(self, payload: Any):\n        if not isinstance(payload, str):\n            try:\n                payload = json.dumps(payload)\n            except Exception as exc:\n                logger.error(\"Failed to parse payload '%s' to JSON - %s\", payload, exc)\n                return\n        with self._lock:\n            try:\n                self._increment += 1\n                redis_key = f\"{self._prefix}:{self._increment}\"\n                # https://redis.io/docs/latest/develop/interact/transactions/\n                redis_pipeline = self._redis_cache.client.pipeline()\n                redis_pipeline.set(\n                    name=redis_key,\n                    value=payload,\n                )\n                redis_pipeline.zadd(\n                    name=\"UsageCollector\",\n                    mapping={redis_key: time.time()},\n                )\n                results = redis_pipeline.execute()\n                if not all(results):\n                    # TODO: partial insert, retry\n                    logger.error(\n                        \"Failed to store payload and sorted set (partial insert): %s\",\n                        results,\n                    )\n            except Exception as exc:\n                logger.error(\"Failed to store usage records '%s', %s\", payload, exc)\n\n    @staticmethod\n    def full() -&gt; bool:\n        return False\n\n    def empty(self) -&gt; bool:\n        return True\n\n    def get_nowait(self) -&gt; List[Dict[str, Any]]:\n        return []\n</code></pre>"},{"location":"docs/reference/inference/usage_tracking/sqlite_queue/","title":"sqlite_queue","text":""},{"location":"docs/reference/inference/usage_tracking/utils/","title":"utils","text":""},{"location":"enterprise/enterprise/","title":"Enterprise Features","text":"<p>Roboflow Enterprise customers have access to advanced Inference features. These include:</p> <ol> <li>Active learning: Actively collect data from your production line for use in training new, more accurate models over time.</li> <li>Parallel processing server: Run requests in parallel to achieve increased throughput and lower latency when running inference on models.</li> <li>A license to run Inference on more than one device.</li> </ol> <p>To learn more about Roboflow's enterprise offerings, contact the sales team.</p>"},{"location":"enterprise/manage_devices/","title":"Manage devices","text":"<p>The Roboflow team is working on support for remote device management with Inference. </p> <p>To learn more about this feature, contact the Roboflow sales team.</p>"},{"location":"enterprise/parallel_processing/","title":"Parallel Inference","text":"<p>Note</p> <p>This feature is only available for Roboflow Enterprise users. Contact our sales team to learn more about Roboflow Enterprise.</p> <p>You can run multiple models in parallel with Inference with parallel processing, a version of Roboflow Inference that processes inference requests asynchronously.</p> <p>Inference Parallel supports all the same features as Roboflow Inference, with the exception that it does not support Core models (i.e. CLIP and SAM).</p> <p>With Inference Parallel, preprocessing, auto batching, inference, and post processing all run in separate threads to increase server FPS throughput.</p> <p>Separate requests to the same model will be batched on the fly as allowed by <code>$MAX_BATCH_SIZE</code>, and then response handling will occurr independently. Images are passed via Python's SharedMemory module to maximize throughput.</p> <p>These changes result in as much as a 76% speedup on one measured workload.</p>"},{"location":"enterprise/parallel_processing/#how-to-use-inference-with-parallel-processing","title":"How To Use Inference with Parallel Processing","text":"<p>You can run Inference with Parallel Processing in two ways: via the CLI or via Docker.</p> BashDocker <p>First, build the parallel server</p> <pre><code>./inference/enterprise/parallel/build.sh\n</code></pre> <p>Then, run the server:</p> <pre><code>./inference/enterprise/parallel/run.sh\n</code></pre> <p>A message will appear in the terminal indicating that the server is running and ready for use.</p> <p>We provide a container at Docker Hub that you can pull using <code>docker pull roboflow/roboflow-inference-server-gpu-parallel:latest</code>. If you are pulling a pinned tag, be sure to change the <code>$TAG</code> variable in <code>run.sh</code>.</p>"},{"location":"enterprise/parallel_processing/#benchmarking","title":"Benchmarking","text":"<p>We evaluated the performance of Inference Parallel on a variety of models from Roboflow Universe. We compared the performance of Inference Parallel to the latest version of Inference Server (0.9.5.rc) on the same hardware.</p> <p>We ran our tests on a computer with eight cores and one GPU. Instance segmentation metrics are calculated using <code>\"mask_decode_mode\": \"fast\"</code> in the request body. Requests are posted concurrently with a parallelism of 1000.</p> <p>Here are the results of our tests:</p> Workspace Model Model Type split 0.9.5.rc FPS 0.9.5.parallel FPS senior-design-project-j9gpp nbafootage/3 object-detection train 30.2 fps 44.03 fps niklas-bommersbach-jyjff dart-scorer/8 object-detection train 26.6 fps 47.0 fps geonu water-08xpr/1 instance-segmentation valid 4.7 fps 6.1 fps university-of-bradford detecting-drusen_1/2 instance-segmentation train 6.2 fps 7.2 fps fy-project-y9ecd cataract-detection-viwsu/2 classification train 48.5 fps 65.4 fps hesunyu playing-cards-ir0wr/1 classification train 44.6 fps 57.7 fps <p>Inference with parallel processing enabled achieved higher FPS on every test. On eome models, the FPS increase by using Inference with parallel processing was greater than 10 FPS.</p>"},{"location":"enterprise/stream_management_api/","title":"Stream Management","text":"<p>[!IMPORTANT]  We require a Roboflow Enterprise License to use this in production. See inference/enterpise/LICENSE.txt for details.</p>"},{"location":"enterprise/stream_management_api/#overview","title":"Overview","text":"<p>This feature is designed to cater to users requiring the execution of inference to generate predictions using Roboflow object-detection models, particularly when dealing with online video streams. It enhances the functionalities of the familiar <code>inference.Stream()</code> and <code>InferencePipeline()</code> interfaces, as found in the open-source version of the library, by introducing a sophisticated management layer. The inclusion of additional capabilities empowers users to remotely manage the state of inference pipelines through the HTTP management interface integrated into this package.</p> <p>This functionality proves beneficial in various scenarios, including but not limited to:</p> <ul> <li>Performing inference across multiple online video streams simultaneously.</li> <li>Executing inference on multiple devices that necessitate coordination.</li> <li>Establishing a monitoring layer to oversee video processing based on the <code>inference</code> package.</li> </ul>"},{"location":"enterprise/stream_management_api/#design","title":"Design","text":""},{"location":"enterprise/stream_management_api/#example-use-case","title":"Example use-case","text":"<p>Joe aims to monitor objects within the footage captured by a fleet of IP cameras installed in his factory. After successfully training an object-detection model on the Roboflow platform, he is now prepared for deployment. With four cameras in his factory, Joe opts for a model that is sufficiently compact, allowing for over 30 inferences per second on his Jetson devices. Considering this computational budget per device, Joe determines that he requires two Jetson devices to efficiently process footage from all cameras, anticipating an inference throughput of approximately 15 frames per second for each video source.</p> <p>To streamline the deployment, Joe chooses to deploy Stream Management containers to all available Jetson devices within his local network. This setup enables him to communicate with each Jetson device via HTTP, facilitating the orchestration of processing tasks. Joe develops a web app through which he can send commands to the devices and retrieve metrics regarding the statuses of the video streams.</p> <p>Finally, Joe implements a UDP server capable of receiving predictions, leveraging the <code>supervision</code> package to effectively track objects in the footage. This comprehensive approach allows Joe to manage and monitor the object-detection process seamlessly across his fleet of Jetson devices.</p>"},{"location":"enterprise/stream_management_api/#how-to-run","title":"How to run?","text":""},{"location":"enterprise/stream_management_api/#in-docker-using-docker-compose","title":"In docker - using <code>docker compose</code>","text":"<p>The most prevalent use-cases are conveniently encapsulated with Docker Compose configurations, ensuring readiness for immediate use. Nevertheless, in specific instances where custom configuration adjustments are required within Docker containers, such as passing camera devices, alternative options may prove more suitable.</p>"},{"location":"enterprise/stream_management_api/#cpu-based-devices","title":"CPU-based devices","text":"<pre><code>repository_root$ docker compose -f ./docker/dockerfiles/stream-management-api.compose-cpu.yaml up\n</code></pre>"},{"location":"enterprise/stream_management_api/#gpu-based-devices","title":"GPU-based devices","text":"<pre><code>repository_root$ docker compose -f ./docker/dockerfiles/stream-management-api.compose-gpu.yaml up\n</code></pre>"},{"location":"enterprise/stream_management_api/#jetson-devices-jetpack-511","title":"Jetson devices (<code>JetPack 5.1.1</code>)","text":"<pre><code>repository_root$ docker-compose -f ./docker/dockerfiles/stream-management-api.compose-jetson.5.1.1.yaml up\n</code></pre> <p>Disclaimer: At Jetson devices, some operations (like container bootstrap or initialisation of model) takes more time than for other ones. In particular - docker compose definition in current form do not define active awaiting TCP socket port to be opened by Stream Manager - which means that initial requests to HTTP API may be responded with HTTP 503.</p>"},{"location":"enterprise/stream_management_api/#in-docker-running-api-and-stream-manager-containers-separately","title":"In docker - running API and stream manager containers separately","text":""},{"location":"enterprise/stream_management_api/#run","title":"Run","text":""},{"location":"enterprise/stream_management_api/#cpu-based-devices_1","title":"CPU-based devices","text":"<pre><code>docker run -d --name stream_manager --network host roboflow/roboflow-inference-stream-manager-cpu:latest\ndocker run -d --name stream_management_api --network host roboflow/roboflow-inference-stream-management-api:latest\n</code></pre>"},{"location":"enterprise/stream_management_api/#gpu-based-devices_1","title":"GPU-based devices","text":"<pre><code>docker run -d --name stream_manager --network host --runtime nvidia roboflow/roboflow-inference-stream-manager-gpu:latest\ndocker run -d --name stream_management_api --network host roboflow/roboflow-inference-stream-management-api:latest\n</code></pre>"},{"location":"enterprise/stream_management_api/#jetson-devices-jetpack-511_1","title":"Jetson devices (<code>JetPack 5.1.1</code>)","text":"<pre><code>docker run -d --name stream_manager --network host --runtime nvidia roboflow/roboflow-inference-stream-manager-jetson-5.1.1:latest\ndocker run -d --name stream_management_api --network host roboflow/roboflow-inference-stream-management-api:latest\n</code></pre>"},{"location":"enterprise/stream_management_api/#configuration-parameters","title":"Configuration parameters","text":""},{"location":"enterprise/stream_management_api/#stream-management-api","title":"Stream Management API","text":"<ul> <li><code>STREAM_MANAGER_HOST</code> - hostname for stream manager container (alter with container name if <code>--network host</code> not used   or used against remote machine)</li> <li><code>STREAM_MANAGER_PORT</code> - port to communicate with stream manager (must match with stream manager container)</li> </ul>"},{"location":"enterprise/stream_management_api/#stream-manager","title":"Stream Manager","text":"<ul> <li><code>PORT</code> - port at which server will be running</li> <li>one can mount volume under container's <code>/tmp/cache</code> to enable permanent storage of models - for faster inference   pipelines initialisation</li> <li>at the level of this container the connectivity to camera must be enabled - so if device passing to docker must   happen - it should happen at this stage</li> </ul>"},{"location":"enterprise/stream_management_api/#build-optional","title":"Build (Optional)","text":""},{"location":"enterprise/stream_management_api/#stream-management-api_1","title":"Stream Management API","text":"<pre><code>docker build -t roboflow/roboflow-inference-stream-management-api:dev -f docker/dockerfiles/Dockerfile.stream_management_api .\n</code></pre>"},{"location":"enterprise/stream_management_api/#stream-manager_1","title":"Stream Manager","text":"<pre><code>docker build -t roboflow/roboflow-inference-stream-manager-{device}:dev -f docker/dockerfiles/Dockerfile.onnx.{device}.stream_manager .\n</code></pre>"},{"location":"enterprise/stream_management_api/#bare-metal-deployment","title":"Bare-metal deployment","text":"<p>In some cases, it would be required to deploy the application at host level. This is possible, although client must resolve the environment in a way that is presented in Stream Manager and Stream Management API dockerfiles appropriate for specific platform. Once this is done the following command should be run:</p> <pre><code>repository_root$ python -m inference.enterprise.stream_management.manager.app  # runs manager\n</code></pre> <pre><code>repository_root$ python -m inference.enterprise.stream_management.api.app  # runs management API\n</code></pre>"},{"location":"enterprise/stream_management_api/#how-to-integrate","title":"How to integrate?","text":"<p>After running <code>roboflow-inference-stream-management-api</code> container, HTTP API will be available under <code>http://127.0.0.1:8080</code> (given that default configuration is used).</p> <p>One can call <code>wget http://127.0.0.1:8080/openapi.json</code> to get OpenApi specification of API that can be rendered here</p> <p>Example Python client is provided below:</p> <pre><code>import requests\nfrom typing import Optional\n\nURL = \"http://127.0.0.1:8080\"\n\ndef list_pipelines() -&gt; dict:\n    response = requests.get(f\"{URL}/list_pipelines\")\n    return response.json()\n\n\ndef get_pipeline_status(pipeline_id: str) -&gt; dict:\n    response = requests.get(f\"{URL}/status/{pipeline_id}\")\n    return response.json()\n\n\ndef pause_pipeline(pipeline_id: str) -&gt; dict:\n    response = requests.post(f\"{URL}/pause/{pipeline_id}\")\n    return response.json()\n\n\ndef resume_pipeline(pipeline_id: str) -&gt; dict:\n    response = requests.post(f\"{URL}/resume/{pipeline_id}\")\n    return response.json()\n\ndef terminate_pipeline(pipeline_id: str) -&gt; dict:\n    response = requests.post(f\"{URL}/terminate/{pipeline_id}\")\n    return response.json()\n\ndef initialise_pipeline(\n    video_reference: str,\n    model_id: str,\n    api_key: str,\n    sink_host: str,\n    sink_port: int,\n    max_fps: Optional[int] = None,\n) -&gt; dict:\n    response = requests.post(\n        f\"{URL}/initialise\",\n        json={\n            \"type\": \"init\",\n            \"sink_configuration\": {\n                \"type\": \"udp_sink\",\n                \"host\": sink_host,\n                \"port\": sink_port,\n            },\n            \"video_reference\": video_reference,\n            \"model_id\": model_id,\n            \"api_key\": api_key,\n            \"max_fps\": max_fps,\n\n        },\n    )\n    return response.json()\n</code></pre>"},{"location":"enterprise/stream_management_api/#important-notes","title":"Important notes","text":"<ul> <li>Please remember that <code>initialise_pipeline()</code> must be filled with <code>video_reference</code> and <code>sink_configuration</code>   in such a way, that any resource (video file / camera device) or URI (stream reference, sink reference) must be   reachable from Stream Manager environment! For instance - in some cases inside docker containers <code>localhost</code> will   be bound into container localhost not the localhost of the machine hosting container.</li> </ul>"},{"location":"enterprise/stream_management_api/#developer-notes","title":"Developer notes","text":"<p>The pivotal element of the implementation is the Stream Manager component, operating as an application in single-threaded, TCP-server mode. It systematically processes requests received from a TCP socket, taking on the responsibility of spawning and overseeing processes that run the <code>InferencePipelineManager</code>. Communication between the <code>InferencePipelineManager</code> processes and the main process of the Stream Manager occurs through multiprocessing queues. These queues facilitate the exchange of input commands and the retrieval of results.</p> <p>Requests directed to the Stream Manager are sequentially handled in blocking mode, ensuring that each request must conclude before the initiation of the next one.</p>"},{"location":"enterprise/stream_management_api/#communication-protocol-requests","title":"Communication protocol - requests","text":"<p>Stream Manager accepts the following binary protocol in communication. Each communication payload contains:</p> <pre><code>[HEADER: 4B, big-endian, not signed - int value with message size][MESSAGE: utf-8 serialised json of size dictated by header]\n</code></pre> <p>Message must be a valid JSON after decoding and represent valid command.</p>"},{"location":"enterprise/stream_management_api/#list_pipelines-command","title":"<code>list_pipelines</code> command","text":"<pre><code>{\n  \"type\": \"list_pipelines\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#init-command","title":"<code>init</code> command","text":"<pre><code>{\n  \"type\": \"init\",\n  \"model_id\": \"some/1\",\n  \"video_reference\": \"rtsp://192.168.0.1:554\",\n  \"sink_configuration\": {\n    \"type\": \"udp_sink\",\n    \"host\": \"192.168.0.3\",\n    \"port\": 9999\n  },\n  \"api_key\": \"YOUR-API-KEY\",\n  \"max_fps\": 16,\n  \"model_configuration\": {\n    \"type\": \"object-detection\",\n    \"class_agnostic_nms\": true,\n    \"confidence\": 0.5,\n    \"iou_threshold\": 0.4,\n    \"max_candidates\": 300,\n    \"max_detections\": 3000\n  },\n  \"video_source_properties\": {\n    \"frame_width\": 1920,\n    \"frame_height\": 1080,\n    \"fps\": 30\n  }\n}\n</code></pre> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p>"},{"location":"enterprise/stream_management_api/#terminate-command","title":"<code>terminate</code> command","text":"<pre><code>{\n  \"type\": \"terminate\",\n  \"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#pause-command","title":"<code>pause</code> command","text":"<pre><code>{\n  \"type\": \"mute\",\n  \"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#resume-command","title":"<code>resume</code> command","text":"<pre><code>{\n  \"type\": \"resume\",\n  \"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#status-command","title":"<code>status</code> command","text":"<pre><code>{\n  \"type\": \"status\",\n  \"pipeline_id\": \"my_pipeline\"\n}\n</code></pre>"},{"location":"enterprise/stream_management_api/#communication-protocol-responses","title":"Communication protocol - responses","text":"<p>Stream Manager, for each request that can be processed (without timeout or source disconnection), will return the result in a format:</p> <pre><code>[HEADER: 4B, big-endian, not signed - int value with result size][RESULT: utf-8 serialised json of size dictated by header]\n</code></pre> <p>Structure of result:</p> <ul> <li><code>request_id</code> - field with random string representing request id assigned by Stream Manager - to ease debugging</li> <li><code>pipeline_id</code> - if command from request can be associated to specific pipeline - its ID will be denoted in response</li> <li><code>response</code> - payload of operation response</li> </ul> <p>Each <code>response</code> has the <code>status</code> key with two values possible: <code>success</code> or <code>failure</code> to denote operation status. Each failed response contain <code>error_type</code> key to dispatch error handling and optional fields <code>error_class</code> and <code>error_message</code> representing inner details of error.</p> <p>Content of successful responses depends on type of operation.</p>"},{"location":"enterprise/stream_management_api/#future-work","title":"Future work","text":"<ul> <li>securing API connection layer (to enable safe remote control)</li> <li>securing TCP socket of Stream Manager</li> </ul>"},{"location":"enterprise/active-learning/active_learning/","title":"Active Learning","text":"<p>Active Learning is a process of iterative improvement of model by retraining models on dataset that grows over time. This process includes data collection (usually with smart selection of datapoints that model would most benefit from), labeling, model re-training, evaluation and deployment - to close the circle and start new iteration.</p> <p>Elements of that process can be partially or fully automated - providing an elegant way of improving dataset over time,  which is important to ensure good quality of model predictions over time (as the data distribution may change and a model trained on old data may not be performant facing the new one). At Roboflow, we've brought automated data collection  mechanism - which is the foundational building block for Active Learning  -- to the platform.</p>"},{"location":"enterprise/active-learning/active_learning/#where-to-start","title":"Where to start?","text":"<p>We suggest clients apply the following strategy to train their models. If it's applicable - start from a small, good  quality dataset labeled manually (making sure that the test set is representative of the problem to be solved) and train  an initial model. Once that is done - deploy your model, enabling Active Learning data collection, and gradually increase  the size of your dataset with data collected in production environment.</p> <p>Alternatively, it is also possible to start the project with a Universe model. Then, for each request you can specify <code>active_learning_target_dataset</code> - pointing to the project where the data should be  saved. This way, if you find a model that meets your minimal quality criteria, you may start generating valuable  predictions from day zero, while collecting good quality data to train even better models in the future.</p>"},{"location":"enterprise/active-learning/active_learning/#how-active-learning-data-collection-works","title":"How Active Learning data collection works?","text":"<p>Here is the standard workflow for Active Learning data collection:</p> <ul> <li>The user initiates the creation of an Active Learning configuration within the Roboflow app.</li> <li>This configuration is then distributed across all active inference instances, which may include those running against video streams and the HTTP API, both on-premises and within the Roboflow platform.</li> <li>During runtime, as predictions are generated, images and model predictions (treated as initial annotations) are dynamically collected and submitted in batches into user project. These batches are then ready for labeling within the Roboflow platform.</li> </ul> <p>How active learning works with Inference is configured in your server active learning configuration. Learn how to configure active learning.</p> <p>Active learning can be disabled by setting <code>ACTIVE_LEARNING_ENABLED=false</code> in the environment where you run <code>inference</code>.</p>"},{"location":"enterprise/active-learning/active_learning/#usage-patterns","title":"Usage patterns","text":"<p>Active Learning data collection may be combined with different components of the Roboflow ecosystem. In particular:</p> <ul> <li>the <code>inference</code> Python package can be used to get predictions from the model and register them on the Roboflow platform</li> <li>one may want to use <code>InferencePipeline</code> to get predictions from video and register its video frames using Active Learning</li> <li>self-hosted <code>inference</code> server - where data is collected while processing requests</li> <li>Roboflow hosted <code>inference</code> - where you let us make sure you get your predictions and data registered. No  infrastructure needs to run on your end, we take care of everything</li> <li>Roboflow <code>workflows</code> - our newest feature - supports <code>Roboflow Dataset Upload block</code></li> </ul>"},{"location":"enterprise/active-learning/active_learning/#sampling-strategies","title":"Sampling Strategies","text":"<p><code>inference</code> makes it possible to configure the way data is selected for registration. One may configure one or more sampling strategies during Active Learning configuration. We support five strategies for sampling image data for use in training new model versions.  These strategies are:</p> <ul> <li>Random sampling: Images are collected at random.</li> <li>Close-to-threshold: Collect data close to a given threshold.</li> <li>Detection count-based (Detection models only): Collect data with a specific number of detections returned by a detection model.</li> <li>Class-based (Classification models only): Collect data with a specific class returned by a classification model.</li> </ul>"},{"location":"enterprise/active-learning/active_learning/#how-data-is-sampled","title":"How Data is Sampled","text":"<p>When you run Inference with an active learning configuration, the following steps are run:</p> <ol> <li>Sampling methods are evaluated to decide which ones are applicable to the image and prediction (evaluation happens in the order of definition in your configuration).</li> <li>A global limit for batch (defined in <code>batching_strategy</code>) is checked. Its violation terminates Active Learning attempt.</li> <li>Matching methods are checked against limits defined within their configurations. The first method with matching limit is selected.</li> </ol> <p>Once a datapoint is selected and there is no limit violation, it will be saved into Roboflow platform with tags relevant for specific strategy (and global tags defined at the level of Active Learning configuration).</p>"},{"location":"enterprise/active-learning/active_learning/#active-learning-configuration","title":"Active Learning Configuration","text":"<p>One may choose to configure their Active Learning with the Roboflow app UI by navigating to the <code>Active Learning</code> panel. Alternatively, requests to Roboflow API may be sent with custom configuration. Here is how to configure Active Learning directly through the API.</p>"},{"location":"enterprise/active-learning/active_learning/#configuration-options","title":"Configuration options","text":"<ul> <li><code>enabled</code>: boolean flag to enable / disable the configuration (required) - <code>{\"enabled\": false}</code> is minimal valid config</li> <li><code>max_image_size</code>: two element list with positive integers (height, width) enforcing down-sizing (with aspect-ratio preservation) of images before submission into Roboflow platform (optional)</li> <li><code>jpeg_compression_level</code>: integer value in range [1, 100]  representing compression level of submitted images  (optional, defaults to <code>95</code>)</li> <li><code>persist_predictions</code>: binary flag to decide if predictions should be collected along with images (required if <code>enabled</code>)</li> <li><code>sampling_strategies</code>: list of sampling strategies (non-empty list required if <code>enabled</code>)</li> <li><code>batching_strategy</code>: configuration of labeling batches creation - details below (required if <code>enabled</code>)</li> <li><code>tags</code>: list of tags (each contains 1-64 characters from range <code>a-z, A-Z, 0-9, and -_:/.[]&lt;&gt;{}@</code>) (optional)</li> </ul>"},{"location":"enterprise/active-learning/active_learning/#batching-strategy","title":"Batching strategy","text":"<p>The <code>batching_strategy</code> field holds a dictionary with the following configuration options:</p> <ul> <li><code>batches_name_prefix</code>: A string representing the prefix of batch names created by Active Learning (required)</li> <li><code>recreation_interval</code>: One of <code>[\"never\", \"daily\", \"weekly\", \"monthly\"]</code>: representing the interval which is to be used to create separate batches. This parameter allows the user to control the flow of labeling batches over time (required).</li> <li><code>max_batch_images</code>: Positive integer representing the maximum size of the batch (applied on top of any strategy limits) to prevent too much data from being collected (optional)</li> </ul>"},{"location":"enterprise/active-learning/active_learning/#strategy-limits","title":"Strategy limits","text":"<p>Each strategy can be configured with <code>limits</code>: list of values limiting how many images can be collected  each minute, hour or day. Each entry on that list can hold two values: * <code>type</code>: one of <code>[\"minutely\", \"hourly\", \"daily\"]</code>: representing the type of limit * <code>value</code>: with limit threshold</p> <p>Limits are enforced with different granularity, as they are implemented based or either Redis or memory cache (bounded into a single process). So, effectively: * if the Redis cache is used - all instances of <code>inference</code> connected to the same Redis service will share limit  enforcements * otherwise, the memory cache of a single instance is used (multiple processes will have their own limits)</p> <p>Self-hosted <code>inference</code> may be connected to your own Redis cache.</p>"},{"location":"enterprise/active-learning/active_learning/#example-configuration","title":"Example configuration","text":"<pre><code>{\n    \"enabled\": true,\n    \"max_image_size\": [1200, 1200],\n    \"jpeg_compression_level\": 75,\n    \"persist_predictions\": true,\n    \"sampling_strategies\": [\n      {\n        \"name\": \"default_strategy\",\n        \"type\": \"random\",\n        \"traffic_percentage\": 0.1,\n        \"limits\": [{\"type\": \"daily\", \"value\": 100}]\n      }\n    ],\n    \"batching_strategy\": {\n      \"batches_name_prefix\": \"al_batch\",\n      \"recreation_interval\": \"daily\"\n    }\n}\n</code></pre>"},{"location":"enterprise/active-learning/active_learning/#set-configuration","title":"Set Configuration","text":"<p>To set an active learning configuration, use the following code:</p> <pre><code>import requests\n\ndef set_active_learning_configuration(\n    workspace: str,\n    project: str,\n    api_key: str,\n    config: dict,\n) -&gt; None:\n    response = requests.post(\n        f\"https://api.roboflow.com/{workspace}/{project}/active_learning?api_key={api_key}\",\n        json={\n            \"config\": config,\n        }\n    )\n    return response.json()\n\nset_active_learning_configuration(\n    workspace=\"yyy\",\n    project=\"zzz\",\n    api_key=\"XXX\",\n    config={\n        \"enabled\": True,\n        \"persist_predictions\": True,\n        \"batching_strategy\": {\n            \"batches_name_prefix\": \"my_batches\",\n            \"recreation_interval\": \"daily\",\n        },\n        \"sampling_strategies\": [\n            {\n                \"name\": \"default_strategy\",\n                \"type\": \"random\",\n                \"traffic_percentage\": 0.01, \n                \"limits\": [{\"type\": \"daily\", \"value\": 100}]\n            }\n        ]\n    }\n)\n</code></pre> <p>Where:</p> <ol> <li><code>workspace</code> is your workspace name;</li> <li><code>project</code> is your project name;</li> <li><code>api_key</code> is your API key, and;</li> <li><code>config</code> is your active learning configuration.</li> </ol>"},{"location":"enterprise/active-learning/active_learning/#retrieve-existing-configuration","title":"Retrieve Existing Configuration","text":"<p>To retrieve an existing active learning configuration, use the following code:</p> <pre><code>import requests\n\ndef get_active_learning_configuration(\n    workspace: str,\n    project: str,\n    api_key: str\n) -&gt; None:\n    response = requests.get(\n        f\"https://api.roboflow.com/{workspace}/{project}/active_learning?api_key={api_key}\",\n    )\n    return response.json()\n</code></pre> <p>Above, replace <code>workspace</code> with your workspace name, <code>project</code> with your project name, and <code>api_key</code> with your API key.</p>"},{"location":"enterprise/active-learning/active_learning/#stubs","title":"Stubs","text":"<p>One may use <code>{dataset_name}/0</code> as <code>model_id</code> while making prediction - to use null model for specific project.  It is going to provide predictions in the following format: <pre><code>{\n    \"time\": 0.0002442499971948564,\n    \"is_stub\": true,\n    \"model_id\": \"asl-poly-instance-seg/0\",\n    \"task_type\": \"instance-segmentation\"\n}\n</code></pre></p> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p> <p>This option, combined with Active Learning (namely <code>random</code> sampling strategy), provides a way to start data collection even prior any model is trained. There are several benefits of such strategy. The most important is building  the dataset representing the true production distribution, before any model is trained.</p> <p>Example client usage: <pre><code>import cv2\nfrom inference_sdk import InferenceHTTPClient\n\nimage = cv2.imread(\"&lt;path_to_your_image&gt;\")\nLOCALHOST_CLIENT = InferenceHTTPClient(\n    api_url=\"http://127.0.0.1:9001\",\n    api_key=\"XXX\"\n)\nLOCALHOST_CLIENT.infer(image, model_id=\"asl-poly-instance-seg/0\")\n</code></pre></p>"},{"location":"enterprise/active-learning/active_learning/#parameters-of-requests-to-inference-server-influencing-the-active-learning-data-collection","title":"Parameters of requests to <code>inference</code> server influencing the Active Learning data collection","text":"<p>There are a few parameters that can be added to request to influence how data collection works, in particular:</p> <ul> <li><code>disable_active_learning</code> - to disable functionality at the level of a single request (if for some reason you do not  want input data to be collected - useful for testing purposes)</li> <li><code>active_learning_target_dataset</code> - making inference from a specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. Please remember that you cannot use incompatible types of models in <code>project_a</code> and <code>project_b</code>; if that is the case, data will not be  registered. For instance, classification predictions cannot be registered in detection-based projects. You are free to mix  tasks like object-detection, instance-segmentation, or keypoints detection, but naturally not every detail of the required label may be available from prediction.</li> </ul> <p>Visit Inference SDK docs to learn more.</p>"},{"location":"enterprise/active-learning/classes_based/","title":"Classes Based","text":"<p>Collect and save images that match a class from a classifier prediction for use in improving your model.</p> <p>Tip</p> <p>Review the Active Learning page for more information about how to use active learning.</p> <p>This strategy is available for the following model types:</p> <ul> <li><code>classification</code></li> </ul>"},{"location":"enterprise/active-learning/classes_based/#configuration","title":"Configuration","text":"<ul> <li><code>name</code>: user-defined name of the strategy - must be non-empty and unique within all strategies defined in a   single configuration (required)</li> <li><code>type</code>: with value <code>classes_based</code> is used to identify close to threshold sampling strategy (required)</li> <li><code>selected_class_names</code>: list of class names to consider during sampling - (required)</li> <li><code>probability</code>: fraction of datapoints that matches sampling criteria that will be persisted. It is meant to be float   value in range [0.0, 1.0] (required)</li> <li><code>tags</code>: list of tags (each contains 1-64 characters from range <code>a-z, A-Z, 0-9, and -_:/.[]&lt;&gt;{}@</code>) (optional)</li> </ul>"},{"location":"enterprise/active-learning/classes_based/#example","title":"Example","text":"<p>Here is an example of a configuration manifest for the close to threshold sampling strategy:</p> <pre><code>{\n  \"name\": \"underrepresented_classes\",\n  \"type\": \"classes_based\",\n  \"selected_class_names\": [\"cat\"],\n  \"probability\": 1.0,\n  \"tags\": [\"hard-classes\"],\n  \"limits\": [\n    { \"type\": \"minutely\", \"value\": 10 },\n    { \"type\": \"hourly\", \"value\": 100 },\n    { \"type\": \"daily\", \"value\": 1000 }\n  ]\n}\n</code></pre>"},{"location":"enterprise/active-learning/close_to_threshold_sampling/","title":"Close to Threshold Sampling","text":"<p>Select data points that lead to specific prediction confidences for particular classes.</p> <p>This method is applicable to both detection and classification models, although the behavior may vary slightly between the two.</p> <p>Tip</p> <p>Review the Active Learning page for more information about how to use active learning.</p> <p>This strategy is available for the following model types:</p> <ul> <li><code>classification</code></li> <li><code>object-detection</code></li> <li><code>instance-segmentation</code></li> <li><code>keypoints-detection</code></li> </ul>"},{"location":"enterprise/active-learning/close_to_threshold_sampling/#configuration","title":"Configuration","text":"<ul> <li><code>name</code>: user-defined name of the strategy - must be non-empty and unique within all strategies defined in a   single configuration (required)</li> <li><code>type</code>: with value <code>close_to_threshold</code> is used to identify close to threshold sampling strategy (required)</li> <li><code>selected_class_names</code>: list of class names to consider during sampling; if not provided, all classes can be sampled. (Optional)</li> <li><code>threshold</code> and <code>epsilon</code>: Represent the center and radius for the confidence range that triggers sampling. Both are   to be float values in range [0.0, 1.0]. For example, if one aims to obtain datapoints where the classifier is highly   confident (0.8, 1.0), set threshold=0.9 and epsilon=0.1. Note that this is limited to outcomes from model   post-processing and threshold filtering - hence not all model predictions may be visible at the level of Active Learning   logic. (required)</li> <li><code>probability</code>: Fraction of datapoints matching sampling criteria that will be persisted. It is meant to be float   value in range [0.0, 1.0] (required)</li> <li><code>minimum_objects_close_to_threshold</code>: (used for detection predictions only) Specify how many detected objects from   selected classes must be close to the threshold to accept the datapoint. If given - must be integer value &gt;= 1.   (Optional - with default to <code>1</code>)</li> <li><code>only_top_classes</code>: (used for classification predictions only) Flag to decide whether only the <code>top</code> or   <code>predicted_classes</code> (for multi-class/multi-label cases, respectively) should be considered. This helps avoid sampling   based on non-leading classes in predictions. Default: <code>True</code>.</li> <li><code>tags</code>: list of tags (each contains 1-64 characters from range <code>a-z, A-Z, 0-9, and -_:/.[]&lt;&gt;{}@</code>) (optional)</li> </ul>"},{"location":"enterprise/active-learning/close_to_threshold_sampling/#example","title":"Example","text":"<p>Here is an example of a configuration manifest for the close to threshold sampling strategy:</p> <pre><code>{\n  \"name\": \"hard_examples\",\n  \"type\": \"close_to_threshold\",\n  \"selected_class_names\": [\"a\", \"b\"],\n  \"threshold\": 0.25,\n  \"epsilon\": 0.1,\n  \"probability\": 0.5,\n  \"tags\": [\"my_tag_1\", \"my_tag_2\"],\n  \"limits\": [\n    { \"type\": \"minutely\", \"value\": 10 },\n    { \"type\": \"hourly\", \"value\": 100 },\n    { \"type\": \"daily\", \"value\": 1000 }\n  ]\n}\n</code></pre>"},{"location":"enterprise/active-learning/detection_number/","title":"Detection Number","text":"<p>Choose specific detections based on count and detection classes. Collect and save images for use in improving your model.</p> <p>Tip</p> <p>Review the Active Learning page for more information about how to use active learning.</p> <p>This strategy is available for the following model types:</p> <ul> <li><code>object-detection</code></li> <li><code>instance-segmentation</code></li> <li><code>keypoints-detection</code></li> </ul>"},{"location":"enterprise/active-learning/detection_number/#configuration","title":"Configuration","text":"<ul> <li><code>name</code>: user-defined name of the strategy - must be non-empty and unique within all strategies defined in a   single configuration (required)</li> <li><code>type</code>: with value <code>detections_number_based</code> is used to identify close to threshold sampling strategy (required)</li> <li><code>selected_class_names</code>: list of class names to consider during sampling; if not provided, all classes can be sampled. (Optional)</li> <li><code>probability</code>: fraction of datapoints that matches sampling criteria that will be persisted. It is meant to be float   value in range [0.0, 1.0] (required)</li> <li><code>more_than</code>: minimal number of detected objects - if given it is meant to be integer &gt;= 0   (optional - if not given - lower limit is not applied)</li> <li><code>less_than</code>: maximum number of detected objects - if given it is meant to be integer &gt;= 0   (optional - if not given - upper limit is not applied)</li> <li>NOTE: if both <code>more_than</code> and <code>less_than</code> is not given - any number of matching detections will match the   sampling condition</li> <li><code>tags</code>: list of tags (each contains 1-64 characters from range <code>a-z, A-Z, 0-9, and -_:/.[]&lt;&gt;{}@</code>) (optional)</li> </ul>"},{"location":"enterprise/active-learning/detection_number/#example","title":"Example","text":"<pre><code>{\n  \"name\": \"multiple_detections\",\n  \"type\": \"detections_number_based\",\n  \"probability\": 0.2,\n  \"more_than\": 3,\n  \"tags\": [\"crowded\"],\n  \"limits\": [\n    { \"type\": \"minutely\", \"value\": 10 },\n    { \"type\": \"hourly\", \"value\": 100 },\n    { \"type\": \"daily\", \"value\": 1000 }\n  ]\n}\n</code></pre>"},{"location":"enterprise/active-learning/random_sampling/","title":"Random Sampling","text":"<p>Randomly select data to be saved for future labeling.</p> <p>Tip</p> <p>Review the Active Learning page for more information about how to use active learning.</p> <p>This strategy is available for the following model types:</p> <ul> <li><code>stub</code></li> <li><code>classification</code></li> <li><code>object-detection</code></li> <li><code>instance-segmentation</code></li> <li><code>keypoints-detection</code></li> </ul>"},{"location":"enterprise/active-learning/random_sampling/#configuration","title":"Configuration","text":"<ul> <li><code>name</code>: user-defined name of the strategy - must be non-empty and unique within all strategies defined in a   single configuration (required)</li> <li><code>type</code>: with value <code>random</code> is used to identify random sampling strategy (required)</li> <li><code>traffic_percentage</code>: float value in range [0.0, 1.0] defining the percentage of traffic to be persisted (required)</li> <li><code>tags</code>: list of tags (each contains 1-64 characters from range <code>a-z, A-Z, 0-9, and -_:/.[]&lt;&gt;{}@</code>) (optional)</li> <li><code>limits</code>: definition of limits for data collection within a specific strategy</li> </ul>"},{"location":"enterprise/active-learning/random_sampling/#example","title":"Example","text":"<p>Here is an example of a configuration manifest for random sampling strategy:</p> <pre><code>{\n  \"name\": \"my_random_sampling\",\n  \"type\": \"random\",\n  \"traffic_percentage\": 0.01,\n  \"tags\": [\"my_tag_1\", \"my_tag_2\"],\n  \"limits\": [\n    { \"type\": \"minutely\", \"value\": 10 },\n    { \"type\": \"hourly\", \"value\": 100 },\n    { \"type\": \"daily\", \"value\": 1000 }\n  ]\n}\n</code></pre>"},{"location":"fine-tuned/yolonas/","title":"YOLO-NAS","text":"<p>YOLO-NAS is a computer vision model architecture developed by Deci AI.</p>"},{"location":"fine-tuned/yolonas/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLO-NAS model types with Inference:</p> <ul> <li>Object Detection</li> <li>Keypoint Detection</li> </ul>"},{"location":"fine-tuned/yolonas/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLO-NAS model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolonas/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLO-NAS Object Detection Model</li> <li>Configure a YOLO-NAS Classification Model</li> <li>Configure a YOLO-NAS Segmentation Model</li> </ul>"},{"location":"fine-tuned/yolonas/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLO-NAS is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov10/","title":"YOLOv10","text":"<p>YOLOv10, released on May 23, 2024, is a real-time object detection model developed by researchers from Tsinghua University. YOLOv10 follows in the long-running series of YOLO models, created by authors from a wide variety of researchers and organizations.</p>"},{"location":"fine-tuned/yolov10/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv10 model types with Inference:</p> <ul> <li>Object Detection</li> </ul>"},{"location":"fine-tuned/yolov10/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv10 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov10/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv10 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov10/#see-also","title":"See Also","text":"<ul> <li>How to Train a YOLOv10 Model</li> <li>Deploy a YOLOv10 Model with Roboflow</li> </ul>"},{"location":"fine-tuned/yolov5/","title":"YOLOv5","text":"<p>YOLOv5 is a computer vision model architecture implemented in the <code>ultralytics</code> Python package.</p>"},{"location":"fine-tuned/yolov5/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv5 model types with Inference:</p> <ul> <li>Object Detection</li> <li>Classification</li> <li>Image Segmentation</li> </ul>"},{"location":"fine-tuned/yolov5/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv5 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov5/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLOv5 Object Detection Model</li> <li>Configure a YOLOv5 Classification Model</li> <li>Configure a YOLOv5 Segmentation Model</li> </ul>"},{"location":"fine-tuned/yolov5/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv5 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov7/","title":"YOLOv7","text":"<p>YOLOv7 is a computer vision model architecture introduced in the paper \"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors\".</p>"},{"location":"fine-tuned/yolov7/#supported-model-types","title":"Supported Model Types","text":"<ul> <li>Classification</li> </ul>"},{"location":"fine-tuned/yolov7/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv8 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov7/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Deploy a YOLOv7 Segmentation Model</li> </ul>"},{"location":"fine-tuned/yolov7/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv7 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov8/","title":"YOLOv8","text":"<p>YOLOv8 is a computer vision model architecture implemented in the <code>ultralytics</code> Python package.</p>"},{"location":"fine-tuned/yolov8/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv8 model types with Inference:</p> <ul> <li>Object Detection</li> <li>Classification</li> <li>Image Segmentation</li> <li>Keypoint Detection</li> </ul>"},{"location":"fine-tuned/yolov8/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv8 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov8/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLOv8 Object Detection Model</li> <li>Configure a YOLOv8 Classification Model</li> <li>Configure a YOLOv8 Segmentation Model</li> <li>Configure a YOLOv8 Keypoint Detection Model</li> </ul>"},{"location":"fine-tuned/yolov8/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv8 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov9/","title":"YOLOv9","text":"<p>YOLOv9 is a computer vision model architecture introduced in the paper \"YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information\".</p>"},{"location":"fine-tuned/yolov9/#supported-model-types","title":"Supported Model Types","text":"<p>You can deploy the following YOLOv9 model types with Inference:</p> <ul> <li>Object Detection</li> </ul>"},{"location":"fine-tuned/yolov9/#supported-inputs","title":"Supported Inputs","text":"<p>Click a link below to see instructions on how to run a YOLOv9 model on different inputs:</p> <ul> <li>Image</li> <li>Video, Webcam, or RTSP Stream</li> </ul>"},{"location":"fine-tuned/yolov9/#available-pretrained-models","title":"Available Pretrained Models","text":"<p>You may use YOLOv9 object detection models available on the Universe.</p>"},{"location":"fine-tuned/yolov9/#configure-your-deployment","title":"Configure Your Deployment","text":"<p>Starting from scratch? Use our Deployment Wizard to get a code snippet tailored to your device and use case.</p> <ul> <li>Configure a YOLOv9 Object Detection Model</li> </ul>"},{"location":"fine-tuned/yolov9/#license","title":"License","text":"<p>See our Licensing Guide for more information about how your use of YOLOv9 is licensed when using Inference to deploy your model.</p>"},{"location":"fine-tuned/yolov9/#see-also","title":"See Also","text":"<ul> <li>YOLOv9 on Roboflow Blog</li> </ul>"},{"location":"foundation/about/","title":"What is a Foundation Model?","text":"<p>Foundation models are machine learning models that have been trained on vast amounts of data to accomplish a specific task.</p> <p>For example, OpenAI trained CLIP, a foundation model. CLIP enables you to classify images. You can also compare the similarity of images and text with CLIP.</p> <p>The CLIP training process, which was run using over 400 million pairs of images and text, allowed the model to build an extensive range of knowledge, which can be applied to a range of domains.</p> <p>Foundation models are being built for a range of vision tasks, from image segmentation to classification to zero-shot object detection.</p> <p>Inference supports the following foundation models:</p> <ul> <li>Gaze (LC2S-Net): Detect the direction in which someone is looking.</li> <li>CLIP: Classify images and compare the similarity of images and text.</li> <li>DocTR: Read characters in images.</li> <li>Grounding DINO: Detect objects in images using text prompts.</li> <li>Segment Anything (SAM): Segment objects in images.</li> <li>CogVLM: A large multimodal model (LMM).</li> </ul> <p>All of these models can be used over a HTTP request with Inference. This means you don't need to spend time setting up and configuring each model.</p>"},{"location":"foundation/about/#how-are-foundation-models-used","title":"How Are Foundation Models Used?","text":"<p>Use cases vary depending on the foundation model with which you are working. For example, CLIP has been used extensively in the field of computer vision for tasks such as:</p> <ol> <li>Clustering images to identify groups of similar images and outliers;</li> <li>Classifying images;</li> <li>Moderating image content;</li> <li>Identifying if two images are too similar or too different, ideal for dataset management and cleaning;</li> <li>Building dataset search experiences,</li> <li>And more.</li> </ol> <p>Grounding DINO, on the other hand, can be used out-of-the-box to detect a range of objects. Or you can use Grounding DINO to automatically label data for use in training a smaller, faster object detection model that is fine-tuned to your use case.</p>"},{"location":"foundation/about/#how-to-use-foundation-models","title":"How to Use Foundation Models","text":"<p>The guides in this section walk through how to use each of the foundation models listed above with Inference. No machine learning experience is required to use each model. Our code snippets and accompanying reference material provide the knowledge you need to get started working with foundation models.</p>"},{"location":"foundation/clip/","title":"CLIP (Classification, Embeddings)","text":"<p>CLIP is a computer vision model that can measure the similarity between text and images.</p> <p>CLIP can be used for, among other things:</p> <ul> <li>Image classification</li> <li>Automated labeling for classification models</li> <li>Image clustering</li> <li>Gathering images for model training that are sufficiently dissimilar from existing samples</li> <li>Content moderation</li> </ul> <p>With Inference, you can calculate CLIP embeddings for images and text in real-time.</p> <p>In this guide, we will show:</p> <ol> <li>How to classify video frames with CLIP in real time, and;</li> <li>How to calculate CLIP image and text embeddings for use in clustering and comparison.</li> </ol>"},{"location":"foundation/clip/#how-can-i-use-clip-model-in-inference","title":"How can I use CLIP model in <code>inference</code>?","text":"<ul> <li>directly from <code>inference[clip]</code> package, integrating the model directly into your code</li> <li>using <code>inference</code> HTTP API (hosted locally, or on the Roboflow platform), integrating via HTTP protocol</li> <li>using <code>inference-sdk</code> package (<code>pip install inference-sdk</code>) and <code>InferenceHTTPClient</code></li> <li>creating custom code to make HTTP requests (see API Reference)</li> </ul>"},{"location":"foundation/clip/#supported-clip-versions","title":"Supported CLIP versions","text":"<ul> <li><code>clip/RN101</code></li> <li><code>clip/RN50</code></li> <li><code>clip/RN50x16</code></li> <li><code>clip/RN50x4</code></li> <li><code>clip/RN50x64</code></li> <li><code>clip/ViT-B-16</code></li> <li><code>clip/ViT-B-32</code></li> <li><code>clip/ViT-L-14-336px</code></li> <li><code>clip/ViT-L-14</code></li> </ul>"},{"location":"foundation/clip/#classify-video-frames","title":"Classify Video Frames","text":"<p>With CLIP, you can classify images and video frames without training a model. This is because CLIP has been pre-trained to recognize many different objects.</p> <p>To use CLIP to classify video frames, you need a prompt. In the example below, we will use the prompt \"cell phone\".</p> <p>We can compare the similarity of \"cell phone\" to each video frame and use that to classify the video frame.</p> <p>Below is a demo of CLIP classifying video frames in real time. The code for the example is below the video.</p> <p>First, install the Inference CLIP extension:</p> <pre><code>pip install \"inference[clip]\"\n</code></pre> <p>Then, create a new Python file and add the following code:</p> <pre><code>import cv2\nimport inference\nfrom inference.core.utils.postprocess import cosine_similarity\n\nfrom inference.models import Clip\nclip = Clip(model_id=\"clip/ViT-B-16\")  # `model_id` has default, but here is how to test other versions\n\nprompt = \"an ace of spades playing card\"\ntext_embedding = clip.embed_text(prompt)\n\ndef render(result, image):\n    # get the cosine similarity between the prompt &amp; the image\n    similarity = cosine_similarity(result[\"embeddings\"][0], text_embedding[0])\n\n    # scale the result to 0-100 based on heuristic (~the best &amp; worst values I've observed)\n    range = (0.15, 0.40)\n    similarity = (similarity-range[0])/(range[1]-range[0])\n    similarity = max(min(similarity, 1), 0)*100\n\n    # print the similarity\n    text = f\"{similarity:.1f}%\"\n    cv2.putText(image, text, (10, 310), cv2.FONT_HERSHEY_SIMPLEX, 12, (255, 255, 255), 30)\n    cv2.putText(image, text, (10, 310), cv2.FONT_HERSHEY_SIMPLEX, 12, (206, 6, 103), 16)\n\n    # print the prompt\n    cv2.putText(image, prompt, (20, 1050), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 10)\n    cv2.putText(image, prompt, (20, 1050), cv2.FONT_HERSHEY_SIMPLEX, 2, (206, 6, 103), 5)\n\n    # display the image\n    cv2.imshow(\"CLIP\", image)\n    cv2.waitKey(1)\n\n# start the stream\ninference.Stream(\n    source=\"webcam\",\n    model=clip,\n\n    output_channel_order=\"BGR\",\n    use_main_thread=True,\n\n    on_prediction=render\n)\n</code></pre> <p>Run the code to use CLIP on your webcam.</p> <p>Note: The model will take a minute or two to load. You will not see output while the model is loading.</p>"},{"location":"foundation/clip/#calculate-a-clip-embedding","title":"Calculate a CLIP Embedding","text":"<p>CLIP enables you to calculate embeddings. Embeddings are numeric, semantic representations of images and text. They are useful for clustering and comparison.</p> <p>You can use CLIP embeddings to compare the similarity of text and images.</p> <p>There are two types of CLIP embeddings: image and text.</p> <p>Below we show how to calculate, then compare, both types of embeddings.</p>"},{"location":"foundation/clip/#image-embedding","title":"Image Embedding","text":"<p>Tip</p> <p>In this example, we assume <code>inference-sdk</code> package installed <pre><code>pip install inference-sdk\n</code></pre></p> <p>In the code below, we calculate an image embedding.</p> <p>Create a new Python file and add this code:</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\nembeddings = CLIENT.get_clip_image_embeddings(inference_input=\"https://media.roboflow.com/inference/people-walking.jpg\")\nprint(embeddings)\n\n# since release `0.9.17`, you may pass extra argument `clip_version` to get_clip_image_embeddings(...) to select\n# model version\n</code></pre>"},{"location":"foundation/clip/#text-embedding","title":"Text Embedding","text":"<p>In the code below, we calculate a text embedding.</p> <p>Tip</p> <p>In this example, we assume <code>inference-sdk</code> package installed <pre><code>pip install inference-sdk\n</code></pre></p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\nembeddings = CLIENT.get_clip_text_embeddings(text=\"the quick brown fox jumped over the lazy dog\")\nprint(embeddings)\n\n# since release `0.9.17`, you may pass extra argument `clip_version` to get_clip_text_embeddings(...) to select\n# model version\n</code></pre>"},{"location":"foundation/clip/#compare-embeddings","title":"Compare Embeddings","text":"<p>To compare embeddings for similarity, you can use cosine similarity.</p> <p>The code you need to compare image and text embeddings is the same.</p> <p>Tip</p> <p>In this example, we assume <code>inference-sdk</code> package installed <pre><code>pip install inference-sdk\n</code></pre></p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\nresult = CLIENT.clip_compare(\n  subject=\"./image.jpg\",\n  prompt=[\"dog\", \"cat\"]\n)\nprint(result)\n# since release `0.9.17`, you may pass extra argument `clip_version` to clip_compare(...) to select\n# model version\n</code></pre> <p>The resulting number will be between 0 and 1. The higher the number, the more similar the image and text are.</p>"},{"location":"foundation/clip/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of CLIP.</p> <ul> <li>CLIP Embed Images: 0.5 seconds per inference (59.55 seconds for 100 inferences).</li> <li>CLIP Embed Text: 0.5 seconds per inference (51.52 seconds for 100 inferences).</li> <li>CLIP Compare Image and Text: 0.58 seconds per inference (58.03 seconds for 100 inferences).</li> </ul>"},{"location":"foundation/clip/#see-also","title":"See Also","text":"<ul> <li>What is CLIP?</li> <li>Build an Image Search Engine with CLIP and Faiss</li> <li>Build a Photo Memories App with CLIP</li> <li>Analyze and Classify Video with CLIP</li> </ul>"},{"location":"foundation/cogvlm/","title":"CogVLM (Multimodal Language Model)","text":"<p>CogVLM is a Large Multimodal Model (LMM). CogVLM is available for use in Inference.</p> <p>You can ask CogVLM questions about the contents of an image and retrieve a text response.</p>"},{"location":"foundation/cogvlm/#model-quantization","title":"Model Quantization","text":"<p>You can run CogVLM through Roboflow Inference with three degrees of quantization. Quantization allows you to make a model smaller, but there is an accuracy trade-off. The three degrees of quantization are:</p> <ul> <li>No quantization: Run the full model. For this, you will need 80 GB of RAM. You could run the model on an 80 GB NVIDIA A100.</li> <li>8-bit quantization: Run the model with less accuracy than no quantization. You will need 32 GB of RAM.You could run this model on an A100 with sufficient virtual RAM.</li> <li>4-bit quantization: Run the model with less accuracy than 8-bit quantization. You will need 16 GB of RAM. You could run this model on an NVIDIA T4.</li> </ul>"},{"location":"foundation/cogvlm/#use-cogvlm-with-inference","title":"Use CogVLM with Inference","text":"<p>To use CogVLM with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. </p> <p>Then, retrieve your API key from the Roboflow dashboard. Learn how to retrieve your API key.</p> <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>We recommend using CogVLM paired with inference HTTP API adjusted to run in GPU environment. It's easy to set up  with our <code>inference-cli</code> tool. Run the following command to set up environment and run the API under  <code>http://localhost:9001</code></p> <p>Warning</p> <p>Make sure that you are running this at machine with an NVidia GPU! Otherwise CogVLM will not be available.</p> <pre><code>pip install inference inference-cli inference-sdk\ninference server start\n</code></pre> <p>Let's ask a question about the following image:</p> <p></p> <p>Use <code>inference-sdk</code> to prompt the model:</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nresult = CLIENT.prompt_cogvlm(\n    visual_prompt=\"./forklift.jpg\",\n    text_prompt=\"Is there a forklift close to a conveyor belt?\",\n)\nprint(result)\n</code></pre> <p>Above, replace <code>forklift.jpeg</code> with the path to the image in which you want to detect objects.</p> <p>Let's use the prompt \"Is there a forklift close to a conveyor belt?\u201d\"</p> <p>The results of CogVLM will appear in your terminal:</p> <pre><code>{\n    'response': 'yes, there is a forklift close to a conveyor belt, and it appears to be transporting a stack of items onto it.',\n    'time': 12.89864671198302\n}\n</code></pre> <p>CogVLM successfully answered our question, noting there is a forklift close to the conveyor belt in the image.</p>"},{"location":"foundation/cogvlm/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of CogVLM.</p> <p>CogVLM ran 100 inferences in 365.22 seconds (11.69 seconds per inference, on average).</p>"},{"location":"foundation/cogvlm/#see-also","title":"See Also","text":"<ul> <li>How to deploy CogVLM</li> </ul>"},{"location":"foundation/doctr/","title":"DocTR (OCR)","text":"<p>DocTR is an Optical Character Recognition (OCR) model.</p> <p>You can use DocTR to read the text in an image.</p> <p>To use DocTR with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. </p> <p>Then, retrieve your API key from the Roboflow dashboard. Learn how to retrieve your API key.</p> <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Let's retrieve the text in the following image:</p> <p></p> <p>Create a new Python file and add the following code:</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://infer.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nresult = CLIENT.ocr_image(inference_input=\"./container.jpg\")  # single image request\nprint(result)\n</code></pre> <p>Above, replace <code>container.jpeg</code> with the path to the image in which you want to detect objects.</p> <p>The results of DocTR will appear in your terminal:</p> <pre><code>{'result': '', 'time': 3.98263641900121, 'result': 'MSKU 0439215', 'time': 3.870879542999319}\n</code></pre>"},{"location":"foundation/doctr/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of DocTR.</p> <p>DocTR ran 100 inferences in 365.22 seconds (3.65 seconds per inference, on average).</p>"},{"location":"foundation/doctr/#see-also","title":"See Also","text":"<ul> <li>How to detect text in images with OCR</li> </ul>"},{"location":"foundation/florence2/","title":"Florence-2","text":"<p>Florence-2 is a multimodal model developed by Microsoft Research.</p> <p>You can use Florence-2 for:</p> <ol> <li>Object detection: Identify the location of all objects in an image. (<code>&lt;OD&gt;</code>)</li> <li>Dense region captioning: Generate dense captions for all identified regions in an image. (<code>&lt;DENSE_REGION_CAPTION&gt;</code>)</li> <li>Image captioning: Generate a caption for a whole image. (<code>&lt;CAPTION&gt;</code> for a short caption, <code>&lt;DETAILED_CAPTION&gt;</code> for a more detailed caption, and <code>&lt;MORE_DETAILED_CAPTION&gt;</code> for an even more detailed caption)</li> <li>Region proposal: Identify regions where there are likely to be objects in an image. (<code>&lt;REGION_PROPOSAL&gt;</code>)</li> <li>Phrase grounding: Identify the location of objects that match a text description. (<code>&lt;CAPTION_TO_PHRASE_GROUNDING&gt;</code>)</li> <li>Referring expression segmentation: Identify a segmentation mask that corresponds with a text input. (<code>&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;</code>)</li> <li>Region to segmentation: Calculate a segmentation mask for an object from a bounding box region. (<code>&lt;REGION_TO_SEGMENTATION&gt;</code>)</li> <li>Open vocabulary detection: Identify the location of objects that match a text prompt. (<code>&lt;OPEN_VOCABULARY_DETECTION&gt;</code>)</li> <li>Region to description: Generate a description for a region in an image. (<code>&lt;REGION_TO_DESCRIPTION&gt;</code>)</li> <li>Optical Character Recognition (OCR): Read the text in an image. (<code>&lt;OCR&gt;</code>)</li> <li>OCR with region: Read the text in a specific region in an image. (<code>&lt;OCR_WITH_REGION&gt;</code>)</li> </ol> <p>You can use Inference for all the Florence-2 tasks above.</p> <p>The text in the parentheses are the task prompts you will need to use each task.</p>"},{"location":"foundation/florence2/#how-to-use-florence-2","title":"How to Use Florence-2","text":"Install <code>inference</code> <p>To install <code>inference</code> with Florence 2 support use the following command on CPU machine:</p> <pre><code>pip install inference[transformers]\n</code></pre> <p>or the following one for GPU machine:</p> <pre><code>pip install inference-gpu[transformers]\n</code></pre> <p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>from inference import get_model\n\nmodel = get_model(\"florence-2-base\", api_key=\"API_KEY\")\n\nresult = model.infer(\n    \"https://media.roboflow.com/inference/seawithdock.jpeg\", \n    prompt=\"&lt;CAPTION&gt;\",\n)\n\nprint(result[0].response)\n</code></pre> <p>Above, replace <code>&lt;CAPTION&gt;</code> with the name of the task you want to use.</p> <p>Replace <code>API_KEY</code> with your Roboflow API key. Learn how to retrieve your Roboflow API key</p> <p>To use PaliGemma with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account.</p> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from your model will be printed to the console.</p>"},{"location":"foundation/gaze/","title":"L2CS-Net (Gaze Detection)","text":"<p>L2CS-Net is a gaze estimation model.</p> <p>You can detect the direction in which someone is looking using the L2CS-Net model.</p>"},{"location":"foundation/gaze/#how-to-use-l2cs-net","title":"How to Use L2CS-Net","text":"<p>To use L2CS-Net with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>L2CS-Net accepts an image and returns pitch and yaw values that you can use to:</p> <ol> <li>Figure out the direction in which someone is looking, and;</li> <li>Estimate, roughly, where someone is looking.</li> </ol> <p>We recommend using L2CS-Net paired with inference HTTP API. It's easy to set up with our <code>inference-cli</code> tool. Run the  following command to set up environment and run the API under <code>http://localhost:9001</code></p> <pre><code>pip install inference inference-cli inference-sdk\ninference server start  # this starts server under http://localhost:9001\n</code></pre> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nCLIENT.detect_gazes(inference_input=\"./image.jpg\")  # single image request\n</code></pre> <p>Above, replace <code>image.jpg</code> with the image in which you want to detect gazes.</p> <p>The code above makes two assumptions:</p> <ol> <li>Faces are roughly one meter away from the camera.</li> <li>Faces are roughly 250mm tall.</li> </ol> <p>These assumptions are a good starting point if you are using a computer webcam with L2CS-Net, where people in the frame are likely to be sitting at a desk.</p> <p>On the first run, the model will be downloaded. On subsequent runs, the model will be cached locally and loaded from the cache. It will take a few moments for the model to download.</p> <p>The results of L2CS-Net will appear in your terminal:</p> <pre><code>[{'face': {'x': 1107.0, 'y': 1695.5, 'width': 1056.0, 'height': 1055.0, 'confidence': 0.9355756640434265, 'class': 'face', 'class_confidence': None, 'class_id': 0, 'tracker_id': None, 'landmarks': [{'x': 902.0, 'y': 1441.0}, {'x': 1350.0, 'y': 1449.0}, {'x': 1137.0, 'y': 1692.0}, {'x': 1124.0, 'y': 1915.0}, {'x': 625.0, 'y': 1551.0}, {'x': 1565.0, 'y': 1571.0}]}, 'yaw': -0.04104889929294586, 'pitch': 0.029525401070713997}]\n</code></pre> <p>We have created a full gaze detection example that shows how to:</p> <ol> <li>Use L2CS-Net with a webcam;</li> <li>Calculate the direction in which and point in space at which someone is looking;</li> <li>Calculate what quadrant of the screen someone is looking at, and;</li> <li>Annotate the image with the direction someone is looking.</li> </ol> <p>This example will let you run L2CS-Net and see the results of the model in real time. Here is an recording of the example working:</p> <p>Learn how to set up the example.</p>"},{"location":"foundation/gaze/#l2cs-net-inference-response","title":"L2CS-Net Inference Response","text":"<p>Here is the structure of the data returned by a gaze request:</p> <pre><code>[{'face': {'class': 'face',\n           'class_confidence': None,\n           'class_id': 0,\n           'confidence': 0.9355756640434265,\n           'height': 1055.0,\n           'landmarks': [{'x': 902.0, 'y': 1441.0},\n                         {'x': 1350.0, 'y': 1449.0},\n                         {'x': 1137.0, 'y': 1692.0},\n                         {'x': 1124.0, 'y': 1915.0},\n                         {'x': 625.0, 'y': 1551.0},\n                         {'x': 1565.0, 'y': 1571.0}],\n           'tracker_id': None,\n           'width': 1056.0,\n           'x': 1107.0,\n           'y': 1695.5},\n  'pitch': 0.029525401070713997,\n  'yaw': -0.04104889929294586}]\n</code></pre>"},{"location":"foundation/gaze/#see-also","title":"See Also","text":"<ul> <li>Gaze Detection and Eye Tracking: A How-To Guide</li> </ul>"},{"location":"foundation/grounding_dino/","title":"Grounding DINO (Object Detection)","text":"<p>Grounding DINO is a zero-shot object detection model.</p> <p>You can use Grounding DINO to identify objects in images and videos using arbitrary text prompts.</p> <p>To use Grounding DINO effectively, we recommend experimenting with the model to understand which text prompts help achieve the desired results.</p> <p>Note</p> <p>Grounding DINO is most effective at identifying common objects (i.e. cars, people, dogs, etc.). It is less effective at identifying uncommon objects (i.e. a specific type of car, a specific person, a specific dog, etc.).</p>"},{"location":"foundation/grounding_dino/#how-to-use-grounding-dino","title":"How to Use Grounding DINO","text":"<p>First, install the Inference Grounding DINO extension:</p> <pre><code>pip install \"inference[grounding-dino]\"\n</code></pre> <p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>from inference.models.grounding_dino import GroundingDINO\n\nmodel = GroundingDINO(api_key=\"\")\n\nresults = model.infer(\n    {\n        \"image\": {\n            \"type\": \"url\",\n            \"value\": \"https://media.roboflow.com/fruit.png\",\n        },\n        \"text\": [\"apple\"],\n\n        # Optional params\n        \"box_threshold\": 0.5\n        \"text_threshold\": 0.5\n    }\n)\n\nprint(results.json())\n</code></pre> <p>In this code, we load Grounding DINO, run Grounding DINO on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>apple</code> with the object you want to detect.</li> <li><code>fruit.png</code> with the path to the image in which you want to detect objects.</li> </ol> <p>Additionally, you can tweak the optional <code>box_threshold</code> and <code>class_threshold</code> params for your specific use case. Both values default to 0.5 if not set. See the Grounding DINO README for an explanation of the model's thresholds.</p> <p>To use Grounding DINO with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The predictions from your model will be printed to the console.</p>"},{"location":"foundation/owlv2/","title":"OwlV2 (Object Detection)","text":"<p>OWLv2 is an open set object detectio model trained by Google. OWLv2 was primarily trained to detect objects from text. The implementation in <code>Inference</code> currently only supports detecting objects from visual examples of that object.</p>"},{"location":"foundation/owlv2/#installation","title":"Installation","text":"<p>To install inference with the extra dependencies necessary to run OWLv2, run</p> <p><code>pip install inference[transformers]</code></p> <p>or</p> <p><code>pip install inference-gpu[transformers]</code></p>"},{"location":"foundation/owlv2/#how-to-use-owlv2","title":"How to Use OWLv2","text":"<p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import inference\nfrom inference.models.owlv2.owlv2 import OwlV2\nfrom inference.core.entities.requests.owlv2 import OwlV2InferenceRequest\nfrom PIL import Image\nimport io\nimport base64\n\nmodel = OwlV2()\n\n\nim_url = \"https://media.roboflow.com/inference/seawithdock.jpeg\"\nimage = {\n    \"type\": \"url\",\n    \"value\": im_url\n}\nrequest = OwlV2InferenceRequest(\n    image=image,\n    training_data=[\n        {\n            \"image\": image,\n            \"boxes\": [{\"x\": 223, \"y\": 306, \"w\": 40, \"h\": 226, \"cls\": \"post\"}],\n        }\n    ],\n    visualize_predictions=True,\n    confidence=0.9999,\n)\n\nresponse = OwlV2().infer_from_request(request)\n\ndef load_image_from_base64(base64_str):\n    image = Image.open(io.BytesIO(base64_str))\n    return image\n\nvisualization = load_image_from_base64(response.visualization)\nvisualization.save(\"owlv2_visualization.jpg\")\n</code></pre> <p>In this code, we run OWLv2 on an image, using example objects from that image. Above, replace:</p> <ol> <li><code>training_data</code> with the locations of the objects you want to detect.</li> <li><code>im_url</code> with the image you would like to perform inference on.</li> </ol> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from your model will be save to disk at <code>owlv2_visualization.jpg</code></p> <p>Note the blue bounding boxes surrounding each pole of the dock.</p> <p></p>"},{"location":"foundation/paligemma/","title":"PaliGemma","text":"<p>PaliGemma is a large multimodal model developed by Google Research.</p> <p>You can use PaliGemma to:</p> <ol> <li>Ask questions about images (Visual Question Answering)</li> <li>Identify the location of objects in an image (object detection)</li> <li>Identify the precise location of objects in an imageh (image segmentation)</li> </ol> <p>You can deploy PaliGemma object detection models with Inference, and use PaliGemma for object detection.</p>"},{"location":"foundation/paligemma/#installation","title":"Installation","text":"<p>To install inference with the extra dependencies necessary to run PaliGemma, run</p> <p><code>pip install inference[transformers]</code></p> <p>or</p> <p><code>pip install inference-gpu[transformers]</code></p>"},{"location":"foundation/paligemma/#how-to-use-paligemma-vqa","title":"How to Use PaliGemma (VQA)","text":"<p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import inference\n\nfrom inference.models.paligemma.paligemma import PaliGemma\n\npg = PaliGemma(\"paligemma-3b-mix-224\", api_key=\"YOUR ROBOFLOW API KEY\")\n\nfrom PIL import Image\n\nimage = Image.open(\"image.jpeg\") # Change to your image\n\nprompt = \"How many dogs are in this image?\"\n\nresult = pg.predict(image,prompt)\n</code></pre> <p>In this code, we load PaliGemma run PaliGemma on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>prompt</code> with the prompt for the model.</li> <li><code>image.jpeg</code> with the path to the image in which you want to detect objects.</li> </ol> <p>To use PaliGemma with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account.</p> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from your model will be printed to the console.</p>"},{"location":"foundation/paligemma/#how-to-use-paligemma-object-detection","title":"How to Use PaliGemma (Object Detection)","text":"<p>Create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import os\nimport transformers\nimport re\nimport numpy as np\nimport supervision as sv\nfrom typing import Tuple, List, Optional\nfrom PIL import Image\n\nimage = Image.open(\"/content/data/dog.jpeg\")\n\ndef from_pali_gemma(response: str, resolution_wh: Tuple[int, int], class_list: Optional[List[str]] = None) -&gt; sv.Detections:\n    _SEGMENT_DETECT_RE = re.compile(\n        r'(.*?)' +\n        r'&lt;loc(\\d{4})&gt;' * 4 + r'\\s*' +\n        '(?:%s)?' % (r'&lt;seg(\\d{3})&gt;' * 16) +\n        r'\\s*([^;&lt;&gt;]+)? ?(?:; )?',\n    )\n\n    width, height = resolution_wh\n    xyxy_list = []\n    class_name_list = []\n\n    while response:\n        m = _SEGMENT_DETECT_RE.match(response)\n        if not m:\n            break\n\n        gs = list(m.groups())\n        before = gs.pop(0)\n        name = gs.pop()\n        y1, x1, y2, x2 = [int(x) / 1024 for x in gs[:4]]\n        y1, x1, y2, x2 = map(round, (y1*height, x1*width, y2*height, x2*width))\n\n        content = m.group()\n        if before:\n            response = response[len(before):]\n            content = content[len(before):]\n\n        xyxy_list.append([x1, y1, x2, y2])\n        class_name_list.append(name.strip())\n        response = response[len(content):]\n\n    xyxy = np.array(xyxy_list)\n    class_name = np.array(class_name_list)\n\n    if class_list is None:\n        class_id = None\n    else:\n        class_id = np.array([class_list.index(name) for name in class_name])\n\n    return sv.Detections(\n        xyxy=xyxy,\n        class_id=class_id,\n        data={'class_name': class_name}\n    )\n\nprompt = \"detect person; car; backpack\"\nresponse = pali_gemma.predict(image, prompt)[0]\nprint(response)\n\ndetections = from_pali_gemma(response=response, resolution_wh=image.size, class_list=['person', 'car', 'backpack'])\n\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotatrd_image = bounding_box_annotator.annotate(image, detections)\nannotatrd_image = label_annotator.annotate(annotatrd_image, detections)\nsv.plot_image(annotatrd_image)\n</code></pre> <p>In this code, we load PaliGemma run PaliGemma on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>prompt</code> with the prompt for the model.</li> <li><code>image.jpeg</code> with the path to the image in which you want to detect objects.</li> </ol> <p>To use PaliGemma with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account.</p> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from the model will be displayed:</p> <p></p>"},{"location":"foundation/sam/","title":"Segment Anything (Segmentation)","text":"<p>Segment Anything is an open source image segmentation model.</p> <p>You can use Segment Anything to identify the precise location of objects in an image.</p> <p>To use Segment Anything, you need to:</p> <ol> <li>Create an embedding for an image, and;</li> <li>Specify the coordinates of the object you want to segment.</li> </ol>"},{"location":"foundation/sam/#how-to-use-segment-anything","title":"How to Use Segment Anything","text":"<p>To use Segment Anything with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre>"},{"location":"foundation/sam/#embed-an-image","title":"Embed an Image","text":"<p>An embedding is a numeric representation of an image. SAM uses embeddings as input to calcualte the location of objects in an image.</p> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\n\ninfer_payload = {\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": \"https://i.imgur.com/Q6lDy8B.jpg\",\n    },\n    \"image_id\": \"example_image_id\",\n}\n\nbase_url = \"http://localhost:9001\"\n\n# Define your Roboflow API Key\napi_key = \"YOUR ROBOFLOW API KEY\"\n\nres = requests.post(\n    f\"{base_url}/sam/embed_image?api_key={api_key}\",\n    json=infer_payload,\n)\n\nembeddings = res.json()['embeddings']\n</code></pre> <p>This code makes a request to Inference to embed an image using SAM.</p> <p>The <code>example_image_id</code> is used to cache the embeddings for later use so you don't have to send them back in future segmentation requests.</p>"},{"location":"foundation/sam/#segment-an-object","title":"Segment an Object","text":"<p>To segment an object, you need to know at least one point in the image that represents the object that you want to use.</p> <p>For testing with a single image, you can upload an image to the Polygon Zone web interface and hover over a point in the image to see the coordinates of that point.</p> <p>You may also opt to use an object detection model to identify an object, then use the center point of the bounding box as a prompt for segmentation.</p> <p>Create a new Python file and add the following code:</p> <pre><code>#Define request payload\ninfer_payload = {\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": \"https://i.imgur.com/Q6lDy8B.jpg\",\n    },\n    \"point_coords\": [[380, 350]],\n    \"point_labels\": [1],\n    \"image_id\": \"example_image_id\",\n}\n\nres = requests.post(\n    f\"{base_url}/sam/embed_image?api_key={api_key}\",\n    json=infer_clip_payload,\n)\n\nmasks = request.json()['masks']\n</code></pre> <p>This request returns segmentation masks that represent the object of interest.</p>"},{"location":"foundation/sam/#see-also","title":"See Also","text":"<ul> <li>What is Segment Anything Model (SAM)?</li> </ul>"},{"location":"foundation/sam2/","title":"Segment Anything 2 (Segmentation)","text":"<p>Segment Anything 2 is an open source image segmentation model.</p> <p>You can use Segment Anything 2 to identify the precise location of objects in an image. This process can generate masks for objects in an image iteratively, by specifying points to be included or discluded from the segmentation mask.</p>"},{"location":"foundation/sam2/#how-to-use-segment-anything","title":"How to Use Segment Anything","text":"<p>To use Segment Anything 2 with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard.</p>"},{"location":"foundation/sam2/#how-to-use-sam2-locally-with-inference","title":"How To Use SAM2 Locally With Inference","text":"<p>We will follow along with the example located at <code>examples/sam2/sam2_example.py</code>.</p> <p>We start with the following image,</p> <p></p> <p>compute the most prominent mask,</p> <p></p> <p>and negative prompt the wrist to obtain only the fist.</p> <p></p>"},{"location":"foundation/sam2/#running-within-docker","title":"Running within docker","text":"<p>Build the dockerfile (make sure your cwd is at the root of inference) with <pre><code>docker build -f docker/dockerfiles/Dockerfile.sam2 -t sam2 .\n</code></pre></p> <p>Start up an interactive terminal with <pre><code>docker run -it --rm --entrypoint bash -v $(pwd)/scratch/:/app/scratch/ -v /tmp/cache/:/tmp/cache/ -v $(pwd)/inference/:/app/inference/ --gpus=all --net=host sam2\n</code></pre> You can save files to <code>/app/scratch/</code> to use them on the host device.</p> <p>Or, start a sam2 server with <pre><code>docker run -it --rm -v /tmp/cache/:/tmp/cache/ -v $(pwd)/inference/:/app/inference/ --gpus=all --net=host sam2\n</code></pre></p> <p>and interact over http.</p>"},{"location":"foundation/sam2/#imports","title":"Imports","text":"<p>Set up your api key, and install Segment Anything 2</p> <p>Note</p> <p>There's currently a problem with sam2 + flash attention on certain gpus, like the L4 or A100. Use the fix in the posted thread, or use the docker image we provide for sam2. </p> <pre><code>import os\n\nos.environ[\"API_KEY\"] = \"&lt;YOUR-API-KEY&gt;\"\nfrom inference.models.sam2 import SegmentAnything2\nfrom inference.core.utils.postprocess import masks2poly\nfrom inference.core.entities.requests.sam2 import Sam2PromptSet\nimport supervision as sv\nfrom PIL import Image\nimport numpy as np\n\nimage_path = \"./examples/sam2/hand.png\"\n</code></pre>"},{"location":"foundation/sam2/#model-loading","title":"Model Loading","text":"<p>Load the model with  <pre><code>m = SegmentAnything2(model_id=\"sam2/hiera_large\")\n</code></pre></p> <p>Other values for <code>model_id</code> are <code>\"hiera_small\", \"hiera_large\", \"hiera_tiny\", \"hiera_b_plus\"</code>.</p>"},{"location":"foundation/sam2/#compute-the-most-prominent-mask","title":"Compute the Most Prominent Mask","text":"<p><pre><code># call embed_image before segment_image to precompute embeddings\nembedding, img_shape, id_ = m.embed_image(image_path)\n\n# segments image using cached embedding if it exists, else computes it on the fly\nraw_masks, raw_low_res_masks = m.segment_image(image_path)\n\n# convert binary masks to polygons\nraw_masks = raw_masks &gt;= m.predictor.mask_threshold\npoly_masks = masks2poly(raw_masks)\n</code></pre> Note that you can embed the image as soon as you know you want to process it, and the embeddings are cached automatically for faster downstream processing.</p> <p>The resulting mask will look like this:</p> <p></p>"},{"location":"foundation/sam2/#negative-prompt-the-model","title":"Negative Prompt the Model","text":"<p><pre><code>point = [250, 800]\nlabel = False\n# give a negative point (point_label 0) or a positive example (point_label 1)\nprompt = Sam2PromptSet(\n    prompts=[{\"points\": [{\"x\": point[0], \"y\": point[1], \"positive\": label}]}]\n)\n\n# uses cached masks from prior call\n\nraw_masks2, raw_low_res_masks2 = m.segment_image(\n    image_path,\n    prompts=prompt,\n)\n\nraw_masks2 = raw_masks2 &gt;= m.predictor.mask_threshold\nraw_masks2 = raw_masks2[0]\n</code></pre> Here we tell the model that the cached mask should not include the wrist.</p> <p>The resulting mask will look like this:</p> <p></p>"},{"location":"foundation/sam2/#annotate","title":"Annotate","text":"<p>Use Supervision to draw the results of the model.</p> <pre><code>image = np.array(Image.open(image_path).convert(\"RGB\"))\n\nmask_annotator = sv.MaskAnnotator()\ndot_annotator = sv.DotAnnotator()\n\ndetections = sv.Detections(\n    xyxy=np.array([[0, 0, 100, 100]]), mask=np.array([raw_masks])\n)\ndetections.class_id = [i for i in range(len(detections))]\nannotated_image = mask_annotator.annotate(image.copy(), detections)\nim = Image.fromarray(annotated_image)\nim.save(\"sam.png\")\n\ndetections = sv.Detections(\n    xyxy=np.array([[0, 0, 100, 100]]), mask=np.array([raw_masks2])\n)\ndetections.class_id = [i for i in range(len(detections))]\nannotated_image = mask_annotator.annotate(image.copy(), detections)\n\ndot_detections = sv.Detections(\n    xyxy=np.array([[point[0] - 1, point[1] - 1, point[0] + 1, point[1] + 1]]),\n    class_id=np.array([1]),\n)\nannotated_image = dot_annotator.annotate(annotated_image, dot_detections)\nim = Image.fromarray(annotated_image)\nim.save(\"sam_negative_prompted.png\")\n</code></pre>"},{"location":"foundation/sam2/#how-to-use-sam2-with-a-local-docker-container-http-server","title":"How To Use SAM2 With a Local Docker Container HTTP Server","text":""},{"location":"foundation/sam2/#build-and-start-the-server","title":"Build and Start The Server","text":"<p>Build the dockerfile (make sure your cwd is at the root of inference) with <pre><code>docker build -f docker/dockerfiles/Dockerfile.sam2 -t sam2 .\n</code></pre> and start a sam2 server with <pre><code>docker run -it --rm -v /tmp/cache/:/tmp/cache/ -v $(pwd)/inference/:/app/inference/ --gpus=all --net=host sam2\n</code></pre></p>"},{"location":"foundation/sam2/#embed-an-image","title":"Embed an Image","text":"<p>An embedding is a numeric representation of an image. SAM uses embeddings as input to calcualte the location of objects in an image.</p> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\n\ninfer_payload = {\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": \"https://i.imgur.com/Q6lDy8B.jpg\",\n    },\n    \"image_id\": \"example_image_id\",\n}\n\nbase_url = \"http://localhost:9001\"\n\n# Define your Roboflow API Key\napi_key = \"YOUR ROBOFLOW API KEY\"\n\nres = requests.post(\n    f\"{base_url}/sam2/embed_image?api_key={api_key}\",\n    json=infer_payload,\n)\n</code></pre> <p>This code makes a request to Inference to embed an image using SAM.</p> <p>The <code>example_image_id</code> is used to cache the embeddings for later use so you don't have to send them back in future segmentation requests.</p>"},{"location":"foundation/sam2/#segment-an-object","title":"Segment an Object","text":"<p>To segment an object, you need to know at least one point in the image that represents the object that you want to use.</p> <p>For testing with a single image, you can upload an image to the Polygon Zone web interface and hover over a point in the image to see the coordinates of that point.</p> <p>You may also opt to use an object detection model to identify an object, then use the center point of the bounding box as a prompt for segmentation.</p> <p>Create a new Python file and add the following code:</p> <pre><code>#Define request payload\ninfer_payload = {\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": \"https://i.imgur.com/Q6lDy8B.jpg\",\n    },\n    \"point_coords\": [[380, 350]],\n    \"point_labels\": [1],\n    \"image_id\": \"example_image_id\",\n}\n\nres = requests.post(\n    f\"{base_url}/sam2/embed_image?api_key={api_key}\",\n    json=infer_payload,\n)\n\nmasks = request.json()['masks']\n</code></pre> <p>This request returns segmentation masks that represent the object of interest.</p>"},{"location":"foundation/trocr/","title":"TrOCR (OCR)","text":"<p>TrOCR is a transformer-based model for text recognition, otherwise known as Optical Character Recognition (OCR).</p> <p>TrOCR works best on focused, single-line printed text.</p> <p>Be sure to use with cropped images since unlike some other OCR models, TrOCR will not perform well on uncropped or multi-line text.</p> <p>Let's try running TrOCR on this image:</p> <p></p> <p>Note</p> <p>TROCR model is only supported in <code>inference</code> Python package and <code>inference</code> server deployed locally (excluding Roboflow Hosted Platform).</p> <p>To run the example, start <code>inference</code> server locally:</p> <pre><code>inference server start\n</code></pre> <p>Make sure you have <code>inference-cli</code> installed - if that's not the case run:</p> <pre><code>pip install inference-cli\n</code></pre> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(api_url=\"http://127.0.0.1:9001\")\n\nresult = CLIENT.ocr_image(inference_input=\"./serial_number.png\", model=\"trocr\")  # single image request\nprint(result)\n</code></pre>"},{"location":"foundation/yolo_world/","title":"YOLO-World (Object Detection)","text":"<p>YOLO-World is a zero-shot object detection model.</p> <p>You can use YOLO-World to identify objects in images and videos using arbitrary text prompts.</p> <p>To use YOLO-World effectively, we recommend experimenting with the model to understand which text prompts help achieve the desired results.</p> <p>YOLO World is faster than many other zero-shot object detection models like YOLO-World. On powerful hardware like a V100 GPU, YOLO World can run in real-time.</p> <p>Note</p> <p>YOLO-World, like most state-of-the-art zero-shot detection models, is most effective at identifying common objects (i.e. cars, people, dogs, etc.). It is less effective at identifying uncommon objects (i.e. a specific type of car, a specific person, a specific dog, etc.).</p> <p>Note</p> <p>In <code>inference</code> package YOLO-World models are identified by <code>yolo_world/&lt;version&gt;</code>, where <code>&lt;version&gt;</code> can be one of the following: <code>s</code>, <code>m</code>, <code>l</code>, <code>x</code>, <code>v2-s</code>, <code>v2-m</code>, <code>v2-l</code>, <code>v2-x</code>. Versions <code>v2-...</code> denote newer models, with improved evaluation metrics.</p>"},{"location":"foundation/yolo_world/#how-to-use-yolo-world","title":"How to Use YOLO-World","text":"Inference Python LibraryInference Server HTTP APIInference Pipeline (Video) <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import cv2\nimport supervision as sv\n\nfrom inference.models.yolo_world.yolo_world import YOLOWorld\n\nimage = cv2.imread(\"image.jpeg\")\n\nmodel = YOLOWorld(model_id=\"yolo_world/l\")\nclasses = [\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"]\nresults = model.infer(\"image.jpeg\", text=classes, confidence=0.03)[0]\n\ndetections = sv.Detections.from_inference(results)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [classes[class_id] for class_id in detections.class_id]\n\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections\n)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\nsv.plot_image(annotated_image)\n</code></pre> <p>Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, you will need to set up an Inference server to use the YOLO World HTTP API.</p> <p>To do this, run:</p> <pre><code>pip install inference inference-sdk\ninference server start\n</code></pre> <p>Then, create a new Python file called <code>app.py</code> and add the following code:</p> <pre><code>import os\nimport cv2\nimport supervision as sv\n\nfrom inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://127.0.0.1:9001\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\n\nresults = client.infer_from_yolo_world(\n    inference_input=[\"https://media.roboflow.com/dog.jpeg\"],\n    class_names=[\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"],\n    model_version=\"l\",\n    confidence=0.1,\n)[0]\n\ndetections = sv.Detections.from_inference(results)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [classes[class_id] for class_id in detections.class_id]\n\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections\n)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\nsv.plot_image(annotated_image)\n</code></pre> <p>Info</p> <pre><code>**Breaking change!** There were versions: `0.9.14` and `0.9.15` where Yolo World was exposed\nbehind `InferencePipeline.init(...)` initializer that you needed to run with specific combination \nof parameters to alter default behavior of pipeline such that it runs against YoloWorld model. \nWe decided to provide an explicit way of running this foundation model in `InferencePipeline` providing\na dedicated init function starting from version `0.9.16`\n</code></pre> <p>You can easily run predictions against <code>YoloWorld</code> model using <code>InferencePipeline</code>. There is a custom init method to ease handling that use-case:</p> <pre><code># import the InferencePipeline interface\nfrom inference import InferencePipeline\n# import a built-in sink called render_boxes (sinks are the logic that happens after inference)\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\npipeline = InferencePipeline.init_with_yolo_world(\n    video_reference=\"./your_video.mp4\",\n    classes=[\"person\", \"dog\", \"car\", \"truck\"],\n    model_size=\"s\",\n    on_prediction=render_boxes,\n)\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre> <p>In this code, we load YOLO-World, run YOLO-World on an image, and annotate the image with the predictions from the model.</p> <p>Above, replace:</p> <ol> <li><code>[\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"]</code> with the objects you want to detect.</li> <li><code>image.jpeg</code> with the path to the image in which you want to detect objects.</li> </ol> <p>Then, run the Python script you have created:</p> <pre><code>python app.py\n</code></pre> <p>The result from YOLO-World will be displayed in a new window.</p> <p></p>"},{"location":"foundation/yolo_world/#benchmarking","title":"Benchmarking","text":"<p>We ran 100 inferences on an NVIDIA T4 GPU to benchmark the performance of YOLO-World.</p> <p>YOLO-World ran 100 inferences in 9.18 seconds (0.09 seconds per inference, on average).</p>"},{"location":"include/install/","title":"Install","text":"<p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"include/model_id/","title":"Model id","text":"<p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p>"},{"location":"inference_helpers/inference_cli/","title":"Inference CLI","text":""},{"location":"inference_helpers/inference_cli/#roboflow-inference-cli","title":"Roboflow Inference CLI","text":"<p>Roboflow Inference CLI offers a lightweight interface for running the Roboflow inference server locally or the Roboflow Hosted API.</p> <p>To create custom Inference server Docker images, go to the parent package, Roboflow Inference.</p> <p>Roboflow has everything you need to deploy a computer vision model to a range of devices and environments. Inference supports object detection, classification, and instance segmentation models, and running foundation models (CLIP and SAM).</p>"},{"location":"inference_helpers/inference_cli/#installation","title":"Installation","text":"<pre><code>pip install roboflow-cli\n</code></pre>"},{"location":"inference_helpers/inference_cli/#examples","title":"Examples","text":""},{"location":"inference_helpers/inference_cli/#inference-server-start","title":"inference server start","text":"<p>Starts a local Inference server. It optionally takes a port number (default is 9001) and will only start the docker container if there is not already a container running on that port.</p> <p>If you would rather run your server on a virtual machine in Google cloud or Amazon cloud, skip to the section titled \"Deploy Inference on Cloud\" below.</p> <p>Before you begin, ensure that you have Docker installed on your machine. Docker provides a containerized environment, allowing the Roboflow Inference Server to run in a consistent and isolated manner, regardless of the host system. If you haven't installed Docker yet, you can get it from Docker's official website.</p> <p>The CLI will automatically detect the device you are running on and pull the appropriate Docker image.</p> <pre><code>inference server start --port 9001 [-e {optional_path_to_file_with_env_variables}]\n</code></pre> <p>Parameter <code>--env-file</code> (or <code>-e</code>) is the optional path for .env file that will be loaded into your Inference server in case that values of internal parameters needs to be adjusted. Any value passed explicitly as command parameter is considered as more important and will shadow the value defined in <code>.env</code> file under the same target variable name.</p>"},{"location":"inference_helpers/inference_cli/#development-mode","title":"Development Mode","text":"<p>Use the <code>--dev</code> flag to start the Inference Server in development mode. Development mode enables the Inference Server's built in notebook environment for easy testing and development.</p>"},{"location":"inference_helpers/inference_cli/#tunnel","title":"Tunnel","text":"<p>Use the <code>--tunnel</code> flag to start the Inference Server with a tunnel to expose inference to external requests on a TLS-enabled endpoint.</p> <p>The random generated address will be on server start output:</p> <pre><code>Tunnel to local inference running on https://somethingrandom-ip-192-168-0-1.roboflow.run\n</code></pre>"},{"location":"inference_helpers/inference_cli/#inference-server-status","title":"inference server status","text":"<p>Checks the status of the local inference server.</p> <pre><code>inference server status\n</code></pre>"},{"location":"inference_helpers/inference_cli/#inference-server-stop","title":"inference server stop","text":"<p>Stops the inference server.</p> <pre><code>inference server stop\n</code></pre>"},{"location":"inference_helpers/inference_cli/#deploy-inference-on-a-cloud-vm","title":"Deploy Inference on a Cloud VM","text":"<p>You can deploy Roboflow Inference containers to virtual machines in the cloud. These VMs are configured to run CPU or GPU-based Inference servers under the hood, so you don't have to deal with OS/GPU drivers/docker installations, etc! The Inference cli currently supports deploying the Roboflow Inference container images into a virtual machine running on Google (GCP) or Amazon cloud (AWS).</p> <p>The Roboflow Inference CLI assumes the corresponding cloud CLI is configured for the project you want to deploy the virtual machine into. Read instructions for setting up Google/GCP - gcloud cli or the Amazon/AWS aws cli.</p> <p>Roboflow Inference cloud deploy is powered by the popular Skypilot project.</p>"},{"location":"inference_helpers/inference_cli/#cloud-deploy-examples","title":"Cloud Deploy Examples","text":"<p>We illustrate Inference cloud deploy with some examples, below.</p> <p>Deploy GPU or CPU inference to AWS or GCP</p> <pre><code># Deploy the roboflow Inference GPU container into a GPU-enabled VM in AWS\n\ninference cloud deploy --provider aws --compute-type gpu\n</code></pre> <pre><code># Deploy the roboflow Inference CPU container into a CPU-only VM in GCP\n\ninference cloud deploy --provider gcp --compute-type cpu\n</code></pre> <p>Note the \"cluster name\" printed after the deployment completes. This handle is used in many subsequent commands. The deploy command also prints helpful debug and cost information about your VM.</p> <p>Deploying Inference into a cloud VM will also print out an endpoint of the form \"http://1.2.3.4:9001\"; you can now run inferences against this endpoint.</p> <p>Note that the port 9001 is automatically opened - check with your security admin if this is acceptable for your cloud/project.</p>"},{"location":"inference_helpers/inference_cli/#view-status-of-deployments","title":"View status of deployments","text":"<pre><code>inference cloud status\n</code></pre>"},{"location":"inference_helpers/inference_cli/#stop-and-start-deployments","title":"Stop and start deployments","text":"<pre><code># Stop the VM, you only pay for disk storage while the VM is stopped\ninference cloud stop &lt;deployment_handle&gt;\n</code></pre>"},{"location":"inference_helpers/inference_cli/#restart-deployments","title":"Restart deployments","text":"<pre><code>inference cloud start &lt;deployment_handle&gt;\n</code></pre>"},{"location":"inference_helpers/inference_cli/#undeploy-delete-the-cloud-deployment","title":"Undeploy (delete) the cloud deployment","text":"<pre><code>inference cloud undeploy &lt;deployment_handle&gt;\n</code></pre>"},{"location":"inference_helpers/inference_cli/#ssh-into-the-cloud-deployment","title":"SSH into the cloud deployment","text":"<p>You can SSH into your cloud deployment with the following command: <pre><code>ssh &lt;deployment_handle&gt;\n</code></pre></p> <p>The required SSH key is automatically added to your .ssh/config, you don't need to configure this manually.</p>"},{"location":"inference_helpers/inference_cli/#cloud-deploy-customization","title":"Cloud Deploy Customization","text":"<p>Roboflow Inference cloud deploy will create VMs based on internally tested templates.</p> <p>For advanced usecases and to customize the template, you can use your sky yaml template on the command-line, like so:</p> <pre><code>inference cloud deploy --custom /path/to/sky-template.yaml\n</code></pre> <p>If you want you can download the standard template stored in the roboflow cli and the modify it for your needs, this command will do that.</p> <pre><code># This command will print out the standard gcp/cpu sky template.\ninference cloud deploy --dry-run --provider gcp --compute-type cpu\n</code></pre> <p>Then you can deploy a custom template based off your changes.</p> <p>As an aside, you can also use the sky cli to control your deployment(s) and access some more advanced functionality.</p> <p>Roboflow Inference deploy currently supports AWS and GCP, please open an issue on the Inference GitHub repository if you would like to see other cloud providers supported.</p>"},{"location":"inference_helpers/inference_cli/#inference-infer","title":"inference infer","text":"<p>It takes input path / url and model version to produce predictions (and optionally make visualisation using  <code>supervision</code>). You can also specify a host to run inference on our hosted inference server.</p> <p>Note</p> <p>If you decided to use hosted inference server - make sure command <code>inference server start</code> was used first </p> <p>Tip</p> <p>Use <code>inference infer --help</code> to display description of parameters</p> <p>Tip</p> <p>Roboflow API key can be provided via <code>ROBOFLOW_API_KEY</code> environment variable</p>"},{"location":"inference_helpers/inference_cli/#local-image","title":"Local image","text":"<p>This command is going to make a prediction from local image using selected model and print the prediction on  the console.</p> <pre><code>inference infer -i ./image.jpg -m {your_project}/{version} --api-key {YOUR_API_KEY}\n</code></pre> <p>To display visualised prediction use <code>-D</code> option. To save prediction and visualisation in a local directory, use <code>-o {path_to_your_directory}</code> option. Those options work also in other modes.</p> <pre><code>inference infer -i ./image.jpg -m {your_project}/{version} --api-key {YOUR_API_KEY} -D -o {path_to_your_output_directory}\n</code></pre>"},{"location":"inference_helpers/inference_cli/#hosted-image","title":"Hosted image","text":"<pre><code>inference infer -i https://[YOUR_HOSTED_IMAGE_URL] -m {your_project}/{version} --api-key {YOUR_API_KEY}\n</code></pre>"},{"location":"inference_helpers/inference_cli/#hosted-api-inference","title":"Hosted API inference","text":"<pre><code>inference infer -i ./image.jpg -m {your_project}/{version} --api-key {YOUR_API_KEY} -h https://detect.roboflow.com\n</code></pre>"},{"location":"inference_helpers/inference_cli/#local-directory","title":"Local directory","text":"<pre><code>inference infer -i {your_directory_with_images} -m {your_project}/{version} -o {path_to_your_output_directory} --api-key {YOUR_API_KEY}\n</code></pre>"},{"location":"inference_helpers/inference_cli/#video-file","title":"Video file","text":"<pre><code>inference infer -i {path_to_your_video_file} -m {your_project}/{version} -o {path_to_your_output_directory} --api-key {YOUR_API_KEY}\n</code></pre>"},{"location":"inference_helpers/inference_cli/#configuration-of-visualisation","title":"Configuration of visualisation","text":"<p>Option <code>-c</code> can be provided with a path to <code>*.yml</code> file configuring <code>supervision</code> visualisation. There are few pre-defined configs: - <code>bounding_boxes</code> - with <code>BoundingBoxAnnotator</code> and <code>LabelAnnotator</code> annotators - <code>bounding_boxes_tracing</code> - with <code>ByteTracker</code> and annotators (<code>BoundingBoxAnnotator</code>, <code>LabelAnnotator</code>) - <code>masks</code> - with <code>MaskAnnotator</code> and <code>LabelAnnotator</code> annotators - <code>polygons</code> - with <code>PolygonAnnotator</code> and <code>LabelAnnotator</code> annotators</p> <p>Custom configuration can be created following the schema: <pre><code>annotators:\n  - type: \"bounding_box\"\n    params:\n      thickness: 2\n  - type: \"label\"\n    params:\n      text_scale: 0.5\n      text_thickness: 2\n      text_padding: 5\n  - type: \"trace\"\n    params:\n      trace_length: 60\n      thickness: 2\ntracking:\n  track_thresh: 0.25\n  track_buffer: 30\n  match_thresh: 0.8\n  frame_rate: 30\n</code></pre> <code>annotators</code> field is a list of dictionaries with two keys: <code>type</code> and <code>param</code>. <code>type</code> points to  name of annotator class: <pre><code>from supervision import *\nANNOTATOR_TYPE2CLASS = {\n    \"bounding_box\": BoundingBoxAnnotator,\n    \"mask\": MaskAnnotator,\n    \"polygon\": PolygonAnnotator,\n    \"color\": ColorAnnotator,\n    \"halo\": HaloAnnotator,\n    \"ellipse\": EllipseAnnotator,\n    \"box_corner\": BoxCornerAnnotator,\n    \"circle\": CircleAnnotator,\n    \"dot\": DotAnnotator,\n    \"label\": LabelAnnotator,\n    \"blur\": BlurAnnotator,\n    \"trace\": TraceAnnotator,\n    \"heat_map\": HeatMapAnnotator,\n    \"pixelate\": PixelateAnnotator,\n    \"triangle\": TriangleAnnotator,\n}\n</code></pre> <code>param</code> is a dictionary of annotator constructor parameters (check them in  <code>supervision</code> docs - you would only be able to use primitive values, classes and enums that are defined in constructors may not be possible to resolve from yaml config).</p> <p><code>tracking</code> is an optional key that holds a dictionary with constructor parameters for <code>ByteTrack</code>.</p>"},{"location":"inference_helpers/inference_cli/#configuration-of-model","title":"Configuration of model","text":"<p><code>-mc</code> parameter can be provided with path to <code>*.yml</code> file that specifies  model configuration (like confidence threshold or IoU threshold). If given, configuration will be used to initialise <code>InferenceConfiguration</code> object from <code>inference_sdk</code> library. See sdk docs to discover which options can be configured via <code>*.yml</code> file - configuration keys must match with names of fields in <code>InferenceConfiguration</code> object.</p>"},{"location":"inference_helpers/inference_cli/#inference-benchmark","title":"inference benchmark","text":"<p>Note</p> <p>The command is introduced in <code>inference_cli&gt;=0.9.10</code></p> <p><code>inference benchmark</code> is a set of command suited to run benchmarks of <code>inference</code>. There are two types of benchmark  available <code>inference benchmark api-speed</code> - to test <code>inference</code> HTTP server and <code>inference benchmark python-package-speed</code> to verify the performance of <code>inference</code> Python package.</p> <p>Tip</p> <p>Use <code>inference benchmark api-speed --help</code> / <code>inference benchmark python-package-speed --help</code> to display all options of benchmark commands.</p> <p>Tip</p> <p>Roboflow API key can be provided via <code>ROBOFLOW_API_KEY</code> environment variable</p>"},{"location":"inference_helpers/inference_cli/#running-benchmark-of-python-package","title":"Running benchmark of Python package","text":"<p>Basic benchmark can be run using the following command: </p> <p><pre><code>inference benchmark python-package-speed \\\n  -m {your_model_id} \\\n  -d {pre-configured dataset name or path to directory with images} \\\n  -o {output_directory}  \n</code></pre> Command runs specified number of inferences using pointed model and saves statistics (including benchmark  parameter, throughput, latency, errors and platform details) in pointed directory.</p>"},{"location":"inference_helpers/inference_cli/#running-benchmark-of-inference-server","title":"Running benchmark of <code>inference server</code>","text":"<p>Note</p> <p>Before running API benchmark - make sure the server is up and running: <pre><code>inference server start\n</code></pre></p> <p>Basic benchmark can be run using the following command: </p> <p><pre><code>inference benchmark api-speed \\\n  -m {your_model_id} \\\n  -d {pre-configured dataset name or path to directory with images} \\\n  -o {output_directory}  \n</code></pre> Command runs specified number of inferences using pointed model and saves statistics (including benchmark  parameter, throughput, latency, errors and platform details) in pointed directory.</p> <p>This benchmark has more configuration options to support different ways HTTP API profiling. In default mode, single client will be spawned, and it will send one request after another sequentially. This may be suboptimal in specific cases, so one may specify number of concurrent clients using <code>-c {number_of_clients}</code> option. Each client will send next request once previous is handled. This option will also not cover all scenarios of tests. For instance one may want to send <code>x</code> requests each second (which is closer to the scenario of production environment where multiple clients are sending requests concurrently). In this scenario, <code>--rps {value}</code>  option can be used (and <code>-c</code> will be ignored). Value provided in <code>--rps</code> option specifies how many requests  are to be spawned each second without waiting for previous requests to be handled. In I/O intensive benchmark  scenarios - we suggest running command from multiple separate processes and possibly multiple hosts.</p>"},{"location":"inference_helpers/inference_cli/#supported-devices","title":"Supported Devices","text":"<p>Roboflow Inference CLI currently supports the following device targets:</p> <ul> <li>x86 CPU</li> <li>ARM64 CPU</li> <li>NVIDIA GPU</li> </ul> <p>For Jetson specific inference server images, check out the Roboflow Inference package, or pull the images directly following instructions in the official Roboflow Inference documentation.</p>"},{"location":"inference_helpers/inference_landing_page/","title":"Inference Landing Page","text":"<p>The Roboflow Inference server hosts a landing page. This page contains links to helpful resources including documentation and examples.</p>"},{"location":"inference_helpers/inference_landing_page/#visit-the-inference-landing-page","title":"Visit the Inference Landing Page","text":"<p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>The easiest way to start an inference server is with the inference CLI. Install it via pip:</p> <pre><code>pip install inference-cli\n</code></pre> <p>Now run the <code>inference sever start</code> command.</p> <pre><code>inference server start\n</code></pre> <p>Now visit localhost:9001 in your browser to see the <code>inference</code> landing page. This page contains links to resources and examples related to <code>inference</code>.</p>"},{"location":"inference_helpers/inference_landing_page/#inference-notebook","title":"Inference Notebook","text":"<p>Roboflow Inference Servers come equipped with a built in Jupyterlab environment. This environment is the fastest way to get up and running with inference for development and testing. To use it, first start an inference server. Be sure to specify the <code>--dev</code> flag so that the notebook environment is enabled (it is disabled by default).</p> <pre><code>inference server start --dev\n</code></pre> <p>Now visit localhost:9001 in your browser to see the <code>inference</code> landing page. From the landing page, select the button labeled \"Jump Into an Inference Enabled Notebook\" to open a new tab for the Jupyterlab environment.</p> <p>This Jupyterlab environment comes preloaded with several example notebooks and all of the dependencies needed to run <code>inference</code>.</p>"},{"location":"inference_helpers/inference_sdk/","title":"Inference Client","text":"<p>The <code>InferenceHTTPClient</code> enables you to interact with Inference over HTTP.</p> <p>You can use this client to run models hosted:</p> <ol> <li>On the Roboflow platform (use client version <code>v0</code>), and;</li> <li>On device with Inference.</li> </ol> <p>For models trained on the Roboflow platform, client accepts the following inputs:</p> <ul> <li>A single image (Given as a local path, URL, <code>np.ndarray</code> or <code>PIL.Image</code>);</li> <li>Multiple images;</li> <li>A directory of images, or;</li> <li>A video file.</li> <li>Single image encoded as <code>base64</code></li> </ul> <p>For core model - client exposes dedicated methods to be used, but standard image loader used accepts file paths, URLs, <code>np.ndarray</code> and <code>PIL.Image</code> formats. Apart from client version (<code>v0</code> or <code>v1</code>) - options provided via configuration are used against models trained at the platform, not the core models.</p> <p>The client returns a dictionary of predictions for each image or frame.</p> <p>Starting from <code>0.9.10</code> - <code>InferenceHTTPClient</code> provides async equivalents for the majority of methods and support for requests parallelism and batching implemented (yet in limited scope, not for all methods).  Further details to be found in specific sections of this document. </p> <p>Tip</p> <p>Read our Run Model on an Image guide to learn how to run a model with the Inference Client.</p>"},{"location":"inference_helpers/inference_sdk/#quickstart","title":"Quickstart","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\nresult = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#asyncio-client","title":"AsyncIO client","text":"<pre><code>import asyncio\nfrom inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\nloop = asyncio.get_event_loop()\nresult = loop.run_until_complete(\n  CLIENT.infer_async(image_url, model_id=\"soccer-players-5fuqs/1\")\n)\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#configuration-options-used-for-models-trained-on-the-roboflow-platform","title":"Configuration options (used for models trained on the Roboflow platform)","text":""},{"location":"inference_helpers/inference_sdk/#configuring-with-context-managers","title":"configuring with context managers","text":"<p>Methods <code>use_configuration(...)</code>, <code>use_api_v0(...)</code>, <code>use_api_v1(...)</code>, <code>use_model(...)</code> are designed to work in context managers. Once context manager is left - old config values are restored.</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\ncustom_configuration = InferenceConfiguration(confidence_threshold=0.8)\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nwith CLIENT.use_api_v0():\n    _ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\nwith CLIENT.use_configuration(custom_configuration):\n    _ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\nwith CLIENT.use_model(\"soccer-players-5fuqs/1\"):\n    _ = CLIENT.infer(image_url)\n\n# after leaving context manager - changes are reverted and `model_id` is still required\n_ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n</code></pre> <p>As you can see - <code>model_id</code> is required to be given for prediction method only when default model is not configured.</p> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p>"},{"location":"inference_helpers/inference_sdk/#setting-the-configuration-once-and-using-till-next-change","title":"Setting the configuration once and using till next change","text":"<p>Methods <code>configure(...)</code>, <code>select_api_v0(...)</code>, <code>select_api_v1(...)</code>, <code>select_model(...)</code> are designed alter the client state and will be preserved until next change.</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\ncustom_configuration = InferenceConfiguration(confidence_threshold=0.8)\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.select_api_v0()\n_ = CLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\n# API v0 still holds\nCLIENT.configure(custom_configuration)\nCLIENT.infer(image_url, model_id=\"soccer-players-5fuqs/1\")\n\n# API v0 and custom configuration still holds\nCLIENT.select_model(model_id=\"soccer-players-5fuqs/1\")\n_ = CLIENT.infer(image_url)\n\n# API v0, custom configuration and selected model - still holds\n_ = CLIENT.infer(image_url)\n</code></pre> <p>One may also initialise in <code>chain</code> mode:</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(api_url=\"http://localhost:9001\", api_key=\"ROBOFLOW_API_KEY\") \\\n    .select_api_v0() \\\n    .select_model(\"soccer-players-5fuqs/1\")\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#overriding-model_id-for-specific-call","title":"Overriding <code>model_id</code> for specific call","text":"<p><code>model_id</code> can be overriden for specific call</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(api_url=\"http://localhost:9001\", api_key=\"ROBOFLOW_API_KEY\") \\\n    .select_model(\"soccer-players-5fuqs/1\")\n\n_ = CLIENT.infer(image_url, model_id=\"another-model/1\")\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#parallel-batch-inference","title":"Parallel / Batch inference","text":"<p>You may want to predict against multiple images at single call. There are two parameters of <code>InferenceConfiguration</code> that specifies batching and parallelism options: - <code>max_concurrent_requests</code> - max number of concurrent requests that can be started  - <code>max_batch_size</code> - max number of elements that can be injected into single request (in <code>v0</code> mode - API only  support a single image in payload for the majority of endpoints - hence in this case, value will be overriden with <code>1</code> to prevent errors)</p> <p>Thanks to that the following improvements can be achieved: - if you run inference container with API on prem on powerful GPU machine - setting <code>max_batch_size</code> properly may bring performance / throughput benefits - if you run inference against hosted Roboflow API - setting <code>max_concurrent_requests</code> will cause multiple images being served at once bringing performance / throughput benefits - combination of both options can be beneficial for clients running inference container with API on cluster of machines, then the load of single node can be optimised and parallel requests to different nodes can be made at a time  ``</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nimage_url = \"https://source.roboflow.com/pwYAXv9BTpqLyFfgQoPZ/u48G0UpWfk8giSw7wrU8/original.jpg\"\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\npredictions = CLIENT.infer([image_url] * 5, model_id=\"soccer-players-5fuqs/1\")\n\nprint(predictions)\n</code></pre> <p>Methods that support batching / parallelism: -<code>infer(...)</code> and <code>infer_async(...)</code> - <code>infer_from_api_v0(...)</code> and <code>infer_from_api_v0_async(...)</code> (enforcing <code>max_batch_size=1</code>) - <code>ocr_image(...)</code> and <code>ocr_image_async(...)</code> (enforcing <code>max_batch_size=1</code>) - <code>detect_gazes(...)</code> and <code>detect_gazes_async(...)</code> - <code>get_clip_image_embeddings(...)</code> and <code>get_clip_image_embeddings_async(...)</code></p>"},{"location":"inference_helpers/inference_sdk/#client-for-core-models","title":"Client for core models","text":"<p><code>InferenceHTTPClient</code> now supports core models hosted via <code>inference</code>. Part of the models can be used on the Roboflow  hosted inference platform (use <code>https://infer.roboflow.com</code> as url), other are possible to be deployed locally (usually local server will be available under <code>http://localhost:9001</code>).</p> <p>Tip</p> <p>Install <code>inference-cli</code> package to easily run <code>inference</code> API locally <pre><code>pip install inference-cli\ninference server start\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#clip","title":"Clip","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.get_clip_image_embeddings(inference_input=\"./my_image.jpg\")  # single image request\nCLIENT.get_clip_image_embeddings(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\nCLIENT.get_clip_text_embeddings(text=\"some\")  # single text request\nCLIENT.get_clip_text_embeddings(text=[\"some\", \"other\"])  # other text request\nCLIENT.clip_compare(\n    subject=\"./my_image.jpg\",\n    prompt=[\"fox\", \"dog\"],\n)\n</code></pre> <p><code>CLIENT.clip_compare(...)</code> method allows to compare different combination of <code>subject_type</code> and <code>prompt_type</code>:</p> <ul> <li><code>(image, image)</code></li> <li><code>(image, text)</code></li> <li><code>(text, image)</code></li> <li><code>(text, text)</code>   Default mode is <code>(image, text)</code>.</li> </ul> <p>Tip</p> <p>Check out async methods for Clip model: <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nasync def see_async_method(): \n  await CLIENT.get_clip_image_embeddings_async(inference_input=\"./my_image.jpg\")  # single image request\n  await CLIENT.get_clip_image_embeddings_async(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\n  await CLIENT.get_clip_text_embeddings_async(text=\"some\")  # single text request\n  await CLIENT.get_clip_text_embeddings_async(text=[\"some\", \"other\"])  # other text request\n  await CLIENT.clip_compare_async(\n      subject=\"./my_image.jpg\",\n      prompt=[\"fox\", \"dog\"],\n  )\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#cogvlm","title":"CogVLM","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.prompt_cogvlm(\n    visual_prompt=\"./my_image.jpg\",\n    text_prompt=\"So - what is your final judgement about the content of the picture?\",\n    chat_history=[(\"I think the image shows XXX\", \"You are wrong - the image shows YYY\")], # optional parameter\n)\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#doctr","title":"DocTR","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.ocr_image(inference_input=\"./my_image.jpg\")  # single image request\nCLIENT.ocr_image(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\n</code></pre> <p>Tip</p> <p>Check out async methods for DocTR model: <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nasync def see_async_method(): \n  await CLIENT.ocr_image(inference_input=\"./my_image.jpg\")  # single image request\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#gaze","title":"Gaze","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # only local hosting supported\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nCLIENT.detect_gazes(inference_input=\"./my_image.jpg\")  # single image request\nCLIENT.detect_gazes(inference_input=[\"./my_image.jpg\", \"./other_image.jpg\"])  # batch image request\n</code></pre> <p>Tip</p> <p>Check out async methods for Gaze model: <pre><code>from inference_sdk import InferenceHTTPClient\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",  # or \"https://infer.roboflow.com\" to use hosted serving\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\nasync def see_async_method(): \n  await CLIENT.detect_gazes(inference_input=\"./my_image.jpg\")  # single image request\n</code></pre></p>"},{"location":"inference_helpers/inference_sdk/#inference-against-stream","title":"Inference against stream","text":"<p>One may want to infer against video or directory of images - and that modes are supported in <code>inference-client</code></p> <pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nfor frame_id, frame, prediction in CLIENT.infer_on_stream(\"video.mp4\", model_id=\"soccer-players-5fuqs/1\"):\n    # frame_id is the number of frame\n    # frame - np.ndarray with video frame\n    # prediction - prediction from the model\n    pass\n\nfor file_path, image, prediction in CLIENT.infer_on_stream(\"local/dir/\", model_id=\"soccer-players-5fuqs/1\"):\n    # file_path - path to the image\n    # frame - np.ndarray with video frame\n    # prediction - prediction from the model\n    pass\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#what-is-actually-returned-as-prediction","title":"What is actually returned as prediction?","text":"<p><code>inference_client</code> returns plain Python dictionaries that are responses from model serving API. Modification is done only in context of <code>visualization</code> key that keep server-generated prediction visualisation (it can be transcoded to the format of choice) and in terms of client-side re-scaling.</p>"},{"location":"inference_helpers/inference_sdk/#methods-to-control-inference-server-in-v1-mode-only","title":"Methods to control <code>inference</code> server (in <code>v1</code> mode only)","text":""},{"location":"inference_helpers/inference_sdk/#getting-server-info","title":"Getting server info","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.get_server_info()\n</code></pre>"},{"location":"inference_helpers/inference_sdk/#listing-loaded-models","title":"Listing loaded models","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.list_loaded_models()\n</code></pre> <p>Tip</p> <p>This method has async equivaluent: <code>list_loaded_models_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#getting-specific-model-description","title":"Getting specific model description","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.get_model_description(model_id=\"some/1\", allow_loading=True)\n</code></pre> <p>If <code>allow_loading</code> is set to <code>True</code>: model will be loaded as side-effect if it is not already loaded. Default: <code>True</code>.</p> <p>Tip</p> <p>This method has async equivaluent: <code>get_model_description_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#loading-model","title":"Loading model","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.load_model(model_id=\"some/1\", set_as_default=True)\n</code></pre> <p>The pointed model will be loaded. If <code>set_as_default</code> is set to <code>True</code>: after successful load, model will be used as default model for the client. Default value: <code>False</code>.</p> <p>Tip</p> <p>This method has async equivaluent: <code>load_model_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#unloading-model","title":"Unloading model","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.unload_model(model_id=\"some/1\")\n</code></pre> <p>Sometimes (to avoid OOM at server side) - unloading model will be required.</p> <p>Tip</p> <p>This method has async equivaluent: <code>unload_model_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#unloading-all-models","title":"Unloading all models","text":"<pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\nCLIENT.unload_all_models()\n</code></pre> <p>Tip</p> <p>This method has async equivaluent: <code>unload_all_models_async()</code></p>"},{"location":"inference_helpers/inference_sdk/#inference-workflows","title":"Inference <code>workflows</code>","text":"<p>Tip</p> <p>This feature is in <code>alpha</code> preview. We encourage you to experiment and reach out to us with issues spotted. Check out documentation of deployment specs, create one and run</p> <p>Tip</p> <p>This feature only works with locally hosted inference container and hosted platform (access may be limited).  Use inefernce-cli to run local container with HTTP API: <pre><code>inference server start\n</code></pre></p> <p>Warning</p> <p>Method <code>infer_from_workflow(...)</code> is deprecated starting from <code>v0.9.21</code> and  will be removed end of Q2 2024. Please migrate - the signature is the same,  what changes is underlying inference server endpoint used to run workflow.</p> <p>New method is called <code>run_workflow(...)</code> and is compatible with Roboflow hosted API and inverence servers in versions <code>0.9.21+</code> </p> <pre><code>from inference_sdk import InferenceHTTPClient\n\n# Replace ROBOFLOW_API_KEY with your Roboflow API Key\nCLIENT = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=\"ROBOFLOW_API_KEY\"\n)\n\n# for older versions of server than v0.9.21 use: CLIENT.infer_from_workflow(...) \nCLIENT.run_workflow(\n    specification={\n        \"version\": \"1.0\",\n        \"inputs\": [\n            {\"type\": \"InferenceImage\", \"name\": \"image\"},\n            {\"type\": \"InferenceParameter\", \"name\": \"my_param\"},\n        ],\n        # ...\n    },\n    # OR\n    # workspace_name=\"my_workspace_name\",\n    # workflow_id=\"my_workflow_id\",\n\n    images={\n        \"image\": \"url or your np.array\",\n    },\n    parameters={\n        \"my_param\": 37,\n    },\n)\n</code></pre> <p>Please note that either <code>specification</code> is provided with specification of workflow as described here or  both <code>workspace_name</code> and <code>workflow_id</code> are given to use workflow predefined in Roboflow app. <code>workspace_name</code> can be found in Roboflow APP URL once browser shows the main panel of workspace.</p> <p>Server-side caching of Workflow definitions</p> <p>In <code>inference v0.22.0</code> we've added server-side caching of Workflows reginsted on Roboflow platform which is enabled by default. When you use <code>run_workflow(...)</code> method with <code>workspace_name</code> and <code>workflow_id</code> server will cache the definition for 15 minutes. If you change the definition in Workflows UI and re-run the method, you may not see the change. To force processing without cache, pass <code>use_cache=False</code> as a parameter of  <code>run_workflow(...)</code> method. </p> <p>Workflows profiling</p> <p>Since <code>inference v0.22.0</code>, you may request profiler trace of your Workflow execution from server passing  <code>enable_profiling=True</code> parameter to <code>run_workflow(...)</code> method. If server configuration enables traces exposure, you will be able to find a JSON file with trace in a directory specified by <code>profiling_directory</code> parameter of  <code>InferenceConfiguration</code> - by default it is <code>inference_profiling</code> directory in your current working directory. The traces can be directly loaded and rendered in Google Chrome - navigate into <code>chrome://tracing</code> in your  borwser and hit \"load\" button. </p>"},{"location":"inference_helpers/inference_sdk/#details-about-client-configuration","title":"Details about client configuration","text":"<p><code>inference-client</code> provides <code>InferenceConfiguration</code> dataclass to hold whole configuration.</p> <pre><code>from inference_sdk import InferenceConfiguration\n</code></pre> <p>Overriding fields in this config changes the behaviour of client (and API serving model). Specific fields are used in specific contexts. In particular:</p>"},{"location":"inference_helpers/inference_sdk/#inference-in-v0-mode","title":"Inference in <code>v0</code> mode","text":"<p>The following fields are passed to API</p> <ul> <li><code>confidence_threshold</code> (as <code>confidence</code>) - to alter model thresholding</li> <li><code>keypoint_confidence_threshold</code> as (<code>keypoint_confidence</code>) - to filter out detected keypoints   based on model confidence</li> <li><code>format</code>: to visualise on server side - use <code>image</code> (just the image) or <code>image_and_json</code> (prediction details and image base64)</li> <li><code>visualize_labels</code> (as <code>labels</code>) - used in visualisation to show / hide labels for classes</li> <li><code>mask_decode_mode</code></li> <li><code>tradeoff_factor</code></li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>iou_threshold</code> (as <code>overlap</code>) - to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>count_inference</code> as <code>countinference</code></li> <li><code>service_secret</code></li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#classification-model-in-v1-mode","title":"Classification model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li> <p><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></p> </li> <li> <p><code>visualize_predictions</code>: flag to enable / disable visualisation</p> </li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li> <p><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful, for instance, while testing the model)</p> </li> <li> <p><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</p> </li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> </ul>"},{"location":"inference_helpers/inference_sdk/#object-detection-model-in-v1-mode","title":"Object detection model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>class_filter</code> to filter out list of classes</li> <li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li> <li><code>fix_batch_size</code></li> <li><code>iou_threshold</code>: to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>max_candidates</code>: max candidates to post-processing from model</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#keypoints-detection-model-in-v1-mode","title":"Keypoints detection model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>keypoint_confidence_threshold</code> as (<code>keypoint_confidence</code>) - to filter out detected keypoints   based on model confidence</li> <li><code>class_filter</code> to filter out list of object classes</li> <li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li> <li><code>fix_batch_size</code></li> <li><code>iou_threshold</code>: to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>max_candidates</code>: max candidates to post-processing from model</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#instance-segmentation-model-in-v1-mode","title":"Instance segmentation model in <code>v1</code> mode:","text":"<ul> <li><code>visualize_predictions</code>: flag to enable / disable visualisation</li> <li><code>visualize_labels</code>: flag to enable / disable labels visualisation if visualisation is enabled</li> <li><code>confidence_threshold</code> as <code>confidence</code></li> <li><code>class_filter</code> to filter out list of classes</li> <li><code>class_agnostic_nms</code>: flag to control whether NMS is class-agnostic</li> <li><code>fix_batch_size</code></li> <li><code>iou_threshold</code>: to dictate NMS IoU threshold</li> <li><code>stroke_width</code>: width of stroke in visualisation</li> <li><code>max_detections</code>: max detections to return from model</li> <li><code>max_candidates</code>: max candidates to post-processing from model</li> <li><code>disable_preproc_auto_orientation</code>, <code>disable_preproc_contrast</code>, <code>disable_preproc_grayscale</code>,   <code>disable_preproc_static_crop</code> to alter server-side pre-processing</li> <li><code>mask_decode_mode</code></li> <li><code>tradeoff_factor</code></li> <li><code>disable_active_learning</code> to prevent Active Learning feature from registering the datapoint (can be useful for   instance while testing model)</li> <li><code>source</code> Optional string to set a \"source\" attribute on the inference call; if using model monitoring, this will get logged with the inference request so you can filter/query inference requests coming from a particular source. e.g. to identify which application, system, or deployment is making the request.</li> <li><code>source_info</code> Optional string to set additional \"source_info\" attribute on the inference call; e.g. to identify a sub component in an app.</li> <li><code>active_learning_target_dataset</code> - making inference from specific model (let's say <code>project_a/1</code>), when we want to save data in another project <code>project_b</code> - the latter should be pointed to by this parameter. **Please remember that you cannot use different type of models in <code>project_a</code> and <code>project_b</code> - if that is the case - data will not be  registered) - since <code>v0.9.18</code></li> </ul>"},{"location":"inference_helpers/inference_sdk/#configuration-of-client","title":"Configuration of client","text":"<ul> <li><code>output_visualisation_format</code>: one of (<code>VisualisationResponseFormat.BASE64</code>, <code>VisualisationResponseFormat.NUMPY</code>,   <code>VisualisationResponseFormat.PILLOW</code>) - given that server-side visualisation is enabled - one may choose what   format should be used in output</li> <li><code>image_extensions_for_directory_scan</code>: while using <code>CLIENT.infer_on_stream(...)</code> with local directory   this parameter controls type of files (extensions) allowed to be processed -   default: <code>[\"jpg\", \"jpeg\", \"JPG\", \"JPEG\", \"png\", \"PNG\"]</code></li> <li><code>client_downsizing_disabled</code>: set to <code>False</code> if you want to perform client-side downsizing - default <code>True</code> (   changed in version <code>0.16.0</code> - previously was <code>False</code>).   Client-side scaling is only supposed to down-scale (keeping aspect-ratio) the input for inference -   to utilise internet connection more efficiently (but for the price of images manipulation / transcoding).   If model registry endpoint is available (mode <code>v1</code>) - model input size information will be used, if not:   <code>default_max_input_size</code> will be in use.</li> <li><code>max_concurrent_requests</code> - max number of concurrent requests that can be started </li> <li><code>max_batch_size</code> - max number of elements that can be injected into single request (in <code>v0</code> mode - API only  support a single image in payload for the majority of endpoints - hence in this case, value will be overriden with <code>1</code> to prevent errors)</li> </ul> <p>Warning</p> <p>The default value for flag <code>client_downsizing_disabled</code> was changed from <code>False</code> to <code>True</code> in release <code>0.16.0</code>! For clients using models with input size above <code>1024x1024</code> running models on hosted  platform it should improve predictions quality (as previous default behaviour was causing that input was downsized  and then artificially upsized on the server side with worse image quality).  There may be some clients that would like to remain previous settings to potentially improve speed ( when internet connection is a bottleneck and large images are submitted despite small  model input size). </p>"},{"location":"inference_helpers/inference_sdk/#configuration-of-workflows-execution","title":"Configuration of Workflows execution","text":"<ul> <li><code>profiling_directory</code>: parameter specify the location where Workflows profiler traces are saved. By default, it is <code>./inference_profiling</code> directory.</li> </ul>"},{"location":"inference_helpers/inference_sdk/#faqs","title":"FAQs","text":""},{"location":"inference_helpers/inference_sdk/#why-does-the-inference-client-have-two-modes-v0-and-v1","title":"Why does the Inference client have two modes (<code>v0</code> and <code>v1</code>)?","text":"<p>We are constantly improving our <code>infrence</code> package - initial version (<code>v0</code>) is compatible with models deployed on the Roboflow platform (task types: <code>classification</code>, <code>object-detection</code>, <code>instance-segmentation</code> and <code>keypoints-detection</code>) are supported. Version <code>v1</code> is available in locally hosted Docker images with HTTP API.</p> <p>Locally hosted <code>inference</code> server exposes endpoints for model manipulations, but those endpoints are not available at the moment for models deployed on the Roboflow platform.</p> <p><code>api_url</code> parameter passed to <code>InferenceHTTPClient</code> will decide on default client mode - URLs with <code>*.roboflow.com</code> will be defaulted to version <code>v0</code>.</p> <p>Usage of model registry control methods with <code>v0</code> clients will raise <code>WrongClientModeError</code>.</p>"},{"location":"models/from_local_weights/","title":"Models: Local Weights","text":"<p>You can upload supported weights to Roboflow and deploy them to your device.</p> <p>This is ideal if you have already trained a model outside of Roboflow that you want to deploy with Inference.</p> <p>To upload weights to Roboflow, you will need:</p> <ol> <li>A Roboflow account</li> <li>A project with your dataset (that does not have a trained model)</li> </ol> <p>To learn how to create a project and a dataset, refer to these guides:</p> <ul> <li>Create a project</li> <li>Create a dataset</li> </ul> <p>Once you have a project with a dataset, you can upload your weights.</p> <p>Install the Roboflow Python package:</p> <pre><code>pip install roboflow\n</code></pre> <p>Then, create a new Python file and add the following code:</p> <pre><code>import roboflow\n\nroboflow.login()\n\nrf = roboflow.Roboflow()\nproject = rf.project(\"your-project-id\")\nversion = project.version(1)\nversion.deploy(\"model-type\", \"path/to/training/results/\")\n</code></pre> <p>The following model types are supported:</p> Model Architecture Task Model Type ID YOLOv5 Object Detection yolov5 YOLOv5 Segmentation yolov5-seg YOLOv7 Object Detection yolov7-seg YOLOv8 Object Detection yolov8 YOLOv8 Segmentation yolov8-seg YOLOv8 Classification yolov8-cls YOLOv8 Pose Estimation yolov8-pose YOLOv9 Object Detection yolov9 YOLOv9 Segmentation yolov9 YOLO-NAS Object Detection yolonas YOLOv10 Object Detection yolov10 PaliGemma Multimodal paligemma-3b-pt-224 PaliGemma Multimodal paligemma-3b-pt-448 PaliGemma Multimodal paligemma-3b-pt-896 Florence-2 Multimodal florence-2-large Florence-2 Multimodal florence-2-base <p>In the code above, replace:</p> <ol> <li><code>your-project-id</code> with the ID of your project. Learn how to retrieve your Roboflow project ID.</li> <li><code>1</code> with the version number of your project.</li> <li><code>model-type</code> with the model type you want to deploy.</li> <li><code>path/to/training/results/</code> with the path to the weights you want to upload. This path will vary depending on what model architecture you are using.</li> </ol> <p>Your model weights will be uploaded to Roboflow. It may take a few minutes for your weights to be processed. Once your weights have been processed, your dataset version page will be updated to say that a model is available with your weights.</p> <p>You can then use the model with Inference following our Run a Private, Fine-Tuned Model model.</p>"},{"location":"notebooks/clip_classification/","title":"CLIP Classify Content of Video","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install supervision opencv-python\n</pre> !pip install supervision opencv-python In\u00a0[25]: Copied! <pre>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport supervision as sv\nfrom tqdm import tqdm\nfrom supervision import get_video_frames_generator\nimport time\n\nINFERENCE_ENDPOINT = \"https://infer.roboflow.com\"\nAPI_KEY = \"YOUR_API_KEY\"\nVIDEO = \"VIDEO_PATH\"\n</pre> import requests import base64 from PIL import Image from io import BytesIO import os import supervision as sv from tqdm import tqdm from supervision import get_video_frames_generator import time  INFERENCE_ENDPOINT = \"https://infer.roboflow.com\" API_KEY = \"YOUR_API_KEY\" VIDEO = \"VIDEO_PATH\"   In\u00a0[\u00a0]: Copied! <pre>#Prompt list to evaluate similarity between each image and each prompt. If something else is selected, then we ignore the caption\n#change this to your desired prompt list\nprompt_list = [['action video game shooting xbox','Drake rapper music','soccer game ball',\n                'marvel combic book','beyonce','Church pope praying',\n                'Mcdonalds French Fries',\"something else\"]]\n</pre> #Prompt list to evaluate similarity between each image and each prompt. If something else is selected, then we ignore the caption #change this to your desired prompt list prompt_list = [['action video game shooting xbox','Drake rapper music','soccer game ball',                 'marvel combic book','beyonce','Church pope praying',                 'Mcdonalds French Fries',\"something else\"]] In\u00a0[26]: Copied! <pre>def classify_image(image: str, prompt: str) -&gt; dict:\n    \n    image_data = Image.fromarray(image)\n\n    buffer = BytesIO()\n    image_data.save(buffer, format=\"JPEG\")\n    image_data = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n\n    payload = {\n        \"api_key\": API_KEY,\n        \"subject\": {\n            \"type\": \"base64\",\n            \"value\": image_data\n        },\n        \"prompt\": prompt,\n    }\n\n    data = requests.post(INFERENCE_ENDPOINT + \"/clip/compare?api_key=\" + API_KEY, json=payload)\n\n    response = data.json()\n    #print(response[\"similarity\"])\n    sim = response[\"similarity\"]\n\n    highest_prediction = 0\n    highest_prediction_index = 0\n\n    for i, prediction in enumerate(response[\"similarity\"]):\n        if prediction &gt; highest_prediction:\n            highest_prediction = prediction\n            highest_prediction_index = i\n\n    return prompt[highest_prediction_index], sim[highest_prediction_index]\n</pre> def classify_image(image: str, prompt: str) -&gt; dict:          image_data = Image.fromarray(image)      buffer = BytesIO()     image_data.save(buffer, format=\"JPEG\")     image_data = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")      payload = {         \"api_key\": API_KEY,         \"subject\": {             \"type\": \"base64\",             \"value\": image_data         },         \"prompt\": prompt,     }      data = requests.post(INFERENCE_ENDPOINT + \"/clip/compare?api_key=\" + API_KEY, json=payload)      response = data.json()     #print(response[\"similarity\"])     sim = response[\"similarity\"]      highest_prediction = 0     highest_prediction_index = 0      for i, prediction in enumerate(response[\"similarity\"]):         if prediction &gt; highest_prediction:             highest_prediction = prediction             highest_prediction_index = i      return prompt[highest_prediction_index], sim[highest_prediction_index] In\u00a0[\u00a0]: Copied! <pre>def process_video_frames(video_path, prompt_list, total_frames=160, total_seconds=80, stride_length=30,max_retries):\n    if not os.path.exists(video_path):\n        print(f\"The specified video file '{video_path}' does not exist.\")\n        return\n\n    frames_per_second = total_frames / total_seconds\n    frame_dict = {}\n\n    for frame_index, frame in enumerate(sv.get_video_frames_generator(source_path=video_path, stride=stride_length, start=0)):\n        frame_second = frame_index * (1 / frames_per_second)\n        frame_key = f\"Frame {frame_index}: {frame_second:.2f} seconds\"\n        frame_dict[frame_key] = []\n\n        print(frame_key)\n        retries = 0\n\n        for prompt in prompt_list:\n            try: \n                label, similarity = classify_image(frame)\n                if label != \"something else\":\n                    print('label found')\n                    frame_dict[frame_key].append({label: similarity})\n                    print('\\n')\n\n            except Exception as e:\n                retries += 1\n                print(f\"Error: {e}\")\n                print(f\"Retrying... (Attempt {retries}/{max_retries})\")\n\n                if retries &gt;= max_retries:\n                    print(\"Max retries exceeded. Skipping frame.\")\n                    break\n\n    return frame_dict\n\n# Example usage:\nmax_retries = 4\nprompt_list = prompt_list\nclip_results = process_video_frames(VIDEO, prompt_list,max_retries)\n</pre>  def process_video_frames(video_path, prompt_list, total_frames=160, total_seconds=80, stride_length=30,max_retries):     if not os.path.exists(video_path):         print(f\"The specified video file '{video_path}' does not exist.\")         return      frames_per_second = total_frames / total_seconds     frame_dict = {}      for frame_index, frame in enumerate(sv.get_video_frames_generator(source_path=video_path, stride=stride_length, start=0)):         frame_second = frame_index * (1 / frames_per_second)         frame_key = f\"Frame {frame_index}: {frame_second:.2f} seconds\"         frame_dict[frame_key] = []          print(frame_key)         retries = 0          for prompt in prompt_list:             try:                  label, similarity = classify_image(frame)                 if label != \"something else\":                     print('label found')                     frame_dict[frame_key].append({label: similarity})                     print('\\n')              except Exception as e:                 retries += 1                 print(f\"Error: {e}\")                 print(f\"Retrying... (Attempt {retries}/{max_retries})\")                  if retries &gt;= max_retries:                     print(\"Max retries exceeded. Skipping frame.\")                     break      return frame_dict  # Example usage: max_retries = 4 prompt_list = prompt_list clip_results = process_video_frames(VIDEO, prompt_list,max_retries)  In\u00a0[\u00a0]: Copied! <pre># Flatten the nested dictionary\ndata = clip_results\n# Define the threshold based on the similarity score returned for the most similar prompt\nthreshold = 0.22\n\n# Filter out key-value pairs below the threshold for each frame\nfiltered_data = [\n    {\n        frame: [\n            {key: value}\n            for item in items\n            for key, value in item.items()\n            if value &gt; threshold\n        ]\n    }\n    for frame, items in data.items()\n]\nprint(filtered_data)\n</pre> # Flatten the nested dictionary data = clip_results # Define the threshold based on the similarity score returned for the most similar prompt threshold = 0.22  # Filter out key-value pairs below the threshold for each frame filtered_data = [     {         frame: [             {key: value}             for item in items             for key, value in item.items()             if value &gt; threshold         ]     }     for frame, items in data.items() ] print(filtered_data) In\u00a0[44]: Copied! <pre># Specify the filename for the JSON file\nimport json\nfilename = f\"{str(threshold)}.json\"\n\n# Write the dictionary to the JSON file\nwith open(filename, 'w') as json_file:\n    json.dump(filtered_data, json_file, indent=4)  # The indent parameter is optional for pretty-printing\n\n#print(f'Data has been written to {filename})\n</pre> # Specify the filename for the JSON file import json filename = f\"{str(threshold)}.json\"  # Write the dictionary to the JSON file with open(filename, 'w') as json_file:     json.dump(filtered_data, json_file, indent=4)  # The indent parameter is optional for pretty-printing  #print(f'Data has been written to {filename})"},{"location":"notebooks/clip_classification/#clip-classify-content-of-video","title":"CLIP Classify Content of Video\u00b6","text":"<p>CLIP is a powerful foundation model for zero-shot classification. In this scenario, we are using CLIP to classify the topics in a Youtube video. Plug in your own video and set of prompts!</p> <p>Click the Open in Colab button to run the cookbook on Google Colab.</p> <p>Let's begin!</p>"},{"location":"notebooks/clip_classification/#install-required-packages","title":"Install required packages\u00b6","text":"<p>In this cookbook, we'll leverage two Python packages - <code>opencv</code> and <code>supervision</code></p>"},{"location":"notebooks/clip_classification/#imports-configure-roboflow-inference-server","title":"Imports &amp; Configure Roboflow Inference Server\u00b6","text":""},{"location":"notebooks/clip_classification/#prompt-list-for-clip-similarity-function","title":"Prompt List for CLIP similarity function\u00b6","text":""},{"location":"notebooks/clip_classification/#clip-endpoint-compare-frame-prompt-list-similarity","title":"CLIP Endpoint Compare Frame &amp; Prompt List Similarity\u00b6","text":""},{"location":"notebooks/clip_classification/#process-video-return-most-similar-prompt-to-frame","title":"Process Video &amp; Return Most Similar Prompt to Frame\u00b6","text":""},{"location":"notebooks/clip_classification/#create-json-file-and-filter-out-low-similarity-classes","title":"Create JSON file and filter out low similarity classes\u00b6","text":""},{"location":"notebooks/inference_pipeline_rtsp/","title":"InferencePipeline on RTSP Stream","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install inference supervision==0.18.0\n</pre> !pip install inference supervision==0.18.0 In\u00a0[\u00a0]: Copied! <pre>from inference.core.interfaces.stream.inference_pipeline import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import render_boxes\nimport supervision as sv\nimport pandas as pd\nfrom collections import defaultdict\nimport cv2\nimport numpy as np\nimport time\n</pre> from inference.core.interfaces.stream.inference_pipeline import InferencePipeline from inference.core.interfaces.stream.sinks import render_boxes import supervision as sv import pandas as pd from collections import defaultdict import cv2 import numpy as np import time  In\u00a0[\u00a0]: Copied! <pre># Create an instance of FPSMonitor\nfps_monitor = sv.FPSMonitor()\n\nREGISTERED_ALIASES = {\n    \"yolov8n-640\": \"coco/3\",\n    \"yolov8n-1280\": \"coco/9\",\n    \"yolov8m-640\": \"coco/8\"\n}\n\nAPI_KEY = \"API_KEY\"\nRTSP_STREAM = \"RTSP_URL\"\n\n# Example alias\nalias = \"yolov8n-640\"\n\n# Function to resolve an alias to the actual model ID\ndef resolve_roboflow_model_alias(model_id: str) -&gt; str:\n    return REGISTERED_ALIASES.get(model_id, model_id)\n\n# Resolve the alias to get the actual model ID\nmodel_name = resolve_roboflow_model_alias(alias)\n\n# Modify the render_boxes function to enable displaying statistics\ndef on_prediction(predictions, video_frame):\n    render_boxes(\n        predictions=predictions,\n        video_frame=video_frame,\n        fps_monitor=fps_monitor,  # Pass the FPS monitor object\n        display_statistics=True,   # Enable displaying statistics\n    )\n    \npipeline = InferencePipeline.init(\n    model_id= model_name,\n    video_reference=RTSP_STREAM,\n    on_prediction=on_prediction,\n    api_key=API_KEY,\n    confidence=0.5,\n)\n\npipeline.start()\npipeline.join()\n</pre> # Create an instance of FPSMonitor fps_monitor = sv.FPSMonitor()  REGISTERED_ALIASES = {     \"yolov8n-640\": \"coco/3\",     \"yolov8n-1280\": \"coco/9\",     \"yolov8m-640\": \"coco/8\" }  API_KEY = \"API_KEY\" RTSP_STREAM = \"RTSP_URL\"  # Example alias alias = \"yolov8n-640\"  # Function to resolve an alias to the actual model ID def resolve_roboflow_model_alias(model_id: str) -&gt; str:     return REGISTERED_ALIASES.get(model_id, model_id)  # Resolve the alias to get the actual model ID model_name = resolve_roboflow_model_alias(alias)  # Modify the render_boxes function to enable displaying statistics def on_prediction(predictions, video_frame):     render_boxes(         predictions=predictions,         video_frame=video_frame,         fps_monitor=fps_monitor,  # Pass the FPS monitor object         display_statistics=True,   # Enable displaying statistics     )      pipeline = InferencePipeline.init(     model_id= model_name,     video_reference=RTSP_STREAM,     on_prediction=on_prediction,     api_key=API_KEY,     confidence=0.5, )  pipeline.start() pipeline.join() In\u00a0[\u00a0]: Copied! <pre>#ByteTrack &amp; Supervision\ntracker = sv.ByteTrack()\nannotator = sv.BoxAnnotator()\nframe_count = defaultdict(int)\ncolors = sv.ColorPalette.default()\n\n#define polygon zone of interest\npolygons = [\nnp.array([\n[390, 543],[1162, 503],[1510, 711],[410, 819],[298, 551],[394, 543]\n])\n]\n\n#create zones, zone_annotator, and box_annotator based on polygon zone of interest\nzones = [\n    sv.PolygonZone(\n        polygon=polygon,\n        frame_resolution_wh=[1440,2560],\n    )\n    for polygon\n    in polygons\n]\nzone_annotators = [\n    sv.PolygonZoneAnnotator(\n        zone=zone,\n        color=colors.by_idx(index),\n        thickness=4,\n        text_thickness=8,\n        text_scale=4\n    )\n    for index, zone\n    in enumerate(zones)\n]\nbox_annotators = [\n    sv.BoxAnnotator(\n        color=colors.by_idx(index),\n        thickness=4,\n        text_thickness=4,\n        text_scale=2\n        )\n    for index\n    in range(len(polygons))\n]\n\n\n#columns for csv output\ncolumns = ['trackerID', 'class_id', 'frame_count','entry_timestamp','exit_timestamp','time_in_zone']\nframe_count_df = pd.DataFrame(columns=columns)\n\n# Define a dictionary to store the first detection timestamp for each tracker_id\nfirst_detection_timestamps = {}\nlast_detection_timestamps = {}\n\ndef render(predictions: dict, video_frame) -&gt; None:\n    detections = sv.Detections.from_inference(predictions)\n    detections = tracker.update_with_detections(detections)\n    \n    for zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):\n        mask = zone.trigger(detections=detections)\n        detections_filtered = detections[mask]\n        \n        image = box_annotator.annotate(scene=video_frame.image, detections=detections, skip_label=False)\n        image = zone_annotator.annotate(scene=image)\n        \n        for tracker_id, class_id in zip(detections_filtered.tracker_id, detections_filtered.class_id):\n            frame_count[tracker_id] += 1\n            \n            # Check if tracker_id is not in first_detection_timestamps, if not, add the timestamp\n            if tracker_id not in first_detection_timestamps:\n                first_detection_timestamps[tracker_id] = time.time()\n            \n            last_detection_timestamps[tracker_id] = time.time()\n            \n            time_difference = last_detection_timestamps[tracker_id] - first_detection_timestamps[tracker_id]\n            \n            # Add data to the DataFrame\n            frame_count_df.loc[tracker_id] = [tracker_id, class_id, frame_count[tracker_id], first_detection_timestamps[tracker_id],last_detection_timestamps[tracker_id], time_difference]\n    \n    frame_count_df.to_csv('demo.csv', index=False)\n    \n    cv2.imshow(\"Prediction\", image)\n    cv2.waitKey(1)\n    \n\n#Initialize &amp; Deploy InferencePipeline\npipeline = InferencePipeline.init(\n    model_id=\"coco/8\",\n    video_reference=\"RTSP_URL\",\n    on_prediction=render,\n    api_key = 'API_KEY',\n    confidence=0.5,\n)\npipeline.start()\npipeline.join()\n</pre> #ByteTrack &amp; Supervision tracker = sv.ByteTrack() annotator = sv.BoxAnnotator() frame_count = defaultdict(int) colors = sv.ColorPalette.default()  #define polygon zone of interest polygons = [ np.array([ [390, 543],[1162, 503],[1510, 711],[410, 819],[298, 551],[394, 543] ]) ]  #create zones, zone_annotator, and box_annotator based on polygon zone of interest zones = [     sv.PolygonZone(         polygon=polygon,         frame_resolution_wh=[1440,2560],     )     for polygon     in polygons ] zone_annotators = [     sv.PolygonZoneAnnotator(         zone=zone,         color=colors.by_idx(index),         thickness=4,         text_thickness=8,         text_scale=4     )     for index, zone     in enumerate(zones) ] box_annotators = [     sv.BoxAnnotator(         color=colors.by_idx(index),         thickness=4,         text_thickness=4,         text_scale=2         )     for index     in range(len(polygons)) ]   #columns for csv output columns = ['trackerID', 'class_id', 'frame_count','entry_timestamp','exit_timestamp','time_in_zone'] frame_count_df = pd.DataFrame(columns=columns)  # Define a dictionary to store the first detection timestamp for each tracker_id first_detection_timestamps = {} last_detection_timestamps = {}  def render(predictions: dict, video_frame) -&gt; None:     detections = sv.Detections.from_inference(predictions)     detections = tracker.update_with_detections(detections)          for zone, zone_annotator, box_annotator in zip(zones, zone_annotators, box_annotators):         mask = zone.trigger(detections=detections)         detections_filtered = detections[mask]                  image = box_annotator.annotate(scene=video_frame.image, detections=detections, skip_label=False)         image = zone_annotator.annotate(scene=image)                  for tracker_id, class_id in zip(detections_filtered.tracker_id, detections_filtered.class_id):             frame_count[tracker_id] += 1                          # Check if tracker_id is not in first_detection_timestamps, if not, add the timestamp             if tracker_id not in first_detection_timestamps:                 first_detection_timestamps[tracker_id] = time.time()                          last_detection_timestamps[tracker_id] = time.time()                          time_difference = last_detection_timestamps[tracker_id] - first_detection_timestamps[tracker_id]                          # Add data to the DataFrame             frame_count_df.loc[tracker_id] = [tracker_id, class_id, frame_count[tracker_id], first_detection_timestamps[tracker_id],last_detection_timestamps[tracker_id], time_difference]          frame_count_df.to_csv('demo.csv', index=False)          cv2.imshow(\"Prediction\", image)     cv2.waitKey(1)       #Initialize &amp; Deploy InferencePipeline pipeline = InferencePipeline.init(     model_id=\"coco/8\",     video_reference=\"RTSP_URL\",     on_prediction=render,     api_key = 'API_KEY',     confidence=0.5, ) pipeline.start() pipeline.join()"},{"location":"notebooks/inference_pipeline_rtsp/#inferencepipeline-on-rtsp-stream","title":"InferencePipeline on RTSP Stream\u00b6","text":"<p>The Roboflow Inference Pipeline is a drop-in replacement for the Hosted Inference API that can be deployed on your own hardware. The Inference Pipeline interface is made for streaming and is likely the best route to go for real time use cases. It is an asynchronous interface that can consume many different video sources including local devices (like webcams), RTSP video streams, video files, etc. With this interface, you define the source of a video stream and sinks.</p> <p>We have optimized Inference Pipeline to get maximum performance from the NVIDIA Jetson line of edge-AI devices. We have done this by specifically tailoring the drivers, libraries, and binaries specifically to its CPU and GPU architectures.</p> <p>Let's begin!</p>"},{"location":"notebooks/inference_pipeline_rtsp/#install-required-packages","title":"Install required packages\u00b6","text":"<p>In this cookbook, we'll leverage two Python packages - <code>inference</code> and <code>supervision</code></p>"},{"location":"notebooks/inference_pipeline_rtsp/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/inference_pipeline_rtsp/#run-inference-pipeline-with-coco-model-aliases-native-fps-monitor","title":"Run Inference Pipeline with COCO Model Aliases &amp; Native FPS Monitor\u00b6","text":""},{"location":"notebooks/inference_pipeline_rtsp/#time-in-zone-with-bytetrack-using-supervision-save-data-to-csv","title":"Time in Zone with Bytetrack using Supervision, save data to CSV\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/","title":"RGB Anomaly Detection","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install sklearn\n</pre> !pip install sklearn In\u00a0[\u00a0]: Copied! <pre>import cv2\nimport numpy as np\nimport time\nimport base64\nimport requests\nimport os, glob\nfrom sklearn.cluster import KMeans\n</pre> import cv2 import numpy as np import time import base64 import requests import os, glob from sklearn.cluster import KMeans In\u00a0[\u00a0]: Copied! <pre>def parse_polygon_annotation(annotation_data, image_shape):\n    width, height = image_shape[1], image_shape[0]\n    return [(int(data['x']), int(data['y'])) for data in annotation_data]\n\ndef extract_polygon_area(image_path, polygon_points):\n    image = cv2.imread(image_path)\n    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n    cv2.drawContours(mask, [np.array(polygon_points)], -1, (255, 255, 255), -1)\n    return cv2.bitwise_and(image, image, mask=mask)\n\ndef compute_average_color(image):\n    mask = np.all(image != [0, 0, 0], axis=2)\n    avg_color = np.mean(image[mask], axis=0)\n    return avg_color\n\ndef color_difference(color1, color2):\n    return np.linalg.norm(np.array(color1) - np.array(color2))\n\ndef count_color_matches(dominant_colors, target_colors, threshold):\n    matches_count = {tuple(target): 0 for target in target_colors}\n    matched_colors = {tuple(target): [] for target in target_colors}\n    \n    for color in dominant_colors:\n        for target in target_colors:\n            difference = color_difference(color, target)\n            \n            if difference &lt; threshold:\n                matches_count[tuple(target)] += 1\n                matched_colors[tuple(target)].append(color)\n                \n    return matches_count, matched_colors\n\ndef get_dominant_colors(image, k=5):\n    image = image.reshape((image.shape[0] * image.shape[1], 3))\n    image = image[np.any(image != [0, 0, 0], axis=1)]\n    kmeans = KMeans(n_clusters=k, n_init='auto')\n    kmeans.fit(image)\n    dominant_colors = kmeans.cluster_centers_\n    return dominant_colors\n\ndef extract_target_colors(target_image_path,inference_server_address, project_id, version_number):\n    target_image = cv2.imread(target_image_path)\n    with open(target_image_path, \"rb\") as f:\n        im_bytes = f.read()        \n    im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n\n    headers = {\n        'Content-Type': 'application/json',\n        'Accept': 'application/json'\n    }\n\n    params = {\n        'api_key': 'FFgkmScNUBERP9t3PJvV',\n    }\n\n    response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)\n    data = response.json()\n\n    for predictions in data['predictions']:\n        Pred_points = predictions['points']\n        target_image = cv2.imread(target_image_path)\n        polygon_points = parse_polygon_annotation(Pred_points, target_image.shape)\n        polygon_image = extract_polygon_area(target_image_path, polygon_points)\n        target_dominant_colors = get_dominant_colors(polygon_image)\n    \n    return target_dominant_colors\n\ndef match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold):\n    global prediction_counter, image_counter\n    total_matches = 0\n    matched_filepaths = []\n\n    extention_images = \".jpg\"\n    get_images = sorted(glob.glob(images_folder + '*' + extention_images))\n\n    for images in get_images:\n        t0 = time.time()\n        print(\"File path: \" + images)\n        img = cv2.imread(images)\n        with open(images, \"rb\") as f:\n            im_bytes = f.read()        \n        im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")\n        headers = {\n            'Content-Type': 'application/json',\n            'Accept': 'application/json'\n        }\n\n        params = {\n            'api_key': '',\n        }\n        \n        response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)\n        data = response.json()\n\n        for predictions in data['predictions']:\n            prediction_counter += 1\n            image_counter += 1\n            Pred_points = predictions['points']\n            image = cv2.imread(images)\n            polygon_points = parse_polygon_annotation(Pred_points, image.shape)\n            polygon_image = extract_polygon_area(images, polygon_points)\n            dominant_colors = get_dominant_colors(polygon_image)\n            matches, matched_colors_list = count_color_matches(dominant_colors, target_dominant_colors, color_threshold)\n        \n        all_matched = all(value &gt; 0 for value in matches.values())\n        \n        if all_matched:\n            matched_filepaths.append(images)\n            total_matches += 1\n\n    print(f\"\\nTotal images where all target colors matched: {total_matches}\")\n    print(f\"\\nMatched images where all target colors matched: {matched_filepaths}\")\n</pre> def parse_polygon_annotation(annotation_data, image_shape):     width, height = image_shape[1], image_shape[0]     return [(int(data['x']), int(data['y'])) for data in annotation_data]  def extract_polygon_area(image_path, polygon_points):     image = cv2.imread(image_path)     mask = np.zeros(image.shape[:2], dtype=np.uint8)     cv2.drawContours(mask, [np.array(polygon_points)], -1, (255, 255, 255), -1)     return cv2.bitwise_and(image, image, mask=mask)  def compute_average_color(image):     mask = np.all(image != [0, 0, 0], axis=2)     avg_color = np.mean(image[mask], axis=0)     return avg_color  def color_difference(color1, color2):     return np.linalg.norm(np.array(color1) - np.array(color2))  def count_color_matches(dominant_colors, target_colors, threshold):     matches_count = {tuple(target): 0 for target in target_colors}     matched_colors = {tuple(target): [] for target in target_colors}          for color in dominant_colors:         for target in target_colors:             difference = color_difference(color, target)                          if difference &lt; threshold:                 matches_count[tuple(target)] += 1                 matched_colors[tuple(target)].append(color)                      return matches_count, matched_colors  def get_dominant_colors(image, k=5):     image = image.reshape((image.shape[0] * image.shape[1], 3))     image = image[np.any(image != [0, 0, 0], axis=1)]     kmeans = KMeans(n_clusters=k, n_init='auto')     kmeans.fit(image)     dominant_colors = kmeans.cluster_centers_     return dominant_colors  def extract_target_colors(target_image_path,inference_server_address, project_id, version_number):     target_image = cv2.imread(target_image_path)     with open(target_image_path, \"rb\") as f:         im_bytes = f.read()             im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")      headers = {         'Content-Type': 'application/json',         'Accept': 'application/json'     }      params = {         'api_key': 'FFgkmScNUBERP9t3PJvV',     }      response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)     data = response.json()      for predictions in data['predictions']:         Pred_points = predictions['points']         target_image = cv2.imread(target_image_path)         polygon_points = parse_polygon_annotation(Pred_points, target_image.shape)         polygon_image = extract_polygon_area(target_image_path, polygon_points)         target_dominant_colors = get_dominant_colors(polygon_image)          return target_dominant_colors  def match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold):     global prediction_counter, image_counter     total_matches = 0     matched_filepaths = []      extention_images = \".jpg\"     get_images = sorted(glob.glob(images_folder + '*' + extention_images))      for images in get_images:         t0 = time.time()         print(\"File path: \" + images)         img = cv2.imread(images)         with open(images, \"rb\") as f:             im_bytes = f.read()                 im_b64 = base64.b64encode(im_bytes).decode(\"utf8\")         headers = {             'Content-Type': 'application/json',             'Accept': 'application/json'         }          params = {             'api_key': '',         }                  response = requests.post(inference_server_address + project_id + str(version_number), params=params, headers=headers, data=im_b64)         data = response.json()          for predictions in data['predictions']:             prediction_counter += 1             image_counter += 1             Pred_points = predictions['points']             image = cv2.imread(images)             polygon_points = parse_polygon_annotation(Pred_points, image.shape)             polygon_image = extract_polygon_area(images, polygon_points)             dominant_colors = get_dominant_colors(polygon_image)             matches, matched_colors_list = count_color_matches(dominant_colors, target_dominant_colors, color_threshold)                  all_matched = all(value &gt; 0 for value in matches.values())                  if all_matched:             matched_filepaths.append(images)             total_matches += 1      print(f\"\\nTotal images where all target colors matched: {total_matches}\")     print(f\"\\nMatched images where all target colors matched: {matched_filepaths}\") In\u00a0[\u00a0]: Copied! <pre>def main():\n    target_image_path = \"TARGET_IMAGE_PATH\"\n    inference_server_address = \"http://detect.roboflow.com/\"\n    version_number = 1\n    project_id = \"PROJECT_ID\"\n    images_folder = \"IMAGE_FOLDER_PATH\"\n    # grab all the .jpg files\n    extention_images = \".jpg\"\n    get_images = sorted(glob.glob(images_folder + '*' + extention_images))\n    MAX_COLOR_DIFFERENCE = 3 * 256 # DO NOT EDIT\n    TARGET_COLOR_PERCENT_THRESHOLD= 0.08 # Value must be between 0 - 1 - DO EDIT\n    color_threshold = int(MAX_COLOR_DIFFERENCE * TARGET_COLOR_PERCENT_THRESHOLD)\n\n\n    target_dominant_colors = extract_target_colors(target_image_path,inference_server_address, project_id, version_number)\n    match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold)\n\nif __name__ == \"__main__\":\n    main()\n</pre> def main():     target_image_path = \"TARGET_IMAGE_PATH\"     inference_server_address = \"http://detect.roboflow.com/\"     version_number = 1     project_id = \"PROJECT_ID\"     images_folder = \"IMAGE_FOLDER_PATH\"     # grab all the .jpg files     extention_images = \".jpg\"     get_images = sorted(glob.glob(images_folder + '*' + extention_images))     MAX_COLOR_DIFFERENCE = 3 * 256 # DO NOT EDIT     TARGET_COLOR_PERCENT_THRESHOLD= 0.08 # Value must be between 0 - 1 - DO EDIT     color_threshold = int(MAX_COLOR_DIFFERENCE * TARGET_COLOR_PERCENT_THRESHOLD)       target_dominant_colors = extract_target_colors(target_image_path,inference_server_address, project_id, version_number)     match_images_with_target_colors(target_dominant_colors, images_folder, inference_server_address, project_id, version_number, color_threshold)  if __name__ == \"__main__\":     main()"},{"location":"notebooks/rgb_anomaly_detection/#rgb-anomaly-detection","title":"RGB Anomaly Detection\u00b6","text":"<p>In this cookbook, we identify color / RGB anomalies for segmented items. Capture a base image to extract your ground truth RGB with Roboflow and compare to neew data collected. In this scenario, we are assessing variations in logo color.</p> <p>Click the Open in Colab button to run the cookbook on Google Colab.</p> <p>Let's begin!</p>"},{"location":"notebooks/rgb_anomaly_detection/#install-required-packages","title":"Install required packages\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/#extract-target-rgb-color-from-polygon-and-run-kmeans","title":"Extract target RGB color from polygon and run Kmeans\u00b6","text":""},{"location":"notebooks/rgb_anomaly_detection/#run-main-function-to-compare-base-color-logo-with-target-colors-and-run-anomaly-detection","title":"Run main function to compare base color logo with target colors and run anomaly detection\u00b6","text":""},{"location":"notebooks/workflow_schema_api/","title":"Workflow Schema API","text":"In\u00a0[\u00a0]: Copied! <pre>from google.colab import userdata\n\n\nWORKSPACE_NAME = \"nicks-workspace\"\nWORKFLOW_ID= \"detect-people\"\nINFERENCE_SERVER_URL = \"https://detect.roboflow.com\"\n\nWORKFLOW_SCHEMA_ENDPOINT = f\"{INFERENCE_SERVER_URL}/{WORKSPACE_NAME}/workflows/{WORKFLOW_ID}/describe_interface\"\nROBOFLOW_API_KEY = userdata.get(\"ROBOFLOW_API_KEY\")\n</pre> from google.colab import userdata   WORKSPACE_NAME = \"nicks-workspace\" WORKFLOW_ID= \"detect-people\" INFERENCE_SERVER_URL = \"https://detect.roboflow.com\"  WORKFLOW_SCHEMA_ENDPOINT = f\"{INFERENCE_SERVER_URL}/{WORKSPACE_NAME}/workflows/{WORKFLOW_ID}/describe_interface\" ROBOFLOW_API_KEY = userdata.get(\"ROBOFLOW_API_KEY\") In\u00a0[\u00a0]: Copied! <pre>import requests\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n}\n\ndata = {\n    \"api_key\": ROBOFLOW_API_KEY,\n}\n\nres = requests.post(WORKFLOW_SCHEMA_ENDPOINT, headers=headers, json=data)\n\nschema = res.json()\n\ninputs = schema[\"inputs\"]\noutputs = schema[\"outputs\"]\nkinds_schemas = schema[\"kinds_schemas\"]\ntyping_hints = schema[\"typing_hints\"]\n</pre> import requests  headers = {     \"Content-Type\": \"application/json\", }  data = {     \"api_key\": ROBOFLOW_API_KEY, }  res = requests.post(WORKFLOW_SCHEMA_ENDPOINT, headers=headers, json=data)  schema = res.json()  inputs = schema[\"inputs\"] outputs = schema[\"outputs\"] kinds_schemas = schema[\"kinds_schemas\"] typing_hints = schema[\"typing_hints\"]  In\u00a0[\u00a0]: Copied! <pre>from pprint import pprint\npprint(inputs)\npprint(outputs)\n</pre> from pprint import pprint pprint(inputs) pprint(outputs) <pre>{'image': ['image'], 'model_name': ['roboflow_model_id']}\n{'model_predictions': ['object_detection_prediction']}\n</pre> In\u00a0[\u00a0]: Copied! <pre>pprint(typing_hints)\n</pre> pprint(typing_hints) <pre>{'image': 'dict',\n 'object_detection_prediction': 'dict',\n 'roboflow_model_id': 'str'}\n</pre> In\u00a0[\u00a0]: Copied! <pre>pprint(kinds_schemas)\n</pre> pprint(kinds_schemas) <pre>{'image': {'properties': {'type': {'const': 'url',\n                                   'enum': ['url'],\n                                   'title': 'Type',\n                                   'type': 'string'},\n                          'value': {'description': 'Value depends on `type` - '\n                                                   'for url, one should '\n                                                   'provide URL to the file, '\n                                                   'for `file` - local path, '\n                                                   'for `base64` - base64 '\n                                                   'string.',\n                                    'title': 'Value'}},\n           'required': ['type', 'value'],\n           'title': 'ImageSchema',\n           'type': 'object'},\n 'object_detection_prediction': {'$defs': {'BoundingBoxSchema': {'properties': {'class': {'description': 'Name '\n                                                                                                         'of '\n                                                                                                         'the '\n                                                                                                         'class '\n                                                                                                         'associated '\n                                                                                                         'to '\n                                                                                                         'bounding '\n                                                                                                         'box',\n                                                                                          'title': 'class',\n                                                                                          'type': 'string'},\n                                                                                'class_id': {'description': 'Identifier '\n                                                                                                            'of '\n                                                                                                            'bounding '\n                                                                                                            'box '\n                                                                                                            'class',\n                                                                                             'title': 'Class '\n                                                                                                      'Id',\n                                                                                             'type': 'integer'},\n                                                                                'confidence': {'description': 'Model '\n                                                                                                              'confidence '\n                                                                                                              'for '\n                                                                                                              'bounding '\n                                                                                                              'box',\n                                                                                               'title': 'Confidence',\n                                                                                               'type': 'number'},\n                                                                                'detection_id': {'description': 'Identifier '\n                                                                                                                'of '\n                                                                                                                'detected '\n                                                                                                                'bounding '\n                                                                                                                'box',\n                                                                                                 'title': 'Detection '\n                                                                                                          'Id',\n                                                                                                 'type': 'string'},\n                                                                                'height': {'anyOf': [{'type': 'integer'},\n                                                                                                     {'type': 'number'}],\n                                                                                           'description': 'Height '\n                                                                                                          'of '\n                                                                                                          'bounding '\n                                                                                                          'box',\n                                                                                           'title': 'Height'},\n                                                                                'parent_id': {'anyOf': [{'type': 'string'},\n                                                                                                        {'type': 'null'}],\n                                                                                              'default': None,\n                                                                                              'description': 'Identifier '\n                                                                                                             'of '\n                                                                                                             'parent '\n                                                                                                             'image '\n                                                                                                             'region. '\n                                                                                                             'Useful '\n                                                                                                             'when '\n                                                                                                             'stack '\n                                                                                                             'of '\n                                                                                                             'detection-models '\n                                                                                                             'is '\n                                                                                                             'in '\n                                                                                                             'use '\n                                                                                                             'to '\n                                                                                                             'refer '\n                                                                                                             'the '\n                                                                                                             'RoI '\n                                                                                                             'being '\n                                                                                                             'the '\n                                                                                                             'input '\n                                                                                                             'to '\n                                                                                                             'inference',\n                                                                                              'title': 'Parent '\n                                                                                                       'Id'},\n                                                                                'width': {'anyOf': [{'type': 'integer'},\n                                                                                                    {'type': 'number'}],\n                                                                                          'description': 'Width '\n                                                                                                         'of '\n                                                                                                         'bounding '\n                                                                                                         'box',\n                                                                                          'title': 'Width'},\n                                                                                'x': {'anyOf': [{'type': 'integer'},\n                                                                                                {'type': 'number'}],\n                                                                                      'description': 'OX '\n                                                                                                     'coordinate '\n                                                                                                     'of '\n                                                                                                     'bounding '\n                                                                                                     'box '\n                                                                                                     'center',\n                                                                                      'title': 'X'},\n                                                                                'y': {'anyOf': [{'type': 'integer'},\n                                                                                                {'type': 'number'}],\n                                                                                      'description': 'OY '\n                                                                                                     'coordinate '\n                                                                                                     'of '\n                                                                                                     'bounding '\n                                                                                                     'box '\n                                                                                                     'center',\n                                                                                      'title': 'Y'}},\n                                                                 'required': ['width',\n                                                                              'height',\n                                                                              'x',\n                                                                              'y',\n                                                                              'confidence',\n                                                                              'class',\n                                                                              'class_id',\n                                                                              'detection_id'],\n                                                                 'title': 'BoundingBoxSchema',\n                                                                 'type': 'object'},\n                                           'ImageMetadataSchema': {'properties': {'height': {'anyOf': [{'type': 'integer'},\n                                                                                                       {'type': 'null'}],\n                                                                                             'description': 'The '\n                                                                                                            'original '\n                                                                                                            'height '\n                                                                                                            'of '\n                                                                                                            'the '\n                                                                                                            'image '\n                                                                                                            'used '\n                                                                                                            'in '\n                                                                                                            'inference',\n                                                                                             'title': 'Height'},\n                                                                                  'width': {'anyOf': [{'type': 'integer'},\n                                                                                                      {'type': 'null'}],\n                                                                                            'description': 'The '\n                                                                                                           'original '\n                                                                                                           'width '\n                                                                                                           'of '\n                                                                                                           'the '\n                                                                                                           'image '\n                                                                                                           'used '\n                                                                                                           'in '\n                                                                                                           'inference',\n                                                                                            'title': 'Width'}},\n                                                                   'required': ['width',\n                                                                                'height'],\n                                                                   'title': 'ImageMetadataSchema',\n                                                                   'type': 'object'}},\n                                 'properties': {'image': {'$ref': '#/$defs/ImageMetadataSchema'},\n                                                'predictions': {'items': {'$ref': '#/$defs/BoundingBoxSchema'},\n                                                                'title': 'Predictions',\n                                                                'type': 'array'}},\n                                 'required': ['image', 'predictions'],\n                                 'title': 'ObjectDetectionSchema',\n                                 'type': 'object'}}\n</pre>"},{"location":"notebooks/workflow_schema_api/#workflow-schema-api","title":"Workflow Schema API\u00b6","text":"<p>Prior to the workflow schema API it was a challenge to be able to programatically know what inputs a given workflow takes and what type of data a workflow will return. The Workflow Schema API aims to solve this problem by providng a list of inputs, outputs, typing hints, and schemas of kinds.</p>"},{"location":"notebooks/workflow_schema_api/#setup","title":"Setup\u00b6","text":"<p>First let's define a few variable for use in our api request. the workspace name and workflow ID can be found and configured on a workflow by clicking the workspace pencil icon.</p> <p></p> <p>You can also point this request to a self hosted, or dedicated deployment inference server url. Make sure to configure your ROBOFLOW_API_KEY in Google Colab secrets if you'd like to run this notebook aswell.</p>"},{"location":"notebooks/workflow_schema_api/#workflow-configuration","title":"Workflow Configuration\u00b6","text":"<p>For this example, let's show a simple workflow that takes one parameter for the model id, and runs the input image on a object detection model. It then outputs the predictions from the model.</p> <p></p>"},{"location":"notebooks/workflow_schema_api/#api-request","title":"API Request\u00b6","text":"<p>Now that we've got our workflow built, Let's take a look at the code required to hit the API.</p>"},{"location":"notebooks/workflow_schema_api/#inputs-and-outputs","title":"Inputs and Outputs\u00b6","text":"<p>The inputs and outputs keys show all of the inputs/outputs the workflow expects to run and return. This workflow requires an image and a model_name and will return a \"model_predictions\".</p>"},{"location":"notebooks/workflow_schema_api/#typing-hints","title":"Typing Hints\u00b6","text":"<p>The typing_hints key show the type of data being returned from the request from a python data type perspective.</p>"},{"location":"notebooks/workflow_schema_api/#kinds-schemas","title":"Kinds Schemas\u00b6","text":"<p>The kinds_schemas keys return an Open API specification with more detailed information about the data type being returned and how to parse it. For example the 'object_detection_prediction' contains information about the nested data that will be present.</p>"},{"location":"quickstart/aliases/","title":"Models: Popular","text":"<p>Inference supports running any of the 50,000+ pre-trained public models hosted on Roboflow Universe, as well as fine-tuned models.</p> <p>We have defined IDs for common models for ease of use. These models do not require an API key for use unlike other public or private models.</p> <p>Using it in <code>inference</code> is as simple as:</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\n\nresults = model.infer(\"https://media.roboflow.com/inference/people-walking.jpg\")\n</code></pre> <p>Tip</p> <p>See the Use a fine-tuned model guide for an example on how to deploy your own model.</p>"},{"location":"quickstart/aliases/#supported-pre-trained-models","title":"Supported Pre-Trained Models","text":"<p>You can click the link associated with a model below to test the model in your browser, and use the ID with Inference to deploy the model to the edge.</p> Model Size Task Model ID Test Model in Browser YOLOv8n 640 Object Detection yolov8n-640 Test in Browser YOLOv8n 1280 Object Detection yolov8n-1280 Test in Browser YOLOv8s 640 Object Detection yolov8s-640 Test in Browser YOLOv8s 1280 Object Detection yolov8s-1280 Test in Browser YOLOv8m 640 640 Object Detection yolov8m-640 Test in Browser YOLOv8m 1280 Object Detection yolov8m-1280 Test in Browser YOLOv8l 640 Object Detection yolov8l-640 Test in Browser YOLOv8l 1280 Object Detection yolov8l-1280 Test in Browser YOLOv8x 640 Object Detection yolov8x-640 Test in Browser YOLOv8x 1280 Object Detection yolov8x-1280 Test in Browser YOLO-NAS (small) 640 Object Detection yolo-nas-s-640 Test in Browser YOLO-NAS (medium) 640 Object Detection yolo-nas-m-640 Test in Browser YOLO-NAS (large) 640 Object Detection yolo-nas-l-640 Test in Browser YOLOv8n Instance Segmentation 640 Instance Segmentation yolov8n-seg-640 Test in Browser YOLOv8n Instance Segmentation 1280 Instance Segmentation yolov8n-seg-1280 Test in Browser YOLOv8s Instance Segmentation 640 Instance Segmentation yolov8s-seg-640 Test in Browser YOLOv8m Instance Segmentation 1280 Instance Segmentation yolov8s-seg-1280 Test in Browser YOLOv8m Instance Segmentation 640 Instance Segmentation yolov8m-seg-640 Test in Browser YOLOv8m Instance Segmentation 1280 Instance Segmentation yolov8m-seg-1280 Test in Browser YOLOv8l Instance Segmentation 640 Instance Segmentation yolov8l-seg-640 Test in Browser YOLOv8l Instance Segmentation 1280 Instance Segmentation yolov8l-seg-1280 Test in Browser YOLOv8x Instance Segmentation 640 Instance Segmentation yolov8x-seg-640 Test in Browser YOLOv8x Instance Segmentation 1280 Instance Segmentation yolov8x-seg-1280 Test in Browser YOLOv8x Keypoint Detection 1280 Keypoint Detection yolov8x-pose-1280 Test in Browser YOLOv8x Keypoint Detection 640 Keypoint Detection yolov8x-pose-640 Test in Browser YOLOv8l Keypoint Detection 640 Keypoint Detection yolov8l-pose-640 Test in Browser YOLOv8m Keypoint Detection 640 Keypoint Detection yolov8m-pose-640 Test in Browser YOLOv8s Keypoint Detection 640 Keypoint Detection yolov8s-pose-640 Test in Browser YOLOv8n Keypoint Detection 640 Keypoint Detection yolov8n-pose-640 Test in Browser YOLOv10n 640 Object Detection yolov10n-640 Test in Browser YOLOv10s 640 Object Detection yolov10s-640 Test in Browser YOLOv10m 640 Object Detection yolov10m-640 Test in Browser YOLOv10b 640 Object Detection yolov10b-640 Test in Browser YOLOv10l 640 Object Detection yolov10l-640 Test in Browser YOLOv10x 640 Object Detection yolov10x-640 Test in Browser YOLOv11n 640 Object Detection yolov11n-640 Test in Browser YOLOv11s 640 Object Detection yolov11s-640 Test in Browser YOLOv11m 640 Object Detection yolov11m-640 Test in Browser YOLOv11l 640 Object Detection yolov11l-640 Test in Browser YOLOv11x 640 Object Detection yolov11x-640 Test in Browser YOLOv11n 1280 Object Detection yolov11n-1280 Test in Browser YOLOv11s 1280 Object Detection yolov11s-1280 Test in Browser YOLOv11m 1280 Object Detection yolov11m-1280 Test in Browser YOLOv11l 1280 Object Detection yolov11l-1280 Test in Browser YOLOv11x 1280 Object Detection yolov11x-1280 Test in Browser YOLOv11n 640 Instance Segmentation yolov11n-seg-640 Test in Browser YOLOv11s 640 Instance Segmentation yolov11s-seg-640 Test in Browser YOLOv11m 640 Instance Segmentation yolov11m-seg-640 Test in Browser YOLOv11l 640 Instance Segmentation yolov11l-seg-640 Test in Browser YOLOv11x 640 Instance Segmentation yolov11x-seg-640 Test in Browser"},{"location":"quickstart/compatability_matrix/","title":"Model Compatability","text":"<p>The table below shows on what devices you can deploy models supported by Inference.</p> <p>See our Docker Getting Started guide for more information on how to deploy Inference on your device.</p> <p>Table key:</p> <ul> <li>\u2705 Fully supported</li> <li>\ud83d\udeab Not supported</li> <li>\ud83d\udea7 On roadmap, not currently supported</li> </ul> Model CPU GPU Jetson 4.5.x Jetson 4.6.x Jetson 5.x Roboflow Hosted Inference YOLOv8 Object Detection \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Classification \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Segmentation \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv5 Object Detection \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Segmentation \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 CLIP \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 DocTR \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Gaze \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \u2705 SAM \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab ViT Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLACT \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"quickstart/configure_api_key/","title":"Roboflow API Key","text":"<p>Throughout these docs you will see references to your Roboflow API key. Using your Roboflow API key grants you access to the models you have trained on Roboflow, public models available on Roboflow Universe, and access to hosted inference API's.</p>"},{"location":"quickstart/configure_api_key/#access-your-roboflow-api-key","title":"Access Your Roboflow API Key","text":"<p>For some examples in the documentation you will need to provide your Roboflow API key. To access your Roboflow API key, you will need to create a free Roboflow account, then follow the docs to retrieve your key.</p>"},{"location":"quickstart/configure_api_key/#use-your-roboflow-api-key","title":"Use Your Roboflow API Key","text":"<p>There are several ways to configure your Roboflow API key when using Inference.</p>"},{"location":"quickstart/configure_api_key/#environment-variable","title":"Environment Variable","text":"<p>The recommended way is to set your Roboflow API key within your environment via the variable <code>ROBOFLOW_API_KEY</code>. In most terminals you can run:</p> <pre><code>export ROBOFLOW_API_KEY=MY_ROBOFLOW_API_KEY\n</code></pre> <p>Then, any command you run within that same terminal session will have access to the environment variable <code>ROBOFLOW_API_KEY</code>.</p>"},{"location":"quickstart/configure_api_key/#python","title":"Python","text":"<p>When using Inference within python, your Roboflow API key can be set via keyword arguments</p> <pre><code>from inference.models.utils import get_model\n\nmodel = get_model(model_id=\"...\", api_key=\"YOUR ROBOFLOW API KEY\")\n</code></pre> <p>Hint</p> <p>If you set your API key in your environment, you do not have to pass it as a keyword argument: <code>model = get_model(model_id=\"...\")</code></p>"},{"location":"quickstart/configure_api_key/#http-request-payload","title":"HTTP Request Payload","text":"<p>When using HTTP requests, your Roboflow API key should be passed as a url parameter, or as part of the request payload, depending on the route you are using.</p> <pre><code>import requests\n\nmy_api_key = \"YOUR ROBOFLOW API KEY\"\n\nurl = f\"http://localhost:9001/soccer-players-5fuqs/1?api_key={my_api_key}\"\nresponse = requests.post(url,...)\n\nurl = \"http://localhost:9001/infer/object_detection\"\npayload = {\n  \"api_key\": my_api_key,\n  \"model_id\": \"soccer-players-5fuqs/1\",\n  ...\n}\nresponse = requests.post(url,json=payload)\n</code></pre>"},{"location":"quickstart/configure_api_key/#docker-configuration","title":"Docker Configuration","text":"<p>If you are running the Roboflow Inference Server locally in a docker container, you can provide your Roboflow API key within the <code>docker run</code> command.</p> <pre><code>docker run -it --rm --network=host -e ROBOFLOW_API_KEY=YOUR_ROBOFLOW_API_KEY roboflow/roboflow-inference-server-cpu:latest\n</code></pre> <p>Requests sent to this server can now omit <code>api_key</code> from the request payload.</p>"},{"location":"quickstart/devices/","title":"What Devices Can I Use?","text":"<p>You can deploy Inference on the edge, in your own cloud, or using the Roboflow hosted inference option.</p>"},{"location":"quickstart/devices/#supported-edge-devices","title":"Supported Edge Devices","text":"<p>You can set up a server to use computer vision models with Inference on the following devices:</p> <ul> <li>ARM CPU (macOS, Raspberry Pi)</li> <li>x86 CPU (macOS, Linux, Windows)</li> <li>NVIDIA GPU</li> <li>NVIDIA Jetson (JetPack 4.5.x, JetPack 4.6.x, JetPack 5.x, JetPack 6.x)</li> </ul>"},{"location":"quickstart/devices/#model-compatability","title":"Model Compatability","text":"<p>The table below shows on what devices you can deploy models supported by Inference.</p> <p>See our Docker Getting Started guide for more information on how to deploy Inference on your device.</p> <p>Table key:</p> <ul> <li>\u2705 Fully supported</li> <li>\ud83d\udeab Not supported</li> <li>\ud83d\udea7 On roadmap, not currently supported</li> </ul> Model CPU GPU Jetson 4.5.x Jetson 4.6.x Jetson 5.x Roboflow Hosted Inference YOLOv8 Object Detection \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Classification \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv8 Segmentation \u2705 \u2705 \ud83d\udeab \ud83d\udeab \u2705 \u2705 YOLOv5 Object Detection \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLOv5 Segmentation \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 CLIP \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 DocTR \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Gaze \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \u2705 SAM \u2705 \u2705 \ud83d\udeab \ud83d\udeab \ud83d\udeab \ud83d\udeab ViT Classification \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 YOLACT \u2705 \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"quickstart/devices/#cloud-platform-support","title":"Cloud Platform Support","text":"<p>You can deploy Inference on any cloud platform such as AWS, GCP, or Azure.</p> <p>The installation and setup instructions are the same as for any edge device, once you have installed the relevant drivers on your cloud platform. We recommend deploying with an official \"Deep Learning\" image from your cloud provider if you are running inference on a GPU device. \"Deep Learning\" images should have the relevant drivers pre-installed so you can set up Inference without configuring GPU drivers manually</p>"},{"location":"quickstart/devices/#use-hosted-inference-from-roboflow","title":"Use Hosted Inference from Roboflow","text":"<p>You can also run your models in the cloud with the Roboflow hosted inference offering. The Roboflow hosted inference solution enables you to deploy your models in the cloud without having to manage your own infrastructure. Roboflow's hosted solution does not support all features available in Inference that you can run on your own infrastructure.</p> <p>To learn more about device compatability with different models, refer to the model compatability matrix.</p>"},{"location":"quickstart/docker/","title":"Running With Docker","text":""},{"location":"quickstart/docker/#setup","title":"Setup","text":"<p>Before you begin, ensure that you have Docker installed on your machine. Docker provides a containerized environment, allowing the Roboflow Inference Server to run in a consistent and isolated manner, regardless of the host system. If you haven't installed Docker yet, you can get it from Docker's official website.</p>"},{"location":"quickstart/docker/#set-up-a-docker-inference-server-via-inference-server-start","title":"Set up a Docker Inference Server via `inference server start``","text":"<p>Another easy way to run the Roboflow Inference Server with Docker is via the command line.</p> <p>First, Install the CLI.</p> <p>Running the Inference Server is as simple as running the following command:</p> <pre><code>inference server start\n</code></pre> <p>This will pull the appropriate Docker image for your machine and start the Inference Server on port 9001. You can then send requests to the server to get predictions from your model, as described in HTTP Inference.</p> <p>Once you have your inference server running, you can check its status with the following command:</p> <pre><code>inference server status\n</code></pre> <p>Roboflow Inference CLI currently supports the following device targets:</p> <ul> <li>x86 CPU</li> <li>ARM64 CPU</li> <li>NVIDIA GPU</li> </ul> <p>For Jetson or TensorRT Runtime inference server images, pull the images directly following the instructions below.</p>"},{"location":"quickstart/docker/#manually-set-up-a-docker-container","title":"Manually Set Up a Docker Container","text":""},{"location":"quickstart/docker/#step-1-pull-from-docker-hub","title":"Step #1: Pull from Docker Hub","text":"<p>If you don't wish to build the Docker image locally or prefer to use the official releases, you can directly pull the pre-built images from the Docker Hub. These images are maintained by the Roboflow team and are optimized for various hardware configurations.</p> <p>docker pull</p> x86 CPUarm64 CPUGPU <p>Official Roboflow Inference Server Docker Image for x86 CPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for ARM CPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia GPU Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-gpu\n</code></pre> <p>=== \"Jetson 4.5.x\" (Deprecated)     Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 4.5.x Targets.</p> <pre><code>```\ndocker pull roboflow/roboflow-inference-server-jetson-4.5.0\n```\n</code></pre> <p>=== \"Jetson 4.6.x\" (Deprecated)     Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 4.6.x Targets.</p> <pre><code>```\ndocker pull roboflow/roboflow-inference-server-jetson-4.6.1\n```\n</code></pre> Jetson 5.xJetson 6.x <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 5.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-jetson-5.1.1\n</code></pre> <p>Official Roboflow Inference Server Docker Image for Nvidia Jetson JetPack 6.x Targets.</p> <pre><code>docker pull roboflow/roboflow-inference-server-jetson-6.0.0\n</code></pre>"},{"location":"quickstart/docker/#step-2-run-the-docker-container","title":"Step #2: Run the Docker Container","text":"<p>Once you have a Docker image (either built locally or pulled from Docker Hub), you can run the Roboflow Inference Server in a container.</p> <p>docker run</p> x86 CPUarm64 CPUGPUJetson 4.5.xJetson 4.6.xJetson 5.x <pre><code>docker run -it --net=host \\\nroboflow/roboflow-inference-server-cpu:latest\n</code></pre> <pre><code>docker run -p 9001:9001 \\\nroboflow/roboflow-inference-server-cpu:latest\n</code></pre> <pre><code>docker run -it --network=host --gpus=all \\\nroboflow/roboflow-inference-server-gpu:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-jetson-4.5.0:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-jetson-4.6.1:latest\n</code></pre> <pre><code>docker run --privileged --net=host --runtime=nvidia \\\nroboflow/roboflow-inference-server-jetson-5.1.1:latest\n</code></pre> <p>Note: The Jetson images come with TensorRT dependencies. To use TensorRT acceleration with your model, pass an additional environment variable at runtime <code>-e ONNXRUNTIME_EXECUTION_PROVIDERS=TensorrtExecutionProvider</code>. This can improve inference speed, however, this also incurs a costly startup expense when the model is loaded. Note: On Windows and macOS, you may need to use <code>-p 9001:9001</code> instead of <code>--net=host</code> to expose the port to the host machine.</p> <p>You may add the flag <code>-e ROBOFLOW_API_KEY=&lt;YOUR API KEY&gt;</code> to your <code>docker run</code> command so that you do not need to provide a Roboflow API key in your requests. Substitute <code>&lt;YOUR API KEY&gt;</code> with your Roboflow API key. Learn how to retrieve your Roboflow API key here.</p> <p>You may add the flag <code>-v $(pwd)/cache:/tmp/cache</code> to create a cache folder on your home device so that you do not need to redownload or recompile model artifacts upon inference container reboot. You can also (preferably) store artificats in a docker volume named <code>inference-cache</code> by adding the flag <code>-v inference-cache:/tmp/cache</code>.</p>"},{"location":"quickstart/docker/#advanced-build-a-docker-container-from-scratch","title":"Advanced: Build a Docker Container from Scratch","text":"<p>To build a Docker image locally, first clone the Inference Server repository.</p> <pre><code>git clone https://github.com/roboflow/inference\n</code></pre> <p>Choose a Dockerfile from the following options, depending on the hardware you want to run Inference Server on.</p> <p>docker build</p> x86 CPUarm64 CPUGPUJetson 4.5.xJetson 4.6.xJetson 5.x <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.cpu \\\n-t roboflow/roboflow-inference-server-cpu .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.cpu \\\n-t roboflow/roboflow-inference-server-cpu .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.gpu \\\n-t roboflow/roboflow-inference-server-gpu .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.jetson \\\n-t roboflow/roboflow-inference-server-jetson-4.5.0 .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.jetson \\\n-t roboflow/roboflow-inference-server-jetson-4.6.1 .\n</code></pre> <pre><code>docker build \\\n-f docker/dockerfiles/Dockerfile.onnx.jetson.5.1.1 \\\n-t roboflow/roboflow-inference-server-jetson-5.1.1 .\n</code></pre>"},{"location":"quickstart/docker_configuration_options/","title":"Docker Configuration Options","text":"<p>Inference servers have a number of configurable parameters which can be set using environment variables. To set an environment variable with the docker run command, use the -e flag with an argument, like this:</p> <pre><code>docker run -it --rm -e ENV_VAR_NAME=env_var_value -p 9001:9001 --gpus all roboflow/roboflow-inference-server-gpu:latest\n</code></pre>"},{"location":"quickstart/docker_configuration_options/#networking","title":"Networking","text":""},{"location":"quickstart/docker_configuration_options/#host","title":"Host","text":"<p>HOST: String (default = 0.0.0.0)</p> <p>Sets the host address used by HTTP interfaces.</p>"},{"location":"quickstart/docker_configuration_options/#inference-server-port","title":"Inference Server Port","text":"<p>PORT: Integer (default = 9001)</p> <p>Sets the port used by HTTP interfaces.</p>"},{"location":"quickstart/docker_configuration_options/#class-agnostic-nms","title":"Class Agnostic NMS","text":"<p>Variable: CLASS_AGNOSTIC_NMS</p> <p>Type: Boolean (default = False)</p> <p>Sets the default non-maximal suppression (NMS) behavior for detection type models (object detection, instance segmentation, etc.).  If True, the default NMS behavior will be class be class agnostic,  meaning overlapping detections from different classes may be removed based on the IoU threshold. If False, only overlapping detections from the same class will be considered for removal by NMS.</p>"},{"location":"quickstart/docker_configuration_options/#allow-origins","title":"Allow Origins","text":"<p>Variable: ALLOW_ORIGINS</p> <p>Type: String (default = \"*\")</p> <p>Sets the allow_origins property on the CORSMiddleware used with FastAPI for HTTP interfaces. Multiple values can be provided separated by a comma (ex. ALLOW_ORIGINS=orig1.com,orig2.com).</p>"},{"location":"quickstart/docker_configuration_options/#clip-model-options","title":"CLIP Model Options","text":""},{"location":"quickstart/docker_configuration_options/#clip-version","title":"CLIP Version","text":"<p>Variable: CLIP_VERSION_ID</p> <p>Type: String (default = ViT-B-16)</p> <p>Sets the OpenAI CLIP version for use by all /clip routes. Available model versions are: RN101, RN50, RN50x16, RN50x4, RN50x64, ViT-B-16, BiT-B-32, BiT-L-14-336px, and ViT-L-14.</p>"},{"location":"quickstart/docker_configuration_options/#clip-batch-size","title":"CLIP Batch Size","text":"<p>Variable: CLIP_MAX_BATCH_SIZE</p> <p>Type: Integer (default = 8)</p> <p>Sets the max batch size accepted by the clip model inference functions.</p>"},{"location":"quickstart/docker_configuration_options/#batch-size","title":"Batch Size","text":"<p>FIX_BATCH_SIZE: Boolean (default = False)</p> <p>If true, the batch size will be fixed to the maximum batch size configured for this server.</p>"},{"location":"quickstart/docker_configuration_options/#license-server","title":"License Server","text":"<p>LICENSE_SERVER: String (default = None)</p> <p>Sets the address of a Roboflow license server.</p>"},{"location":"quickstart/docker_configuration_options/#maximum-active-models","title":"Maximum Active Models","text":"<p>MAX_ACTIVE_MODELS: Integer (default = 8)</p> <p>Sets the maximum number of models the internal model manager will store in memory at one time. By default, the model queue will remove the least recently accessed model when making space for a new model.</p>"},{"location":"quickstart/docker_configuration_options/#maximum-candidates","title":"Maximum Candidates","text":"<p>MAX_CANDIDATES: Integer (default = 3000)</p> <p>The maximum number of candidates for detection.</p>"},{"location":"quickstart/docker_configuration_options/#maximum-detections","title":"Maximum Detections","text":"<p>MAX_DETECTIONS: Integer (default = 300)</p> <p>Sets the maximum number of detections returned by a model.</p>"},{"location":"quickstart/docker_configuration_options/#model-cache-directory","title":"Model Cache Directory","text":"<p>MODEL_CACHE_DIR: String (default = /tmp/cache)</p> <p>Sets the container path for the root model cache directory.</p>"},{"location":"quickstart/docker_configuration_options/#number-of-workers","title":"Number of Workers","text":"<p>NUM_WORKERS: Integer (default = 1)</p> <p>Sets the number of workers used by HTTP interfaces. </p>"},{"location":"quickstart/docker_configuration_options/#tensorrt-cache-directory","title":"TensorRT Cache Directory","text":"<p>TENSORRT_CACHE_PATH: String (default = MODEL_CACHE_DIR)</p> <p>Sets the container path to the TensorRT cache directory. Setting this path in conjunction with mounting a host volume can reduce the cold start time of TensorRT based servers.</p>"},{"location":"quickstart/explore_models/","title":"Models: Fine-tuned","text":"<p>With Inference, you can run private, fine-tuned models that you have trained or uploaded to Roboflow.</p> <p>All models run on your own hardware.</p>"},{"location":"quickstart/explore_models/#run-a-private-fine-tuned-model","title":"Run a Private, Fine-Tuned Model","text":"<p>To run a model, first go to your Roboflow dashboard. Then, choose the model you want to run.</p> <p></p> <p>Click the \"Deploy\" link in the sidebar to find the information you will need to use your model with Inference.</p> <p>Copy the model ID on the page (in this case, <code>taylor-swift-records/3</code>).</p> <p></p> <p>Then, create a new Python file and add the following code:</p> <pre><code>from inference import get_model\nimport supervision as sv\nimport cv2\n\n# define the image url to use for inference\nimage_file = \"taylor-swift-album-1989.jpeg\"\nimage = cv2.imread(image_file)\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"taylor-swift-records/3\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)[0]\n\n# load the results into the supervision Detections api\ndetections = sv.Detections.from_inference(results)\n\n# create supervision annotators\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# annotate the image with our inference results\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n\n# display the image\nsv.plot_image(annotated_image)\n</code></pre> <p>The <code>taylor-swift-album-1989.jpeg</code> file is hosted here.</p> <p>Replace <code>taylor-swift-records/3</code> with the model ID from your private model and ensure your API key is in your environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>You should see your model's predictions visualized on your screen.</p> <p></p>"},{"location":"quickstart/http_inference/","title":"HTTP Inference","text":"<p>The Roboflow Inference Server provides a standard API through which to run inference on computer vision models.</p> <p>In this guide, we show how to run inference on object detection, classification, and segmentation models using the Inference Server.</p> <p>Currently, the server is compatible with models trained on Roboflow, but stay tuned as we actively develop support for bringing your own models.</p> <p>To run inference with the server, we will:</p> <ol> <li>Install the server</li> <li>Download a model for use on the server</li> <li>Run inference</li> </ol>"},{"location":"quickstart/http_inference/#step-1-install-the-inference-server","title":"Step #1: Install the Inference Server","text":"<p>You can skip this step if you already have Inference installed and running.</p> <p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using.</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>Start the server using <code>inference server start</code>. After you have installed the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p>"},{"location":"quickstart/http_inference/#step-2-run-inference","title":"Step #2: Run Inference","text":"<p>You can send a URL with an image, a NumPy array, or a base64-encoded image to an Inference server. The server will return a JSON response with the predictions.</p> <p>There are two generations of routes in a Roboflow inference server To see what routes are available for a running inference server instance, visit the <code>/docs</code> route in a browser. Roboflow hosted inference endpoints (<code>detect.roboflow.com</code>) only support V1 routes.</p>"},{"location":"quickstart/http_inference/#run-inference-on-a-v2-route","title":"Run Inference on a v2 Route","text":"<p>Run</p> URLBase64 ImageBatch InferenceNumpy Array <p>Create a new Python file and add the following code:</p> <pre><code>import requests\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nimage_url = \"https://storage.googleapis.com/com-roboflow-marketing/inference/soccer.jpg\"\nconfidence = 0.75\niou_thresh = 0.5\napi_key = \"YOUR API KEY\"\ntask = \"object_detection\"\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"url\",\n        \"value\": image_url,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>image_url</code>: The URL of the image you want to run inference on.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"base64\",\n        \"value\": img_str,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Object detection models support batching. Utilize batch inference by passing a list of image objects in a request payload:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": [\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        }\n    ],\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = cv2.imread(file_name)\nnumpy_data = pickle.dumps(image)\nimg_str = base64.b64encode(numpy_data)\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"numpy\",\n        \"value\": img_str,\n    },\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre>"},{"location":"quickstart/http_inference/#run-inference-on-a-v1-route","title":"Run Inference on a v1 Route","text":"<p>Run</p> URL <pre><code>The Roboflow hosted API uses the V1 route and requests take a slightly different form:\n\n```python\nimport requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nimage_url = \"https://storage.googleapis.com/com-roboflow-marketing/inference/soccer.jpg\n\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}&amp;image={image_url}\",\n)\n\npredictions = res.json()\nprint(predictions)\n```\n\nAbove, specify:\n\n1. `project_id`, `model_version`: Your project ID and model version number. &lt;a href=\"https://docs.roboflow.com/api-reference/workspace-and-project-ids\" target=\"_blank\"&gt;Learn how to retrieve your project ID and model version number&lt;/a&gt;.\n2. `confidence`: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.\n3. `api_key`: Your Roboflow API key. &lt;a href=\"https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key\" target=\"_blank\"&gt;Learn how to retrieve your Roboflow API key&lt;/a&gt;.\n4. `task`: The type of task you want to run. Choose from `object_detection`, `classification`, or `segmentation`.\n5. `filename`: The path to the image you want to run inference on.\n\nThen, run the Python script:\n\n```\npython app.py\n```\n</code></pre> Base64 ImageNumPy ArrayBatch Inference <p>The Roboflow hosted API uses the V1 route and requests take a slightly different form:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}\",\n    data=img_str,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Numpy arrays can be pickled and passed to the inference server for quicker processing. Note, Roboflow hosted APIs to not accept numpy inputs for security reasons:</p> <pre><code>import requests\nimport cv2\nimport pickle\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\napi_key = \"YOUR API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = cv2.imread(file_name)\nnumpy_data = pickle.dumps(image)\n\nres = requests.post(\n    f\"http://localhost:9001/{project_id}/{model_version}?api_key={api_key}&amp;image_type=numpy\",\n    data=numpy_data,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Batch inference is not currently supported by V1 routes.</p> <p>The code snippets above will run inference on a computer vision model. On the first request, the model weights will be downloaded and set up with your local inference server. This request may take some time depending on your network connection and the size of the model. Once your model has downloaded, subsequent requests will be much faster.</p> <p>The Inference Server comes with a <code>/docs</code> route at <code>localhost:9001/docs</code> or <code>localhost:9001/redoc</code> that provides OpenAPI-powered documentation. You can use this to reference the routes available, and the configuration options for each route.</p>"},{"location":"quickstart/http_inference/#batching-requests","title":"Batching Requests","text":"<p>Object detection models trained with Roboflow support batching, which allow you to upload multiple images of any type at once:</p> <pre><code>infer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": [\n        {\n            \"type\": \"url\",\n            \"value\": image_url_1,\n        },\n        {\n            \"type\": \"url\",\n            \"value\": image_url_2,\n        },\n        {\n            \"type\": \"url\",\n            \"value\": image_url_3,\n        },\n    ],\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n</code></pre>"},{"location":"quickstart/inference_101/","title":"How Do I Run Inference?","text":"<p>There are three ways to run Inference:</p> <ul> <li>Using the Python SDK (Images and Videos)</li> <li>Using the Python HTTP SDK (Images)</li> <li>Using the HTTP SDK (Images for all languages)</li> </ul> <p>We document every method in the \"Inputs\" section of the Inference documentation.</p> <p>Below, we talk about when you would want to use each method.</p>"},{"location":"quickstart/inference_101/#using-the-python-sdk-images-and-videos","title":"Using the Python SDK (Images and Videos)","text":"<p>You can use the Python SDK to run models on images and videos directly using the Inference code, without using Docker.</p> <p>Any code example that imports from <code>inference.models</code> uses the model directly.</p> <p>To use the Python SDK, you need to install:</p> <pre><code>pip install inference\n</code></pre>"},{"location":"quickstart/inference_101/#python-http-sdk","title":"Python HTTP SDK","text":"<p>You can use the Python HTTP SDK to run models using Inference with Docker.</p> <p>Any code example that imports from <code>inference_sdk</code> uses the HTTP API.</p> <p>To use this method, you will need an Inference server running, or you can use the Roboflow endpoint for your model.</p>"},{"location":"quickstart/inference_101/#self-hosted-inference-server","title":"Self-Hosted Inference Server","text":"<p>You can set up and install and Inference server using:</p> <pre><code>pip install inference\ninference server start\n</code></pre>"},{"location":"quickstart/inference_101/#roboflow-hosted-api","title":"Roboflow Hosted API","text":"<p>First, run:</p> <pre><code>pip install inference inference-sdk\n</code></pre> <p>Then, use your Roboflow hosted API endpoint to access your model. You can find this in the Deploy tab of your Roboflow model.</p>"},{"location":"quickstart/inference_101/#http-sdk","title":"HTTP SDK","text":"<p>You can deploy your model with Inference and Docker and use the API in any programming language (i.e. Swift, Node.js, and more).</p> <p>To use this method, you will need an Inference server running. You can set up and install and Inference server using:</p>"},{"location":"quickstart/inference_101/#self-hosted-inference-server_1","title":"Self-Hosted Inference Server","text":"<p>You can set up and install and Inference server using:</p> <pre><code>pip install inference\ninference server start\n</code></pre>"},{"location":"quickstart/inference_101/#roboflow-hosted-api_1","title":"Roboflow Hosted API","text":"<p>Use your Roboflow hosted API endpoint to access your model. You can find this in the Deploy tab of your Roboflow model.</p>"},{"location":"quickstart/inference_101/#benefits-of-using-inference-over-http","title":"Benefits of Using Inference Over HTTP","text":"<p>You can run Inference directly from your codebase or using a HTTP microservice deployed with Docker.</p> <p>Running Inference this way can have several advantages:</p> <ul> <li>No Dependency Management: When running Inference within one of Roboflow's published Inference Server containers, all the dependencies are built and isolated so they wont interfere with other dependencies in your code.</li> <li>Microservice Architecture: Running Inference as a separate service allows you to operate and maintain your computer vision models separate from other logic within your software, including scaling up and down to meet dynamic loads.</li> <li>Roboflow Hosted API: Roboflow hosts a powerful and infinitely scalable version of the Roboflow Inference Server. This makes it even easier to integrate computer vision models into your software without adding any maintenance burden. And, since the Roboflow hosted APIs are running using the Inference package, it's easy to switch between using a hosted server and an on prem server without having to reinvent your client code.</li> <li>Non-Python Clients: Running Inference within an HTTP server allows you to interact with it from any language you prefer.</li> </ul>"},{"location":"quickstart/inference_101/#advanced-usage-interfaces","title":"Advanced Usage &amp; Interfaces","text":"<p>There are several advanced interfaces that enhance the capabilities of the base Inference package.</p> <ul> <li>Active Learning: Active learning helps improve your model over time by contributing real world data back to your Roboflow dataset in real time. Docs and Examples</li> <li>Parallel HTTP API: A highly parallel server capable of accepting requests from many different clients and batching them dynamically in real time to make the most efficient use of the host hardware. Docs and Examples</li> <li>Stream Manager API: An API for starting, stopping, and managing Inference Pipeline instances. This interfaces combines the advantages of running Inference realtime on a stream while also fitting nicely into a microservice architecture. Docs and Examples</li> </ul> <p>To learn more, contact the Roboflow sales team.</p>"},{"location":"quickstart/inference_gpu_windows/","title":"Install the <code>inference-gpu</code> Python Package and NVIDIA CUDA on Windows","text":"<p>Warning</p> <p>We strongly recommend installing Inference with Docker on Windows. The guide below should only be used if you are unable to use Docker on your system.</p> <p>You can use Inference with <code>inference-gpu</code> and NVIDIA CUDA on Windows devices.</p> <p>This guide walks through how to configure your Windows GPU setup.</p>"},{"location":"quickstart/inference_gpu_windows/#prerequisites","title":"Prerequisites","text":"<p>To follow this guide, you must have a Windows machine that runs either Windows 10 or Windows 11. Your machine must have an NVIDIA GPU.</p>"},{"location":"quickstart/inference_gpu_windows/#step-1-install-python","title":"Step #1: Install Python","text":"<p>Download and the latest Python 3.11.x from the Python Windows version list.</p> <p>Do not install the Python version from the windows store as it is not compatible with onnxruntime.</p> <p>Click the \"Windows Installer (64 Bit)\" link and follow instructions to install python on the machine.</p> <p>When the installation is finished, type <code>py --version</code> to ensure your Python installation was successful.</p> <p>You should see a message showing your Python version.</p>"},{"location":"quickstart/inference_gpu_windows/#step-2-install-inference-gpu","title":"Step #2: Install Inference GPU","text":"<p>In a powershell terminal, run:</p> <pre><code>py -m pip install inference-gpu\n</code></pre>"},{"location":"quickstart/inference_gpu_windows/#step-3-install-cuda-toolkit-118","title":"Step #3: Install CUDA Toolkit 11.8","text":"<p>Next, we need to install CUDA Toolkit 11.8. This software will allow Inference to use CUDA.</p> <p>Download CUDA Toolkit.</p> <p>From the download page, choose the correct parameters for your system, and then choose \"exe (Network)\" and follow the link to download the toolkit.</p> <p></p> <p>Open the installation file and accept all defaults in order to install the toolkit.</p>"},{"location":"quickstart/inference_gpu_windows/#step-3-install-cudnn","title":"Step #3: Install cuDNN","text":"<p>Next, we need to install cuDNN.</p> <p>Navigate to the cuDNN installation page on the NVIDIA website.</p> <p>Select \"cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x\"</p> <p>Choose the link for 11.x, as others will not be compatible with your CUDA version and will fail.</p> <p>Choose \"Local Installer for Windows (Zip)\" and continue to download the ZIP file. You will need to sign up for an NVIDIA account in order to download the software.</p> <p>Open the file in downloads folder, right click and choose \"Extract All\" to extract all files to the download folder.</p> <p>Type ctrl+n to open a new Explorer window, and in this window navigate to <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8</code>.</p> <p>Copy all .dll files from the bin/ folder of the cuDNN download into the bin/ folder of the CUDA toolkit:</p> <p></p> <p>Copy all .h files from the include/ folder of the cuDNN download into the include/ folder of the CUDA toolkit:</p> <p></p> <p>Copy the x64 folder from the lib/ directory of the cuDNN download into the lib/ directory of the CUDA installation:</p> <p></p> <p>Right click the start menu and choose System \u2192 Advanced system settings \u2192 Environment Variables.</p> <p>Create a new environment variable called CUDNN with the value:</p> <pre><code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\include;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\lib;\n</code></pre>"},{"location":"quickstart/inference_gpu_windows/#step-4-install-zlib","title":"Step #4: Install zlib.","text":"<p>Find the file C:\\Program Files\\NVIDIA Corporation\\Nsight Systems 2022.4.2\\host-windows-x64\\zlib.dll Right click and choose \"Copy\".</p> <p>Now navigate to C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin in the finder.</p> <p>Paste the zlib.dll file into this folder and rename to zlibwapi.dll.</p>"},{"location":"quickstart/inference_gpu_windows/#step-5-install-visual-studio-2019-c-runtime","title":"Step #5: Install Visual Studio 2019 C++ Runtime","text":"<p>Finally, install the Visual Studio 2019 C++ runtime (download link).</p> <p>Create a new file with the following contents:</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\npipeline = InferencePipeline.init(\n    api_key=\"REPLACE API KEY\", ## &lt;- update API KEY\n    model_id=\"rock-paper-scissors-sxsw/11\",\n    video_reference='https://media.roboflow.com/rock-paper-scissors.mp4',\n    on_prediction=render_boxes,\n)\npipeline.start()\npipeline.join()\n</code></pre> <p>Using a text editor (we recommend Visual Studio Code), add your Roboflow API key in the string on line 5.</p> <p>Open a PowerShell terminal in the location of the file, and type py infer.py. If the installation is successful, you should see a few frames of annotated images displayed, with no errors or warnings in the console.</p>"},{"location":"quickstart/inference_notebook/","title":"Inference notebook","text":"<p>Roboflow Inference Servers come equipped with a built in Jupyterlab environment. This environment is the fastest way to get up and running with inference for development and testing. To use it, first start an inference server.</p> <p>The easiest way to start an inference server is with the inference CLI. Install it via pip:</p> <pre><code>pip install inference-cli\n</code></pre> <p>Now run the <code>inference sever start</code> command. Be sure to specify the <code>--dev</code> flag so that the notebook environment is enabled (it is disabled by default).</p> <pre><code>inference server start --dev\n</code></pre> <p>Now visit localhost:9001 in your browser to see the <code>inference</code> landing page. This page contains links to resources and examples related to <code>inference</code>. It also contains a link to the built in Jupyterlab environment.</p> <p>From the landing page, select the button labeled \"Jump Into an Inference Enabled Notebook\" to open a new tab for the Jupyterlab environment. </p> <p>This Jupyterlab environment comes preloaded with several example notebooks and all of the dependencies needed to run <code>inference</code>.</p>"},{"location":"quickstart/licensing/","title":"Licensing","text":""},{"location":"quickstart/licensing/#inference-source-code-license","title":"Inference Source Code License","text":"<p>See Roboflow Licensing for information on how Inference and models supported in Inference are licensed.</p>"},{"location":"quickstart/load_from_universe/","title":"Models: Universe","text":"<p>With Inference, you can run any of the 50,000+ models available on Roboflow Universe.</p> <p>All models run on your own hardware.</p>"},{"location":"quickstart/load_from_universe/#run-a-model-from-roboflow-universe","title":"Run a Model From Roboflow Universe","text":"<p>In the first example, we showed how to run a people detection model. This model was hosted on Universe. Let's find another model to try.</p> <p>Go to the Roboflow Universe homepage and use the search bar to find a model.</p> <p></p> <p>Info</p> <p>Add \"model\" to your search query to only find models.</p> <p>Browse the search page to find a model.</p> <p></p> <p>When you have found a model, click on the model card to learn more. Click the \"Model\" link in the sidebar to get the information you need to use the model.</p> <p>Then, install Inference and supervision, which we will use to run our model and handle model predictions, respectively:</p> <pre><code>pip install inference supervision\n</code></pre> <p>Next, create a new Python file and add the following code:</p> <pre><code># import a utility function for loading Roboflow models\nfrom inference import get_model\n# import supervision to visualize our results\nimport supervision as sv\n# import cv2 to helo load our image\nimport cv2\n\n# define the image url to use for inference\nimage_file = \"people-walking.jpg\"\nimage = cv2.imread(image_file)\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"yolov8n-640\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)\n\n# load the results into the supervision Detections api\ndetections = sv.Detections.from_inference(results[0].dict(by_alias=True, exclude_none=True))\n\n# create supervision annotators\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# annotate the image with our inference results\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n\n# display the image\nsv.plot_image(annotated_image)\n</code></pre> <p>Tip</p> <p>To see more models, check out the Pre-Trained Models page and Roboflow Universe.</p> <p>The <code>people-walking.jpg</code> file is hosted here.</p> <p>Replace <code>yolov8n-640</code> with the model ID you found on Universe, replace <code>image</code> with the image of your choosing, and be sure to export your API key:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>You should see your model's predictions visualized on your screen.</p> <p></p>"},{"location":"quickstart/roboflow_ecosystem/","title":"Introduction to Roboflow Ecosystem","text":"<p>Roboflow provides everything you need to label, train, and deploy computer vision solutions. It helps you manage and refine datasets, provides tools to streamline and speed up data labelling, helps train and deploy models in the cloud and on edge devices.</p> <p>Inference is what allows you to deploy and run computer vision models. It enables you to perform object detection, classification, instance segmentation and keypoint detection, and utilize foundation models like CLIP, Segment Anything, and YOLO-World, through a Python-native package, a self-hosted inference server, or a fully managed API.</p> <p>Roboflow offers both a free tier, and paid plans, encompassing all of its products.</p> <ul> <li>Over half of Fortune 100 companies build with Roboflow. If you're an enterprise customer and interested in a custom solution, parallel processing, active learning or licenses for more than one cloud instance or edge device - Reach Out to our sales team!</li> </ul>"},{"location":"quickstart/roboflow_ecosystem/#related-products","title":"Related Products","text":"<p>Inference is commonly used with several other roboflow products.</p>"},{"location":"quickstart/roboflow_ecosystem/#roboflow-app","title":"Roboflow App","text":"<p>Roboflow App This is your central dashboard. Here you can upload data, annotate images, define datasets, train and deploy models. You can find the API key (scoped to workspace)</p> <ul> <li>Roboflow App</li> <li>Docs: Getting Started with Roboflow</li> <li>App: API key</li> <li>Docs: How to Retrieve the API key</li> </ul>"},{"location":"quickstart/roboflow_ecosystem/#roboflow-package","title":"<code>roboflow</code> Package","text":"<p>Your workspace can be managed via the dashboard UI. If you'd like to do it via Python, install the <code>roboflow</code> package. This lets you manage your workspace, upload datasets and model weights, and even run model inference (a bit outdated).</p> <p>However, If all you need is to run a deployed model, you likely won't need <code>roboflow</code> at all. Where possible, we recommend <code>inference</code>. That's what we use on our servers!</p> <p>If you wish to use the <code>roboflow</code> package, instructions can be found in Roboflow Python Package Docs.</p>"},{"location":"quickstart/roboflow_ecosystem/#universe","title":"Universe","text":"<p>Universe is our space for sharing datasets and models.</p> <p>Search for models, test out their performance on your images, track model versions, access in inference, build on top.</p> <p>The <code>model_id</code> you pass into inference can be a <code>model_id</code> from Universe.</p> <ul> <li>Run Model from Roboflow Universe</li> </ul>"},{"location":"quickstart/roboflow_ecosystem/#supervision","title":"Supervision","text":"<p>What happens when you infer some results from a model? <code>supervision</code> lets you plot bounding boxes and segmentation masks, track objects, merge various detections. With supervision tools such as <code>InferenceSlicer</code> you can detect small objects in an image by running the model on smaller patches.</p> <p>It also encodes model results from various sources - <code>inference</code>, Hugging Face, Ultralytics and more, into a common format. https://supervision.roboflow.com/latest/</p> <ul> <li>Get started with Supervision</li> </ul>"},{"location":"quickstart/roboflow_ecosystem/#workflows","title":"Workflows","text":"<p>Workflows are an new <code>inference</code> feature. Instead of writing code, you may chain together blocks to build your computer vision algorithms from scratch. There's an expanding library of blocks available - see if you find anything you like!</p>"},{"location":"quickstart/run_a_model/","title":"Getting started","text":"<p>Let's run a computer vision model with Inference. The quickest way to get started with Inference is to simply load a model, and then call the model's <code>infer(...)</code> method.</p>"},{"location":"quickstart/run_a_model/#install-inference","title":"Install Inference","text":"<p>First, we need to install Inference:</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>To help us visualize our results in the example below, we will install Supervision:</p> <pre><code>pip install supervision\n</code></pre> <p>Create a new Python file called <code>app.py</code> and add the following code:</p>"},{"location":"quickstart/run_a_model/#load-a-model-and-run-inference","title":"Load a Model and Run Inference","text":"<pre><code># import a utility function for loading Roboflow models\nfrom inference import get_model\n\n# define the image url to use for inference\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"yolov8n-640\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)\n</code></pre> <p>In the code above, we loaded a model and then we used that model's <code>infer(...)</code> method to run an image through it.</p> <p>Tip</p> <p>When you run inference on an image, the same augmentations you applied when you generated a version in Roboflow will be applied at inference time. This helps improve model performance.</p>"},{"location":"quickstart/run_a_model/#visualize-results","title":"Visualize Results","text":"<p>Running inference is fun but it's not much to look at. Let's add some code to visualize our results.</p> <pre><code>from io import BytesIO\n\nimport requests\nimport supervision as sv\nfrom inference import get_model\nfrom PIL import Image\nfrom PIL.ImageFile import ImageFile\n\n\ndef load_image_from_url(url: str) -&gt; ImageFile:\n    response = requests.get(url)\n    response.raise_for_status()  # check if the request was successful\n    image = Image.open(BytesIO(response.content))\n    return image\n\n\n# load the image from an url\nimage = load_image_from_url(\"https://media.roboflow.com/inference/people-walking.jpg\")\n\n# load a pre-trained yolov8n model\nmodel = get_model(model_id=\"yolov8n-640\")\n\n# run inference on our chosen image, image can be a url, a numpy array, a PIL image, etc.\nresults = model.infer(image)[0]\n\n# load the results into the supervision Detections api\ndetections = sv.Detections.from_inference(results)\n\n# create supervision annotators\nbounding_box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# annotate the image with our inference results\nannotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\nannotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n\n# display the image\nsv.plot_image(annotated_image)\n</code></pre> <p></p>"},{"location":"quickstart/run_a_model/#summary","title":"Summary","text":"<p>Huzzah! We used Inference to load a computer vision model, run inference on an image, then visualize the results! But this is just the start. There are many different ways to use Inference and how you use it is likely to depend on your specific use case and deployment environment. Learn more about how to use inference here.</p>"},{"location":"quickstart/run_keypoint_detection/","title":"Keypoint Detection","text":"<p>Running a keypoint detection model on Roboflow is very similar to segmentation or detection.</p> <p>You may run it locally, hosted on our inference servers, or using a docker container.</p> \ud83d\udca1 model weights  In all cases, model weights need to be downloaded from Roboflow's servers first.  If you have the weights locally, you may upload the weights to our servers using the [From Local Weights](https://inference.roboflow.com/models/from_local_weights/) guide.  For offline usage, run inference with the Python API once. The weights will be downloaded and cached in the format our inference runtime can parse."},{"location":"quickstart/run_keypoint_detection/#quickstart","title":"Quickstart","text":"<p>Install dependencies:</p> <pre><code>pip install inference\n</code></pre> <p>Set up the API key:</p> <pre><code>export ROBOFLOW_API_KEY=MY_ROBOFLOW_API_KEY\n</code></pre> <p>Run with Python API:</p> <pre><code>from inference import get_model\n\n\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\nmodel = get_model(model_id=\"yolov8x-pose-640\")\nresults = model.infer(image)[0]\n</code></pre>"},{"location":"quickstart/run_keypoint_detection/#details","title":"Details","text":""},{"location":"quickstart/run_keypoint_detection/#inference-setup","title":"Inference Setup","text":"<p>In all cases, you'll need the <code>inference</code> package.</p> <pre><code>pip install inference\n</code></pre> <p>By default, it runs on the CPU. Instead, you may install the GPU module with the following command:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"quickstart/run_keypoint_detection/#api-keys","title":"API Keys","text":"<p>You'll need the API key to access the fine-tuned models or models on the Roboflow Universe. A guide can be found in Retrieve Your API Key.</p> <pre><code>export ROBOFLOW_API_KEY=MY_ROBOFLOW_API_KEY\n</code></pre>"},{"location":"quickstart/run_keypoint_detection/#available-pretrained-models","title":"Available Pretrained Models","text":"<p>You may use keypoint detection models available on the Universe. Alternatively, here's a few <code>model_ids</code> that we support out-of-the-box:</p> <ul> <li><code>yolov8x-pose-1280</code> (largest)</li> <li><code>yolov8x-pose-640</code></li> <li><code>yolov8l-pose-640</code></li> <li><code>yolov8m-pose-640</code></li> <li><code>yolov8s-pose-640</code></li> <li><code>yolov8n-pose-640</code> (smallest)</li> </ul> <p>Run</p> Python API - ImageInference Pipeline - StreamHostedDocker server <p>Run the model locally, without needing to set up a docker container. This pulls the model from roboflow servers and runs it on your machine. It can take both images and videos as input.</p> <p>Run:</p> <pre><code>from inference import get_model\n\n\n# This can be a URL, a np.ndarray or a PIL image.\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\nmodel = get_model(model_id=\"yolov8x-pose-640\")\nresults = model.infer(image)[0]\n</code></pre> <p>Inference Pipeline allows running inference on videos, webcams and RTSP streams. You may define a custom sink to extract pose results.</p> <p>More details can be found on Predict on a Video, Webcam or RTSP Stream</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef my_custom_sink(predictions: dict, video_frame: VideoFrame):\n    print(predictions)\n\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-pose-640\", # Roboflow model to use\n    video_reference=0, # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url\n    on_prediction=my_custom_sink, # Function to run after each prediction\n)\npipeline.start()\npipeline.join()\n</code></pre> <p>Send an image to our servers and get the detected keypoint response. Only images are supported (URL, <code>np.ndarray</code>, <code>PIL</code>).</p> <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\n\n# This can be a URL, a np.ndarray or a PIL image.\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\nresults = client.infer(image, model_id=\"yolov8x-pose-640\")\n</code></pre> <p>With this method, you may self-host a server container, similar to Hosted model API. Only images are supported (URL, <code>np.ndarray</code>, <code>PIL</code>).</p> <p>Note that the model weights still need to be retrieved from our servers at least once. Check out From Local Weights for instructions on how to upload yours.</p> <p>Start the inference server:</p> <pre><code>inference server start\n</code></pre> <p>Run: <pre><code>import os\nfrom inference_sdk import InferenceHTTPClient\n\n\n# This can be a URL, a np.ndarray or a PIL image.\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"]\n)\nresults = client.infer(image, model_id=\"yolov8x-pose-640\")\n</code></pre></p>"},{"location":"quickstart/run_keypoint_detection/#visualize","title":"Visualize","text":"<p>With supervision you may visualize the results, carry out post-processing. Supervision library standardizes results from various keypoint detection and pose estimation models into a consistent format, using adaptors such as <code>from_inference</code>.</p> <p>Example usage:</p> <pre><code>import os\nimport cv2\nfrom inference import get_model\nimport supervision as sv\n\n\n# Model accepts URLs, np.arrays (cv2.imread), and PIL images.\n# Annotators accept np.arrays (cv2.imread), and PIL images\nimage = \"https://media.roboflow.com/inference/people-walking.jpg\"\n\nmodel = get_model(model_id=\"yolov8x-pose-640\")\nresults = model.infer(image)[0]\n\n# Any results object would work, regardless of which inference API is used\nkeypoints = sv.KeyPoints.from_inference(results)\n\n# Convert to numpy image\nimg_name = \"people-walking.jpg\"\nif not os.path.exists(img_name):\n    os.system(f\"wget -O {img_name} {image}\")\nimage_np = cv2.imread(img_name)\n\nannotated_image = sv.EdgeAnnotator(\n    color=sv.Color.GREEN,\n    thickness=5\n).annotate(image, keypoints)\n</code></pre>"},{"location":"quickstart/run_model_on_image/","title":"Predict on an Image Over HTTP","text":"<p>A Roboflow Inference server provides a standard API through which to run inference on computer vision models.</p> <p>In this guide, we show how to run inference on object detection, classification, and segmentation models using Inference.</p> <p>Note</p> <p>Inference is compatible with models trained on Roboflow, but stay tuned as we actively develop support for bringing your own models.</p> <p>You can run inference on images from:</p> <ol> <li>URLs, which will be downloaded from the internet</li> <li>File names, which will be read from disk</li> <li>PIL images</li> <li>NumPy arrays</li> </ol>"},{"location":"quickstart/run_model_on_image/#step-1-install-the-inference-server","title":"Step #1: Install the Inference Server","text":"<p>You can skip this step if you already have Inference installed and running.</p> <p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using.</p> <p>Start the server using <code>inference server start</code>. After you have installed the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p>"},{"location":"quickstart/run_model_on_image/#step-2-run-inference","title":"Step #2: Run Inference","text":"<p>You can send a URL with an image, a NumPy array, or a base64-encoded image to an Inference server. The server will return a JSON response with the predictions.</p> <p>There are two generations of routes in a Roboflow inference server To see what routes are available for a running inference server instance, visit the <code>/docs</code> route in a browser. Roboflow hosted inference endpoints (<code>detect.roboflow.com</code>) only support V1 routes.</p>"},{"location":"quickstart/run_model_on_image/#run-inference-on-a-v2-route","title":"Run Inference on a v2 Route","text":"<p>Run</p> URLPIL ImageNumPy ArrayBatch Inference <p>Create a new Python file and add the following code:</p> <pre><code># import client from inference_sdk\nfrom inference_sdk import InferenceHTTPClient,\n# import os to get the API_KEY from the environment\nimport os\n\n# set the project_id, model_version, image_url\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nimage_url = \"https://media.roboflow.com/inference/soccer.jpg\"\n\n# create a client object\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"API_KEY\"],\n)\n\n# run inference on the image\nresults = client.infer(image_url, model_id=f\"{project_id}/{model_version}\")\n\n# print the results\nprint(results)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>image_url</code>: The URL of the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code># import client from inference sdk\nfrom inference_sdk import InferenceHTTPClient\n# import PIL for loading image\nfrom PIL import Image\n# import os for getting api key from environment\nimport os\n\n# set the project_id, model_version, image_url\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nfilename = \"path/to/local/image.jpg\"\n\n# create a client object\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"API_KEY\"],\n)\n\n# load the image\npil_image = Image.open(filename)\n\n# run inference\nresults = client.infer(pil_image, model_id=f\"{project_id}/{model_version}\")\n\nprint(results)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Create a new Python file and add the following code:</p> <pre><code># import client from inference-sdk\nfrom inference_sdk import InferenceHTTPClient\n# import opencv for image loading\nimport cv2\n# import os to read api key from environment\nimport os\n\n# set the project_id, model_version, image_url\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nfilename = \"path/to/local/image.jpg\"\n\n# create client\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"API_KEY\"],\n)\n\n# load image with opencv\nnumpy_image = cv2.imread(filename)\n\n# run inference\nresults = client.infer(numpy_image, model_id=f\"{project_id}/{model_version}\")\n\nprint(results)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Object detection models support batching. Utilize batch inference by passing a list of image objects in a request payload:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\ninfer_payload = {\n    \"model_id\": f\"{project_id}/{model_version}\",\n    \"image\": [\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        },\n        {\n            \"type\": \"base64\",\n            \"value\": img_str,\n        }\n    ],\n    \"confidence\": confidence,\n    \"iou_threshold\": iou_thresh,\n    \"api_key\": api_key,\n}\n\nres = requests.post(\n    f\"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>See more docs on the inference-sdk</p>"},{"location":"quickstart/run_model_on_image/#run-inference-on-a-v1-route","title":"Run Inference on a v1 Route","text":"<p>Run</p> URLBase64 ImageNumPy ArrayBatch Inference <p>The Roboflow hosted API uses the V1 route and requests take a slightly different form:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nimage_url = \"https://storage.googleapis.com/com-roboflow-marketing/inference/soccer.jpg\"\n\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}&amp;image={image_url}\",\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>The Roboflow hosted API uses the V1 route and requests take a slightly different form:</p> <pre><code>import requests\nimport base64\nfrom PIL import Image\nfrom io import BytesIO\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\nconfidence = 0.5\niou_thresh = 0.5\napi_key = \"YOUR ROBOFLOW API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = Image.open(file_name)\n\nbuffered = BytesIO()\n\nimage.save(buffered, quality=100, format=\"JPEG\")\n\nimg_str = base64.b64encode(buffered.getvalue())\nimg_str = img_str.decode(\"ascii\")\n\nres = requests.post(\n    f\"https://detect.roboflow.com/{project_id}/{model_version}?api_key={api_key}&amp;confidence={confidence}&amp;overlap={iou_thresh}\",\n    data=img_str,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Numpy arrays can be pickled and passed to the inference server for quicker processing. Note, Roboflow hosted APIs to not accept numpy inputs for security reasons:</p> <pre><code>import requests\nimport cv2\nimport pickle\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = 1\ntask = \"object_detection\"\napi_key = \"YOUR API KEY\"\nfile_name = \"path/to/local/image.jpg\"\n\nimage = cv2.imread(file_name)\nnumpy_data = pickle.dumps(image)\n\nres = requests.post(\n    f\"http://localhost:9001/{project_id}/{model_version}?api_key={api_key}&amp;image_type=numpy\",\n    data=numpy_data,\n    headers={\"Content-Type\": \"application/json\"},\n)\n\npredictions = res.json()\nprint(predictions)\n</code></pre> <p>Above, specify:</p> <ol> <li><code>project_id</code>, <code>model_version</code>: Your project ID and model version number. Learn how to retrieve your project ID and model version number.</li> <li><code>confidence</code>: The confidence threshold for predictions. Predictions with a confidence score below this threshold will be filtered out.</li> <li><code>api_key</code>: Your Roboflow API key. Learn how to retrieve your Roboflow API key.</li> <li><code>task</code>: The type of task you want to run. Choose from <code>object_detection</code>, <code>classification</code>, or <code>segmentation</code>.</li> <li><code>filename</code>: The path to the image you want to run inference on.</li> </ol> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Batch inference is not currently supported by V1 routes.</p> <p>The code snippets above will run inference on a computer vision model. On the first request, the model weights will be downloaded and set up with your local inference server. This request may take some time depending on your network connection and the size of the model. Once your model has downloaded, subsequent requests will be much faster.</p> <p>The Inference Server comes with a <code>/docs</code> route at <code>localhost:9001/docs</code> or <code>localhost:9001/redoc</code> that provides OpenAPI-powered documentation. You can use this to reference the routes available, and the configuration options for each route.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/","title":"Predict on a Video, Webcam or RTSP Stream","text":"<p>You can run computer vision models on webcam stream frames, RTSP stream frames, and video frames with Inference.</p> <p>Follow our Run a Fine-Tuned Model on Images guide to learn how to find a model to run.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#installation","title":"Installation","text":"<p>To use fine-tuned models with Inference, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre> <p>Learn more about using Roboflow API keys in Inference</p> <p>Then, install Inference:</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre>"},{"location":"quickstart/run_model_on_rtsp_webcam/#inference-on-video","title":"Inference on Video","text":"<p>Next, create an Inference Pipeline. Once you have selected a model to run, create a new Python file and add the following code:</p> <pre><code># Import the InferencePipeline object\nfrom inference import InferencePipeline\n# Import the built in render_boxes sink for visualizing results\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\n# initialize a pipeline object\npipeline = InferencePipeline.init(\n    model_id=\"rock-paper-scissors-sxsw/11\", # Roboflow model to use\n    video_reference=0, # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url\n    on_prediction=render_boxes, # Function to run after each prediction\n)\npipeline.start()\npipeline.join()\n</code></pre> <p>This code will run a model on frames from a webcam stream. To use RTSP, set the <code>video_reference</code> value to an RTSP stream URL. To use video, set the <code>video_reference</code> value to a video file path.</p> <p>Predictions are annotated using the <code>render_boxes</code> helper function. You can specify any function to process each prediction in the <code>on_prediction</code> parameter.</p> <p>Replace <code>rock-paper-scissors-sxsw/11</code> with the model ID associated with the model you want to run.</p> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p> <p>Then, run the Python script:</p> <pre><code>python app.py\n</code></pre> <p>Your webcam will open and you can see the model running:</p> <p>Tip</p> <p>When you run inference on an image, the same augmentations you applied when you generated a version in Roboflow will be applied at inference time. This helps improve model performance.</p> <p>Presto! We used an InferencePipeline to run inference on our webcam and learned how we could modify it to run on other video sources (like video files or RTSP streams). See the Inference Pipeline docs to learn more about other configurable parameters and built in sinks.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#define-custom-prediction-logic","title":"Define Custom Prediction Logic","text":"<p>In Inference a sink is a function used to execute logic on the inference results within an <code>InferencePipeline</code>. Inference has some built in sinks for convenience. We used one above to plot bounding boxes.</p> <p>Below, we describe how to define custom prediction logic.</p> <p>The <code>on_prediction</code> parameter in the <code>InferencePipeline</code> constructor allows you to define custom prediction handlers. You can use this to define custom logic for how predictions are processed.</p> <p>This function provides two parameters:</p> <ul> <li><code>predictions</code>: A dictionary that contains all predictions returned by the model for the frame, and;</li> <li><code>video_frame</code>: A dataclass</li> </ul> <p>A VideoFrame object contains:</p> <ul> <li><code>image</code>: The video frame as a NumPy array,</li> <li><code>frame_id</code>: The frame ID, and;</li> <li><code>frame_timestamp</code>: The timestamp of the frame.</li> <li><code>source_id</code>: The index of the video_reference element which was passed to InferencePipeline (useful when multiple streams are passed to InferencePipeline).</li> </ul> <p>Let's start by just printing the frame ID to the console.</p> <pre><code>from inference import InferencePipeline\n# import VideoFrame for type hinting\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\n# define sink function\ndef my_custom_sink(predictions: dict, video_frame: VideoFrame):\n    # print the frame ID of the video_frame object\n    print(f\"Frame ID: {video_frame.frame_id}\")\n\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\",\n    video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\",\n    on_prediction=my_custom_sink,\n)\n\npipeline.start()\npipeline.join()\n</code></pre> <p>The output should look something like:</p> <pre><code>Frame ID: 1\nFrame ID: 2\nFrame ID: 3\n</code></pre> <p>Now let's do something a little more useful and use our custom sink to visualize our predictions.</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\n# import opencv to display our annotated images\nimport cv2\n# import supervision to help visualize our predictions\nimport supervision as sv\n\n# create a bounding box annotator and label annotator to use in our custom sink\nlabel_annotator = sv.LabelAnnotator()\nbox_annotator = sv.BoundingBoxAnnotator()\n\ndef my_custom_sink(predictions: dict, video_frame: VideoFrame):\n    # get the text labels for each prediction\n    labels = [p[\"class\"] for p in predictions[\"predictions\"]]\n    # load our predictions into the Supervision Detections api\n    detections = sv.Detections.from_inference(predictions)\n    # annotate the frame using our supervision annotator, the video_frame, the predictions (as supervision Detections), and the prediction labels\n    image = label_annotator.annotate(\n        scene=video_frame.image.copy(), detections=detections, labels=labels\n    )\n    image = box_annotator.annotate(image, detections=detections)\n    # display the annotated image\n    cv2.imshow(\"Predictions\", image)\n    cv2.waitKey(1)\n\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\",\n    video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\",\n    on_prediction=my_custom_sink,\n)\n\npipeline.start()\npipeline.join()\n</code></pre> <p>You should see something like this on your screen:</p> <p>And there you have it! We created a custom sink that takes the outputs of our Inference Pipeline, annotates an image, and displays it to our screen. See the Inference Pipeline docs to learn more about other configurable parameters and built in sinks.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#existing-video-sinks","title":"Existing Video Sinks","text":""},{"location":"quickstart/run_model_on_rtsp_webcam/#built-in-sinks","title":"Built In Sinks","text":"<p>Inference has several sinks built in that are ready to use.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#render_boxes","title":"<code>render_boxes(...)</code>","text":"<p>The render boxes sink is made to visualize predictions and overlay them on a stream. It uses Supervision annotators to render the predictions and display the annotated frame. It only works for Roboflow models that yields detection-based output (<code>object-detection</code>, <code>instance-segmentation</code>, <code>keypoint-detection</code>), yet not all details of predictions may be  displayed by default (like detected key-points).</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#udpsink","title":"<code>UDPSink(...)</code>","text":"<p>The UDP sink is made to broadcast predictions with a UDP port. This port can be listened to by client code for further processing. It uses Python-default json serialisation - so predictions must be serializable, otherwise error will be thrown.  </p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#multi_sink","title":"<code>multi_sink(...)</code>","text":"<p>The Multi-Sink is a way to combine multiple sinks so that multiple actions can happen on a single inference result.</p>"},{"location":"quickstart/run_model_on_rtsp_webcam/#videofilesink","title":"<code>VideoFileSink(...)</code>","text":"<p>The Video File Sink visualizes predictions, similar to the <code>render_boxes(...)</code> sink, however, instead of displaying the annotated frames, it saves them to a video file. All constraints related to <code>render_boxes(...)</code> apply.</p>"},{"location":"quickstart/run_model_over_udp/","title":"Predict Over UDP","text":"<p>You can run Inference directly on frames using UDP.</p> <p>This is ideal for real-time use cases where reducing latency is essential (i.e. sports broadcasting).</p> <p>This feature only works on devices with a CUDA-enabled GPU.</p> <p>Inference has been used at sports broadcasting events around the world for real-time object detection.</p> <p>Follow our Run a Fine-Tuned Model on Images guide to learn how to find a model to run.</p>"},{"location":"quickstart/run_model_over_udp/#run-a-vision-model-on-a-udp-stream","title":"Run a Vision Model on a UDP Stream","text":"<p>To run inference on frames from a UDP stream, you will need to:</p> <ol> <li>Set up a listening server to receive predictions from Inference, and;</li> <li>Run Inference, connected directly to your stream.</li> </ol>"},{"location":"quickstart/run_model_over_udp/#authenticate-with-roboflow","title":"Authenticate with Roboflow","text":"<p>To use Inference with a UDP stream, you will need a Roboflow API key. If you don't already have a Roboflow account, sign up for a free Roboflow account. Then, retrieve your API key from the Roboflow dashboard. Run the following command to set your API key in your coding environment:</p> <pre><code>export ROBOFLOW_API_KEY=&lt;your api key&gt;\n</code></pre>"},{"location":"quickstart/run_model_over_udp/#configure-a-listening-server","title":"Configure a Listening Server","text":"<p>You need a server to receive predictions from Inference. This server is where you can write custom logic to process predictions.</p> <p>Create a new Python file and add the following code:</p> <pre><code>import socket\nimport json\nimport time\n\n\nHOST = \"localhost\"\nPORT = 8000\n\nfps_array = []\n\n# Create a datagram (UDP) socket\nUDPClientSocket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)\n\n# Bind to the given IP address and port\nUDPClientSocket.bind((HOST, PORT))\n\nprint(f\"UDP server up and listening on http://{HOST}:{PORT}\")\n\n# Listen for incoming datagrams\nwhile True:\n    t0 = time.time()\n\n    bytesAddressPair = UDPClientSocket.recvfrom(1024)\n    message = bytesAddressPair[0]\n    address = bytesAddressPair[1]\n\n    clientMsg = json.loads(message)\n    clientIP = \"Client IP Address:{}\".format(address)\n\n    print(clientMsg)\n    print(clientIP)\n\n    t = time.time() - t0\n    fps_array.append(1 / t)\n    fps_array[-150:]\n    fps_average = sum(fps_array) / len(fps_array)\n    print(\"AVERAGE FPS: \" + str(fps_average))\n</code></pre> <p>Above, replace <code>port</code> with the port on which you want to run your server.</p>"},{"location":"quickstart/run_model_over_udp/#run-a-broadcasting-server","title":"Run a Broadcasting Server","text":"<ul> <li>set up socket</li> <li>render will broadcast</li> <li>https://hub.docker.com/repository/docker/roboflow/roboflow-inference-server-udp-gpu/general</li> </ul>"},{"location":"scripts/gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages.\"\"\"\n</pre> \"\"\"Generate the code reference pages.\"\"\" In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>src = Path(__file__).parent.parent.parent / \"inference\"\nSKIP_MODULES = [\n    \"inference.enterprise.device_manager.command_handler\",\n    \"inference.enterprise.parallel.celeryconfig\",\n]\n</pre> src = Path(__file__).parent.parent.parent / \"inference\" SKIP_MODULES = [     \"inference.enterprise.device_manager.command_handler\",     \"inference.enterprise.parallel.celeryconfig\", ] In\u00a0[\u00a0]: Copied! <pre>for path in sorted(p for p in src.rglob(\"*.py\") if \"landing\" not in p.parts):\n    module_path = path.relative_to(src.parent).with_suffix(\"\")\n    doc_path = path.relative_to(src.parent).with_suffix(\".md\")\n    full_doc_path = Path(\"docs\", \"reference\", doc_path)\n\n    parts = list(module_path.parts)\n    identifier = \".\".join(parts)\n    if parts[-1] == \"__main__\" or parts[-1] == \"__init__\" or identifier in SKIP_MODULES:\n        print(\"SKIPPING\", identifier)\n        continue\n\n    nav[parts] = doc_path.as_posix()\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        fd.write(f\"::: {identifier}\")\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n</pre> for path in sorted(p for p in src.rglob(\"*.py\") if \"landing\" not in p.parts):     module_path = path.relative_to(src.parent).with_suffix(\"\")     doc_path = path.relative_to(src.parent).with_suffix(\".md\")     full_doc_path = Path(\"docs\", \"reference\", doc_path)      parts = list(module_path.parts)     identifier = \".\".join(parts)     if parts[-1] == \"__main__\" or parts[-1] == \"__init__\" or identifier in SKIP_MODULES:         print(\"SKIPPING\", identifier)         continue      nav[parts] = doc_path.as_posix()      with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         fd.write(f\"::: {identifier}\")      mkdocs_gen_files.set_edit_path(full_doc_path, path) In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"docs/reference/nav.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"docs/reference/nav.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"},{"location":"server_configuration/accepted_input_formats/","title":"Input formats accepted by <code>inference</code> server","text":""},{"location":"server_configuration/accepted_input_formats/#why-should-i-care","title":"Why should I care?","text":"<p>The Roboflow team has designed the inference server to be as user-friendly and straightforward to integrate as  possible. We understand that some users prioritize ease of use, which is why we did not restrict the use of  potentially less secure data loading methods. This approach caters to those who prefer a simple and accessible  serving mechanism without the need for rigorous security measures.</p> <p>However, we also recognize the importance of having a production-ready solution. Therefore, we offer configuration  options that allow users to disable potentially unsafe behaviors.</p> <p>In this document, we explain how to configure the server to either enhance security or enable more  flexible behaviors, depending on your needs.</p>"},{"location":"server_configuration/accepted_input_formats/#deserialization-of-pickled-numpy-objects","title":"Deserialization of pickled <code>numpy</code> objects","text":"<p>One of the ways to send requests to the inference server is via serialized numpy objects:</p> <pre><code>import cv2\nimport pickle\nimport requests\n\nimage = cv2.imread(\"...\")\nimg_str = pickle.dumps(image)\n\ninfer_payload = {\n    \"model_id\": \"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"numpy\",\n        \"value\": img_str,\n    },\n    \"api_key\": \"YOUR-API-KEY\",\n}\n\nres = requests.post(\n    \"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n</code></pre> <p>Starting from version <code>v0.14.0</code>, deserialization of this type of payload is disabled by default. However, you can  enable it by setting an environmental variable, <code>ALLOW_NUMPY_INPUT=True</code>. Check inference cli docs to see how to run the server with that flag. This option is not available in Roboflow's Hosted Inference API.</p> <p>Warning</p> <p>Roboflow advises all users hosting the inference server in production environments not to enable this option if  the server is open to requests from the open Internet or is not locked down to accept only authenticated requests from your workspace's API key.</p>"},{"location":"server_configuration/accepted_input_formats/#sending-urls-to-inference-images","title":"Sending URLs to inference images","text":"<p>Making GET requests to obtain images from URLs can expose the server to  server-side request forgery (SSRF) attacks. However, it is also very convenient to simply provide an image URL  for requests: <pre><code>import requests\n\n\ninfer_payload = {\n    \"model_id\": \"{project_id}/{model_version}\",\n    \"image\": {\n        \"type\": \"numpy\",\n        \"value\": \"https://some.com/image.jpg\",\n    },\n    \"api_key\": \"YOUR-API-KEY\",\n}\n\nres = requests.post(\n    \"http://localhost:9001/infer/{task}\",\n    json=infer_payload,\n)\n</code></pre></p> <p>This option is enabled by default, but we recommend configuring the server to enhance security using one or more of the following environment variables: * <code>ALLOW_URL_INPUT</code> - Set to <code>False</code> disable image URLs of any kind to be accepted by server - default: <code>True</code>. * <code>ALLOW_NON_HTTPS_URL_INPUT</code> - set to <code>False</code> to only allow https protocol in URLs (useful to make sure domain names are not maliciously resolved) - default: <code>False</code> * <code>ALLOW_URL_INPUT_WITHOUT_FQDN</code> - set to <code>False</code> to enforce URLs with fully qualified domain names only - and reject URLs based on IPs - default: <code>False</code> * <code>WHITELISTED_DESTINATIONS_FOR_URL_INPUT</code> - Optionally, you can specify a comma-separated list of allowed destinations  for URL requests. For example: <code>WHITELISTED_DESTINATIONS_FOR_URL_INPUT=192.168.0.15,some.site.com</code>. URLs pointing to  other targets will be rejected. * <code>BLACKLISTED_DESTINATIONS_FOR_URL_INPUT</code> - Optionally, you can specify a comma-separated list of forbidden  destinations for URL requests. For example:  <code>BLACKLISTED_DESTINATIONS_FOR_URL_INPUT=192.168.0.15,some.site.com</code>. URLs pointing to these targets will be rejected.</p> <p>Check inference cli docs to see how to run server with specific flags.</p>"},{"location":"server_configuration/environmental_variables/","title":"Environmental vaiables","text":"<p><code>Inference</code> behavior can be controlled by set of environmental variables. All environmental variables are listed in inference/core/env.py</p> <p>Below is a list of some environmental values that require more in-depth explanation.</p> Environmental variable Description Default <code>ONNXRUNTIME_EXECUTION_PROVIDERS</code> List of execution providers in priority order, warning message will be displayed if provider is not supported on user platform See here <code>SAM2_MAX_EMBEDDING_CACHE_SIZE</code> The number of sam2 embeddings that will be held in memory. The embeddings will be held in gpu memory. Each embedding takes 16777216 bytes. 100 <code>SAM2_MAX_LOGITS_CACHE_SIZE</code> The number of sam2 logits that will be held in memory. The the logits will be in cpu memory. Each logit takes 262144 bytes. 1000 <code>DISABLE_SAM2_LOGITS_CACHE</code> If set to True, disables the caching of SAM2 logits. This can be useful for debugging or in scenarios where memory usage needs to be minimized, but may result in slower performance for repeated similar requests. False <code>ENABLE_WORKFLOWS_PROFILING</code> If set to True, in <code>inference</code> server allows the server to output Workflows profiler traces the client, running in Python package with <code>InferencePipeline</code> it enables profiling. False <code>WORKFLOWS_PROFILER_BUFFER_SIZE</code> Size of profiler buffer (number of consecutive Wrofklows Execution Engine <code>run(...)</code> invocations to trace in buffer. 64 <code>ENABLE_STREAM_API</code> Flag to enable Stream Management API in <code>inference</code> server - see more. False <code>RUNS_ON_JETSON</code> Boolean flag to tell if <code>inference</code> runs on Jetson device - set to <code>True</code> in all docker builds for Jetson architecture. False <code>WORKFLOWS_DEFINITION_CACHE_EXPIRY</code> Number of seconds to cache Workflows definitions as a result of <code>get_workflow_specification(...)</code> function call <code>15 * 60</code> - 15 minutes <code>DOCKER_SOCKET_PATH</code> Path to the local socket mounted to the container - by default empty, if provided - enables pooling docker container stats from the docker deamon socket. See more here Not Set <code>ENABLE_PROMETHEUS</code> Boolean flag to enable Prometeus <code>/metrics</code> enpoint. True for docker images in dockerhub"},{"location":"server_configuration/service_telemetry/","title":"Inference server telemetry","text":"<p>Service telemetry provides essential real-time data on system health, performance, and usage. It enables:</p> <ul> <li> <p>Monitoring and Diagnostics: Early detection of issues for quick resolution.</p> </li> <li> <p>Performance Optimization: Identifying bottlenecks to improve efficiency.</p> </li> <li> <p>Usage Insights: Understanding user behavior to guide improvements.</p> </li> <li> <p>Security: Detecting suspicious activities and ensuring compliance.</p> </li> <li> <p>Scalability: Predicting and managing resource demands.</p> </li> </ul> <p>In <code>inference</code> server, we enabled:</p> <ul> <li> <p><code>prometeus</code> metrics</p> </li> <li> <p>docker container metrics provided by Docker daemon</p> </li> </ul>"},{"location":"server_configuration/service_telemetry/#prometeus-in-inference-server","title":"\ud83d\udd25 <code>prometeus</code> in <code>inference</code> server","text":"<p>To enable metrics, set environmental variable <code>ENABLE_PROMETHEUS=True</code> in your docker container:</p> <pre><code>docker run -p 9001:9001 -e ENABLE_PROMETHEUS=True roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Then use <code>GET /metrics</code> endpoint to fetch the metrics in Python:</p> <pre><code>import requests\n\nresult = requests.get(\"http://127.0.0.1:9001/metrics\")\nresult.raise_for_status()\n\nprint(result.text)\n</code></pre> <p>or using curl: <pre><code>curl http://127.0.0.1:9001/metrics\n</code></pre></p>"},{"location":"server_configuration/service_telemetry/#docker-container-metrics","title":"Docker container metrics","text":"<p>Potential security issue</p> <p>This feature rely on docker daemon socket exposure inside container. This way we can expose docker container resource utilisation metrics without the need for \"supervisor\" service, but may be seen as security violation. We disable that ooption by default. Please acknowledge  potential security risks before enabling this option</p> <p>To expose container metrics you need to run docker container with <code>inference</code> server in a specific way:</p> <pre><code>docker run -p 9001:9001 \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -e DOCKER_SOCKET_PATH=/var/run/docker.sock\n  roboflow/roboflow-inference-server-cpu\n</code></pre> <p>Explanation:</p> <ul> <li> <p>In line <code>2</code>, you mount a docker daemon socket from your host (typically <code>/var/run/docker.sock</code>,  but you need to verify your setup) into some location inside container (for convenience also <code>/var/run/docker.sock</code>)</p> </li> <li> <p>In line <code>3</code>, you expose environmental variable <code>DOCKER_SOCKET_PATH</code> with the location of the docker daemon socket  inside the container - in this case <code>/var/run/docker.sock</code>, as per specification in line <code>2</code></p> </li> </ul> <p>Then you would be able to reach <code>GET /device/stats</code> endpoint using cURL: <pre><code>curl http://127.0.0.1:9001/device/stats\n</code></pre></p> <p>or using Python: <pre><code>import requests\n\nresult = requests.get(\"http://127.0.0.1:9001/device/stats\")\nresult.raise_for_status()\n\nprint(result.json())\n</code></pre></p>"},{"location":"using_inference/http_api/","title":"Http api","text":"<p>The HTTP Inference API provides a standard API through which to run inference on computer vision models. The HTTP API is a helpful way to treat your machine learning models as their own microservice. With this interface, you will run a docker container and make requests over HTTP. The requests contain all of the information Inference needs to run a computer vision model including the model ID, the input image data, and any configurable parameters used during processing (e.g. confidence threshold).</p>"},{"location":"using_inference/http_api/#quickstart","title":"Quickstart","text":""},{"location":"using_inference/http_api/#install-the-inference-server","title":"Install the Inference Server","text":"<p>You can skip this step if you already have Inference installed and running.</p> <p>The Inference Server runs in Docker. Before we begin, make sure you have installed Docker on your system. To learn how to install Docker, refer to the official Docker installation guide.</p> <p>Once you have Docker installed, you are ready to download Roboflow Inference. The command you need to run depends on what device you are using.</p> <p>Start the server using <code>inference server start</code>. After you have started the Inference Server, the Docker container will start running the server at <code>localhost:9001</code>.</p>"},{"location":"using_inference/http_api/#run-inference","title":"Run Inference","text":"<p>You can send a URL with an image, a NumPy array, or a base64-encoded image to an Inference server. The server will return a JSON response with the predictions. The easiest way to interact with the Roboflow Inference server is to sue the Inference SDK. To do this first install it with pip:</p> <pre><code>pip install inference-sdk\n</code></pre> <p>Next, instantiate a client and use the <code>infer(...)</code> method:</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nproject_id = \"soccer-players-5fuqs\"\nmodel_version = \"1\"\nmodel_id = project_id + \"/\" + model_version\nimage_url = \"https://media.roboflow.com/inference/soccer.jpg\"\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\n\nresults = client.infer(image_url, model_id=model_id)\n</code></pre> <p>Note</p> <p>The model id is composed of the string <code>&lt;project_id&gt;/&lt;version_id&gt;</code>. You can find these pieces of information by following the guide here.</p> <p>Hint</p> <p>See full docs for the Inference SDK.</p>"},{"location":"using_inference/http_api/#visualize-results","title":"Visualize Results","text":"<pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nmodel_id = \"soccer-players-5fuqs/1\"\nimage_file = \"soccer.jpg\"\n\nimage = cv2.imread(image_file)\n\n#Configure client\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\", # route for local inference server\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"], # api key for your workspace\n)\n\n#Run inference\nresult = client.infer(image, model_id=model_id)\n\n#Load results into Supervision Detection API\ndetections = sv.Detections.from_inference(result)\n\n#Create Supervision annotators\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n#Extract labels array from inference results\nlabels = [p['class'] for p in result['predictions']]\n\n#Apply results to image using Supervision annotators\nannotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\n#Write annotated image to file or display image\nwith sv.ImageSink(target_dir_path=\"./results/\", overwrite=True) as sink:\n    sink.save_image(annotated_image)\n#or sv.plot_image(annotated_image)\n</code></pre>"},{"location":"using_inference/http_api/#hosted-api","title":"Hosted API","text":"<p>Roboflow hosts a powerful and infinitely scalable version of the Roboflow Inference Server. This makes it even easier to integrate computer vision models into your software without adding any maintenance burden. And, since the Roboflow hosted APIs are running using the Inference package, it's easy to switch between using a hosted server and an on prem server without having to reinvent your client code. To use the hosted API, simply replace the <code>api_url</code> parameter passed to the Inference SDK client configuration. The hosted API base URL is <code>https://detect.roboflow.com</code>.</p> <pre><code>from inference_sdk import InferenceHTTPClient, InferenceConfiguration\n\nproject_id = \"soccer-players-5fuqs/1\"\nimage_url = \"https://media.roboflow.com/inference/soccer.jpg\"\n\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\",\n    api_key=os.environ[\"ROBOFLOW_API_KEY\"],\n)\n\nresults = client.infer(image_url, model_id=f\"{model_id}\")\n</code></pre>"},{"location":"using_inference/inference_pipeline/","title":"Inference Pipeline","text":"<p>The Inference Pipeline interface is made for streaming and is likely the best route to go for real time use cases.  It is an asynchronous interface that can consume many different video sources including local devices (like webcams),  RTSP video streams, video files, etc. With this interface, you define the source of a video stream and sinks.</p> <p>Now, since version <code>v0.9.18</code> <code>InferencePipeline</code> supports multiple sources of video at the same time! </p>"},{"location":"using_inference/inference_pipeline/#quickstart","title":"Quickstart","text":"<p>First, install Inference:</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>Next, create an Inference Pipeline:</p> <pre><code># import the InferencePipeline interface\nfrom inference import InferencePipeline\n# import a built-in sink called render_boxes (sinks are the logic that happens after inference)\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\napi_key = \"YOUR_ROBOFLOW_API_KEY\"\n\n# create an inference pipeline object\npipeline = InferencePipeline.init(\n    model_id=\"yolov8x-1280\", # set the model id to a yolov8x model with in put size 1280\n    video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\", # set the video reference (source of video), it can be a link/path to a video file, an RTSP stream url, or an integer representing a device id (usually 0 for built in webcams)\n    on_prediction=render_boxes, # tell the pipeline object what to do with each set of inference by passing a function\n    api_key=api_key, # provide your roboflow api key for loading models from the roboflow api\n)\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre> <p>Let's break down the example line by line:</p> <p><code>pipeline = InferencePipeline.init(...)</code></p> <p>Here, we are calling a class method of InferencePipeline.</p> <p><code>model_id=\"yolov8x-1280\"</code></p> <p>We set the model ID to a YOLOv8x model pre-trained on COCO with input resolution <code>1280x1280</code>.</p> <p><code>video_reference=\"https://storage.googleapis.com/com-roboflow-marketing/inference/people-walking.mp4\"</code></p> <p>We set the video reference to a URL. Later we will show the various values that can be used as a video reference.</p> <p><code>on_prediction=render_boxes</code></p> <p>The <code>on_prediction</code> argument defines our sink (or a list of sinks).</p> <p><code>pipeline.start(); pipeline.join()</code></p> <p>Here, we start and join the thread that processes the video stream.</p>"},{"location":"using_inference/inference_pipeline/#what-is-video-reference","title":"What is video reference?","text":"<p>Inference Pipelines can consume many different types of video streams.</p> <ul> <li>Device Id (integer): Providing an integer instructs a pipeline to stream video from a local device, like a webcam. Typically, built in webcams show up as device <code>0</code>.</li> <li>Video File (string): Providing the path to a video file will result in the pipeline reading every frame from the file, running inference with the specified model, then running the <code>on_prediction</code> method with each set of resulting predictions.</li> <li>Video URL (string): Providing the path to a video URL is equivalent to providing a video file path and voids needing to first download the video.</li> <li>RTSP URL (string): Providing an RTSP URL will result in the pipeline streaming frames from an RTSP stream as fast as possible, then running the <code>on_prediction</code> callback on the latest available frame.</li> <li>Since version <code>0.9.18</code> - list of elements that may be any of values described above.</li> </ul>"},{"location":"using_inference/inference_pipeline/#how-the-inferencepipeline-works","title":"How the <code>InferencePipeline</code> works?","text":"<p><code>InferencePipeline</code> spins a video source consumer thread for each provided video reference. Frames from videos are grabbed by video multiplexer that awaits <code>batch_collection_timeout</code> (if source will not provide frame, smaller batch  will be passed to <code>on_video_frame(...)</code>, but missing frames and predictions will be filled with <code>None</code> before passing to <code>on_prediction(...)</code>). <code>on_prediction(...)</code> may work in <code>SEQUENTIAL</code> mode (only one element at once), or <code>BATCH</code>  mode - all batch elements at a time and that can be controlled by <code>sink_mode</code> parameter.</p> <p>For static video files, <code>InferencePipeline</code> processes all frames by default, for streams - it is possible to drop frames from the buffers - in favour of always processing the most recent data (when model inference is slow, more frames can be accumulated in buffer - stream processing drop older frames and only processes the most recent one).</p> <p>To enhance stability, in case of streams processing - video sources will be automatically re-connected once  connectivity is lost during processing. That is meant to prevent failures in production environment when the pipeline can run long hours and need to gracefully handle sources downtimes.</p>"},{"location":"using_inference/inference_pipeline/#how-to-provide-a-custom-inference-logic-to-inferencepipeline","title":"How to provide a custom inference logic to <code>InferencePipeline</code>","text":"<p>As of <code>inference&gt;=0.9.16</code>, Inference Pipelines support running custom inference logic. This means, instead of passing  a model ID, you can pass a custom callable. This callable should accept and <code>VideoFrame</code> return a dictionary with  results from the processing (as <code>on_video_frame</code> handler). It can be model predictions or results of any other processing you wish to execute. It is important to note that the sink being used (<code>on_prediction</code> handler you use) - must be adjusted to the specific format of <code>on_video_frame(...)</code> response. This way, you can shape video processing in a way you want.</p> <pre><code># This is example, reference implementation - you need to adjust the code to your purposes\nimport os\nimport json\nfrom inference.core.interfaces.camera.entities import VideoFrame\nfrom inference import InferencePipeline\nfrom typing import Any, List\n\nTARGET_DIR = \"./my_predictions\"\n\nclass MyModel:\n\n  def __init__(self, weights_path: str):\n    self._model = your_model_loader(weights_path)\n\n  # before v0.9.18  \n  def infer(self, video_frame: VideoFrame) -&gt; Any:\n    return self._model(video_frame.image)\n\n  # after v0.9.18  \n  def infer(self, video_frames: List[VideoFrame]) -&gt; List[Any]: \n    # result must be returned as list of elements representing model prediction for single frame\n    # with order unchanged.\n    return self._model([v.image for v in video_frames])\n\ndef save_prediction(prediction: dict, video_frame: VideoFrame) -&gt; None:\n  with open(os.path.join(TARGET_DIR, f\"{video_frame.frame_id}.json\")) as f:\n    json.dump(prediction, f)\n\nmy_model = MyModel(\"./my_model.pt\")\n\npipeline = InferencePipeline.init_with_custom_logic(\n  video_reference=\"./my_video.mp4\",\n  on_video_frame=my_model.infer,\n  on_prediction=save_prediction,\n)\n\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre>"},{"location":"using_inference/inference_pipeline/#inferencepipeline-and-roboflow-workflows","title":"<code>InferencePipeline</code> and Roboflow <code>workflows</code>","text":"<p>Info</p> <p>This is feature preview. Please refer to workflows docs.</p> <p>Feature preview do not support multiple videos input!</p> <p>We are working to make <code>workflows</code> compatible with <code>InferencePipeline</code>. Since version <code>0.9.16</code> we introduce  an initializer to be used with workflow definitions. Here is the example:</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\nfrom inference.core.interfaces.stream.sinks import render_boxes\n\ndef workflows_sink(\n    predictions: dict,\n    video_frame: VideoFrame,\n) -&gt; None:\n    render_boxes(\n        predictions[\"predictions\"][0],\n        video_frame,\n        display_statistics=True,\n    )\n\n\n# here you may find very basic definition of workflow - with a single object detection model.\n# Please visit workflows docs: https://github.com/roboflow/inference/tree/main/inference/enterprise/workflows to\n# find more examples.\nworkflow_specification = {\n    \"specification\": {\n        \"version\": \"1.0\",\n        \"inputs\": [\n            {\"type\": \"InferenceImage\", \"name\": \"image\"},\n        ],\n        \"steps\": [\n            {\n                \"type\": \"ObjectDetectionModel\",\n                \"name\": \"step_1\",\n                \"image\": \"$inputs.image\",\n                \"model_id\": \"yolov8n-640\",\n                \"confidence\": 0.5,\n            }\n        ],\n        \"outputs\": [\n            {\"type\": \"JsonField\", \"name\": \"predictions\", \"selector\": \"$steps.step_1.*\"},\n        ],\n    }\n}\npipeline = InferencePipeline.init_with_workflow(\n    video_reference=\"./my_video.mp4\",\n    workflow_specification=workflow_specification,\n    on_prediction=workflows_sink,\n    image_input_name=\"image\",  # adjust according to name of WorkflowImage input you define\n    video_metadata_input_name=\"video_metadata\" # AVAILABLE from v0.17.0! adjust according to name of WorkflowVideoMetadata input you define\n)\n\n# start the pipeline\npipeline.start()\n# wait for the pipeline to finish\npipeline.join()\n</code></pre> <p>Additionally, since <code>v0.9.21</code>, you can initialise <code>InferencePipeline</code> with <code>workflow</code> registered in Roboflow App - providing your <code>workspace_name</code> and <code>workflow_id</code>:</p> <pre><code>pipeline = InferencePipeline.init_with_workflow(\n    video_reference=\"./my_video.mp4\",\n    workspace_name=\"&lt;your_workspace&gt;\",\n    workflow_id=\"&lt;your_workflow_id_to_be_found_in_workflow_url&gt;\",\n    on_prediction=workflows_sink,\n)\n</code></pre> <p>Workflows profiling</p> <p>Since <code>inference v0.22.0</code>, you may profile your Workflow execution inside <code>InferencePipeline</code> when  you export environmental variable <code>ENABLE_WORKFLOWS_PROFILING=True</code>. Additionally, you can tune the  number of frames you keep in profiler buffer via another environmental variable <code>WORKFLOWS_PROFILER_BUFFER_SIZE</code>. <code>init_with_workflow(...)</code> was also given a new parameter <code>profiling_directory</code> which can be adjusted to  dictate where to save the trace. </p>"},{"location":"using_inference/inference_pipeline/#sinks","title":"Sinks","text":"<p>Sinks define what an Inference Pipeline should do with each prediction. A sink is a function with signature:</p>"},{"location":"using_inference/inference_pipeline/#before-v0918","title":"Before <code>v0.9.18</code>","text":"<pre><code>from inference.core.interfaces.camera.entities import VideoFrame\n\n\ndef on_prediction(\n    predictions: dict,\n    video_frame: VideoFrame,\n) -&gt; None:\n    pass\n</code></pre> <p>The arguments are:</p> <ul> <li><code>predictions</code>: A dictionary that is the response object resulting from a call to a model's <code>infer(...)</code> method.</li> <li><code>video_frame</code>: A VideoFrame object containing metadata and pixel data from the video frame.</li> </ul>"},{"location":"using_inference/inference_pipeline/#after-v0918","title":"After <code>v0.9.18</code>","text":"<pre><code>from typing import Union, List, Optional\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef on_prediction(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    for prediction, frame in zip(predictions, video_frame):\n        if prediction is None:\n            # EMPTY FRAME\n            continue\n        # SOME PROCESSING\n</code></pre> <p>See more info in Custom Sink section on how to create sink.</p>"},{"location":"using_inference/inference_pipeline/#usage","title":"Usage","text":"<p>You can also make <code>on_prediction</code> accepting other parameters that configure its behaviour, but those needs to be  latched in function closure before injection into <code>InferencePipeline</code> init methods.</p> <pre><code>from functools import partial\nfrom inference.core.interfaces.camera.entities import VideoFrame\nfrom inference import InferencePipeline\n\n\ndef on_prediction(\n    predictions: dict,\n    video_frame: VideoFrame,\n    my_parameter: int,\n) -&gt; None:\n    # you need to implement your logic here, with `my_parameter` used\n    pass\n\npipeline = InferencePipeline.init(\n  video_reference=\"./my_video.mp4\",\n  model_id=\"yolov8n-640\",\n  on_prediction=partial(on_prediction, my_parameter=42),\n)\n</code></pre>"},{"location":"using_inference/inference_pipeline/#custom-sinks","title":"Custom Sinks","text":"<p>To create a custom sink, define a new function with the appropriate signature.</p> <pre><code>from typing import Union, List, Optional, Any\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef on_prediction(\n    predictions: Union[Any, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    if not issubclass(type(predictions), list):\n      # this is required to support both sequential and batch processing with single code\n      # if you use only one mode - you may create function that handles with only one type\n      # of input\n      predictions = [predictions]\n      video_frame = [video_frame]\n    for prediction, frame in zip(predictions, video_frame):\n        if prediction is None:\n            # EMPTY FRAME\n            continue\n        # SOME PROCESSING\n</code></pre> <p>In <code>v0.9.18</code> we introduced <code>InferencePipeline</code> parameter called <code>sink_mode</code> - here is how it works. With <code>SinkMode.SEQUENTIAL</code> - each frame and prediction triggers separate call for sink, in case of <code>SinkMode.BATCH</code> -  list of frames and predictions will be provided to sink, always aligned in the order of video sources - with None  values in the place of vide_frames / predictions that were skipped due to <code>batch_collection_timeout</code>.  <code>SinkMode.ADAPTIVE</code> is a middle ground (and default mode) - all old sources will work in that mode against a single  video input, as the pipeline will behave as if running in <code>SinkMode.SEQUENTIAL</code>. To handle multiple videos -  sink needs to accept <code>predictions: List[Optional[dict]]</code> and <code>video_frame: List[Optional[VideoFrame]]</code>. It is also  possible to process multiple videos using old sinks - but then <code>SinkMode.SEQUENTIAL</code> is to be used, causing sink to be called on each prediction element.</p>"},{"location":"using_inference/inference_pipeline/#why-there-is-optional-in-listoptionaldict-and-listoptionalvideoframe","title":"Why there is <code>Optional</code> in  <code>List[Optional[dict]]</code> and <code>List[Optional[VideoFrame]]</code>?","text":"<p>It may happen that it is not possible to collect video frames from all the video sources (for instance when one of the  source disconnected and re-connection is attempted). <code>predictions</code> and <code>video_frame</code> are ordered matching the order of <code>video_reference</code> list of <code>InferencePipeline</code> and <code>None</code> elements will appear in position of missing frames. We provide this information to sink, as some sinks may require all predictions and video frames from the batch to be provided (even if missing) - for example: <code>render_boxes(...)</code> sink needs that information to maintain the position of frames in tiles mosaic.</p> <p>Info</p> <p>See our tutorial on creating a custom Inference Pipeline sink!</p> <p>prediction</p> <p>Predictions are provided to the sink as a dictionary containing keys:</p> <ul> <li><code>predictions</code>: predictions - either for single frame or batch of frames. Content depends on which model runs behind  <code>InferencePipeline</code> - for Roboflow models - it will come as dict or list of dicts. The schema of elements is given  below.</li> </ul> <p>Depending on the model output, predictions look differently. You must adjust sink to the prediction format. For instance, Roboflow object-detection prediction contains the following keys:</p> <ul> <li><code>x</code>: The center x coordinate of the predicted bounding box in pixels</li> <li><code>y</code>: The center y coordinate of the predicted bounding box in pixels</li> <li><code>width</code>: The width of the predicted bounding box in pixels</li> <li><code>height</code>: The height of the predicted bounding box in pixels</li> <li><code>confidence</code>: The confidence value of the prediction (between 0 and 1)</li> <li><code>class</code>: The predicted class name</li> <li><code>class_id</code>: The predicted class ID</li> </ul>"},{"location":"using_inference/inference_pipeline/#other-pipeline-configuration","title":"Other Pipeline Configuration","text":"<p>Inference Pipelines are highly configurable. Configurations include:</p> <ul> <li><code>max_fps</code>: Used to set the maximum rate of frame processing.</li> <li><code>confidence</code>: Confidence threshold used for inference.</li> <li><code>iou_threshold</code>: IoU threshold used for inference.</li> <li><code>video_source_properties</code>: Optional dictionary of properties to configure the video source, corresponding to cv2 VideoCapture properties cv2.CAP_PROP_*. See the OpenCV Documentation for a list of all possible properties.</li> </ul> <pre><code>from inference import InferencePipeline\npipeline = InferencePipeline.init(\n    ...,\n    max_fps=10,\n    confidence=0.75,\n    iou_threshold=0.4,\n    video_source_properties={\n        \"frame_width\": 1920.0,\n        \"frame_height\": 1080.0,\n        \"fps\": 30.0,\n    },\n)\n</code></pre> <p>See the reference docs for the full list of Inference Pipeline parameters.</p> <p>Breaking change planned at the end of Q4 2024</p> <p>We've discovered that the behaviour of <code>max_fps</code> parameter is not in line with <code>inference</code> clients expectations regarding processing of video files. Current implementation for vides waits before processing the next  video frame, instead droping the frames to modulate video FPS. </p> <p>We have added a way to change this suboptimal behaviour in release <code>v0.26.0</code> - new behaviour of  <code>InferencePipeline</code> can be enabled setting environmental variable flag  <code>ENABLE_FRAME_DROP_ON_VIDEO_FILE_RATE_LIMITING=True</code>. </p> <p>Please note that the new behaviour will be the default one end of Q4 2024!</p>"},{"location":"using_inference/inference_pipeline/#performance","title":"Performance","text":"<p>We tested the performance of Inference on a variety of hardware devices.</p> <p>Below are the results of our benchmarking tests for Inference.</p>"},{"location":"using_inference/inference_pipeline/#macbook-m2","title":"MacBook M2","text":"Test FPS yolov8-n ~26 yolov8-s ~12 yolov8-m ~5 <p>Tested against the same 1080p 60fps RTSP stream emitted by localhost.</p>"},{"location":"using_inference/inference_pipeline/#jetson-orin-nano","title":"Jetson Orin Nano","text":"Test FPS yolov8-n ~25 yolov8-s ~18 yolov8-m ~8 <p>With old version reaching at max 6-7 fps. This test was executed against 4K@60fps stream, which is not possible to be decoded in native pace due to resource constrains. New implementation proved to run without stability issues for few hours straight.</p>"},{"location":"using_inference/inference_pipeline/#tesla-t4","title":"Tesla T4","text":"<p>GPU workstation with Tesla T4 was able to run 4 concurrent HD streams at 15FPS utilising ~80% GPU - reaching over 60FPS throughput per GPU (against <code>yolov8-n</code>).</p>"},{"location":"using_inference/inference_pipeline/#migrating-from-inferencestream-to-inferencepipeline","title":"Migrating from <code>inference.Stream</code> to <code>InferencePipeline</code>","text":"<p>Inference is deprecating support for <code>inference.Stream</code>, our video stream inference interface. <code>inference.Stream</code> is being replaced with <code>InferencePipeline</code>, which has feature parity and achieves better performance. There are also new, more advanced features available in <code>InferencePipeline</code>.</p>"},{"location":"using_inference/inference_pipeline/#new-features-in-inferencepipeline","title":"New Features in <code>InferencePipeline</code>","text":""},{"location":"using_inference/inference_pipeline/#stability","title":"Stability","text":"<p>New implementation allows <code>InferencePipeline</code> to re-connect to a video source, eliminating the need to create additional logic to run inference against streams for long hours in fault-tolerant mode.</p>"},{"location":"using_inference/inference_pipeline/#granularity-of-control","title":"Granularity of control","text":"<p>New implementation let you decide how to handle video sources - and provided automatic selection of mode. Your videos will be processed frame-by-frame with each frame being passed to model, and streams will be processed in a way to provide continuous, up-to-date predictions on the most fresh frames - and the system will automatically adjust to performance of the hardware to ensure best experience.</p>"},{"location":"using_inference/inference_pipeline/#observability","title":"Observability","text":"<p>New implementation allows to create reports about InferencePipeline state in runtime - providing an easy way to build monitoring on top of it.</p>"},{"location":"using_inference/inference_pipeline/#migrate-from-inferencestream-to-inferencepipeline","title":"Migrate from <code>inference.Stream</code> to <code>InferencePipeline</code>","text":"<p>Let's assume you used <code>inference.Stream(...)</code> with your custom handlers:</p> <pre><code>import numpy as np\n\n\ndef on_prediction(predictions: dict, image: np.ndarray) -&gt; None:\n    pass\n</code></pre> <p>Now, the structure of handlers has changed into:</p> <pre><code>import numpy as np\n\ndef on_prediction(predictions, video_frame) -&gt; None:\n    pass\n</code></pre> <p>With predictions being still dict (passed as second parameter) in the same, standard Roboflow format, but <code>video_frame</code> is a dataclass with the following property:</p> <ul> <li><code>image</code>: which is video frame (<code>np.ndarray</code>)</li> <li><code>frame_id</code>: int value representing the place of the frame in stream order</li> <li><code>frame_timestamp</code>: time of frame grabbing - the exact moment when frame appeared in the file/stream   on the receiver side (<code>datetime.datetime</code>)</li> </ul> <p>Additionally, it eliminates the need of grabbing <code>.frame_id</code> from <code>inference.Stream()</code>.</p> <p><code>InferencePipeline</code> exposes interface to manage its state (possibly from different thread) - including functions like <code>.start()</code>, <code>.pause()</code>, <code>.terminate()</code>.</p>"},{"location":"using_inference/inference_pipeline/#migrate-to-changes-introduced-in-v0918","title":"Migrate to changes introduced in <code>v0.9.18</code>","text":"<p>List of changes: 1. <code>VideoFrame</code> got new parameter: <code>source_id</code> - indicating which video source yielded the frame 2. <code>on_prediction</code> callable signature changed: <pre><code>from typing import Callable, Any, Optional, List, Union\nfrom inference.core.interfaces.camera.entities import VideoFrame\n# OLD\nSinkHandler = Callable[[Any, VideoFrame], None]\n\n# NEW\nSinkHandler = Optional[\n    Union[\n        Callable[[Any, VideoFrame], None],\n        Callable[[List[Optional[Any]], List[Optional[VideoFrame]]], None],\n    ]\n]\n</code></pre> this change is non-breaking, as there is new parameter of <code>InferencePipeline.init*()</code> functions - <code>sink_mode</code> with default  value on <code>ADAPTIVE</code> - which forces single video frame and prediction to be provided for sink invocation if one video  only is specified. Old sinks were adjusted to work in dual mode - for instance in the demo you see <code>render_boxes(...)</code>  displaying image tiles.</p> <p>Example: <pre><code>from typing import Union, List, Optional\nimport json\n\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef save_prediction(predictions: dict, file_name: str) -&gt; None:\n  with open(file_name, \"w\") as f:\n    json.dump(predictions, f)\n\ndef on_prediction_old(predictions: dict, video_frame: VideoFrame) -&gt; None:\n  save_prediction(\n    predictions=predictions,\n    file_name=f\"frame_{video_frame.frame_id}.json\"\n  )\n\ndef on_prediction_new(\n    predictions: Union[dict, List[Optional[dict]]],\n    video_frame: Union[VideoFrame, List[Optional[VideoFrame]]],\n) -&gt; None:\n    for prediction, frame in zip(predictions, video_frame):\n        if prediction is None:\n            # EMPTY FRAME\n            continue\n        save_prediction(\n        predictions=prediction,\n        file_name=f\"source_{frame.source_id}_frame_{frame.frame_id}.json\"\n      )\n</code></pre></p> <ol> <li><code>on_video_frame</code> callable used in InferencePipeline.init_with_custom_logic(...)<code>changed: Previously:</code>InferenceHandler = Callable[[VideoFrame], Any]<code>Now:</code>InferenceHandler = Callable[[List[VideoFrame]], List[Any]]`</li> </ol> <p>Example: <pre><code>from inference.core.interfaces.camera.entities import VideoFrame\nfrom typing import Any, List\n\nMY_MODEL = ...\n\n# before v0.9.18  \ndef on_video_frame_old(video_frame: VideoFrame) -&gt; Any:\n  return MY_MODEL(video_frame.image)\n\n# after v0.9.18  \ndef on_video_frame_new(video_frames: List[VideoFrame]) -&gt; List[Any]: \n  # result must be returned as list of elements representing model prediction for single frame\n  # with order unchanged.\n  return MY_MODEL([v.image for v in video_frames])\n</code></pre></p> <ol> <li>The interface for <code>PipelineWatchdog</code> changed - and there is also a side effect change in form of pipeline state report  that is emitted being changed.</li> </ol> <p>Old watchdog: <pre><code>class PipelineWatchDog(ABC):\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def register_video_source(self, video_source: VideoSource) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_status_update(self, status_update: StatusUpdate) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_inference_started(\n        self, frame_timestamp: datetime, frame_id: int\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_prediction_ready(\n        self, frame_timestamp: datetime, frame_id: int\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def get_report(self) -&gt; Optional[PipelineStateReport]:\n        pass\n</code></pre></p> <p>New watchdog: <pre><code>class PipelineWatchDog(ABC):\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def register_video_sources(self, video_sources: List[VideoSource]) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_status_update(self, status_update: StatusUpdate) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_inference_started(\n        self,\n        frames: List[VideoFrame],\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def on_model_prediction_ready(\n        self,\n        frames: List[VideoFrame],\n    ) -&gt; None:\n        pass\n\n    @abstractmethod\n    def get_report(self) -&gt; Optional[PipelineStateReport]:\n        pass\n</code></pre></p> <p>Old report: <pre><code>@dataclass(frozen=True)\nclass PipelineStateReport:\n    video_source_status_updates: List[StatusUpdate]\n    latency_report: LatencyMonitorReport\n    inference_throughput: float\n    source_metadata: Optional[SourceMetadata]\n</code></pre></p> <p>New report: <pre><code>@dataclass(frozen=True)\nclass PipelineStateReport:\n    video_source_status_updates: List[StatusUpdate]\n    latency_reports: List[LatencyMonitorReport]  # now - one report for each source\n    inference_throughput: float\n    sources_metadata: List[SourceMetadata] # now - one metadata for each source\n</code></pre></p> <p>If there was custom watchdog created on your end - reimplementation should be easy, as all the data passed to methods previously for single video source / frame are now provided for all sources / frames.</p>"},{"location":"using_inference/native_python_api/","title":"Native python api","text":"<p>The native python API is the most simple and involves accessing the base package APIs directly. Going this route, you will import Inference modules directly into your python code. You will load models, run inference, and handle the results all within your own logic. You will also need to manage the dependencies within your python environment. If you are creating a simple app or just testing, the native Python API is a great place to start.</p> <p>Using the native python API centers on loading models, then calling their <code>infer(...)</code> method to get inference results.</p>"},{"location":"using_inference/native_python_api/#quickstart","title":"Quickstart","text":"<p>This example shows how to load a model, run inference, then display the results.</p> <p>Info</p> <p>Prior to installation, you may want to configure a python virtual environment to isolate dependencies of inference.</p> <p>To install Inference via pip:</p> <pre><code>pip install inference\n</code></pre> <p>If you have an NVIDIA GPU, you can accelerate your inference with:</p> <pre><code>pip install inference-gpu\n</code></pre> <p>Next, import a model:</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n</code></pre> <p>The <code>get_model</code> method is a utility function which will help us load a computer vision model from Roboflow. We load a model by referencing its <code>model_id</code>. For Roboflow models, the model ID is a combination of a project name and a version number <code>f\"{project_name}/{version_number}\"</code>.</p> <p>Hint</p> <p>You can find your models project name and version number in the Roboflow App. You can also browse public models that are ready to use on Roboflow Universe. In this example, we are using a special model ID that is an alias of a COCO pretrained model on Roboflow Universe. You can see the list of model aliases here.</p> <p>Next, we can run inference with our model by providing an input image:</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n\nresults = model.infer(\"people-walking.jpg\") # replace with path to your image\n</code></pre> <p>The results object is an inference response object. It contains some meta data (e.g. processing time) as well as an array of the predictions. The type of response and its attributes will depend on the type of model. See all of the Inference Response objects.</p> <p>Now, lets visualize the results using Supervision:</p> <pre><code>from inference import get_model\nimport supervision as sv\nimport cv2\n\n# Load model\nmodel = get_model(model_id=\"yolov8x-1280\")\n\n# Load image with cv2\nimage = cv2.imread(\"people-walking.jpg\")\n\n# Run inference\nresults = model.infer(image)[0]\n\n# Load results into Supervision Detection API\ndetections = sv.Detections.from_inference(results)\n\n# Create Supervision annotators\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\n# Extract labels array from inference results\nlabels = [p.class_name for p in results[0].predictions]\n\n\n\n# Apply results to image using Supervision annotators\nannotated_image = bounding_box_annotator.annotate(scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels\n)\n\n# Write annotated image to file or display image\nsv.plot_image(annotated_image)\n</code></pre> <p></p>"},{"location":"using_inference/native_python_api/#different-image-types","title":"Different Image Types","text":"<p>In the previous example, we saw that we can provide different image types to the <code>infer(...)</code> method. The <code>infer(...)</code> method accepts images in many forms including PIL images, OpenCV images (Numpy arrays), paths to local images, image URLs, and more. Under the hood, models use the <code>load_image(...)</code> method in the <code>image_utils</code> module.</p> <pre><code>from inference import get_model\n\nimport cv2\nfrom PIL import Image\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n\nimage_url = \"https://media.roboflow.com/inference/people-walking.jpg\"\nlocal_image_file = \"people-walking.jpg\"\npil_image = Image.open(local_image_file)\nnumpy_image = cv2.imread(local_image_file)\n\nresults = model.infer(image_url)\n#or     = model.infer(local_image_file)\n#or     = model.infer(pil_image)\n#or     = model.infer(numpy_image)\n</code></pre>"},{"location":"using_inference/native_python_api/#inference-parameters","title":"Inference Parameters","text":"<p>The <code>infer(...)</code> method accepts keyword arguments to set inference parameters. The example below shows setting the confidence threshold and the IoU threshold.</p> <pre><code>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\n\nresults = model.infer(\"people-walking.jpg\", confidence=0.75, iou_threshold=0.5)\n</code></pre>"},{"location":"workflows/about/","title":"Workflows","text":""},{"location":"workflows/about/#what-is-roboflow-workflows","title":"What is Roboflow Workflows?","text":"<p>Roboflow Workflows is an ecosystem that enables users to create machine learning applications using a wide range  of pluggable and reusable blocks. These blocks are organized in a way that makes it easy for users to design  and connect different components. Graphical interface allows to visually construct workflows  without needing extensive technical expertise. Once the workflow is designed, Workflows engine runs the  application, ensuring all the components work together seamlessly, providing a rapid transition  from prototype to production-ready solutions, allowing you to quickly iterate and deploy applications.  </p> <p>Roboflow offers a growing selection of workflows blocks, and the community can also create new blocks, ensuring  that the ecosystem is continuously expanding and evolving. Moreover, Roboflow provides flexible deployment options,  including on-premises and cloud-based solutions, allowing users to deploy their applications in the environment  that best suits their needs.</p> <p>With Workflows, you can:</p> <ul> <li> <p>Detect, classify, and segment objects in images using state-of-the-art models.</p> </li> <li> <p>Use Large Multimodal Models (LMMs) to make determinations at any stage in a workflow.</p> </li> <li> <p>Introduce elements of business logic to translate model predictions into your domain language</p> </li> </ul> Explore all Workflows blocks Begin building with Workflows <p></p> <p>In this section of documentation, we walk through what you need to know to create and run workflows. Let\u2019s get started! </p> <p>Create and run a workflow.</p>"},{"location":"workflows/blocks/","title":"Blocks Gallery","text":"Blocks"},{"location":"workflows/blocks_bundling/","title":"Bundling Workflows blocks","text":"<p>To efficiently manage the Workflows ecosystem, a standardized method for building and distributing blocks is  essential. This allows users to create their own blocks and bundle them into Workflow plugins. A Workflow plugin  is essentially a Python library that implements a defined interface and can be structured in various ways. </p> <p>This page outlines the mandatory interface requirements and suggests a structure for blocks that aligns with  the Workflows versioning guidelines.</p>"},{"location":"workflows/blocks_bundling/#proposed-structure-of-plugin","title":"Proposed structure of plugin","text":"<p>We propose the following structure of plugin:</p> <pre><code>.\n\u251c\u2500\u2500 requirements.txt   # file with requirements\n\u251c\u2500\u2500 setup.py           # use different package creation method if you like\n\u251c\u2500\u2500 {plugin_name}\n\u2502   \u251c\u2500\u2500 __init__.py    # main module that contains loaders\n\u2502   \u251c\u2500\u2500 kinds.py       # optionally - definitions of custom kinds\n\u2502   \u251c\u2500\u2500 {block_name}   # package for your block\n\u2502   \u2502   \u251c\u2500\u2500 v1.py      # version 1 of your block\n\u2502   \u2502   \u251c\u2500\u2500 ...        # ... next versions\n\u2502   \u2502   \u2514\u2500\u2500 v5.py      # version 5 of your block\n\u2502   \u2514\u2500\u2500 {block_name}   # package for another block\n\u2514\u2500\u2500 tests              # tests for blocks\n</code></pre>"},{"location":"workflows/blocks_bundling/#required-interface","title":"Required interface","text":"<p>Plugin must only provide few extensions to <code>__init__.py</code> in main package compared to standard Python library:</p> <ul> <li> <p><code>load_blocks()</code> function to provide list of blocks' classes (required)</p> </li> <li> <p><code>load_kinds()</code> function to return all custom kinds the plugin defines (optional)</p> </li> <li> <p><code>REGISTERED_INITIALIZERS</code> module property which is a dict mapping name of block  init parameter into default value or parameter-free function constructing that value - optional </p> </li> </ul>"},{"location":"workflows/blocks_bundling/#load_blocks-function","title":"<code>load_blocks()</code> function","text":"<p>Function is supposed to enlist all blocks in the plugin - it is allowed to define  a block once.</p> <p>Example:</p> <pre><code>from typing import List, Type\nfrom inference.core.workflows.prototypes.block import WorkflowBlock\n\n# example assumes that your plugin name is `my_plugin` and\n# you defined the blocks that are imported here\nfrom my_plugin.block_1.v1 import Block1V1\nfrom my_plugin.block_2.v1 import Block2V1\n\ndef load_blocks() -&gt; List[Type[WorkflowBlock]]:\n    return [\n        Block1V1,\n        Block2V1,\n]\n</code></pre>"},{"location":"workflows/blocks_bundling/#load_kinds-function","title":"<code>load_kinds()</code> function","text":"<p><code>load_kinds()</code> function to return all custom kinds the plugin defines. It is optional as your blocks may not need custom kinds.</p> <p>Example:</p> <pre><code>from typing import List\nfrom inference.core.workflows.execution_engine.entities.types import Kind\n\n# example assumes that your plugin name is `my_plugin` and\n# you defined the imported kind\nfrom my_plugin.kinds import MY_KIND\n\n\ndef load_kinds() -&gt; List[Kind]:\n    return [MY_KIND]\n</code></pre>"},{"location":"workflows/blocks_bundling/#registered_initializers-dictionary","title":"<code>REGISTERED_INITIALIZERS</code> dictionary","text":"<p>As you know from the docs describing the Workflows Compiler  and the blocks development guide, Workflow blocs are dynamically initialized during compilation and may require constructor  parameters. Those parameters can default to values registered in the <code>REGISTERED_INITIALIZERS</code> dictionary. To expose default a value for an init parameter of your block -  simply register the name of the init param and its value (or a function generating a value) in the dictionary. This is optional part of the plugin interface, as not every block requires a constructor.</p> <p>Example:</p> <pre><code>import os\n\ndef init_my_param() -&gt; str:\n    # do init here\n    return \"some-value\"\n\nREGISTERED_INITIALIZERS = {\n    \"param_1\": 37,\n    \"param_2\": init_my_param,\n}\n</code></pre>"},{"location":"workflows/blocks_bundling/#enabling-plugin-in-your-workflows-ecosystem","title":"Enabling plugin in your Workflows ecosystem","text":"<p>To load a plugin you must:</p> <ul> <li> <p>install the Python package with the plugin in the environment you run Workflows</p> </li> <li> <p>export an environment variable named <code>WORKFLOWS_PLUGINS</code> set to a comma-separated list of names of plugins you want to load. </p> </li> <li> <p>Example: to load two plugins <code>plugin_a</code> and <code>plugin_b</code>, you need to run    <code>export WORKFLOWS_PLUGINS=\"plugin_a,plugin_b\"</code></p> </li> </ul>"},{"location":"workflows/blocks_connections/","title":"Rules dictating which blocks can be connected","text":"<p>A natural question you might ask is: How do I know which blocks to connect to achieve my desired outcome?  This is a crucial question, which is why we've created auto-generated  documentation for all supported Workflow blocks. In this guide, we\u2019ll show you how to use  these docs effectively and explain key details that help you understand why certain connections between  blocks are possible, while others may not be.</p> <p>Note</p> <p>Using the Workflows UI in the Roboflow APP you may find compatible connections between steps found automatically without need for your input. This page explains briefly how to deduce if two  blocks can be connected, making it possible to connect steps manually if needed. Logically, the page must appear before a link to blocks gallery, as it explains  how to effectively use these docs. At the same time, it introduces references to concepts  further explained in the User and Developer Guide. Please continue reading those sections if you find some concepts presented here needing further explanation.</p>"},{"location":"workflows/blocks_connections/#navigation-the-blocks-documentation","title":"Navigation the blocks documentation","text":"<p>When you open the blocks documentation, you\u2019ll see a list of all blocks supported by Roboflow. Each block entry  includes a name, brief description, category, and license for the block. You can click on any block to see more  detailed information.</p> <p>On the block details page, you\u2019ll find documentation for all supported versions of that block,  starting with the latest version. For each version, you\u2019ll see:</p> <ul> <li> <p>detailed description of the block</p> </li> <li> <p>type identifier, which is required in Workflow definitions to identify the specific block used for a step</p> </li> <li> <p>table of configuration properties, listing the fields that can be specified in a Workflow definition,  including their types, descriptions, and whether they can accept a dynamic selector or just a fixed value.</p> </li> <li> <p>Available connections, showing which blocks can provide inputs to this block and which can use its outputs.</p> </li> <li> <p>A list of input and output bindings:</p> </li> <li> <p>input bindings are the names of step definition properties that can hold selectors, along with the type    (or <code>kind</code>) of data they pass.</p> </li> <li> <p>output bindings are names and kinds for block outputs that can be used as inputs by steps defined in    Workflow definition</p> </li> <li> <p>An example of a Workflow step based on the documented block.</p> </li> </ul> <p>The <code>kind</code> mentioned above refers to the type of data flowing through the connection during execution,  and this is further explained in the developer guide.</p>"},{"location":"workflows/blocks_connections/#what-makes-connections-valid","title":"What makes connections valid?","text":"<p>Each block provides a manifest that lists the fields to be included in the Workflow Definition when creating a step.  The Values of these fields in a Workflow Definition may contain:</p> <ul> <li> <p>References (selectors) to data the block will process, such as step outputs or  batch-oriented workflow inputs</p> </li> <li> <p>Configuration values: Specific settings for the step or references (selectors) that  provide configuration parameters dynamically during execution.</p> </li> </ul> <p>The manifest also includes the block's outputs.</p> <p>For each step definition field (if it can hold a selector) and step output,  the expected kind is specified. A kind is a high-level definition  of the type of data that will be passed during workflow execution. Simply put, it describes the data that  will replace the selector during block execution.</p> <p>To ensure steps are correctly connected, the Workflow Compiler checks if the input and output kinds match. If they do, the connection is valid.</p> <p>Additionally, the <code>dimensionality level</code> of the data is considered when  validating connections. This ensures that data from multiple sources is compatible across the entire Workflow,  not just between two connected steps. More details on dimensionality levels can be found in the  user guide describing workflow execution.</p>"},{"location":"workflows/create_and_run/","title":"How to Create and Run a Workflow","text":"<p>In this example, we are going to build a Workflow from scratch that detects dogs, classifies their breeds and visualizes results.</p>"},{"location":"workflows/create_and_run/#step-1-create-a-workflow","title":"Step 1: Create a Workflow","text":"<p>Open https://app.roboflow.com/ in your browser, and navigate to the Workflows tab to click  the Create Workflows button. Select Custom Workflow to start the creation process.</p> <p></p>"},{"location":"workflows/create_and_run/#step-2-add-an-object-detection-model","title":"Step 2: Add an object detection model","text":"<p>We need to add a block with an object detection model to the existing workflow. We will use the <code>yolov8n-640</code> model.</p> <p></p>"},{"location":"workflows/create_and_run/#step-3-crop-each-detected-object-to-run-breed-classification","title":"Step 3: Crop each detected object to run breed classification","text":"<p>Next, we are going to add a block to our Workflow that crops the objects that our first model detects.</p> <p></p>"},{"location":"workflows/create_and_run/#step-4-classify-dog-breeds-with-second-stage-model","title":"Step 4: Classify dog breeds with second stage model","text":"<p>We are then going to add a classification model that runs on each crop to classify its content. We will use Roboflow Universe model <code>dog-breed-xpaq6/1</code>. Please make sure that in the block configuration, the <code>Image</code> property points to the <code>crops</code> output of the  Dynamic Crop block.</p> <p></p>"},{"location":"workflows/create_and_run/#step-5-replace-bounding-box-classes-with-classification-model-predictions","title":"Step 5: Replace Bounding Box classes with classification model predictions","text":"<p>When each crop is classified, we would like to assign the class predicted for each crop (dog breed) as a class  of the dog bounding boxes from the object detection model . To do this we use Detections Classes Replacement block,  which accepts a reference to predictions of a object detection model, as well as a reference to the classification  results on the crops.</p> <p></p>"},{"location":"workflows/create_and_run/#step-6-visualise-predictions","title":"Step 6: Visualise predictions","text":"<p>As a final step of the workflow, we would like to visualize our predictions. We will use two  visualization blocks: Bounding Box Visualization and Label Visualization chained together. At first, add Bounding Box Visualization referring to <code>$inputs.image</code> for the Image property (that's the image sent as your input to workflow), the second step (Label Visualization) however, should point to  the output of Bounding Box Visualization step. Both visualization steps should refer to predictions  from the Detections Classes Replacement step.</p> <p></p>"},{"location":"workflows/create_and_run/#step-7-construct-output","title":"Step 7: Construct output","text":"<p>You have everything ready to construct your workflow output. You can use any intermediate step output that you need, but in this example we will only select bounding boxes with replaced classes (output from Detections  Classes Replacement step) and visualisation (output from Label Visualization step).</p>"},{"location":"workflows/create_and_run/#step-8-running-the-workflow","title":"Step 8: Running the workflow","text":"<p>Now your workflow, is ready. You can click the <code>Save</code> button and move to the <code>Run Preview</code> panel.</p> <p>We will run our workflow against the following example image <code>https://media.roboflow.com/inference/dog.jpeg</code>. Here are the results</p> <p></p> <p>Clicking on the <code>Show Visual</code> button you will find results of our visualization efforts. </p>"},{"location":"workflows/create_and_run/#different-ways-of-running-your-workflow","title":"Different ways of running your workflow","text":"<p>Your workflow is now saved on the Roboflow Platform. This means you can run it in multiple different ways, including:</p> <ul> <li> <p>HTTP request to Roboflow Hosted API</p> </li> <li> <p>HTTP request to your local instance of <code>inference server</code></p> </li> <li> <p>on video</p> </li> </ul> <p>To see code snippets, click the <code>Deploy Workflow</code> button: </p>"},{"location":"workflows/create_and_run/#workflow-definition-for-quick-reproduction","title":"Workflow definition for quick reproduction","text":"<p>To make it easier to reproduce the workflow, below you can find a workflow definition you can copy-paste to UI editor.</p> Workflow definition <pre><code>{\n  \"version\": \"1.0\",\n  \"inputs\": [\n    {\n      \"type\": \"InferenceImage\",\n      \"name\": \"image\"\n    }\n  ],\n  \"steps\": [\n    {\n      \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n      \"name\": \"model\",\n      \"images\": \"$inputs.image\",\n      \"model_id\": \"yolov8n-640\"\n    },\n    {\n      \"type\": \"roboflow_core/dynamic_crop@v1\",\n      \"name\": \"dynamic_crop\",\n      \"images\": \"$inputs.image\",\n      \"predictions\": \"$steps.model.predictions\"\n    },\n    {\n      \"type\": \"roboflow_core/roboflow_classification_model@v1\",\n      \"name\": \"model_1\",\n      \"images\": \"$steps.dynamic_crop.crops\",\n      \"model_id\": \"dog-breed-xpaq6/1\"\n    },\n    {\n      \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n      \"name\": \"detections_classes_replacement\",\n      \"object_detection_predictions\": \"$steps.model.predictions\",\n      \"classification_predictions\": \"$steps.model_1.predictions\"\n    },\n    {\n      \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n      \"name\": \"bounding_box_visualization\",\n      \"predictions\": \"$steps.detections_classes_replacement.predictions\",\n      \"image\": \"$inputs.image\"\n    },\n    {\n      \"type\": \"roboflow_core/label_visualization@v1\",\n      \"name\": \"label_visualization\",\n      \"predictions\": \"$steps.detections_classes_replacement.predictions\",\n      \"image\": \"$steps.bounding_box_visualization.image\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"type\": \"JsonField\",\n      \"name\": \"detections\",\n      \"coordinates_system\": \"own\",\n      \"selector\": \"$steps.detections_classes_replacement.predictions\"\n    },\n    {\n      \"type\": \"JsonField\",\n      \"name\": \"visualisation\",\n      \"coordinates_system\": \"own\",\n      \"selector\": \"$steps.label_visualization.image\"\n    }\n  ]\n}\n</code></pre>"},{"location":"workflows/create_and_run/#next-steps","title":"Next Steps","text":"<p>Now that you have created and run your first workflow, you can explore our other supported blocks and create a more complex workflow.</p> <p>Refer to our Supported Blocks documentation to learn more about what blocks are supported. We also recommend reading the Understanding workflows page.</p>"},{"location":"workflows/create_workflow_block/","title":"Creating Workflow blocks","text":"<p>Workflows blocks development requires an understanding of the Workflow Ecosystem. Before diving deeper into the details, let's summarize the  required knowledge:</p> <p>Understanding of Workflow execution, in particular:</p> <ul> <li> <p>what is the relation of Workflow blocks and steps in Workflow definition</p> </li> <li> <p>how Workflow blocks and their manifests are used by Workflows Compiler</p> </li> <li> <p>what is the <code>dimensionality level</code> of batch-oriented data passing through Workflow</p> </li> <li> <p>how Execution Engine interacts with step, regarding  its inputs and outputs</p> </li> <li> <p>what is the nature and role of Workflow <code>kinds</code></p> </li> <li> <p>understanding how <code>pydantic</code> works</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#environment-setup","title":"Environment setup","text":"<p>As you will soon see, creating a Workflow block is simply a matter of defining a Python class that implements  a specific interface. This design allows you to run the block using the Python interpreter, just like any  other Python code. However, you may encounter difficulties when assembling all the required inputs, which would  normally be provided by other blocks during Workflow execution. Therefore, it's important to set up the development  environment properly for a smooth workflow. We recommend following these steps as part of the standard development  process (initial steps can be skipped for subsequent contributions):</p> <ol> <li> <p>Set up the <code>conda</code> environment and install main dependencies of <code>inference</code>, as described in <code>inference</code> contributor guide.</p> </li> <li> <p>Familiarize yourself with the organization of the Workflows codebase.</p> Workflows codebase structure - cheatsheet <p>Below are the key packages and directories in the Workflows codebase, along with their descriptions:</p> <ul> <li> <p><code>inference/core/workflows</code> - the main package for Workflows.</p> </li> <li> <p><code>inference/core/workflows/core_steps</code> - contains Workflow blocks that are part of the Roboflow Core plugin. At the top levels, you'll find block categories, and as you go deeper, each block has its own package, with modules hosting different versions, starting from <code>v1.py</code></p> </li> <li> <p><code>inference/core/workflows/execution_engine</code> - contains the Execution Engine. You generally won\u2019t need to modify this package unless contributing to Execution Engine functionality.</p> </li> <li> <p><code>tests/workflows/</code> - the root directory for Workflow tests</p> </li> <li> <p><code>tests/workflows/unit_tests/</code> - suites of unit tests for the Workflows Execution Engine and core blocks. This is where you can test utility functions used in your blocks.</p> </li> <li> <p><code>tests/workflows/integration_tests/</code> - suites of integration tests for the Workflows Execution Engine and core blocks. You can run end-to-end (E2E) tests of your workflows in combination with other blocks here.</p> </li> </ul> </li> <li> <p>Create a minimalistic block \u2013 You\u2019ll learn how to do this in the following sections. Start by implementing a simple block manifest and basic logic to ensure the block runs as expected.</p> </li> <li> <p>Add the block to the plugin \u2013 Once your block is created, add it to the list of blocks exported from the plugin. If you're adding the block to the Roboflow Core plugin, make sure to add an entry for your block in the loader.py. If you forget this step, your block won\u2019t be visible!</p> </li> <li> <p>Iterate and refine your block \u2013 Continue developing and running your block until you\u2019re satisfied with the results. The sections below explain how to iterate on your block in various scenarios.</p> </li> </ol>"},{"location":"workflows/create_workflow_block/#running-your-blocks-using-workflows-ui","title":"Running your blocks using Workflows UI","text":"<p>We recommend running the inference server with a mounted volume (which is much faster than re-building <code>inference</code>  server on each change):</p> <p><pre><code>inference_repo$ docker run -p 9001:9001 \\\n   -v ./inference:/app/inference \\\n   roboflow/roboflow-inference-server-cpu:latest\n</code></pre> and connecting your local server to Roboflow UI:</p> <p>to quickly run previews:</p> My block requires extra dependencies - I cannot use pre-built <code>inference</code> server <p>It's natural that your blocks may sometimes require additional dependencies. To add a dependency, simply include it in one of the  requirements fileshat are installed in the relevant Docker image  (usually the CPU build  of the <code>inference</code> server).</p> <p>Afterward, run:</p> <pre><code>inference_repo$ docker build \\\n   -t roboflow/roboflow-inference-server-cpu:test \\ \n   -f docker/dockerfiles/Dockerfile.onnx.cpu .\n</code></pre> <p>You can then run your local build by specifying the test tag you just created:</p> <pre><code>inference_repo$ inference_repo$ docker run -p 9001:9001 \\\n   -v ./inference:/app/inference \\\n   roboflow/roboflow-inference-server-cpu:test\n</code></pre>"},{"location":"workflows/create_workflow_block/#running-your-blocks-without-workflows-ui","title":"Running your blocks without Workflows UI","text":"<p>For contributors without access to the Roboflow platform, we recommend running the server as mentioned in the  section above. However, instead of using the UI editor, you will need to create a simple Workflow definition and  send a request to the server.</p> Running your Workflow without UI <p>The following code snippet demonstrates how to send a request to the <code>inference</code> server to run a Workflow.  The <code>inference_sdk</code> is included with the <code>inference</code> package as a lightweight client library for our server.</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nYOUR_WORKFLOW_DEFINITION = ...\n\nclient = InferenceHTTPClient(\n    api_url=object_detection_service_url,\n    api_key=\"XXX\",  # optional, only required if Workflow uses Roboflow Platform\n)\nresult = client.run_workflow(\n    specification=YOUR_WORKFLOW_DEFINITION,\n    images={\n        \"image\": your_image_np,   # this is example input, adjust it\n    },\n    parameters={\n        \"my_parameter\": 37,   # this is example input, adjust it\n    },\n)\n</code></pre>"},{"location":"workflows/create_workflow_block/#recommended-way-for-regular-contributors","title":"Recommended way for regular contributors","text":"<p>Creating integration tests in the <code>tests/workflows/integration_tests/execution</code> directory is a natural part of the  development iteration process. This approach allows you to develop and test simultaneously, providing valuable  feedback as you refine your code. Although it requires some experience, it significantly enhances  long-term code maintenance.</p> <p>The process is straightforward:</p> <ol> <li> <p>Create a New Test Module: For example, name it <code>test_workflows_with_my_custom_block.py</code>.</p> </li> <li> <p>Develop Example Workflows: Create one or more example Workflows. It would be best if your block cooperates  with other blocks from the ecosystem. </p> </li> <li> <p>Run Tests with Sample Data: Execute these Workflows in your tests using sample data  (you can explore our  fixtures to find example data we usually use).</p> </li> <li> <p>Assert Expected Results: Validate that the results match your expectations.</p> </li> </ol> <p>By incorporating testing into your development flow, you ensure that your block remains stable over time and  effectively interacts with existing blocks, enhancing the expressiveness of your work!</p> <p>You can run your test using the following command:</p> <p><pre><code>pytest tests/workflows/integration_tests/execution/test_workflows_with_my_custom_block\n</code></pre> Feel free to reference other tests for examples or use the following template:</p> Integration test template <pre><code>def test_detection_plus_classification_workflow_when_XXX(\n    model_manager: ModelManager,\n    dogs_image: np.ndarray, \n    roboflow_api_key: str,\n) -&gt; None:\n    # given\n    workflow_init_parameters = {\n        \"workflows_core.model_manager\": model_manager,\n        \"workflows_core.api_key\": roboflow_api_key,\n        \"workflows_core.step_execution_mode\": StepExecutionMode.LOCAL,\n    }\n    execution_engine = ExecutionEngine.init(\n        workflow_definition=&lt;YOUR-EXAMPLE-WORKLFOW&gt;,\n        init_parameters=workflow_init_parameters,\n        max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n    )\n\n    # when\n    result = execution_engine.run(\n        runtime_parameters={\n            \"image\": dogs_image,\n        }\n    )\n\n    # then\n    assert isinstance(result, list), \"Expected list to be delivered\"\n    assert len(result) == 1, \"Expected 1 element in the output for one input image\"\n    assert set(result[0].keys()) == {\n        \"predictions\",\n    }, \"Expected all declared outputs to be delivered\"\n    assert (\n        len(result[0][\"predictions\"]) == 2\n    ), \"Expected 2 dogs crops on input image, hence 2 nested classification results\"\n    assert [result[0][\"predictions\"][0][\"top\"], result[0][\"predictions\"][1][\"top\"]] == [\n        \"116.Parson_russell_terrier\",\n        \"131.Wirehaired_pointing_griffon\",\n    ], \"Expected predictions to be as measured in reference run\"\n</code></pre> <ul> <li> <p>In line <code>2</code>, you\u2019ll find the <code>model_manager</code> fixture, which is typically required by model blocks. This fixture provides the <code>ModelManager</code> abstraction from <code>inference</code>, used for loading and unloading models.</p> </li> <li> <p>Line <code>3</code> defines a fixture that includes an image of two dogs (explore other fixtures to find more example images).</p> </li> <li> <p>Line <code>4</code> is an optional fixture you may want to use if any of the blocks in your tested workflow require a Roboflow API key. If that\u2019s the case, export the <code>ROBOFLOW_API_KEY</code> environment variable with a valid key before running the test.</p> </li> <li> <p>Lines <code>7-11</code> provide the setup for the initialization parameters of the blocks that the Execution Engine will create at runtime, based on your Workflow definition.</p> </li> <li> <p>Lines <code>19-23</code> demonstrate how to run a Workflow by injecting input parameters. Please ensure that the keys in runtime_parameters match the inputs declared in your Workflow definition.</p> </li> <li> <p>Starting from line <code>26</code>, you\u2019ll find example assertions within the test.</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#prototypes","title":"Prototypes","text":"<p>To create a Workflow block you need some amount of imports from the core of Workflows library. Here is the list of imports that you may find useful while creating a block:</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import (\n    Batch,  # batches of data will come in Batch[X] containers\n    OutputDefinition,  # class used to declare outputs in your manifest\n    WorkflowImageData,  # internal representation of image\n    # - use whenever your input kind is image\n)\n\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,  # type alias for result of `run(...)` method\n    WorkflowBlock,  # base class for your block\n    WorkflowBlockManifest,  # base class for block manifest\n)\n\nfrom inference.core.workflows.execution_engine.entities.types import *  \n# module with `kinds` from the core library\n</code></pre> <p>The most important are:</p> <ul> <li> <p><code>WorkflowBlock</code> - base class for your block</p> </li> <li> <p><code>WorkflowBlockManifest</code> - base class for block manifest</p> </li> </ul> <p>Understanding internal data representation</p> <p>You may have noticed that we recommend importing the <code>Batch</code> and <code>WorkflowImageData</code> classes, which are    fundamental components used when constructing building blocks in our system. For a deeper understanding of    how these classes fit into the overall architecture, we encourage you to refer to the    Data Representations page for more detailed information. </p>"},{"location":"workflows/create_workflow_block/#block-manifest","title":"Block manifest","text":"<p>A manifest is a crucial component of a Workflow block that defines a prototype  for step declaration that can be placed in a Workflow definition to use the block.  In particular, it: </p> <ul> <li> <p>Uses <code>pydantic</code> to power syntax parsing of Workflows definitions:  It inherits from  <code>pydantic BaseModel</code> features to parse and  validate Workflow definitions. This schema can also be automatically exported to a format compatible with the  Workflows UI, thanks to <code>pydantic's</code> integration with the OpenAPI standard.</p> </li> <li> <p>Defines Data Bindings: It specifies which fields in the manifest are selectors for data flowing through  the workflow during execution and indicates their kinds.</p> </li> <li> <p>Describes Block Outputs: It outlines the outputs that the block will produce.</p> </li> <li> <p>Specifies Dimensionality: It details the properties related to input and output dimensionality.</p> </li> <li> <p>Indicates Batch Inputs and Empty Values: It informs the Execution Engine whether the step accepts batch  inputs and empty values.</p> </li> <li> <p>Ensures Compatibility: It dictates the compatibility with different Execution Engine versions to maintain  stability. For more details, see versioning.</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#scaffolding-for-manifest","title":"Scaffolding for manifest","text":"<p>To understand how manifests work, let's define one step-by-step. The example block that we build here will be  calculating images similarity. We start from imports and class scaffolding:</p> <pre><code>from typing import Literal\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n</code></pre> <p>This is the minimal representation of a manifest. It defines two special fields that are important for  Compiler and Execution engine:</p> <ul> <li> <p><code>type</code> - required to parse syntax of Workflows definitions based on dynamic pool of blocks - this is the  <code>pydantic</code> type discriminator that lets the Compiler understand which block manifest is to be verified when  parsing specific steps in a Workflow definition</p> </li> <li> <p><code>name</code> - this property will be used to give the step a unique name and let other steps selects it via selectors</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#adding-batch-oriented-inputs","title":"Adding batch-oriented inputs","text":"<p>We want our step to take two batch-oriented inputs with images to be compared - so effectively we will be creating SIMD block. </p> Adding batch-oriented inputs <p>Let's see how to add definitions of those inputs to manifest: </p> <pre><code>from typing import Literal, Union\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepOutputImageSelector,\n    WorkflowImageSelector,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    # all properties apart from `type` and `name` are treated as either \n    # definitions of batch-oriented data to be processed by block or its \n    # parameters that influence execution of steps created based on block\n    image_1: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"Second image to calculate similarity\",\n    )\n</code></pre> <ul> <li> <p>in the lines <code>2-9</code>, we've added a couple of imports to ensure that we have everything needed</p> </li> <li> <p>line <code>17</code> defines <code>image_1</code> parameter - as manifest is prototype for Workflow Definition,  the only way to tell about image to be used by step is to provide selector - we have  two specialised types in core library that can be used - <code>WorkflowImageSelector</code> and <code>StepOutputImageSelector</code>. If you look deeper into codebase, you will discover those are type aliases - telling <code>pydantic</code> to expect string matching <code>$inputs.{name}</code> and <code>$steps.{name}.*</code> patterns respectively, additionally providing  extra schema field metadata that tells Workflows ecosystem components that the <code>kind</code> of data behind selector is  image.</p> </li> <li> <p>denoting <code>pydantic</code> <code>Field(...)</code> attribute in the last parts of line <code>17</code> is optional, yet appreciated,  especially for blocks intended to cooperate with Workflows UI </p> </li> <li> <p>starting in line <code>20</code>, you can find definition of <code>image_2</code> parameter which is very similar to <code>image_1</code>.</p> </li> </ul> <p>Such definition of manifest can handle the following step declaration in Workflow definition:</p> <pre><code>{\n  \"type\": \"my_plugin/images_similarity@v1\",\n  \"name\": \"my_step\",\n  \"image_1\": \"$inputs.my_image\",\n  \"image_2\": \"$steps.image_transformation.image\"\n}\n</code></pre> <p>This definition will make the Compiler and Execution Engine:</p> <ul> <li> <p>select as a step prototype the block which declared manifest with type discriminator being  <code>my_plugin/images_similarity@v1</code></p> </li> <li> <p>supply two parameters for the steps run method:</p> </li> <li> <p><code>input_1</code> of type <code>WorkflowImageData</code> which will be filled with image submitted as Workflow execution input</p> </li> <li> <p><code>imput_2</code> of type <code>WorkflowImageData</code> which will be generated at runtime, by another step called    <code>image_transformation</code></p> </li> </ul>"},{"location":"workflows/create_workflow_block/#adding-parameter-to-the-manifest","title":"Adding parameter to the manifest","text":"<p>Let's now add the parameter that will influence step execution. The parameter is not assumed to be  batch-oriented and will affect all batch elements passed to the step.</p> Adding parameter to the manifest <pre><code>from typing import Literal, Union\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepOutputImageSelector,\n    WorkflowImageSelector,\n    FloatZeroToOne,\n    WorkflowParameterSelector,\n    FLOAT_ZERO_TO_ONE_KIND,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    # all properties apart from `type` and `name` are treated as either \n    # definitions of batch-oriented data to be processed by block or its \n    # parameters that influence execution of steps created based on block\n    image_1: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        FloatZeroToOne,\n        WorkflowParameterSelector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n</code></pre> <ul> <li> <p>line <code>9</code> imports <code>FloatZeroToOne</code> which is type alias providing validation  for float values in range 0.0-1.0 - this is based on native <code>pydantic</code> mechanism and everyone could create this type annotation locally in module hosting block</p> </li> <li> <p>line <code>10</code> imports function <code>WorkflowParameterSelector(...)</code> capable to dynamically create  <code>pydantic</code> type annotation for selector to workflow input parameter (matching format <code>$inputs.param_name</code>),  declaring union of kinds compatible with the field</p> </li> <li> <p>line <code>11</code> imports <code>float_zero_to_one</code> <code>kind</code> definition which will be used later</p> </li> <li> <p>in line <code>26</code> we start defining parameter called <code>similarity_threshold</code>. Manifest will accept  either float values (in range <code>[0.0-1.0]</code>) or selector to workflow input of <code>kind</code> <code>float_zero_to_one</code>. Please point out on how  function creating type annotation (<code>WorkflowParameterSelector(...)</code>) is used -  in particular, expected <code>kind</code> of data is passed as list of <code>kinds</code> - representing union of expected data <code>kinds</code>.</p> </li> </ul> <p>Such definition of manifest can handle the following step declaration in Workflow definition:</p> <pre><code>{\n  \"type\": \"my_plugin/images_similarity@v1\",\n  \"name\": \"my_step\",\n  \"image_1\": \"$inputs.my_image\",\n  \"image_2\": \"$steps.image_transformation.image\",\n  \"similarity_threshold\": \"$inputs.my_similarity_threshold\"\n}\n</code></pre> <p>or alternatively:</p> <pre><code>{\n  \"type\": \"my_plugin/images_similarity@v1\",\n  \"name\": \"my_step\",\n  \"image_1\": \"$inputs.my_image\",\n  \"image_2\": \"$steps.image_transformation.image\",\n  \"similarity_threshold\": \"0.5\"\n}\n</code></pre> LEARN MORE: Selecting step outputs <p>Our siplified example showcased declaration of properties that accept selectors to images produced by other steps via <code>StepOutputImageSelector</code>.</p> <p>You can use function <code>StepOutputSelector(...)</code> creating field annotations dynamically to express the that block accepts batch-oriented outputs from other steps of specified kinds</p> <pre><code>from typing import Literal, Union\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepOutputImageSelector,\n    WorkflowImageSelector,\n    StepOutputSelector,\n    NUMPY_ARRAY_KIND,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    # all properties apart from `type` and `name` are treated as either \n    # definitions of batch-oriented data to be processed by block or its \n    # parameters that influence execution of steps created based on block\n    image_1: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    example: StepOutputSelector(kind=[NUMPY_ARRAY_KIND])\n</code></pre>"},{"location":"workflows/create_workflow_block/#declaring-block-outputs","title":"Declaring block outputs","text":"<p>Our manifest is ready regarding properties that can be declared in Workflow definitions,  but we still need to provide additional information for the Execution Engine to successfully  run the block.</p> Declaring block outputs <p>Minimal set of information required is outputs description. Additionally,  to increase block stability, we advise to provide information about execution engine  compatibility.</p> <pre><code>from typing import Literal, Union, List, Optional\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepOutputImageSelector,\n    WorkflowImageSelector,\n    FloatZeroToOne,\n    WorkflowParameterSelector,\n    FLOAT_ZERO_TO_ONE_KIND,\n    BOOLEAN_KIND,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    image_1: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        FloatZeroToOne,\n        WorkflowParameterSelector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"images_match\", \n            kind=[BOOLEAN_KIND],\n          )\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n</code></pre> <ul> <li> <p>line <code>1</code> contains additional imports from <code>typing</code></p> </li> <li> <p>line <code>5</code> imports class that is used to describe step outputs</p> </li> <li> <p>line <code>13</code> imports <code>boolean</code> <code>kind</code> to be used  in outputs definitions</p> </li> <li> <p>lines <code>33-40</code> declare class method to specify outputs from the block -  each entry in list declare one return property for each batch element and its <code>kind</code>. Our block will return boolean flag <code>images_match</code> for each pair of images.</p> </li> <li> <p>lines <code>42-44</code> declare compatibility of the block with Execution Engine - see versioning page for more details</p> </li> </ul> <p>As a result of those changes:</p> <ul> <li> <p>Execution Engine would understand that steps created based on this block  are supposed to deliver specified outputs and other steps can refer to those outputs in their inputs</p> </li> <li> <p>the blocks loading mechanism will not load the block given that Execution Engine is not in version <code>v1</code></p> </li> </ul> LEARN MORE: Dynamic outputs <p>Some blocks may not be able to arbitrailry define their outputs using  classmethod - regardless of the content of step manifest that is available after  parsing. To support this we introduced the following convention:</p> <ul> <li> <p>classmethod <code>describe_outputs(...)</code> shall return list with one element of  name <code>*</code> and kind <code>*</code> (aka <code>WILDCARD_KIND</code>)</p> </li> <li> <p>additionally, block manifest should implement instance method <code>get_actual_outputs(...)</code> that provides list of actual outputs that can be generated based on filled manifest data </p> </li> </ul> <pre><code>from typing import Literal, Union, List, Optional\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepOutputImageSelector,\n    WorkflowImageSelector,\n    FloatZeroToOne,\n    WorkflowParameterSelector,\n    FLOAT_ZERO_TO_ONE_KIND,\n    BOOLEAN_KIND,\n    WILDCARD_KIND,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    image_1: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        FloatZeroToOne,\n        WorkflowParameterSelector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n    outputs: List[str]\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"*\", \n            kind=[WILDCARD_KIND],\n          ),\n        ]\n\n    def get_actual_outputs(self) -&gt; List[OutputDefinition]:\n        # here you have access to `self`:\n        return [\n          OutputDefinition(name=e, kind=[BOOLEAN_KIND])\n          for e in self.outputs\n        ]\n</code></pre>"},{"location":"workflows/create_workflow_block/#definition-of-block-class","title":"Definition of block class","text":"<p>At this stage, the manifest of our simple block is ready, we will continue  with our example. You can check out the advanced topics section for more details that would just  be a distractions now.</p>"},{"location":"workflows/create_workflow_block/#base-implementation","title":"Base implementation","text":"<p>Having the manifest ready, we can prepare baseline implementation of the  block.</p> Block scaffolding <pre><code>from typing import Literal, Union, List, Optional, Type\nfrom pydantic import Field\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    WorkflowBlock,\n    BlockResult,\n)\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepOutputImageSelector,\n    WorkflowImageSelector,\n    FloatZeroToOne,\n    WorkflowParameterSelector,\n    FLOAT_ZERO_TO_ONE_KIND,\n    BOOLEAN_KIND,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    image_1: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        FloatZeroToOne,\n        WorkflowParameterSelector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"images_match\", \n            kind=[BOOLEAN_KIND],\n          ),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n\nclass ImagesSimilarityBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return ImagesSimilarityManifest\n\n    def run(\n        self,\n        image_1: WorkflowImageData,\n        image_2: WorkflowImageData,\n        similarity_threshold: float,\n    ) -&gt; BlockResult:\n        pass\n</code></pre> <ul> <li> <p>lines <code>1</code>, <code>5-6</code> and <code>8-9</code> added changes into import surtucture to  provide additional symbols required to properly define block class and all of its methods signatures</p> </li> <li> <p>line <code>59</code> defines class method <code>get_manifest(...)</code> to simply return  the manifest class we cretaed earlier</p> </li> <li> <p>lines <code>62-68</code> define <code>run(...)</code> function, which Execution Engine will invoke with data to get desired results</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#providing-implementation-for-block-logic","title":"Providing implementation for block logic","text":"<p>Let's now add an example implementation of  the <code>run(...)</code> method to our block, such that it can produce meaningful results.</p> <p>Note</p> <p>The Content of this section is supposed to provide examples on how to interact  with the Workflow ecosystem as block creator, rather than providing robust  implementation of the block.</p> Implementation of <code>run(...)</code> method <pre><code>from typing import Literal, Union, List, Optional, Type\nfrom pydantic import Field\nimport cv2\n\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    WorkflowBlock,\n    BlockResult,\n)\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepOutputImageSelector,\n    WorkflowImageSelector,\n    FloatZeroToOne,\n    WorkflowParameterSelector,\n    FLOAT_ZERO_TO_ONE_KIND,\n    BOOLEAN_KIND,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    image_1: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        FloatZeroToOne,\n        WorkflowParameterSelector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"images_match\", \n            kind=[BOOLEAN_KIND],\n          ),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n\nclass ImagesSimilarityBlock(WorkflowBlock):\n\n    def __init__(self):\n        self._sift = cv2.SIFT_create()\n        self._matcher = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return ImagesSimilarityManifest\n\n    def run(\n        self,\n        image_1: WorkflowImageData,\n        image_2: WorkflowImageData,\n        similarity_threshold: float,\n    ) -&gt; BlockResult:\n        image_1_gray = cv2.cvtColor(image_1.numpy_image, cv2.COLOR_BGR2GRAY)\n        image_2_gray = cv2.cvtColor(image_2.numpy_image, cv2.COLOR_BGR2GRAY)\n        kp_1, des_1 = self._sift.detectAndCompute(image_1_gray, None)\n        kp_2, des_2 = self._sift.detectAndCompute(image_2_gray, None)\n        matches = self._matcher.knnMatch(des_1, des_2, k=2)\n        good_matches = []\n        for m, n in matches:\n            if m.distance &lt; similarity_threshold * n.distance:\n                good_matches.append(m)\n        return {\n            \"images_match\": len(good_matches) &gt; 0,\n        }\n</code></pre> <ul> <li> <p>in line <code>3</code> we import OpenCV</p> </li> <li> <p>lines <code>56-58</code> defines block constructor, thanks to this - state of block  is initialised once and live through consecutive invocation of <code>run(...)</code> method - for  instance when Execution Engine runs on consecutive frames of video</p> </li> <li> <p>lines <code>70-81</code> provide implementation of block functionality - the details are trully not important regarding Workflows ecosystem, but there are few details you should focus:</p> <ul> <li> <p>lines <code>70</code> and <code>71</code> make use of <code>WorkflowImageData</code> abstraction, showcasing how  <code>numpy_image</code> property can be used to get <code>np.ndarray</code> from internal representation of images in Workflows. We advise to expole remaining properties of <code>WorkflowImageData</code> to discover more.</p> </li> <li> <p>result of workflow block execution, declared in lines <code>79-81</code> is in our case just a dictionary  with the keys being the names of outputs declared in manifest, in line <code>44</code>. Be sure to provide all declared outputs - otherwise Execution Engine will raise error.</p> </li> </ul> </li> </ul> <p>You may ask yourself how it is possible that implemented block accepts batch-oriented workflow input, but do not  operate on batches directly. This is due to the fact that the default block behaviour is to run one-by-one against all elements of input batches. We will show how to change that in advanced topics section.</p> <p>Note</p> <p>One important note: blocks, like all other classes, have constructors that may initialize a state. This state can  persist across multiple Workflow runs when using the same instance of the Execution Engine. If the state management  needs to be aware of which batch element it processes (e.g., in object tracking scenarios), the block creator  should use dedicated batch-oriented inputs. These inputs, provide relevant metadatadata \u2014 like the  <code>WorkflowVideoMetadata</code> input, which is crucial for tracking use cases and can be used along with <code>WorkflowImage</code>  input in a block implementing tracker.</p> <p>The ecosystem is evolving, and new input types will be introduced over time. If a specific input type needed for  a use case is not available, an alternative is to design the block to process entire input batches. This way,  you can rely on the Batch container's indices property, which provides an index for each batch element, allowing  you to maintain the correct order of processing.</p>"},{"location":"workflows/create_workflow_block/#exposing-block-in-plugin","title":"Exposing block in <code>plugin</code>","text":"<p>Now, your block is ready to be used, but if you declared step using it in your Workflow definition you  would see an error. This is because no plugin exports the block you just created. Details of blocks bundling  will be covered in separate page, but the remaining thing to do is to  add block class into list returned from your plugins' <code>load_blocks(...)</code> function:</p> <pre><code># __init__.py of your plugin\n\nfrom my_plugin.images_similarity.v1 import  ImagesSimilarityBlock  \n# this is example import! requires adjustment\n\ndef load_blocks():\n    return [ImagesSimilarityBlock]\n</code></pre>"},{"location":"workflows/create_workflow_block/#advanced-topics","title":"Advanced topics","text":""},{"location":"workflows/create_workflow_block/#blocks-processing-batches-of-inputs","title":"Blocks processing batches of inputs","text":"<p>Sometimes, performance of your block may benefit if all input data is processed at once as batch. This may happen for models running on GPU. Such mode of operation is supported for Workflows blocks - here is the example on how to use it for your block.</p> Implementation of blocks accepting batches <pre><code>from typing import Literal, Union, List, Optional, Type\nfrom pydantic import Field\nimport cv2\n\nfrom inference.core.workflows.prototypes.block import (\n    WorkflowBlockManifest,\n    WorkflowBlock,\n    BlockResult,\n)\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    WorkflowImageData,\n    Batch,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepOutputImageSelector,\n    WorkflowImageSelector,\n    FloatZeroToOne,\n    WorkflowParameterSelector,\n    FLOAT_ZERO_TO_ONE_KIND,\n    BOOLEAN_KIND,\n)\n\nclass ImagesSimilarityManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/images_similarity@v1\"] \n    name: str\n    image_1: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"First image to calculate similarity\",\n    )\n    image_2: Union[WorkflowImageSelector, StepOutputImageSelector] = Field(\n        description=\"Second image to calculate similarity\",\n    )\n    similarity_threshold: Union[\n        FloatZeroToOne,\n        WorkflowParameterSelector(kind=[FLOAT_ZERO_TO_ONE_KIND]),\n    ] = Field(\n        default=0.4,\n        description=\"Threshold to assume that images are similar\",\n    )\n\n    @classmethod\n    def accepts_batch_input(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"images_match\", \n            kind=[BOOLEAN_KIND],\n          ),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n\nclass ImagesSimilarityBlock(WorkflowBlock):\n\n    def __init__(self):\n        self._sift = cv2.SIFT_create()\n        self._matcher = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return ImagesSimilarityManifest\n\n    def run(\n        self,\n        image_1: Batch[WorkflowImageData],\n        image_2: Batch[WorkflowImageData],\n        similarity_threshold: float,\n    ) -&gt; BlockResult:\n        results = []\n        for image_1_element, image_2_element in zip(image_1, image_2): \n          image_1_gray = cv2.cvtColor(image_1_element.numpy_image, cv2.COLOR_BGR2GRAY)\n          image_2_gray = cv2.cvtColor(image_2_element.numpy_image, cv2.COLOR_BGR2GRAY)\n          kp_1, des_1 = self._sift.detectAndCompute(image_1_gray, None)\n          kp_2, des_2 = self._sift.detectAndCompute(image_2_gray, None)\n          matches = self._matcher.knnMatch(des_1, des_2, k=2)\n          good_matches = []\n          for m, n in matches:\n              if m.distance &lt; similarity_threshold * n.distance:\n                  good_matches.append(m)\n          results.append({\"images_match\": len(good_matches) &gt; 0})\n        return results\n</code></pre> <ul> <li> <p>line <code>13</code> imports <code>Batch</code> from core of workflows library - this class represent container which is  veri similar to list (but read-only) to keep batch elements</p> </li> <li> <p>lines <code>41-43</code> define class method that changes default behaviour of the block and make it capable  to process batches</p> </li> <li> <p>changes introduced above made the signature of <code>run(...)</code> method to change, now <code>image_1</code> and <code>image_2</code> are not instances of <code>WorkflowImageData</code>, but rather batches of elements of this type</p> </li> <li> <p>lines <code>75-78</code>, <code>86-87</code> present changes that needed to be introduced to run processing across all batch  elements - showcasing how to iterate over batch elements if needed</p> </li> <li> <p>it is important to note how outputs are constructed in line <code>86</code> - each element of batch will be given its entry in the list which is returned from <code>run(...)</code> method. Order must be aligned with order of batch  elements. Each output dictionary must provide all keys declared in block outputs.</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#implementation-of-flow-control-block","title":"Implementation of flow-control block","text":"<p>Flow-control blocks differs quite substantially from other blocks that just process the data. Here we will show  how to create a flow control block, but first - a little bit of theory:</p> <ul> <li> <p>flow-control block is the block that declares compatibility with step selectors in their manifest (selector to step is defined as <code>$steps.{step_name}</code> - similar to step output selector, but without specification of output name)</p> </li> <li> <p>flow-control blocks cannot register outputs, they are meant to return <code>FlowControl</code> objects</p> </li> <li> <p><code>FlowControl</code> object specify next steps (from selectors provided in step manifest) that for given  batch element (SIMD flow-control) or whole workflow execution (non-SIMD flow-control) should pick up next</p> </li> </ul> Implementation of flow-control - SIMD block <p>Example provides and comments out implementation of random continue block</p> <pre><code>from typing import List, Literal, Optional, Type, Union\nimport random\n\nfrom pydantic import Field\nfrom inference.core.workflows.execution_engine.entities.base import (\n  OutputDefinition,\n  WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepSelector,\n    WorkflowImageSelector,\n    StepOutputImageSelector,\n)\nfrom inference.core.workflows.execution_engine.v1.entities import FlowControl\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/random_continue@v1\"]\n    name: str\n    image: Union[WorkflowImageSelector, StepOutputImageSelector] = ImageInputField\n    probability: float\n    next_steps: List[StepSelector] = Field(\n        description=\"Reference to step which shall be executed if expression evaluates to true\",\n        examples=[[\"$steps.on_true\"]],\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return []\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n\nclass RandomContinueBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        image: WorkflowImageData,\n        probability: float,\n        next_steps: List[str],\n    ) -&gt; BlockResult:\n        if not next_steps or random.random() &gt; probability:\n            return FlowControl()\n        return FlowControl(context=next_steps)\n</code></pre> <ul> <li> <p>line <code>10</code> imports type annotation for step selector which will be used to  notify Execution Engine that the block controls the flow</p> </li> <li> <p>line <code>14</code> imports <code>FlowControl</code> class which is the only viable response from flow-control block</p> </li> <li> <p>line <code>26</code> specifies <code>image</code> which is batch-oriented input making the block SIMD -  which means that for each element of images batch, block will make random choice on  flow-control - if not that input block would operate in non-SIMD mode</p> </li> <li> <p>line <code>28</code> defines list of step selectors which effectively turns the block into flow-control one</p> </li> <li> <p>lines <code>55</code> and <code>56</code> show how to construct output - <code>FlowControl</code> object accept context being <code>None</code>, <code>string</code> or  <code>list of strings</code> - <code>None</code> represent flow termination for the batch element, strings are expected to be selectors  for next steps, passed in input.</p> </li> </ul> Implementation of flow-control non-SIMD block <p>Example provides and comments out implementation of random continue block</p> <pre><code>from typing import List, Literal, Optional, Type, Union\nimport random\n\nfrom pydantic import Field\nfrom inference.core.workflows.execution_engine.entities.base import (\n  OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepSelector,\n)\nfrom inference.core.workflows.execution_engine.v1.entities import FlowControl\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/random_continue@v1\"]\n    name: str\n    probability: float\n    next_steps: List[StepSelector] = Field(\n        description=\"Reference to step which shall be executed if expression evaluates to true\",\n        examples=[[\"$steps.on_true\"]],\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return []\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n\nclass RandomContinueBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        probability: float,\n        next_steps: List[str],\n    ) -&gt; BlockResult:\n        if not next_steps or random.random() &gt; probability:\n            return FlowControl()\n        return FlowControl(context=next_steps)\n</code></pre> <ul> <li> <p>line <code>9</code> imports type annotation for step selector which will be used to  notify Execution Engine that the block controls the flow</p> </li> <li> <p>line <code>11</code> imports <code>FlowControl</code> class which is the only viable response from flow-control block</p> </li> <li> <p>lines <code>24-27</code> defines list of step selectors which effectively turns the block into flow-control one</p> </li> <li> <p>lines <code>50</code> and <code>51</code> show how to construct output - <code>FlowControl</code> object accept context being <code>None</code>, <code>string</code> or  <code>list of strings</code> - <code>None</code> represent flow termination for the batch element, strings are expected to be selectors  for next steps, passed in input.</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#nested-selectors","title":"Nested selectors","text":"<p>Some block will require list of selectors or dictionary of selectors to be  provided in block manifest field. Version <code>v1</code> of Execution Engine supports only  one level of nesting - so list of lists of selectors or dictionary with list of selectors  will not be recognised properly.</p> <p>Practical use cases showcasing usage of nested selectors are presented below.</p>"},{"location":"workflows/create_workflow_block/#fusion-of-predictions-from-variable-number-of-models","title":"Fusion of predictions from variable number of models","text":"<p>Let's assume that you want to build a block to get majority vote on multiple classifiers predictions - then you would  like your run method to look like that:</p> <pre><code># pseud-code here\ndef run(self, predictions: List[dict]) -&gt; BlockResult:\n    predicted_classes = [p[\"class\"] for p in predictions]\n    counts = Counter(predicted_classes)\n    return {\"top_class\": counts.most_common(1)[0]}\n</code></pre> Nested selectors - models ensemble <pre><code>from typing import List, Literal, Optional, Type\n\nfrom pydantic import Field\nimport supervision as sv\nfrom inference.core.workflows.execution_engine.entities.base import (\n  OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepOutputSelector,\n    OBJECT_DETECTION_PREDICTION_KIND,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/fusion_of_predictions@v1\"]\n    name: str\n    predictions: List[StepOutputSelector(kind=[OBJECT_DETECTION_PREDICTION_KIND])] = Field(\n        description=\"Selectors to step outputs\",\n        examples=[[\"$steps.model_1.predictions\", \"$steps.model_2.predictions\"]],\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(\n            name=\"predictions\", \n            kind=[OBJECT_DETECTION_PREDICTION_KIND],\n          )\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n\nclass FusionBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        predictions: List[sv.Detections],\n    ) -&gt; BlockResult:\n        merged = sv.Detections.merge(predictions)\n        return {\"predictions\": merged}\n</code></pre> <ul> <li> <p>lines <code>23-26</code> depict how to define manifest field capable of accepting  list of selectors</p> </li> <li> <p>line <code>50</code> shows what to expect as input to block's <code>run(...)</code> method -  list of objects which are representation of specific kind. If the block accepted  batches, the input type of <code>predictions</code> field would be <code>List[Batch[sv.Detections]</code></p> </li> </ul> <p>Such block is compatible with the following step declaration:</p> <pre><code>{\n  \"type\": \"my_plugin/fusion_of_predictions@v1\",\n  \"name\": \"my_step\",\n  \"predictions\": [\n    \"$steps.model_1.predictions\",\n    \"$steps.model_2.predictions\"  \n  ]\n}\n</code></pre>"},{"location":"workflows/create_workflow_block/#block-with-data-transformations-allowing-dynamic-parameters","title":"Block with data transformations allowing dynamic parameters","text":"<p>Occasionally, blocks may need to accept group of \"named\" selectors,  which names and values are to be defined by creator of Workflow definition.  In such cases, block manifest shall accept dictionary of selectors, where keys serve as names for those selectors.</p> Nested selectors - named selectors <pre><code>from typing import List, Literal, Optional, Type, Any\n\nfrom pydantic import Field\nimport supervision as sv\nfrom inference.core.workflows.execution_engine.entities.base import (\n  OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    StepOutputSelector,\n    WorkflowParameterSelector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/named_selectors_example@v1\"]\n    name: str\n    data: Dict[str, StepOutputSelector(), WorkflowParameterSelector()] = Field(\n        description=\"Selectors to step outputs\",\n        examples=[{\"a\": $steps.model_1.predictions\", \"b\": \"$Inputs.data\"}],\n    )\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n          OutputDefinition(name=\"my_output\", kind=[]),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n\nclass BlockWithNamedSelectorsV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        data: Dict[str, Any],\n    ) -&gt; BlockResult:\n        ...\n        return {\"my_output\": ...}\n</code></pre> <ul> <li> <p>lines <code>23-26</code> depict how to define manifest field capable of accepting  list of selectors</p> </li> <li> <p>line <code>47</code> shows what to expect as input to block's <code>run(...)</code> method -  dict of objects which are reffered with selectors. If the block accepted  batches, the input type of <code>data</code> field would be <code>Dict[str, Union[Batch[Any], Any]]</code>. In non-batch cases, non-batch-oriented data referenced by selector is automatically  broadcasted, whereas for blocks accepting batches - <code>Batch</code> container wraps only  batch-oriented inputs, with other inputs being passed as singular values.</p> </li> </ul> <p>Such block is compatible with the following step declaration:</p> <pre><code>{\n  \"type\": \"my_plugin/named_selectors_example@v1\",\n  \"name\": \"my_step\",\n  \"data\": {\n    \"a\": \"$steps.model_1.predictions\",\n    \"b\": \"$inputs.my_parameter\"  \n  }\n}\n</code></pre> <p>Practical implications will be the following:</p> <ul> <li> <p>under <code>data[\"a\"]</code> inside <code>run(...)</code> you will be able to find model's predictions -  like <code>sv.Detections</code> if <code>model_1</code> is object-detection model</p> </li> <li> <p>under <code>data[\"b\"]</code> inside <code>run(...)</code>, you will find value of input parameter named <code>my_parameter</code></p> </li> </ul>"},{"location":"workflows/create_workflow_block/#inputs-and-output-dimensionality-vs-run-method","title":"Inputs and output dimensionality vs <code>run(...)</code> method","text":"<p>The dimensionality of block inputs plays a crucial role in shaping the <code>run(...)</code> method\u2019s signature, and that's  why the system enforces strict bounds on the differences in dimensionality levels between inputs  (with the maximum allowed difference being <code>1</code>). This restriction is critical for ensuring consistency and  predictability when writing blocks.</p> <p>If dimensionality differences weren't controlled, it would be difficult to predict the structure of  the <code>run(...)</code> method, making development harder and less reliable. That\u2019s why validation of this property  is strictly enforced during the Workflow compilation process.</p> <p>Similarly, the output dimensionality also affects the method signature and the format of the expected output.  The ecosystem supports the following scenarios:</p> <ul> <li> <p>all inputs have the same dimensionality and outputs does not change dimensionality - baseline case</p> </li> <li> <p>all inputs have the same dimensionality and output decreases dimensionality</p> </li> <li> <p>all inputs have the same dimensionality and output increases dimensionality</p> </li> <li> <p>inputs have different dimensionality and output is allowed to keep the dimensionality of  reference input</p> </li> </ul> <p>Other combinations of input/output dimensionalities are not allowed to ensure consistency and to prevent ambiguity in  the method signatures.</p> Impact of dimensionality on <code>run(...)</code> method - batches disabled output dimensionality increaseoutput dimensionality decreasedifferent input dimensionalities <p>In this example, we perform dynamic crop of image based on predictions.</p> <pre><code>from typing import Dict, List, Literal, Optional, Type, Union\nfrom uuid import uuid4\n\nfrom inference.core.workflows.execution_engine.constants import DETECTION_ID_KEY\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    WorkflowImageData,\n    ImageParentMetadata,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    IMAGE_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    StepOutputImageSelector,\n    StepOutputSelector,\n    WorkflowImageSelector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_block/dynamic_crop@v1\"]\n    image: Union[WorkflowImageSelector, StepOutputImageSelector]\n    predictions: StepOutputSelector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND],\n    )\n\n    @classmethod\n    def get_output_dimensionality_offset(cls) -&gt; int:\n        return 1\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(name=\"crops\", kind=[IMAGE_KIND]),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\nclass DynamicCropBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        image: WorkflowImageData,\n        predictions: sv.Detections,\n    ) -&gt; BlockResult:\n        crops = []\n        for (x_min, y_min, x_max, y_max) in predictions.xyxy.round().astype(dtype=int):\n            cropped_image = image.numpy_image[y_min:y_max, x_min:x_max]\n            parent_metadata = ImageParentMetadata(parent_id=f\"{uuid4()}\")\n            if cropped_image.size:\n                result = WorkflowImageData(\n                    parent_metadata=parent_metadata,\n                    numpy_image=cropped_image,\n                )\n            else:\n                result = None\n            crops.append({\"crops\": result})\n        return crops\n</code></pre> <ul> <li> <p>in lines <code>30-32</code> manifest class declares output dimensionality  offset - value <code>1</code> should be understood as adding <code>1</code> to dimensionality level</p> </li> <li> <p>point out, that in line <code>65</code>, block eliminates empty images from further processing but  placing <code>None</code> instead of dictionatry with outputs. This would utilise the same  Execution Engine behaviour that is used for conditional execution - datapoint will be eliminated from downstream processing (unless steps requesting empty inputs  are present down the line).</p> </li> <li> <p>in lines <code>66-67</code> results for single input <code>image</code> and <code>predictions</code> are collected -  it is meant to be list of dictionares containing all registered outputs as keys. Execution engine will understand that the step returns batch of elements for each input element and create nested sturcures of indices to keep track of during execution of downstream steps.</p> </li> </ul> <p>In this example, the block visualises crops predictions and creates tiles presenting all crops predictions in single output image.</p> <pre><code>from typing import List, Literal, Type, Union\n\nimport supervision as sv\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    IMAGE_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    StepOutputImageSelector,\n    StepOutputSelector,\n    WorkflowImageSelector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/tile_detections@v1\"]\n    crops: Union[WorkflowImageSelector, StepOutputImageSelector]\n    crops_predictions: StepOutputSelector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND]\n    )\n\n    @classmethod\n    def get_output_dimensionality_offset(cls) -&gt; int:\n        return -1\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(name=\"visualisations\", kind=[IMAGE_KIND]),\n        ]\n\n\nclass TileDetectionsBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        crops: Batch[WorkflowImageData],\n        crops_predictions: Batch[sv.Detections],\n    ) -&gt; BlockResult:\n        annotator = sv.BoundingBoxAnnotator()\n        visualisations = []\n        for image, prediction in zip(crops, crops_predictions):\n            annotated_image = annotator.annotate(\n                image.numpy_image.copy(),\n                prediction,\n            )\n            visualisations.append(annotated_image)\n        tile = sv.create_tiles(visualisations)\n        return {\"visualisations\": tile}\n</code></pre> <ul> <li> <p>in lines <code>31-33</code> manifest class declares output dimensionality  offset - value <code>-1</code> should be understood as decreasing dimensionality level by <code>1</code></p> </li> <li> <p>in lines <code>50-51</code> you can see the impact of output dimensionality decrease on the method signature. Both inputs are artificially wrapped in <code>Batch[]</code> container. This is done by Execution Engine automatically on output dimensionality decrease when  all inputs have the same dimensionality to enable access to all elements occupying  the last dimensionality level. Obviously, only elements related to the same element  from top-level batch will be grouped. For instance, if you had two input images that you  cropped - crops from those two different images will be grouped separately.</p> </li> <li> <p>lines <code>61-62</code> illustrate how output is constructed - single value is returned and that value  will be indexed by Execution Engine in output batch with reduced dimensionality</p> </li> </ul> <p>In this example, block merges detections which were predicted based on  crops of original image - result is to provide single detections with  all partial ones being merged.</p> <pre><code>from copy import deepcopy\nfrom typing import Dict, List, Literal, Optional, Type, Union\n\nimport numpy as np\nimport supervision as sv\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    OBJECT_DETECTION_PREDICTION_KIND,\n    StepOutputImageSelector,\n    StepOutputSelector,\n    WorkflowImageSelector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/stitch@v1\"]\n    image: Union[WorkflowImageSelector, StepOutputImageSelector]\n    image_predictions: StepOutputSelector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND],\n    )\n\n    @classmethod\n    def get_input_dimensionality_offsets(cls) -&gt; Dict[str, int]:\n        return {\n            \"image\": 0,\n            \"image_predictions\": 1,\n        }\n\n    @classmethod\n    def get_dimensionality_reference_property(cls) -&gt; Optional[str]:\n        return \"image\"\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(\n                name=\"predictions\",\n                kind=[\n                    OBJECT_DETECTION_PREDICTION_KIND,\n                ],\n            ),\n        ]\n\n\nclass StitchDetectionsNonBatchBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        image: WorkflowImageData,\n        image_predictions: Batch[sv.Detections],\n    ) -&gt; BlockResult:\n        image_predictions = [deepcopy(p) for p in image_predictions if len(p)]\n        for p in image_predictions:\n            coords = p[\"parent_coordinates\"][0]\n            p.xyxy += np.concatenate((coords, coords))\n        return {\"predictions\": sv.Detections.merge(image_predictions)}\n</code></pre> <ul> <li> <p>in lines <code>32-37</code> manifest class declares input dimensionalities offset, indicating <code>image</code> parameter being top-level and <code>image_predictions</code> being nested batch of predictions</p> </li> <li> <p>whenever different input dimensionalities are declared, dimensionality reference property must be pointed (see lines <code>39-41</code>) - this dimensionality level would be used to calculate  output dimensionality - in this particular case, we specify <code>image</code>. This choice  has an implication in the expected format of result - in the chosen scenario we are supposed to return single dictionary with all registered outputs keys. If our choice is <code>image_predictions</code>, we would return list of dictionaries (of size equal to length of <code>image_predictions</code> batch). In other worlds, <code>get_dimensionality_reference_property(...)</code> which dimensionality level should be associated to the output.</p> </li> <li> <p>lines <code>63-64</code> present impact of dimensionality offsets specified in lines <code>32-37</code>. It is clearly visible that <code>image_predictions</code> is a nested batch regarding <code>image</code>. Obviously, only nested predictions relevant for the specific <code>images</code> are grouped in batch and provided to the method in runtime.</p> </li> <li> <p>as mentioned earlier, line <code>70</code> construct output being single dictionary, as we register output  at dimensionality level of <code>image</code> (which was also shipped as single element)</p> </li> </ul> Impact of dimensionality on <code>run(...)</code> method - batches enabled output dimensionality increaseoutput dimensionality decreasedifferent input dimensionalities <p>In this example, we perform dynamic crop of image based on predictions.</p> <pre><code>from typing import Dict, List, Literal, Optional, Type, Union\nfrom uuid import uuid4\n\nfrom inference.core.workflows.execution_engine.constants import DETECTION_ID_KEY\nfrom inference.core.workflows.execution_engine.entities.base import (\n    OutputDefinition,\n    WorkflowImageData,\n    ImageParentMetadata,\n    Batch,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    IMAGE_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    StepOutputImageSelector,\n    StepOutputSelector,\n    WorkflowImageSelector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_block/dynamic_crop@v1\"]\n    image: Union[WorkflowImageSelector, StepOutputImageSelector]\n    predictions: StepOutputSelector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND],\n    )\n\n    @classmethod\n    def accepts_batch_input(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def get_output_dimensionality_offset(cls) -&gt; int:\n        return 1\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(name=\"crops\", kind=[IMAGE_KIND]),\n        ]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\nclass DynamicCropBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        image: Batch[WorkflowImageData],\n        predictions: Batch[sv.Detections],\n    ) -&gt; BlockResult:\n        results = []\n        for single_image, detections in zip(image, predictions):\n            crops = []\n            for (x_min, y_min, x_max, y_max) in detections.xyxy.round().astype(dtype=int):\n                cropped_image = single_image.numpy_image[y_min:y_max, x_min:x_max]\n                parent_metadata = ImageParentMetadata(parent_id=f\"{uuid4()}\")\n                if cropped_image.size:\n                    result = WorkflowImageData(\n                        parent_metadata=parent_metadata,\n                        numpy_image=cropped_image,\n                    )\n                else:\n                    result = None\n                crops.append({\"crops\": result})\n            results.append(crops)\n        return results\n</code></pre> <ul> <li> <p>in lines <code>31-33</code> manifest declares that block accepts batches of inputs</p> </li> <li> <p>in lines <code>35-37</code> manifest class declares output dimensionality  offset - value <code>1</code> should be understood as adding <code>1</code> to dimensionality level</p> </li> <li> <p>in lines <code>57-68</code>, signature of input parameters reflects that the <code>run(...)</code> method runs against inputs of the same dimensionality and those inputs are provided in batches</p> </li> <li> <p>point out, that in line <code>72</code>, block eliminates empty images from further processing but  placing <code>None</code> instead of dictionatry with outputs. This would utilise the same  Execution Engine behaviour that is used for conditional execution - datapoint will be eliminated from downstream processing (unless steps requesting empty inputs  are present down the line).</p> </li> <li> <p>construction of the output, presented in lines <code>73-75</code> indicates two levels of nesting. First of all, block operates on batches, so it is expected to return list of outputs, one  output for each input batch element. Additionally, this output element for each input batch  element turns out to be nested batch - hence for each input iage and prediction, block  generates list of outputs - elements of that list are dictionaries providing values  for each declared output.</p> </li> </ul> <p>In this example, the block visualises crops predictions and creates tiles presenting all crops predictions in single output image.</p> <pre><code>from typing import List, Literal, Type, Union\n\nimport supervision as sv\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    IMAGE_KIND,\n    OBJECT_DETECTION_PREDICTION_KIND,\n    StepOutputImageSelector,\n    StepOutputSelector,\n    WorkflowImageSelector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/tile_detections@v1\"]\n    images_crops: Union[WorkflowImageSelector, StepOutputImageSelector]\n    crops_predictions: StepOutputSelector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND]\n    )\n\n    @classmethod\n    def accepts_batch_input(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def get_output_dimensionality_offset(cls) -&gt; int:\n        return -1\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(name=\"visualisations\", kind=[IMAGE_KIND]),\n        ]\n\n\nclass TileDetectionsBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        images_crops: Batch[Batch[WorkflowImageData]],\n        crops_predictions: Batch[Batch[sv.Detections]],\n    ) -&gt; BlockResult:\n        annotator = sv.BoundingBoxAnnotator()\n        visualisations = []\n        for image_crops, crop_predictions in zip(images_crops, crops_predictions):\n            visualisations_batch_element = []\n            for image, prediction in zip(image_crops, crop_predictions):\n                annotated_image = annotator.annotate(\n                    image.numpy_image.copy(),\n                    prediction,\n                )\n                visualisations_batch_element.append(annotated_image)\n            tile = sv.create_tiles(visualisations_batch_element)\n            visualisations.append({\"visualisations\": tile})\n        return visualisations\n</code></pre> <ul> <li> <p>lines <code>31-33</code> manifest that block is expected to take batches as input</p> </li> <li> <p>in lines <code>35-37</code> manifest class declares output dimensionality  offset - value <code>-1</code> should be understood as decreasing dimensionality level by <code>1</code></p> </li> <li> <p>in lines <code>54-55</code> you can see the impact of output dimensionality decrease and batch processing on the method signature. First \"layer\" of <code>Batch[]</code> is a side effect of the  fact that manifest declared that block accepts batches of inputs. The second \"layer\" comes  from output dimensionality decrease. Execution Engine wrapps up the dimension to be reduced into  additional <code>Batch[]</code> container porvided in inputs, such that programmer is able to collect all nested batches elements that belong to specific top-level batch element.</p> </li> <li> <p>lines <code>68-69</code> illustrate how output is constructed - for each top-level batch element, block aggregates all crops and predictions and creates a single tile. As block accepts batches of inputs, this procedure end up with one tile for each top-level batch element - hence list of dictionaries is expected to be returned.</p> </li> </ul> <p>In this example, block merges detections which were predicted based on  crops of original image - result is to provide single detections with  all partial ones being merged.</p> <pre><code>from copy import deepcopy\nfrom typing import Dict, List, Literal, Optional, Type, Union\n\nimport numpy as np\nimport supervision as sv\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n    WorkflowImageData,\n)\nfrom inference.core.workflows.execution_engine.entities.types import (\n    OBJECT_DETECTION_PREDICTION_KIND,\n    StepOutputImageSelector,\n    StepOutputSelector,\n    WorkflowImageSelector,\n)\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/stitch@v1\"]\n    images: Union[WorkflowImageSelector, StepOutputImageSelector]\n    images_predictions: StepOutputSelector(\n        kind=[OBJECT_DETECTION_PREDICTION_KIND],\n    )\n\n    @classmethod\n    def accepts_batch_input(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def get_input_dimensionality_offsets(cls) -&gt; Dict[str, int]:\n        return {\n            \"image\": 0,\n            \"image_predictions\": 1,\n        }\n\n    @classmethod\n    def get_dimensionality_reference_property(cls) -&gt; Optional[str]:\n        return \"image\"\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [\n            OutputDefinition(\n                name=\"predictions\",\n                kind=[\n                    OBJECT_DETECTION_PREDICTION_KIND,\n                ],\n            ),\n        ]\n\n\nclass StitchDetectionsBatchBlock(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        images: Batch[WorkflowImageData],\n        images_predictions: Batch[Batch[sv.Detections]],\n    ) -&gt; BlockResult:\n        result = []\n        for image, image_predictions in zip(images, images_predictions):\n            image_predictions = [deepcopy(p) for p in image_predictions if len(p)]\n            for p in image_predictions:\n                coords = p[\"parent_coordinates\"][0]\n                p.xyxy += np.concatenate((coords, coords))\n            merged_prediction = sv.Detections.merge(image_predictions)\n            result.append({\"predictions\": merged_prediction})\n        return result\n</code></pre> <ul> <li> <p>lines <code>32-34</code> manifest that block is expected to take batches as input</p> </li> <li> <p>in lines <code>36-41</code> manifest class declares input dimensionalities offset, indicating <code>image</code> parameter being top-level and <code>image_predictions</code> being nested batch of predictions</p> </li> <li> <p>whenever different input dimensionalities are declared, dimensionality reference property must be pointed (see lines <code>43-45</code>) - this dimensionality level would be used to calculate  output dimensionality - in this particular case, we specify <code>image</code>. This choice  has an implication in the expected format of result - in the chosen scenario we are supposed to return single dictionary for each element of <code>image</code> batch. If our choice is <code>image_predictions</code>, we would return list of dictionaries (of size equal to length of nested <code>image_predictions</code> batch) for each input <code>image</code> batch element.</p> </li> <li> <p>lines <code>67-68</code> present impact of dimensionality offsets specified in lines <code>36-41</code> as well as  the declararion of batch processing from lines <code>32-34</code>. First \"layer\" of <code>Batch[]</code> container comes  from the latter, nested <code>Batch[Batch[]]</code> for <code>images_predictions</code> comes from the definition of input  dimensionality offset. It is clearly visible that <code>image_predictions</code> holds batch of predictions relevant for specific elements of <code>image</code> batch.</p> </li> <li> <p>as mentioned earlier, lines <code>77-78</code> construct output being single dictionary for each element of <code>image</code>  batch</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#block-accepting-empty-inputs","title":"Block accepting empty inputs","text":"<p>As discussed earlier, some batch elements may become \"empty\" during the execution of a Workflow.  This can happen due to several factors:</p> <ul> <li> <p>Flow-control mechanisms: Certain branches of execution can mask specific batch elements, preventing them  from being processed in subsequent steps.</p> </li> <li> <p>In data-processing blocks: In some cases, a block may not be able to produce a meaningful output for  a specific data point. For example, a Dynamic Crop block cannot generate a cropped image if the bounding box  size is zero.</p> </li> </ul> <p>Some blocks are designed to handle these empty inputs, such as block that can replace missing outputs with default  values. This block can be particularly useful when constructing structured outputs in a Workflow, ensuring  that even if some elements are empty, the output lacks missing elements making it harder to parse.</p> Block accepting empty inputs <pre><code>from typing import Any, List, Literal, Optional, Type\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import StepOutputSelector\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/first_non_empty_or_default@v1\"]\n    data: List[StepOutputSelector()]\n    default: Any\n\n    @classmethod\n    def accepts_empty_values(cls) -&gt; bool:\n        return True\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [OutputDefinition(name=\"output\")]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n\nclass FirstNonEmptyOrDefaultBlockV1(WorkflowBlock):\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        data: Batch[Optional[Any]],\n        default: Any,\n    ) -&gt; BlockResult:\n        result = default\n        for data_element in data:\n            if data_element is not None:\n                return {\"output\": data_element}\n        return {\"output\": result}\n</code></pre> <ul> <li> <p>in lines <code>20-22</code> you may find declaration stating that block acccepts empt inputs </p> </li> <li> <p>a consequence of lines <code>20-22</code> is visible in line <code>41</code>, when signature states that  input <code>Batch</code> may contain empty elements that needs to be handled. In fact - the block  generates \"artificial\" output substituting empty value, which makes it possible for  those outputs to be \"visible\" for blocks not accepting empty inputs that refer to the  output of this block. You should assume that each input that is substituted by Execution Engine with data generated in runtime may provide optional elements.</p> </li> </ul>"},{"location":"workflows/create_workflow_block/#block-with-custom-constructor-parameters","title":"Block with custom constructor parameters","text":"<p>Some blocks may require objects constructed by outside world to work. In such scenario, Workflows Execution Engine job is to transfer those entities to the block,  making it possible to be used. The mechanism is described in  the page presenting Workflows Compiler, as this is the  component responsible for dynamic construction of steps from blocks classes.</p> <p>Constructor parameters must be:</p> <ul> <li> <p>requested by block - using class method <code>WorkflowBlock.get_init_parameters(...)</code></p> </li> <li> <p>provided in the environment running Workflows Execution Engine:</p> <ul> <li> <p>directly, as shown in this example</p> </li> <li> <p>using defaults registered for Workflow plugin</p> </li> </ul> </li> </ul> <p>Let's see how to request init parameters while defining block.</p> Block requesting constructor parameters <pre><code>from typing import Any, List, Literal, Optional, Type\n\nfrom inference.core.workflows.execution_engine.entities.base import (\n    Batch,\n    OutputDefinition,\n)\nfrom inference.core.workflows.execution_engine.entities.types import StepOutputSelector\nfrom inference.core.workflows.prototypes.block import (\n    BlockResult,\n    WorkflowBlock,\n    WorkflowBlockManifest,\n)\n\n\nclass BlockManifest(WorkflowBlockManifest):\n    type: Literal[\"my_plugin/example@v1\"]\n    data: List[StepOutputSelector()]\n\n    @classmethod\n    def describe_outputs(cls) -&gt; List[OutputDefinition]:\n        return [OutputDefinition(name=\"output\")]\n\n    @classmethod\n    def get_execution_engine_compatibility(cls) -&gt; Optional[str]:\n        return \"&gt;=1.0.0,&lt;2.0.0\"\n\n\nclass ExampleBlock(WorkflowBlock):\n\n    def __init__(my_parameter: int):\n        self._my_parameter = my_parameter\n\n    @classmethod\n    def get_init_parameters(cls) -&gt; List[str]:\n        return [\"my_parameter\"]\n\n    @classmethod\n    def get_manifest(cls) -&gt; Type[WorkflowBlockManifest]:\n        return BlockManifest\n\n    def run(\n        self,\n        data: Batch[Any],\n    ) -&gt; BlockResult:\n        pass\n</code></pre> <ul> <li> <p>lines <code>30-31</code> declare class constructor which is not parameter-free</p> </li> <li> <p>to inform Execution Engine that block requires custom initialisation,  <code>get_init_parameters(...)</code> method in lines <code>33-35</code> enlists names of all  parameters that must be provided</p> </li> </ul>"},{"location":"workflows/custom_python_code_blocks/","title":"Dynamic Python blocks","text":"<p>When the syntax for Workflow definitions was outlined, one key  aspect was not covered: the ability to define blocks directly within the Workflow definition itself. This section can include the manifest and Python code for blocks defined in-place, which are dynamically interpreted by the  Execution Engine. These in-place blocks function similarly to those statically defined in  plugins, yet provide much more flexibility.</p> <p>Warning</p> <p>Dynamic blocks only work in your local deployment of <code>inference</code> and are not supported  on the Roboflow hosted platform.</p> <p>If you wish to disable the functionality, <code>export ALLOW_CUSTOM_PYTHON_EXECUTION_IN_WORKFLOWS=False</code></p>"},{"location":"workflows/custom_python_code_blocks/#theory","title":"Theory","text":"<p>The high-level overview of Dynamic Python blocks functionality:</p> <ul> <li> <p>user provides definition of dynamic block in JSON</p> </li> <li> <p>definition contains information required by Execution Engine to construct <code>WorkflowBlockManifest</code> and <code>WorkflowBlock</code> out of the  document</p> </li> <li> <p>in runtime, Compiler turns definition into dynamically created  Python classes - exactly the same as statically defined blocks</p> </li> <li> <p>In Workflow definition, you may declare steps that use dynamic blocks,  as if dynamic blocks were standard static ones</p> </li> </ul>"},{"location":"workflows/custom_python_code_blocks/#example","title":"Example","text":"<p>Let's take a look and discuss example workflow with dynamic Python blocks.</p> Workflow with dynamic block <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"dynamic_blocks_definitions\": [\n        {\n            \"type\": \"DynamicBlockDefinition\",\n            \"manifest\": {\n                \"type\": \"ManifestDescription\",\n                \"block_type\": \"OverlapMeasurement\",\n                \"inputs\": {\n                    \"predictions\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"selector_types\": [\n                            \"step_output\"\n                        ]\n                    },\n                    \"class_x\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"value_types\": [\n                            \"string\"\n                        ]\n                    },\n                    \"class_y\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"value_types\": [\n                            \"string\"\n                        ]\n                    }\n                },\n                \"outputs\": {\n                    \"overlap\": {\n                        \"type\": \"DynamicOutputDefinition\",\n                        \"kind\": []\n                    }\n                }\n            },\n            \"code\": {\n                \"type\": \"PythonCode\",\n                \"run_function_code\": \"\\ndef run(self, predictions: sv.Detections, class_x: str, class_y: str) -&gt; BlockResult:\\n    bboxes_class_x = predictions[predictions.data[\\\"class_name\\\"] == class_x]\\n    bboxes_class_y = predictions[predictions.data[\\\"class_name\\\"] == class_y]\\n    overlap = []\\n    for bbox_x in bboxes_class_x:\\n        bbox_x_coords = bbox_x[0]\\n        bbox_overlaps = []\\n        for bbox_y in bboxes_class_y:\\n            if bbox_y[-1][\\\"detection_id\\\"] == bbox_x[-1][\\\"detection_id\\\"]:\\n                continue\\n            bbox_y_coords = bbox_y[0]\\n            x_min = max(bbox_x_coords[0], bbox_y_coords[0])\\n            y_min = max(bbox_x_coords[1], bbox_y_coords[1])\\n            x_max = min(bbox_x_coords[2], bbox_y_coords[2])\\n            y_max = min(bbox_x_coords[3], bbox_y_coords[3])\\n            # compute the area of intersection rectangle\\n            intersection_area = max(0, x_max - x_min + 1) * max(0, y_max - y_min + 1)\\n            box_x_area = (bbox_x_coords[2] - bbox_x_coords[0] + 1) * (bbox_x_coords[3] - bbox_x_coords[1] + 1)\\n            local_overlap = intersection_area / (box_x_area + 1e-5)\\n            bbox_overlaps.append(local_overlap)\\n        overlap.append(bbox_overlaps)\\n    return  {\\\"overlap\\\": overlap}\\n\"\n            }\n        },\n        {\n            \"type\": \"DynamicBlockDefinition\",\n            \"manifest\": {\n                \"type\": \"ManifestDescription\",\n                \"block_type\": \"MaximumOverlap\",\n                \"inputs\": {\n                    \"overlaps\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"selector_types\": [\n                            \"step_output\"\n                        ]\n                    }\n                },\n                \"outputs\": {\n                    \"max_value\": {\n                        \"type\": \"DynamicOutputDefinition\",\n                        \"kind\": []\n                    }\n                }\n            },\n            \"code\": {\n                \"type\": \"PythonCode\",\n                \"run_function_code\": \"\\ndef run(self, overlaps: List[List[float]]) -&gt; BlockResult:\\n    max_value = -1\\n    for overlap in overlaps:\\n        for overlap_value in overlap:\\n            if not max_value:\\n                max_value = overlap_value\\n            else:\\n                max_value = max(max_value, overlap_value)\\n    return {\\\"max_value\\\": max_value}\\n\"\n            }\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"model\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"OverlapMeasurement\",\n            \"name\": \"overlap_measurement\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"class_x\": \"dog\",\n            \"class_y\": \"dog\"\n        },\n        {\n            \"type\": \"ContinueIf\",\n            \"name\": \"continue_if\",\n            \"condition_statement\": {\n                \"type\": \"StatementGroup\",\n                \"statements\": [\n                    {\n                        \"type\": \"BinaryStatement\",\n                        \"left_operand\": {\n                            \"type\": \"DynamicOperand\",\n                            \"operand_name\": \"overlaps\",\n                            \"operations\": [\n                                {\n                                    \"type\": \"SequenceLength\"\n                                }\n                            ]\n                        },\n                        \"comparator\": {\n                            \"type\": \"(Number) &gt;=\"\n                        },\n                        \"right_operand\": {\n                            \"type\": \"StaticOperand\",\n                            \"value\": 1\n                        }\n                    }\n                ]\n            },\n            \"evaluation_parameters\": {\n                \"overlaps\": \"$steps.overlap_measurement.overlap\"\n            },\n            \"next_steps\": [\n                \"$steps.maximum_overlap\"\n            ]\n        },\n        {\n            \"type\": \"MaximumOverlap\",\n            \"name\": \"maximum_overlap\",\n            \"overlaps\": \"$steps.overlap_measurement.overlap\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"overlaps\",\n            \"selector\": \"$steps.overlap_measurement.overlap\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"max_overlap\",\n            \"selector\": \"$steps.maximum_overlap.max_value\"\n        }\n    ]\n}\n</code></pre> <p>Let's start the analysis from <code>dynamic_blocks_definitions</code> - this is the part of  Workflow Definition that provides a list of dynamic blocks. Each block contains two sections:</p> <ul> <li> <p><code>manifest</code> - providing JSON representation of <code>BlockManifest</code> - refer blocks development guide</p> </li> <li> <p><code>code</code> - shipping Python code</p> </li> </ul>"},{"location":"workflows/custom_python_code_blocks/#definition-of-block-manifest","title":"Definition of block manifest","text":"<p>Manifest definition contains several fields, including:</p> <ul> <li> <p><code>block_type</code> - equivalent of <code>type</code> field in block manifest - must provide unique block identifier</p> </li> <li> <p><code>inputs</code> - dictionary with names and definitions of dynamic inputs</p> </li> <li> <p><code>outputs</code> - dictionary with names and definitions of dynamic outputs</p> </li> <li> <p><code>output_dimensionality_offset</code> - field specifies output dimensionality</p> </li> <li> <p><code>accepts_batch_input</code> - field dictates if input data in runtime is to be provided in batches by Execution Engine</p> </li> <li> <p><code>accepts_empty_values</code> - field deciding if empty inputs will be ignored while  constructing step inputs</p> </li> </ul> <p>In any doubt, refer to blocks development guide, as the dynamic blocks replicates standard blocs capabilities.</p>"},{"location":"workflows/custom_python_code_blocks/#definition-of-dynamic-input","title":"Definition of dynamic input","text":"<p>Dynamic inputs define fields of dynamically created block manifest. In other words,  this is definition based on which <code>BlockManifest</code> class will be created in runtime.</p> <p>Each input may define the following properties:</p> <ul> <li> <p><code>has_default_value</code> - flag to decide if dynamic manifest field has default</p> </li> <li> <p><code>default_value</code> - default value (used only if <code>has_default_value=True</code></p> </li> <li> <p><code>is_optional</code> - flag to decide if dynamic manifest field is optional</p> </li> <li> <p><code>is_dimensionality_reference</code> - flag to decide if dynamic manifest field ship selector to be used in runtime as dimensionality reference</p> </li> <li> <p><code>dimensionality_offset</code> - dimensionality offset for configured input property  of dynamic manifest</p> </li> <li> <p><code>selector_types</code> - type of selectors that may be used by property (one of  <code>input_image</code>, <code>step_output_image</code>, <code>input_parameter</code>, <code>step_output</code>). Step may not hold selector, but then must provide definition of specific type.</p> </li> <li> <p><code>selector_data_kind</code> - dictionary with list of selector kinds specific for each selector type</p> </li> <li> <p><code>value_types</code> - definition of specific type that is to be placed in manifest -  this field specifies typing of dynamically created manifest fields w.r.t Python types. Selection of types: <code>any</code>, <code>integer</code>, <code>float</code>, <code>boolean</code>, <code>dict</code>, <code>list</code>, <code>strig</code></p> </li> </ul>"},{"location":"workflows/custom_python_code_blocks/#definition-of-dynamic-output","title":"Definition of dynamic output","text":"<p>Definitions of outputs are quite simple, hold optional list of <code>kinds</code> declared for given output.</p>"},{"location":"workflows/custom_python_code_blocks/#definition-of-python-code","title":"Definition of Python code","text":"<p>Python code is shipped in JSON document with the following fields:</p> <ul> <li> <p><code>run_function_code</code> - code of <code>run(...)</code> method of your dynamic block </p> </li> <li> <p><code>run_function_name</code> - name of run function</p> </li> <li> <p><code>init_function_code</code> - optional code for your init function that will  assemble step state - it is expected to return dictionary, which will be available for <code>run()</code> function under <code>self._init_results</code></p> </li> <li> <p><code>init_function_name</code> - name of init function</p> </li> <li> <p><code>imports</code> - list of additional imports (you may only use libraries from your environment, no dependencies will be  automatically installed)</p> </li> </ul>"},{"location":"workflows/custom_python_code_blocks/#how-to-create-run-method","title":"How to create <code>run(...)</code> method?","text":"<p>You must know the following:</p> <ul> <li> <p><code>run(...)</code> function must be defined, as if that was class instance method - with  the first argument being <code>self</code> and remaining arguments compatible with dynamic block manifest declared in definition of dynamic block</p> </li> <li> <p>you should expect baseline symbols to be provided, including your import statements and the following:</p> </li> </ul> <pre><code>from typing import Any, List, Dict, Set, Optional\nimport supervision as sv\nimport numpy as np\nimport math\nimport time\nimport json\nimport os\nimport requests\nimport cv2\nimport shapely\nfrom inference.core.workflows.execution_engine.entities.base import Batch, WorkflowImageData\nfrom inference.core.workflows.prototypes.block import BlockResult\n</code></pre> <p>So example function may look like the following (for clarity, we provide here Python code formatted nicely, but you must stringify the code to place it in definition):</p> <pre><code>def run(self, predictions: sv.Detections, class_x: str, class_y: str) -&gt; BlockResult:\n    bboxes_class_x = predictions[predictions.data[\"class_name\"] == class_x]\n    bboxes_class_y = predictions[predictions.data[\"class_name\"] == class_y]\n    overlap = []\n    for bbox_x in bboxes_class_x:\n        bbox_x_coords = bbox_x[0]\n        bbox_overlaps = []\n        for bbox_y in bboxes_class_y:\n            if bbox_y[-1][\"detection_id\"] == bbox_x[-1][\"detection_id\"]:\n                continue\n            bbox_y_coords = bbox_y[0]\n            x_min = max(bbox_x_coords[0], bbox_y_coords[0])\n            y_min = max(bbox_x_coords[1], bbox_y_coords[1])\n            x_max = min(bbox_x_coords[2], bbox_y_coords[2])\n            y_max = min(bbox_x_coords[3], bbox_y_coords[3])\n            # compute the area of intersection rectangle\n            intersection_area = max(0, x_max - x_min + 1) * max(0, y_max - y_min + 1)\n            box_x_area = (bbox_x_coords[2] - bbox_x_coords[0] + 1) * (bbox_x_coords[3] - bbox_x_coords[1] + 1)\n            local_overlap = intersection_area / (box_x_area + 1e-5)\n            bbox_overlaps.append(local_overlap)\n        overlap.append(bbox_overlaps)\n    return  {\"overlap\": overlap}\n</code></pre>"},{"location":"workflows/custom_python_code_blocks/#how-to-create-init-method","title":"How to create <code>init(...)</code> method?","text":"<p>Init function is supposed to build <code>self._init_results</code> dictionary.</p> <p>Example:</p> <pre><code>def my_init() -&gt; Dict[str, Any]:\n    return {\"some\": \"value\"}\n</code></pre>"},{"location":"workflows/custom_python_code_blocks/#usage-of-dynamic-python-block-as-step","title":"Usage of Dynamic Python block as step","text":"<p>As shown in example Workflow definition, you may simply use the block  as if that was normal block exposed through static plugin:</p> <pre><code>{\n    \"type\": \"OverlapMeasurement\",\n    \"name\": \"overlap_measurement\",\n    \"predictions\": \"$steps.model.predictions\",\n    \"class_x\": \"dog\",\n    \"class_y\": \"dog\"\n}\n</code></pre>"},{"location":"workflows/definitions/","title":"Understanding Workflows Definitions syntax","text":"<p>In Roboflow Workflows, the Workflow Definition is the internal \"programming language\". It provides a structured  way to define how different blocks interact, specifying the necessary inputs, outputs, and configurations.  By using this syntax, users can create workflows without UI.</p> <p>Let's start from examining the Workflow Definition created in this tutorial and analyse it step by step.</p> Workflow definition <pre><code>{\n  \"version\": \"1.0\",\n  \"inputs\": [\n    {\n      \"type\": \"InferenceImage\",\n      \"name\": \"image\"\n    },\n    {\n      \"type\": \"WorkflowParameter\",\n      \"name\": \"model\",\n      \"default_value\": \"yolov8n-640\"\n    }\n  ],\n  \"steps\": [\n    {\n      \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n      \"name\": \"model\",\n      \"images\": \"$inputs.image\",\n      \"model_id\": \"$inputs.model\"\n    },\n    {\n      \"type\": \"roboflow_core/dynamic_crop@v1\",\n      \"name\": \"dynamic_crop\",\n      \"images\": \"$inputs.image\",\n      \"predictions\": \"$steps.model.predictions\"\n    },\n    {\n      \"type\": \"roboflow_core/roboflow_classification_model@v1\",\n      \"name\": \"model_1\",\n      \"images\": \"$steps.dynamic_crop.crops\",\n      \"model_id\": \"dog-breed-xpaq6/1\"\n    },\n    {\n      \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n      \"name\": \"detections_classes_replacement\",\n      \"object_detection_predictions\": \"$steps.model.predictions\",\n      \"classification_predictions\": \"$steps.model_1.predictions\"\n    },\n    {\n      \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n      \"name\": \"bounding_box_visualization\",\n      \"predictions\": \"$steps.detections_classes_replacement.predictions\",\n      \"image\": \"$inputs.image\"\n    },\n    {\n      \"type\": \"roboflow_core/label_visualization@v1\",\n      \"name\": \"label_visualization\",\n      \"predictions\": \"$steps.detections_classes_replacement.predictions\",\n      \"image\": \"$steps.bounding_box_visualization.image\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"type\": \"JsonField\",\n      \"name\": \"detections\",\n      \"coordinates_system\": \"own\",\n      \"selector\": \"$steps.detections_classes_replacement.predictions\"\n    },\n    {\n      \"type\": \"JsonField\",\n      \"name\": \"visualisation\",\n      \"coordinates_system\": \"own\",\n      \"selector\": \"$steps.label_visualization.image\"\n    }\n  ]\n}\n</code></pre>"},{"location":"workflows/definitions/#version-marker","title":"Version marker","text":"<p>Every Workflow Definition begins with the version parameter, which specifies the compatible version of the  Workflows Execution Engine. Roboflow utilizes Semantic Versioning to manage these  versions and maintains one version from each major release to ensure backward compatibility.  This means that a workflow defined for Execution Engine version <code>1.0.0</code> will function with version <code>1.3.4</code> and other  newer versions, but workflows created for more recent versions may not be compatible with earlier ones.</p> <p>List of Execution Engine versions loaded on the Roboflow Hosted platform is available  here.</p>"},{"location":"workflows/definitions/#inputs","title":"Inputs","text":"<p>Our example workflow specifies two inputs: <pre><code>[\n    {\n      \"type\": \"InferenceImage\", \"name\": \"image\"\n    },\n    {\n      \"type\": \"WorkflowParameter\", \"name\": \"model\", \"default_value\": \"yolov8n-640\"\n    }\n]\n</code></pre> This entry in definition creates two placeholders that can be filled with data while running workflow. </p> <p>The first placeholder is named <code>image</code> and is of type <code>InferenceImage</code>. This special input type is batch-oriented,  meaning it can accept one or more images at runtime to be processed as a single batch. You can add multiple inputs  of the type <code>InferenceImage</code>, and it is expected that the data provided to these placeholders will contain  the same number of elements. Alternatively, you can mix inputs of sizes <code>N</code> and 1, where <code>N</code> represents the number  of elements in the batch.</p> <p>The second placeholder is a straightforward <code>WorkflowParameter</code> called model. This type of input allows users to  inject hyperparameters \u2014 such as model variants, confidence thresholds, and reference values \u2014 at runtime. The value is not expected to be a batch of elements, so when you provide a list, it will be interpreted as list of  elements, rather than batch of elements, each to be processed individually.</p> <p>More details about the nature of batch-oriented data processing in workflows can be found  here.</p>"},{"location":"workflows/definitions/#steps","title":"Steps","text":"<p>As mentioned here, steps are instances of Workflow blocks connected with inputs and outputs  of other steps to dictate how data flows through the workflow. Let's see example step definition:</p> <pre><code>{\n  \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n  \"name\": \"model\",\n  \"images\": \"$inputs.image\",\n  \"model_id\": \"$inputs.model\"\n}\n</code></pre> <p>Two common properties for each step are <code>type</code> and <code>name</code>. Type tells which block to load and name gives the step  unique identifier, based on which other steps may refer to output of given step.</p> <p>Two remaining properties declare <code>selectors</code> (this is how we call references in Workflows) to inputs - <code>image</code> and <code>model</code>. While running the workflow, data passed into those placeholders will be provided for block to process.</p> <p>Our documentation showcases what is the structure of each block and provides examples of how each block can be  used as workflow step. Explore our blocks collection here where you can find what are  block data inputs, outputs and configuration properties.</p> <p>Input data bindings of blocks (like <code>images</code> property) can be filled with selectors to batch-oriented inputs and  step outputs. Configuration properties of blocks (like <code>model_id</code>) usually can be filled with either values hardcoded in workflow definition (they cannot be altered in runtime) or selectors to inputs of type <code>WorkflowParameter</code>. For instance, valid definition can be obtained when <code>model_id</code> is either <code>\"$inputs.image\"</code> or <code>yolov8n-640</code>.</p> <p>Let's see now how step outputs are referred as inputs of another step: <pre><code>{\n  \"type\": \"roboflow_core/dynamic_crop@v1\",\n  \"name\": \"dynamic_crop\",\n  \"images\": \"$inputs.image\",\n  \"predictions\": \"$steps.model.predictions\"\n}\n</code></pre> In this particular case, <code>predictions</code> property defines output of step named <code>model</code>. Construction of selector is the following: <code>$steps.{step_name}.{step_output_name}</code>. Thanks to this reference, <code>model</code> step is connected with  <code>dynamic_crop</code> and in runtime model predictions will be passed into dynamic crop and will be reference for image  cropping procedure.</p>"},{"location":"workflows/definitions/#outputs","title":"Outputs","text":"<p>This section of Workflow Definition specifies how response from workflow execution looks like. Definitions of  each response field looks like that:</p> <pre><code>{\n  \"type\": \"JsonField\",\n  \"name\": \"detections\",\n  \"selector\": \"$steps.detections_classes_replacement.predictions\"\n}\n</code></pre> <p>The <code>selector</code> can reference either an input or a step output. Additionally, you can specify the <code>\"coordinates_system\"</code>  property, which accepts two values: <code>\"own\"</code> or <code>\"parent\"</code>. This property is relevant for outputs that provide model  detections and determines the coordinate system used for the detections. This becomes crucial when applying a  secondary object detection model on image crops derived from predictions of a primary model. In such cases,  the secondary model\u2019s predictions are based on the coordinates of the crops, not the original input image.  To ensure these coordinates are not translated back to the parent coordinate system, set  <code>\"coordinates_system\": \"own\"</code> (<code>parent</code> is default option).</p> <p>Additionally, outputs selectors support wildcards (<code>$steps.step_nane.*\"</code>) to grab all outputs of specific step.</p> <p>To fully understand how output structure is created - read about  data processing in Workflows.</p>"},{"location":"workflows/execution_engine_changelog/","title":"Execution Engine Changelog","text":"<p>Below you can find the changelog for Execution Engine.</p>"},{"location":"workflows/execution_engine_changelog/#execution-engine-v120-inference-v0230","title":"Execution Engine <code>v1.2.0</code> | inference <code>v0.23.0</code>","text":"<ul> <li>The <code>video_metadata</code> kind has been deprecated, and we strongly recommend discontinuing its use for building  blocks moving forward. As an alternative, the <code>image</code> kind has been extended to support the same metadata as  <code>video_metadata</code> kind, which can now be provided optionally. This update is  non-breaking for existing blocks, but some older blocks that produce images may become incompatible with  future video processing blocks.</li> </ul> Potential blocks incompatibility <p>As previously mentioned, adding <code>video_metadata</code> as an optional field to the internal representation of  <code>image</code> kind (<code>WorkflowImageData</code> class)  may introduce some friction between existing blocks that output the <code>image</code> kind and  future video processing blocks that rely on <code>video_metadata</code> being part of <code>image</code> representation. </p> <p>The issue arises because, while we can provide default values for <code>video_metadata</code> in <code>image</code> without  explicitly copying them from the input, any non-default metadata that was added upstream may be lost.  This can lead to downstream blocks that depend on the <code>video_metadata</code> not functioning as expected.</p> <p>We've updated all existing <code>roboflow_core</code> blocks to account for this, but blocks created before this change in external repositories may cause issues in workflows where their output images are used by video processing blocks.</p> <ul> <li>While the deprecated <code>video_metadata</code> kind is still available for use, it will be fully removed in  Execution Engine version <code>v2.0.0</code>.</li> </ul> <p>Breaking change planned - Execution Engine <code>v2.0.0</code></p> <p><code>video_metadata</code> kind got deprecated and will be removed in <code>v2.0.0</code></p> <ul> <li>As a result of the changes mentioned above, the internal representation of the <code>image</code> kind has been updated to  include a new <code>video_metadata</code> property. This property can be optionally set in the constructor; if not provided,  a default value with reasonable defaults will be used. To simplify metadata manipulation within blocks, we have  introduced two new class methods: <code>WorkflowImageData.copy_and_replace(...)</code> and <code>WorkflowImageData.create_crop(...)</code>.  For more details, refer to the updated <code>WoorkflowImageData</code> usage guide.</li> </ul>"},{"location":"workflows/gallery_index/","title":"Workflows gallery","text":"<p>The Workflows gallery offers example workflow definitions to help you understand what can be achieved with workflows.  Browse through the various categories to find inspiration and ideas for building your own workflows.</p> <ul> <li>Workflows with multiple models</li> <li>Workflows enhanced by Roboflow Platform</li> <li>Basic Workflows</li> <li>Workflows with classical Computer Vision methods</li> <li>Workflows with Visual Language Models</li> <li>Data analytics in Workflows</li> <li>Workflows with dynamic Python Blocks</li> <li>Workflows for OCR</li> <li>Workflows with data transformations</li> <li>Workflows with flow control</li> <li>Workflows with business logic</li> <li>Advanced inference techniques</li> <li>Workflows with foundation models</li> <li>Workflows with visualization blocks</li> </ul>"},{"location":"workflows/internal_data_types/","title":"Data representations in Workflows","text":"<p>Many frameworks enforce standard data types for developers to work with, and the Workflows ecosystem is no  exception. While the kind in a Workflow represents a high-level abstraction of the data  being passed through, it's important to understand the specific data types that will be provided to the  <code>WorkflowBlock.run(...)</code> method when building Workflow blocks.</p> <p>And this is exactly what you will learn here.</p>"},{"location":"workflows/internal_data_types/#batch","title":"<code>Batch</code>","text":"<p>When a Workflow block declares batch processing, it uses a special container type called Batch.  All batch-oriented parameters are wrapped with <code>Batch[X]</code>, where X is the data type:</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import Batch\nfrom inference.core.workflows.prototypes.block import BlockResult\n\n# run method of Workflow block\ndef run(self, x: Batch[int], y: Batch[float]) -&gt; BlockResult:\n   pass\n</code></pre> <p>The <code>Batch</code> type functions similarly to a Python list, with one key difference: it is read-only. You cannot modify  its elements, nor can you add or remove elements. However, several useful operations are available:</p>"},{"location":"workflows/internal_data_types/#iteration-through-elements","title":"Iteration through elements","text":"<pre><code>from inference.core.workflows.execution_engine.entities.base import Batch\n\n\ndef iterate(batch: Batch[int]) -&gt; None:\n    for element in batch:\n        print(element)\n</code></pre>"},{"location":"workflows/internal_data_types/#zipping-multiple-batches","title":"Zipping multiple batches","text":"<p>Do not worry about batches alignment</p> <p>The Execution Engine ensures that batches provided to the run method are of equal size, preventing any loss of  elements due to unequal batch sizes during iteration.</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import Batch\n\n\ndef zip_batches(batch_1: Batch[int], batch_2: Batch[int]) -&gt; None:\n    for element_1, element_2 in zip(batch_1, batch_2):\n        print(element_1, element_2)\n</code></pre>"},{"location":"workflows/internal_data_types/#getting-batch-element-indices","title":"Getting batch element indices","text":"<p>This returns a list of tuples where each tuple represents the position of the batch element in  potentially nested batch structures.</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import Batch\n\n\ndef discover_indices(batch: Batch[int]) -&gt; None:\n    for index in batch.indices:\n        print(index)  # e.g., (0,) for 1D batch, (1, 3) for 2D nested batch, etc.\n</code></pre>"},{"location":"workflows/internal_data_types/#iterating-while-retrieving-both-elements-and-their-indices","title":"Iterating while retrieving both elements and their indices","text":"<pre><code>from inference.core.workflows.execution_engine.entities.base import Batch\n\n\ndef iterate_with_indices(batch: Batch[int]) -&gt; None:\n    for index, element in batch.iter_with_indices():\n        print(index, element)\n</code></pre>"},{"location":"workflows/internal_data_types/#additional-methods-of-batch-container","title":"Additional methods of <code>Batch</code> container","text":"<p>There are other methods in the Batch interface, such as <code>remove_by_indices(...)</code> or <code>broadcast(...)</code>, but these  are not intended for use within Workflow blocks. These methods are primarily used  by the Execution Engine when providing data to the block.</p>"},{"location":"workflows/internal_data_types/#workflowimagedata","title":"<code>WorkflowImageData</code>","text":"<p><code>WorkflowImageData</code> is a dataclass that encapsulates an image along with its metadata, providing useful methods to  manipulate the image representation within a Workflow block.</p> <p>Some users may expect an <code>np.ndarray</code> to be provided directly by the Execution Engine when an <code>image</code> kind is declared.  While that could be a convenient and straightforward approach, it introduces limitations, such as:</p> <ul> <li> <p>Lack of metadata: With only an <code>np.ndarray</code>, there's no way to attach metadata such as data lineage or the  image's location within the original file (e.g., when working with cropped images).</p> </li> <li> <p>Inability to cache multiple representations: If multiple blocks need to serialize and send the image over  HTTP, WorkflowImageData allows caching of different image representations, such as base64-encoded versions,  improving efficiency.</p> </li> </ul> <p>Video Metadata</p> <p>Since Execution Enginge <code>v1.2.0</code>, we have added <code>video_metadata</code> into <code>WorkflowImageData</code>. This  object is supposed to hold the context of video processing and will only be relevant for video processing blocks. Other blocks may ignore it's existance if not creating output image (covered in the next section). </p> <p>Operating on <code>WorkflowImageData</code> is fairly simple once you understand its interface. Here are some of the key methods and properties:</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import WorkflowImageData\n\n\ndef operate_on_image(workflow_image: WorkflowImageData) -&gt; None:\n    # Getting an np.ndarray representing the image.\n    numpy_image = workflow_image.numpy_image  \n\n    # Getting base64-encoded JPEG representation of the image, ideal for API transmission.\n    base64_image: str = workflow_image.base64_image  \n\n    # Converting the image into a format compatible with inference models.\n    inference_format = workflow_image.to_inference_format()  \n\n    # Accesses metadata related to the parent image\n    parent_metadata = workflow_image.parent_metadata\n    print(parent_metadata.parent_id)  # parent identifier\n    origin_coordinates = parent_metadata.origin_coordinates  # optional object with coordinates\n    print(\n        origin_coordinates.left_top_x, origin_coordinates.left_top_y, \n        origin_coordinates.origin_width, origin_coordinates.origin_height,\n    )\n\n    # or the same for root metadata (the oldest ancestor of the image - Workflow input image)\n    root_metadata = workflow_image.workflow_root_ancestor_metadata\n\n    # retrieving `VideoMetadata` object - see the usage guide section below\n    # if `workflow_image` is not provided with `VideoMetadata` - default metadata object will \n    # be created on accessing the property\n    video_metadata = workflow_image.video_metadata \n</code></pre> <p>Below you can find an example showcasing how to preserve metadata, while transforming image</p> <pre><code>import numpy as np\n\nfrom inference.core.workflows.execution_engine.entities.base import WorkflowImageData\n\n\ndef transform_image(image: WorkflowImageData) -&gt; WorkflowImageData:\n    transformed_image = some_transformation(image.numpy_image)\n    # `WorkflowImageData` exposes helper method to return a new object with\n    # updated image, but with preserved metadata. Metadata preservation\n    # should only be used when the output image is compatible regarding\n    # data lineage (the predecessor-successor relation for images).\n    # Lineage is not preserved for cropping and merging images (without common predecessor)\n    # - below you may find implementation tips.\n    return WorkflowImageData.copy_and_replace(\n        origin_image_data=image,\n        numpy_image=transformed_image,\n    )\n\n\ndef some_transformation(image: np.ndarray) -&gt; np.ndarray:\n    ...\n</code></pre> Images cropping <p>When your block increases dimensionality and provides output with <code>image</code> kind - usually that means cropping the  image. In such cases input image <code>video_metadata</code> is to be removed (as usually it does not make sense to keep them, as underlying video processing blocks will not work correctly when for dynamically created blocks).</p> <p>Below you can find scratch of implementation for that operation:</p> <pre><code>from typing import List, Tuple\n\nfrom dataclasses import replace\nfrom inference.core.workflows.execution_engine.entities.base import WorkflowImageData\n\ndef crop_images(\n    image: WorkflowImageData, \n    crops: List[Tuple[str, int, int, int, int]],\n) -&gt; List[WorkflowImageData]:\n    crops = []\n    original_image = image.numpy_image\n    for crop_id, x_min, y_min, x_max, y_max in crops:\n        cropped_image = original_image[y_min:y_max, x_min:x_max]\n        if not cropped_image.size:\n            # discarding empty crops\n            continue\n        result_crop = WorkflowImageData.create_crop(\n            origin_image_data=image, \n            crop_identifier=crop_id,\n            cropped_image=cropped_image,\n            offset_x=x_min,\n            offset_y=y_min,\n        )\n        crops.append(result_crop)\n    return crops\n</code></pre> <p>In some cases you may want to preserve <code>video_metadata</code>. Example of such situation is when  your block produces crops based on fixed coordinates (like video single footage with multiple fixed Regions of  Interest to be applied individual trackers) - then you want result crops to be processed in context of video, as if they were produced by separate cameras. To adjust behaviour of <code>create_crop(...)</code> method, simply add  <code>preserve_video_metadata=True</code>:</p> <pre><code>def crop_images(\n    image: WorkflowImageData, \n    crops: List[Tuple[str, int, int, int, int]],\n) -&gt; List[WorkflowImageData]:\n    # [...]\n    result_crop = WorkflowImageData.create_crop(\n        origin_image_data=image, \n        crop_identifier=crop_id,\n        cropped_image=cropped_image,\n        offset_x=x_min,\n        offset_y=y_min,\n        preserve_video_metadata=True\n    )\n    # [...]\n</code></pre> Merging images without common predecessor <p>If common <code>parent_metadata</code> cannot be pointed for multiple images you try to merge, you should denote that \"a new\" image appears in the Workflow. To do it simply:</p> <pre><code>from typing import List, Tuple\n\nfrom dataclasses import replace\nfrom inference.core.workflows.execution_engine.entities.base import \\\n    WorkflowImageData, ImageParentMetadata\n\ndef merge_images(image_1: WorkflowImageData, image_2: WorkflowImageData) -&gt; WorkflowImageData:\n    merged_image = some_mergin_operation(\n        image_1=image_1.numpy_image,\n        image_2=image_2.numpy_image\n    )\n    new_parent_metadata = ImageParentMetadata(\n        # this is just one of the option for creating id, yet sensible one\n        parent_id=f\"{image_1.parent_metadata.parent_id} + {image_2.parent_metadata.parent_id}\"\n    )\n    return WorkflowImageData(\n        parent_metadata=new_parent_metadata,\n        numpy_image=merged_imagem\n    )\n</code></pre>"},{"location":"workflows/internal_data_types/#videometadata","title":"<code>VideoMetadata</code>","text":"<p>Deprecation</p> <p><code>video_metadata</code> kind is deprecated - we advise not using that kind in new  blocks. <code>VideoMetadata</code> data representation became a member of <code>WorkflowImageData</code> in Execution Engine <code>v1.2.0</code>  (<code>inference</code> release <code>v0.23.0</code>)</p> <p><code>VideoMetadata</code> is a dataclass that provides the following metadata about video frame and video source:</p> <pre><code>from inference.core.workflows.execution_engine.entities.base import VideoMetadata\n\n\ndef inspect_vide_metadata(video_metadata: VideoMetadata) -&gt; None:\n    # Identifier string for video. To be treated as opaque.\n    print(video_metadata.video_identifier)\n\n    # Sequential number of the frame\n    print(video_metadata.frame_number)\n\n    # The timestamp of video frame. When processing video it is suggested that \"\n    # \"blocks rely on `fps` and `frame_number`, as real-world time-elapse will not \"\n    # \"match time-elapse in video file\n    print(video_metadata.frame_timestamp)\n\n    # Field represents FPS value (if possible to be retrieved) (optional)\n    print(video_metadata.fps)\n\n    # Field is a flag telling if frame comes from video file or stream.\n    # If not possible to be determined - None\n    print(video_metadata.comes_from_video_file)\n</code></pre>"},{"location":"workflows/kinds/","title":"Workflows kinds","text":"<p>In Workflows, some values cannot be defined when the Workflow Definition is created. To address this, the Execution  Engine supports selectors, which are references to step outputs or workflow inputs. To help the Execution Engine  understand what type of data will be provided once a reference is resolved, we use a simple type system known as  <code>kinds</code>.</p> <p><code>Kinds</code> are used to represent the semantic meaning of the underlying data. When a step outputs data of a specific  <code>kind</code> and another step requires input of that same <code>kind</code>, the system assumes that the data will be compatible.  This reduces the need for extensive type-compatibility checks.</p> <p>For example, we have different kinds to distinguish between predictions from <code>object detection</code> and  <code>instance segmentation</code> models, even though representation of those <code>kinds</code> is <code>sv.Detections(...)</code>. This distinction ensures that each  block that needs a segmentation mask clearly indicates this requirement, avoiding the need to repeatedly check  for the presence of a mask in the input.</p> <p>Note</p> <p>As for now, <code>kinds</code> are such simplistic that do not support types polymorphism - and developers are asked to use unions of kinds to solve that problem. As defining extensive unions of kinds may be  problematic, this problem will probably be addressed in Execution Engine <code>v2</code>.</p> <p>Warning</p> <p>In <code>inference</code> release <code>0.18.0</code> we decided to make drastic move to heal the ecosystem  from the problem with ambiguous kinds names (<code>Batch[X]</code> vs <code>X</code> - see more  here). </p> <p>The change is breaking only if there is remote Workflow plugin depending on imports from <code>inference.core.workflows.execution_engine.entities.types</code> module, which is not the case to the best of our knowledge. We removed problematic kinds as if they never existed in the ecosystem and fixed all blocks from <code>roboflow_core</code> plugin. If there is anyone impacted by the change - here is the  migration guide.</p>"},{"location":"workflows/kinds/#kinds-declared-in-roboflow-plugins","title":"Kinds declared in Roboflow plugins","text":"<ul> <li><code>classification_prediction</code>: Predictions from classifier</li> <li><code>qr_code_detection</code>: Prediction with QR code detection</li> <li><code>parent_id</code>: Identifier of parent for step output</li> <li><code>roboflow_model_id</code>: Roboflow model id</li> <li><code>bar_code_detection</code>: Prediction with barcode detection</li> <li><code>roboflow_api_key</code>: Roboflow API key</li> <li><code>language_model_output</code>: LLM / VLM output</li> <li><code>object_detection_prediction</code>: Prediction with detected bounding boxes in form of sv.Detections(...) object</li> <li><code>detection</code>: Single element of detections-based prediction (like <code>object_detection_prediction</code>)</li> <li><code>keypoint_detection_prediction</code>: Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object</li> <li><code>boolean</code>: Boolean flag</li> <li><code>point</code>: Single point in 2D</li> <li><code>zone</code>: Definition of polygon zone</li> <li><code>list_of_values</code>: List of values of any type</li> <li><code>bytes</code>: This kind represent bytes</li> <li><code>instance_segmentation_prediction</code>: Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object</li> <li><code>dictionary</code>: Dictionary</li> <li><code>float_zero_to_one</code>: <code>float</code> value in range <code>[0.0, 1.0]</code></li> <li><code>prediction_type</code>: String value with type of prediction</li> <li><code>image_metadata</code>: Dictionary with image metadata required by supervision</li> <li><code>top_class</code>: String value representing top class predicted by classification model</li> <li><code>float</code>: Float value</li> <li><code>contours</code>: List of numpy arrays where each array represents contour points</li> <li><code>integer</code>: Integer value</li> <li><code>rgb_color</code>: RGB color</li> <li><code>image_keypoints</code>: Image keypoints detected by classical Computer Vision method</li> <li><code>*</code>: Equivalent of any element</li> <li><code>image</code>: Image in workflows</li> <li><code>serialised_payloads</code>: Serialised element that is usually accepted by sink</li> <li><code>string</code>: String value</li> <li><code>video_metadata</code>: Video image metadata</li> <li><code>roboflow_project</code>: Roboflow project name</li> <li><code>numpy_array</code>: Numpy array</li> </ul>"},{"location":"workflows/modes_of_running/","title":"What are the options for running workflows?","text":"<p>There are few ways on how to run Workflow, including:</p> <ul> <li> <p>Request to HTTP API (Roboflow Hosted API or self-hosted <code>inference</code> server) running Workflows Execution Engine</p> </li> <li> <p>Video processing using InferencePipeline</p> </li> <li> <p><code>inference</code> Python package, where you can use Workflows Execution Engine directly in your Python app</p> </li> </ul>"},{"location":"workflows/modes_of_running/#http-api-request","title":"HTTP API request","text":"<p>This way of running Workflows is ideal for clients who:</p> <ul> <li> <p>Want to use Workflows as a stand-alone, independent part of their systems.</p> </li> <li> <p>Maintain their main applications in languages other than Python.</p> </li> <li> <p>Prefer to offload compute-heavy tasks to dedicated servers.</p> </li> </ul> <p>Roboflow offers a hosted HTTP API that clients can use without needing their own infrastructure.  Alternatively, the <code>inference</code> server (which can run Workflows) can be set up on-site if needed.</p> <p>Running Workflows with Roboflow Hosted API has several limitations:</p> <ul> <li> <p>Workflow runtime is limited to 20s</p> </li> <li> <p>Response payload is limited to 6MB, which means that some blocks (especially visualization ones) if used in too large numbers, or with input images that are too large may result in failed request</p> </li> </ul> <p>Integrating via HTTP is simple: just send a request to the server. You can do this using a HTTP client library in your preferred programming language,  leverage our Inference SDK in Python, or even use cURL. Explore the examples below to see how it\u2019s done.</p> <p>HTTP integration</p> cURLInference SDK in Python (Roboflow Hosted API)Inference SDK in Python (on-prem) <p>To run your workflow created in Roboflow APP with <code>cURL</code>, use the following command:</p> <pre><code>curl --location 'https://detect.roboflow.com/infer/workflows/&lt;your-workspace-name&gt;/&lt;your-workflow-id&gt;' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n    \"api_key\": \"&lt;YOUR-API-KEY&gt;\",\n    \"inputs\": {\n        \"image\": {\"type\": \"url\", \"value\": \"https://your-image-url\"},\n        \"parameter\": \"some-value\"\n    }\n}'\n</code></pre> <p>Please note that:</p> <ul> <li> <p><code>&lt;your-workspace-name&gt;</code>, <code>&lt;your-workflow-id&gt;</code>, <code>&lt;YOUR-API-KEY&gt;</code> must be replaced with actual values -  valid for your Roboflow account</p> </li> <li> <p>keys of <code>inputs</code> dictionary are dictated by your Workflow, names may differ dependent on  parameters you define</p> </li> <li> <p>values of <code>inputs</code> dictionary are also dependent on your Workflow definition - inputs declared as <code>WorkflowImage</code> have special structure - dictionary with <code>type</code> and <code>value</code> keys - using cURL your  options are <code>url</code> and <code>base64</code> as <code>type</code> - and value adjusted accordingly</p> </li> </ul> <p>To run your workflow created in Roboflow APP with <code>InferenceClient</code>:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\",\n    api_key=\"&lt;YOUR-API-KEY&gt;\",\n)\n\nresult = client.run_workflow(\n    workspace_name=\"&lt;your-workspace-name&gt;\",\n    workflow_id=\"&lt;your-workflow-id&gt;\",\n    images={\n        \"image\": [\"https://your-image-url\", \"https://your-other-image-url\"]\n    },\n    parameters={\n        \"parameter\": \"some-value\"\n    },\n)\n</code></pre> <p>Please note that:</p> <ul> <li> <p><code>&lt;your-workspace-name&gt;</code>, <code>&lt;your-workflow-id&gt;</code>, <code>&lt;YOUR-API-KEY&gt;</code> must be replaced with actual values -  valid for your Roboflow account</p> </li> <li> <p>method parameter named <code>images</code> is supposed to be filled with dictionary that contains names and values for all Workflow inputs declared as <code>WorkflowImage</code>. Names must match your Workflow definition, as value you can pass either <code>np.array</code>, <code>PIL.Image</code>, URL to your image, local path to your image or image in <code>base64</code> string. It is optional if Workflow does not define images as inputs. </p> </li> <li> <p>Batch input for images is supported - simply pass list of images under given input name.</p> </li> <li> <p>method parameter named <code>parameters</code> is supposed to be filled with dictionary that contains names and values for all Workflow inputs of type <code>WorkflowParameter</code>. It's optional and must be filled according to Workflow definition.</p> </li> </ul> <p>Note</p> <p>Please make sure you have <code>inference-sdk</code> package installed in your environment</p> <p>To run your workflow created in Roboflow APP with <code>InferenceClient</code>:</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://127.0.0.1:9001\",  # please modify that value according to URL of your server\n    api_key=\"&lt;YOUR-API-KEY&gt;\",\n)\n\nresult = client.run_workflow(\n    workspace_name=\"&lt;your-workspace-name&gt;\",\n    workflow_id=\"&lt;your-workflow-id&gt;\",\n    images={\n        \"image\": [\"https://your-image-url\", \"https://your-other-image-url\"]\n    },\n    parameters={\n        \"parameter\": \"some-value\"\n    }    \n)\n</code></pre> <p>Please note that:</p> <ul> <li> <p><code>&lt;your-workspace-name&gt;</code>, <code>&lt;your-workflow-id&gt;</code>, <code>&lt;YOUR-API-KEY&gt;</code> must be replaced with actual values -  valid for your Roboflow account</p> </li> <li> <p>method parameter named <code>images</code> is supposed to be filled with dictionary that contains names and values for all Workflow inputs declared as <code>WorkflowImage</code>. Names must match your Workflow definition, as value you can pass either <code>np.array</code>, <code>PIL.Image</code>, URL to your image, local path to your image or image in <code>base64</code> string. It is optional if Workflow does not define images as inputs.</p> </li> <li> <p>Batch input for images is supported - simply pass list of images under given input name.</p> </li> <li> <p>method parameter named <code>parameters</code> is supposed to be filled with dictionary that contains names and values for all Workflow inputs of type <code>WorkflowParameter</code>. It's optional and must be filled according to Workflow definition.</p> </li> </ul> <p>Note</p> <ul> <li> <p>Please make sure you have <code>inference-sdk</code> package installed in your environment.</p> </li> <li> <p>Easiest way to run <code>inference</code> server on-prem is to use <code>inference-cli</code> package command: <pre><code>inference server start\n</code></pre></p> </li> </ul> <p>The above examples present how to run Workflow created and saved in Roboflow APP. It is also possible to create and run workflow that is created from scratch and may not contain API-KEY gated blocks (for instance  your own blocks). Then you should use the  following endpoint or Inference SDK as showcased in docs.</p>"},{"location":"workflows/modes_of_running/#video-processing-using-inferencepipeline","title":"Video processing using <code>InferencePipeline</code>","text":"<p>For use cases involving video files or streams, we recommend using InferencePipeline, which can run  Workflows on each video frame.</p> <p>This option is ideal for clients who:</p> <ul> <li> <p>Need low-latency, high-throughput video processing.</p> </li> <li> <p>Design workflows with single-frame processing times that meet real-time requirements (though complex workflows  might not be suitable for real-time processing)</p> </li> </ul> <p>Explore the example below to see how to combine <code>InferencePipeline</code> with Workflows.</p> <p>Integration with InferencePipeline</p> <pre><code>from inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\n\ndef my_sink(result: dict, video_frame: VideoFrame):\n    print(result) # here you can find dictionary with outputs from your workflow\n\n\n# initialize a pipeline object\npipeline = InferencePipeline.init_with_workflow(\n    api_key=\"&lt;YOUR-API-KEY&gt;\",\n    workspace_name=\"&lt;your-workspace-name&gt;\",\n    workflow_id=\"&lt;your-workflow-id&gt;\",\n    video_reference=0, # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url\n    on_prediction=my_sink,\n    image_input_name=\"image\",  # this parameter holds the name of Workflow input that represents \n    # image to be processed - please ADJUST it to your Workflow Definition \n)\npipeline.start() #start the pipeline\npipeline.join() #wait for the pipeline thread to finish\n</code></pre> <p>Please note that:</p> <ul> <li> <p><code>&lt;your-workspace-name&gt;</code>, <code>&lt;your-workflow-id&gt;</code>, <code>&lt;YOUR-API-KEY&gt;</code> must be replaced with actual values -  valid for your Roboflow account</p> </li> <li> <p>your Workflow must accept video frames under <code>image</code> parameter - when multiple video streams are  given for processing, all collected video frames will be submitted in batch under <code>image</code> parameter for workflow run. <code>image</code> parameter must be single batch oriented input of your workflow</p> </li> <li> <p>additional (non-batch oriented) inputs for your workflow can be passed as parameter to <code>init_with_workflow(...)</code>  method see docs</p> </li> </ul> <p>Note</p> <p>Make sure you have <code>inference</code> or <code>inference-gpu</code> package installed in your Python environment</p>"},{"location":"workflows/modes_of_running/#workflows-in-python-package","title":"Workflows in Python package","text":"<p>Workflows Compiler and Execution Engine are bundled with <code>inference</code> package. Running Workflow directly may be ideal for clients who:</p> <ul> <li> <p>maintain their applications in Python</p> </li> <li> <p>agree for resource-heavy computations directly in their app</p> </li> <li> <p>want to avoid additional latency and errors related to sending HTTP requests</p> </li> <li> <p>expect full control over Workflow execution</p> </li> </ul> <p>In this scenario, you are supposed to provide all required initialisation values for blocks used in your Workflow, what makes this mode most technologically challenging, requiring you to understand handful of topics that we cover in  developer guide.</p> <p>Here you can find example on how to run simple workflow in Python code.</p> <p>Integration in Python</p> <pre><code>from inference.core.env import WORKFLOWS_MAX_CONCURRENT_STEPS\nfrom inference.core.managers.base import ModelManager\nfrom inference.core.workflows.core_steps.common.entities import StepExecutionMode\nfrom inference.core.env import MAX_ACTIVE_MODELS\nfrom inference.core.managers.base import ModelManager\nfrom inference.core.managers.decorators.fixed_size_cache import WithFixedSizeCache\nfrom inference.core.registries.roboflow import RoboflowModelRegistry\nfrom inference.models.utils import ROBOFLOW_MODEL_TYPES\n\n# initialisation of Model registry to manage models load into memory \n# (required by core blocks exposing Roboflow models)\nmodel_registry = RoboflowModelRegistry(ROBOFLOW_MODEL_TYPES)\nmodel_manager = ModelManager(model_registry=model_registry)\nmodel_manager = WithFixedSizeCache(model_manager, max_size=MAX_ACTIVE_MODELS)\n\n# workflow definition\nOBJECT_DETECTION_WORKFLOW = {\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\"type\": \"WorkflowImage\", \"name\": \"image\"},\n        {\"type\": \"WorkflowParameter\", \"name\": \"model_id\"},\n        {\"type\": \"WorkflowParameter\", \"name\": \"confidence\", \"default_value\": 0.3},\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\",\n        }\n    ],\n    \"outputs\": [\n        {\"type\": \"JsonField\", \"name\": \"result\", \"selector\": \"$steps.detection.*\"}\n    ],\n}\n\n# example init paramaters for blocks - dependent on set of blocks \n# used in your workflow\nworkflow_init_parameters = {\n    \"workflows_core.model_manager\": model_manager,\n    \"workflows_core.api_key\": \"&lt;YOUR-API-KEY&gt;,\n    \"workflows_core.step_execution_mode\": StepExecutionMode.LOCAL,\n}\n\n# instance of Execution Engine - init(...) method invocation triggers\n# the compilation process\nexecution_engine = ExecutionEngine.init(\n    workflow_definition=OBJECT_DETECTION_WORKFLOW,\n    init_parameters=workflow_init_parameters,\n    max_concurrent_steps=WORKFLOWS_MAX_CONCURRENT_STEPS,\n)\n\n# runing the workflow\nresult = execution_engine.run(\n    runtime_parameters={\n        \"image\": [&lt;your-image&gt;],\n        \"model_id\": \"yolov8n-640\",\n    }\n)\n</code></pre>"},{"location":"workflows/schema_api/","title":"Workflows Schema API","text":"<p>The Workflows Schema API provides developers with a clear, programmatic understanding of a workflow's structure, inputs, and outputs. It addresses the challenge of programmatically determining a workflow's input requirements and output types.</p>"},{"location":"workflows/schema_api/#purpose-and-benefits","title":"Purpose and Benefits","text":"<p>The API offers structured access to:</p> <ol> <li>Input Parameters: Required workflow inputs.</li> <li>Output Structure: Details of the returned data.</li> <li>Type Hints: Expected data types for inputs and outputs.</li> <li>Schemas of Kinds: Detailed schemas for complex data types.</li> </ol> <p>This enables developers to:</p> <ul> <li>Validate inputs programmatically</li> <li>Understand output data structures</li> <li>Integrate workflows into larger systems</li> <li>Generate documentation or UIs based on workflow requirements</li> </ul>"},{"location":"workflows/schema_api/#using-the-api","title":"Using the API","text":"<pre><code>import requests\n\nWORKSPACE_NAME = \"workspace-name\"\nWORKFLOW_ID = \"workflow-id\"\nINFERENCE_SERVER_URL = \"https://detect.roboflow.com\"\n\nWORKFLOW_SCHEMA_ENDPOINT = f\"{INFERENCE_SERVER_URL}/{WORKSPACE_NAME}/workflows/{WORKFLOW_ID}/describe_interface\"\nROBOFLOW_API_KEY = \"Your Roboflow API Key\"\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n}\n\ndata = {\n    \"api_key\": ROBOFLOW_API_KEY,\n}\n\nres = requests.post(WORKFLOW_SCHEMA_ENDPOINT, headers=headers, json=data)\n\nschema = res.json()\n\ninputs = schema[\"inputs\"]\noutputs = schema[\"outputs\"]\nkinds_schemas = schema[\"kinds_schemas\"]\ntyping_hints = schema[\"typing_hints\"]\n</code></pre>"},{"location":"workflows/schema_api/#inputs-and-outputs","title":"Inputs and Outputs","text":"<p>The <code>inputs</code> and <code>outputs</code> keys show all of the inputs and outputs the workflow expects to run and return.</p>"},{"location":"workflows/schema_api/#typing-hints","title":"Typing Hints","text":"<p>The <code>typing_hints</code> key shows the data types of the inputs and outputs.</p>"},{"location":"workflows/schema_api/#kinds-schemas","title":"Kinds Schemas","text":"<p>The <code>kinds_schemas</code> key returns an OpenAPI specification with more detailed information about the data  types being returned and how to parse them. For example, the <code>object_detection_prediction</code> contains  information about the nested data that will be present.</p>"},{"location":"workflows/testing/","title":"Testing in Workflows","text":"<p>Testing can be challenging when not done properly, which is why we recommend a practical approach  for testing blocks that you create. Since a block is not a standalone element in the ecosystem, testing  might seem complex, but with the right methodology, it becomes manageable.</p> <p>We suggest the following approach when adding a new block:</p> <ul> <li> <p>Unit tests should cover:</p> <ul> <li> <p>Parsing of the manifest, especially when aliases are in use. </p> </li> <li> <p>Utility functions within the block module. If written correctly, these functions should simply  transform input data into output data, making them easy to test.</p> </li> <li> <p>The <code>run(...)</code> method should be tested unit-wise only if assembling the test is straightforward. Otherwise, we recommend focusing on integration tests for Workflow definitions that include the block.</p> </li> <li> <p>Examples can be found here</p> </li> </ul> </li> <li> <p>Integration tests should contain:</p> <ul> <li> <p>practical use cases where the block is used in collaboration with others</p> </li> <li> <p>assertions for results, particularly for model predictions. These assertions should be based on empirical  verification, such as by visualizing and inspecting predictions to ensure they are accurate.</p> </li> <li> <p>When adopting models or inference techniques from external sources (e.g., open-source models),  assertions should confirm that the results are consistent with what you would get outside the Workflows ecosystem,  ensuring compatibility and correctness.</p> </li> <li> <p>Examples can be found here</p> </li> </ul> </li> </ul>"},{"location":"workflows/understanding/","title":"The pillars of Workflows","text":"<p>In the Roboflow Workflows ecosystem, various components work together seamlessly to power your applications.  Some of these elements will be part of your daily interactions, while others operate behind the scenes to ensure  smooth and efficient application performance. The core pillars of the ecosystem include:</p> <ul> <li> <p>Workflows UI: The intuitive interface where you design and manage your workflows.</p> </li> <li> <p>Workflow Definition: An interchangeable format that serves as a program written in the \"workflows\" language,  defining how your workflows operate.</p> </li> <li> <p>Workflows Blocks: Modular components that perform specific tasks within your workflows, organised in plugins  which can be easily created and injected into ecosystem.</p> </li> <li> <p>Workflows Compiler and Execution Engine: The systems that compile your workflow definitions and execute them,  ensuring everything runs smoothly in your environment of choice.</p> </li> </ul> <p>We will explore each of these components, providing you with a foundational understanding to help you navigate and  utilize the full potential of Roboflow Workflows effectively.</p>"},{"location":"workflows/understanding/#workflows-ui","title":"Workflows UI","text":"<p>Traditionally, building machine learning applications involves complex coding and deep technical expertise.  Roboflow Workflows simplifies this process in two key ways: providing pre-built blocks (which will be described later),  and delivering user-friendly GUI. </p> <p>The interface allows you to design applications without needing to write code. You can easily connect components  together and achieve your goals without a deep understanding of Python or the underlying workflow language.</p> <p>Thanks to UI, creating powerful machine learning solutions is straightforward and accessible, allowing you to focus  on innovation rather than intricate programming.</p> <p>While not essential, the UI is a highly valuable component of the Roboflow Workflows ecosystem. At the end of the  workflow creation process it creates workflow definition required for Compiler and Execution engine to run the workflow.</p> <p></p>"},{"location":"workflows/understanding/#workflow-definition","title":"Workflow definition","text":"<p>A workflow definition is essentially a document written in the internal programming language of Roboflow Workflows.  It allows you to separate the design of your workflow from its execution. You can create a workflow definition once  and run it in various environments using the Workflows Compiler and Execution Engine.</p> <p>You have two options for creating a workflow definition: UI to design it visually or write it from scratch  if you\u2019re comfortable with the workflows language. More details on writing definitions manually  are available here. For now, it's important to grasp the role of the definition  within the ecosystem.</p> <p>A workflow definition is in fact JSON document which outlines:</p> <ul> <li> <p>Inputs: These are either images or configuration parameters that influence how the workflow operates.  Instead of hardcoding values, inputs are placeholders that will be replaced with actual data during execution.</p> </li> <li> <p>Steps: These are instances of workflow blocks. Each step takes inputs from either the workflow inputs or the  outputs of previous steps. The sequence and connections between steps determine the execution order.</p> </li> <li> <p>Outputs: specify field names in execution result and reference step outputs. During runtime, referred values are dynamically provide based on results of workflow execution.</p> </li> </ul>"},{"location":"workflows/understanding/#workflow-blocks","title":"Workflow blocks","text":"<p>For users of Roboflow Workflows, blocks are essentially black boxes engineered to perform specific operations.  They act as templates for the steps executed within a workflow, each defining its own set of inputs,  configuration properties, and outputs.</p> <p>When adding a block to your workflow, you need to provide its inputs by referencing either the workflow\u2019s input  or the output of another step. You also specify the values for any required parameters. Once the step is incorporated,  its outputs can be referenced by subsequent steps, allowing for seamless integration and chaining of operations.</p> <p>The creation of blocks is a more advanced topic, which you can explore here.  It\u2019s essential to understand that blocks are grouped in workflow plugins, which are standard Python libraries.  Roboflow offers its own set of plugins, and community members are encouraged to create their own.  The process of importing a plugin into your environment is detailed here.</p> <p>Feel encouraged to explore Workflows blocks prepared by Roboflow.</p>"},{"location":"workflows/understanding/#workflows-compiler-and-execution-engine","title":"Workflows Compiler and Execution Engine","text":"<p>The Compiler and Execution Engine are essential components of the Roboflow Workflows ecosystem, doing the heavy  lifting so you don't have to.</p> <p>Much like a traditional programming compiler or interpreter, these components translate your workflow definition \u2014  a program you create using reusable blocks \u2014 into a format that can be executed by a computer. The workflow definition  acts as a blueprint, with blocks functioning like functions in programming, connected to produce the desired outcomes.</p> <p>Roboflow provides these tools as part of their Inference Server (which can be deployed locally or  accessed via the Roboflow Hosted platform), video processing component,  and Python package, making it easy to run your workflows in  various environments.</p> <p>For a deeper dive into the Compiler and Execution Engine, please refer to our detailed documentation.</p>"},{"location":"workflows/versioning/","title":"Workflows versioning","text":"<p>Understanding the life-cycle of Workflows ecosystem is an important topic,  especially from blocks developer perspective. Those are rules that apply:</p> <ul> <li> <p>Workflows is part of <code>inference</code> - the package itself has a release whenever  any of its component changes and those changes are ready to be published</p> </li> <li> <p>Workflows Execution Engine declares it's version. The game plan is the following:</p> <ul> <li> <p>core of workflows is capable to host multiple versions of Execution Engine -  for instance current stable version and development version</p> </li> <li> <p>stable version is maintained and new features are added until there is  need for new version and the new version is accepted</p> </li> <li> <p>since new version is fully operational, previously stable version starts  being deprecated - there is grace period when old version will be patched  with bug fixes (but new features will not be added), after that it will be left as is. During grace period we will call blocks creators to upgrade  their plugins according to requirements of new version</p> </li> <li> <p>core library only maintains single Execution Engine version for each major - making a promise that features within major will be non-breaking and Workflow  created under version <code>1.0.0</code> will be fully functional under version <code>1.4.3</code> of  Execution Engine</p> </li> </ul> </li> <li> <p>to ensure stability of the ecosystem over time:</p> <ul> <li> <p>Each Workflow Definition declares Execution Engine version it is compatible with.  Since the core library only maintains single version for Execution Engine,  <code>version: 1.1.0</code> in Workflow Definition actually request Execution Engine in version <code>&gt;=1.1.0,&lt;2.0.0</code></p> </li> <li> <p>Each block, in its manifest should provide reasonable Execution Engine compatibility - for instance - if block rely on Execution Engine feature introduced in <code>1.3.7</code> it should  specify <code>&gt;=1.3.7,&lt;2.0.0</code> as compatible versions of Engine</p> </li> </ul> </li> <li> <p>Workflows blocks may be optionally versioned (which we recommend and apply for Roboflow plugins).</p> <ul> <li> <p>we propose the following naming convention for blocks' type identifiers:  <code>{plugin_name}/{block_family_name}@v{X}</code> to ensure good utilisation of blocks identifier  namespace</p> </li> <li> <p>we suggest to only modify specific version of the block if bug-fix is needed,  all other changes to block should yield new version</p> </li> <li> <p>each version of the block is to be submitted into new module (as suggested here) - even for the price of code duplication as we think stability is more important than DRY in this particular case</p> </li> <li> <p>on the similar note, we suggest each block to be as independent as possible,  as code which is shared across blocks, may unintentionally modify other blocks  destroying the stability of your Workflows</p> </li> </ul> </li> </ul>"},{"location":"workflows/workflow_execution/","title":"How Workflow execution looks like?","text":"<p>Workflow execution is a complex subject, but you don\u2019t need to understand every detail to get started effectively.  Grasping some basic concepts can significantly speed up your learning process with the Workflows ecosystem.  This document provides a clear and straightforward overview, designed to help you quickly understand the  fundamentals and build more powerful applications.</p> <p>For those interested in a deeper technical understanding, we invite you to explore the developer guide  for more detailed information.</p>"},{"location":"workflows/workflow_execution/#compilation","title":"Compilation","text":"<p>Workflow execution begins with compiling the Workflow definition. As you know, a Workflow definition is a  JSON document that outlines inputs, steps, outputs, and connections between elements. To turn this document  into an executable format, it must be compiled.</p> <p>From the Execution Engine\u2019s perspective, this process involves creating a computation graph and checking its  integrity and correctness. This verification step is crucial because it helps identify and alert you to errors  early on, making it easier and faster to debug issues. For instance, if you connect incompatible blocks, use an invalid selector, or create a loop in your workflow, the compiler will notify you with error messages. </p> <p>Once the compilation is complete, it means your Workflow is ready to run. This confirms that:</p> <ul> <li> <p>Your Workflow is compatible with the version of the Execution Engine in your environment.</p> </li> <li> <p>All blocks in your Workflow were successfully loaded and initialized.</p> </li> <li> <p>The connections between blocks are valid.</p> </li> <li> <p>The input data you provided for the Workflow has been validated.</p> </li> </ul> <p>At this point, the Execution Engine can begin execution of the Workflow.</p>"},{"location":"workflows/workflow_execution/#data-in-workflow-execution","title":"Data in Workflow execution","text":"<p>When you run a Workflow, you provide input data each time. Just like a function in programming that  can handle different input values, a Workflow can process different pieces of data each time you run it.  Let's see what happens with the data once you trigger Workflow execution. </p> <p>You provide input data substituting inputs' placeholders defined in the Workflow. These placeholders are  referenced by steps of your Workflow using selectors. When a step runs, the actual piece of data you  provided at that moment is used to make the computation. Its outputs can be later used by other steps, based on steps outputs selectors declared in Workflow definition, continuing this process until the Workflow  completes and all outputs are generated.</p> <p>Apart from parameters with fixed values in the Workflow definition, the definition itself does not include  actual data values. It simply tells the Execution Engine how to direct and handle the data you provide as input.</p>"},{"location":"workflows/workflow_execution/#what-is-the-data","title":"What is the data?","text":"<p>Input data in a Workflow can be divided into two types:</p> <ul> <li> <p>Data to be processed: This can be submitted as a batch of data points.</p> </li> <li> <p>Parameters: These are single values used for specific settings or configurations.</p> </li> </ul> <p>To clarify the difference, consider this simple Python function:</p> <p><pre><code>def is_even(number: int) -&gt; bool:\n    return number % 2 == 0\n</code></pre> You use this function like this, providing one number at a time:</p> <pre><code>is_even(number=1)\nis_even(number=2)\nis_even(number=3)\n</code></pre> <p>The situation becomes more complex with machine learning models. Unlike a simple function like <code>is_even(...)</code>,  which processes one number at a time, ML models often handle multiple pieces of data at once. For example,  instead of providing just one image to a classification model, you can usually submit a list of images and  receive predictions for all of them at once.</p> <p>This is different from our <code>is_even(...)</code> function, which would need to be called separately  for each number to get a list of results. The difference comes from how ML models work, especially how  GPUs process data - applying the same operation to many pieces of data simultaneously. </p> <p></p> <p>The <code>is_even(...)</code> function can be adapted to handle batches of data by using a loop, like this:</p> <pre><code>results = []\nfor number in [1, 2, 3, 4]:\n    results.append(is_even(number))\n</code></pre> <p>In Workflows, similar methods are used to handle non-batch-oriented steps facing batch input data. But what if  step expects batch-oriented data and is given singular data point? Let's look at inference process from example classification model:</p> <pre><code>images = [PIL.Image(...), PIL.Image(...), PIL.Image(...), PIL.Image(...)]\nmodel = MyClassificationModel()\n\npredictions = model.infer(images=images, confidence_threshold=0.5)\n</code></pre> <p>As you may imagine, this code has chance to run correctly, as there is substantial difference in meaning of <code>images</code> and <code>confidence_threshold</code> parameter. Former is batch of data to apply single operation (prediction  from a model) and the latter is parameter influencing the processing for all elements in the batch. Virtually,  <code>confidence_threshold</code> gets propagated (broadcast) at each element of <code>images</code> list with the same value,  as if <code>confidence_threshold</code> was the following list: <code>[0.5, 0.5, 0.5, 0.5]</code>.</p> <p>As mentioned earlier, Workflow inputs can be of two types:</p> <ul> <li> <p><code>WorkflowImage</code>: This is similar to the images parameter in our example.</p> </li> <li> <p><code>WorkflowParameters</code>: This works like the confidence_threshold.</p> </li> </ul> <p>When you provide a single image as a <code>WorkflowImage</code> input, it is automatically expanded to form a batch.  If your Workflow definition includes multiple <code>WorkflowImage</code> placeholders, the actual data you provide for  execution must have the same batch size for all these inputs. The only exception is when you submit a  single image; it will be broadcast to fit the batch size requirements of other inputs.</p> <p>Currently, <code>WorkflowImage</code> is the only type of batch-oriented input you can use in Workflows.  This was introduced because the ecosystem started in the Computer Vision field, where images are a key data type.  However, as the field evolves and expands to include multi-modal models (LMMs) and other types of data,  you can expect additional batch-oriented data types to be introduced in the future.</p>"},{"location":"workflows/workflow_execution/#steps-interactions-with-data","title":"Steps interactions with data","text":"<p>If we asked you about the nature of step outputs in these scenarios:</p> <ul> <li> <p>A: The step receives non-batch-oriented parameters as input.</p> </li> <li> <p>B: The step receives batch-oriented data as input.</p> </li> <li> <p>C: The step receives both non-batch-oriented parameters and batch-oriented data as input.</p> </li> </ul> <p>You would likely say:</p> <ul> <li> <p>In option A, the output will be non-batch.</p> </li> <li> <p>In options B and C, the output will be a batch. In option C, the non-batch-oriented parameters will be  broadcast to match the batch size of the data.</p> </li> </ul> <p>And you\u2019d be correct. If you understand that, you probably only have two more concepts to understand before you can comfortably say you understand everything needed to successfully build and run complex Workflows.</p> <p>Let\u2019s say you want to create a Workflow with these steps:</p> <ol> <li> <p>Detect objects in a batch of input images.</p> </li> <li> <p>Crop each detected object from the images.</p> </li> <li> <p>Classify each cropped object with a second model to add detailed labels.</p> </li> </ol> <p>Here\u2019s what happens with the data in the cropping step:</p> <ol> <li> <p>You start with a batch of images, let\u2019s say you have <code>n</code> images.</p> </li> <li> <p>The object detection model finds a different number of objects in each image.</p> </li> <li> <p>The cropping step then creates new images for each detected object, resulting in a new batch of images  for each original image.</p> </li> </ol> <p>So, you end up with a nested list of images, with sizes like <code>[(k[1], ), (k[2], ), ... (k[n])]</code>, where each <code>k[i]</code>  is a batch of images with a variable size based on the number of detections. The second model (classifier) will process these nested batches of cropped images. There is also nothing that stops you from going deeper  in nested batches world.</p> <p>Here\u2019s where it gets tricky, but Execution Engine simplifies this complexity. It manages the nesting of  data virtually, so blocks always receive data in a flattened, non-nested format. This makes it easier to apply  the same block, like an object detection model or classifier, regardless of how deeply nested your data is. But  there is a price - the notion of <code>dimensionality level</code> which dictates which steps may be connected, which not.</p> <p><code>dimensionality level</code> concept refers to the level of nesting of batch. Batch oriented Workflow inputs  have <code>dimensionality level 1</code>, crops that we described in our example have <code>dimensionality level 2</code> and so on. What matters from the perspective of plugging inputs to specific step is:</p> <ul> <li> <p>the difference in <code>dimensionality level</code> across step inputs</p> </li> <li> <p>the impact of step on <code>dimensionality level</code> of output (step may decrease, keep the same or increase dimensionality)</p> </li> </ul> <p>Majority of blocks are designed to work with inputs at the same dimensionality level, not changing dimensionality of its outputs, with some being exceptions to that rule. In our example, predictions from object-detection model occupy <code>dimensionality level 1</code>, while classification results are at <code>dimensionality level 2</code>, due to the fact that cropping step introduced new, dynamic level of dimensionality.</p> <p>Now, if you can find a block that accepts both object detection predictions and classification predictions, you could use our predictions together only if block specifies explicitly it accepts such combination of <code>dimensionality levels</code>,  otherwise you would end up seeing compilation error. Hopefully, there is a block you could use in this context. </p> <p></p> <p>Detections Classes Replacement block is designed to substitute bounding boxes classes labels with predictions from classification model performed at crops of original image with respect to bounding boxes predicted by first model.</p> <p>Warning</p> <p>We are working hard to change it, but so far the Workflow UI in Roboflow APP is not capable of displaying the  concept of <code>dimensionality level</code>. We know that it is suboptimal from UX perspective and very confusing but we must ask for patience until this situation gets better.</p> <p>Note</p> <p>Workflows Compiler keeps track of <code>data lineage</code> in Workflow definition, making it impossible to mix  together data at higher <code>dimensionality levels</code> that do not come from the same origin. This concept is  described in details in developer guide. From the user perspective it is important to understand that if image is cropped based on predictions from different models (or even the same model, using cropping step twice),  cropping outputs despite being at the same dimensionality level cannot be used as inputs to the same step.</p>"},{"location":"workflows/workflow_execution/#conditional-execution","title":"Conditional execution","text":"<p>Let\u2019s be honest\u2014programmers love branching, and for good reason. It\u2019s a common and useful construct in  programming languages.</p> <p>For example, it\u2019s easy to understand what\u2019s happening in this code:</p> <pre><code>def is_string_lower_cased(my_string: str) -&gt; str:\n    if my_string.lower() == my_string:\n        return \"String was lower-cased\"\n    return \"String was not lower-cased\"\n</code></pre> <p>But what about this code?</p> <pre><code>def is_string_lower_cased_batched(my_string: Batch[str]) -&gt; str:\n    pass\n</code></pre> <p>In this case, it\u2019s not immediately clear how branching would work with a batch of strings.  The concept of handling decisions for a single item is straightforward, but when working with batches,  the logic needs to account for multiple inputs at once. The problem arises due to the fact that independent decision must be made for each element of batch - which may lead to different execution branches for  different elements of a batch. In such simplistic example as provided it can be easily addressed:</p> <pre><code>def is_string_lower_cased_batched(my_string: Batch[str]) -&gt; Batch[str]:\n    result = []\n    for element in my_string:\n        if element.lower() == my_string:\n            result.append(\"String was lower-cased\")\n        else:\n            result.append(\"String was not lower-cased\") \n    return result\n</code></pre> <p>In Workflows, however we want blocks to decide where execution goes, not implement conditional statements inside block body and return merged results. This is why whole mechanism of conditional execution  emerged in Workflows Execution engine. This concept is important and has its own technical depth, but from  user perspective there are few things important to understand:</p> <ul> <li> <p>some Workflows blocks can impact execution flow - steps made out of those blocks will be specified a bunch  of step selectors, dictating possible next steps to be decided for each element of batch (non-batch oriented steps work as traditional if-else statements in programming)</p> </li> <li> <p>once data element is discarded from batch by conditional execution, it will be hidden from all  affected steps down the processing path and denoted in outputs as <code>None</code></p> </li> <li> <p>multiple flow-control steps may affect single next step, union of conditional execution masks will be created and dynamically applied</p> </li> <li> <p>step may be not executed if there is no inputs to the step left after conditional execution logic evaluation</p> </li> <li> <p>there are special blocks capable of merging alternative execution branches, such that data from that branches can be referred by single selector (for instance to build outputs). Example of such block is  <code>First Non Empty Or Default</code> - which collapses execution branches taking first value encountered or defaulting to specified value if no value spotted</p> </li> <li> <p>conditional execution usually impacts Workflow outputs - all values that are affected by branching are in  fact optional (if special blocks filling empty values are not used) and nested results may not be filled with data,  leaving empty (potentially nested) lists in results - see details  in section describing output construction.</p> </li> </ul>"},{"location":"workflows/workflow_execution/#output-construction","title":"Output construction","text":"<p>The most important thing to understand is that a Workflow's output is aligned with its input regarding  batch elements order. This means the output will always be a list of dictionaries, with each dictionary  corresponding to an item in the input batch. This structure makes it easier to parse results and handle  them iteratively, matching the outputs to the inputs.</p> <pre><code>input_images = [...]\nworkflow_results = execution_engine.run(\n    runtime_parameters={\"images\": input_images}\n)\n\nfor image, result in zip(input_images, workflow_results):\n    pass\n</code></pre> <p>Each element of the list is a dictionary with keys specified in Workflow definition via declaration like:</p> <pre><code>{\"type\": \"JsonField\", \"name\": \"predictions\", \"selector\": \"$steps.detection.predictions\"}\n</code></pre> <p>what you may expect as a value under those keys, however, is dependent on the structure of the workflow.  All non-batch results got broadcast and placed in each and every output dictionary with the same value.  Elements at <code>dimensionality level 1</code> will be distributed evenly, with values in each dictionary corresponding  to the alignment of input data (predictions for input image 3, will be placed in third dictionary). Elements at  higher <code>dimensionality levels</code> will be embedded into lists of objects of types specific to the step output  being referred. </p> <p>For example, let's consider again our example with object-detection model, crops and secondary classification model. Assuming that predictions from object detection model are registered in the output under the name  <code>\"object_detection_predictions\"</code> and results of classifier are registered as <code>\"classifier_predictions\"</code>, you  may expect following output once three images are submitted as input for Workflow execution:</p> <pre><code>[\n  {\n    \"object_detection_predictions\": \"here sv.Detections object with 2 bounding boxes\",\n    \"classifier_predictions\": [\n      {\"classifier_prediction\":  \"for first crop\"},\n      {\"classifier_prediction\":  \"for second crop\"}\n    ]\n  },\n  {\n    \"object_detection_predictions\": \"empty sv.Detections\",\n    \"classifier_predictions\": []\n  },\n  {\n    \"object_detection_predictions\": \"here sv.Detections object with 3 bounding boxes\",\n    \"classifier_predictions\": [\n      {\"classifier_prediction\":  \"for first crop\"},\n      {\"classifier_prediction\":  \"for second crop\"},\n      {\"classifier_prediction\":  \"for third crop\"}\n    ]\n  }\n]\n</code></pre> <p>As you can see, <code>\"classifier_predictions\"</code> field is populated with list of results, of size equivalent to number  of bounding boxes for <code>\"object_detection_predictions\"</code>. </p> <p>Interestingly, if our workflows has ContinueIf block that only runs cropping and classifier if number of bounding boxes is different from two - it will turn <code>classifier_predictions</code> in first dictionary into empty list. If conditional  execution excludes steps at higher <code>dimensionality levels</code> from producing outputs as a side effect of execution -  output field selecting that values will be presented as nested list of empty lists, with depth matching <code>dimensionality level - 1</code> of referred output.</p> <p>Some outputs would require serialisation when Workflows Execution Engine runs behind HTTP API. We use the following serialisation strategies:</p> <ul> <li> <p>images got serialised into <code>base64</code></p> </li> <li> <p>numpy arrays are serialised into lists</p> </li> <li> <p>sv.Detections are serialised into <code>inference</code> format which can be decoded on the other end of the wire using  <code>sv.Detections.from_inference(...)</code></p> </li> </ul> <p>Note</p> <p>sv.Detections, which is our standard representation of detection-based predictions is treated specially  by output constructor. <code>JsonField</code> output definition can specify optionally <code>coordinates_system</code> property, which may enforce translation of detection coordinates into coordinates system of parent image in workflow. See more in docs page describing outputs definitions</p>"},{"location":"workflows/workflows_compiler/","title":"Compilation of Workflow Definition","text":"<p>Compilation is a process that takes a document written in a programming language, checks its correctness,  and transforms it into a format that the execution environment can understand.</p> <p>A similar process happens in the Workflows ecosystem whenever you want to run a Workflow Definition.  The Workflows Compiler performs several steps to transform a JSON document into a computation graph, which  is then executed by the Workflows Execution Engine. While this process can be complex, understanding it can  be helpful for developers contributing to the ecosystem. In this document, we outline key details of the  compilation process to assist in building Workflow blocks and encourage contributions to the core Execution Engine.</p> <p>Note</p> <p>This document covers the design of Execution Engine <code>v1</code> (which is current stable version). Please  acknowledge information about versioning to understand Execution Engine  development cycle.</p>"},{"location":"workflows/workflows_compiler/#stages-of-compilation","title":"Stages of compilation","text":"<p>Workflow compilation involves several stages, including:</p> <ol> <li> <p>Loading available blocks: Gathering all the blocks that can be used in the workflow based on  configuration of execution environment</p> </li> <li> <p>Compiling dynamic blocks: Turning dynamic blocks definitions into  standard Workflow Blocks</p> </li> <li> <p>Parsing the Workflow Definition: Reading and interpreting the JSON document that defines the workflow, detecting  syntax errors</p> </li> <li> <p>Building Workflow Execution Graph: Creating a graph that defines how data will flow through the workflow  during execution and verifying Workflow integrity</p> </li> <li> <p>Initializing Workflow steps from blocks: Setting up the individual workflow steps based on the available blocks,  steps definitions and configuration of execution environment.</p> </li> </ol> <p>Let's take a closer look at each of the workflow compilation steps.</p>"},{"location":"workflows/workflows_compiler/#workflows-blocks-loading","title":"Workflows blocks loading","text":"<p>As described in the blocks bundling guide, a group of Workflow blocks can be packaged  into a workflow plugin. A plugin is essentially a standard Python library that, in its main module, exposes specific  functions allowing Workflow Blocks to be dynamically loaded.</p> <p>The Workflows Compiler and Execution Engine are designed to be independent of specific Workflow Blocks, and the  Compiler has the ability to discover and load blocks from plugins.</p> <p>Roboflow provides the <code>roboflow_core</code> plugin, which includes a set of basic Workflow Blocks that are always  loaded by the Compiler, as both the Compiler and these blocks are bundled in the <code>inference</code> package.</p> <p>For custom plugins, once they are installed in the Python environment, they need to be referenced using an environment  variable called <code>WORKFLOWS_PLUGINS</code>. This variable should contain the names of the Python packages that contain the  plugins, separated by commas. </p> <p>For example, if you have two custom plugins, <code>numpy_plugin</code> and <code>pandas_plugin</code>, you can enable them in  your Workflows environment by setting: <pre><code>export WORKFLOWS_PLUGINS=\"numpy_plugin,pandas_plugin\"\n</code></pre></p> <p>Both <code>numpy_plugin</code> and <code>pandas_plugin</code> are not paths to library repositories, but rather names of the main modules of libraries shipping plugins (<code>import numpy_plugin</code> must work in your  Python environment for the plugin to be possible to be loaded).</p> <p>Once Compiler loads all plugins it is ready for the next stage of compilation.</p>"},{"location":"workflows/workflows_compiler/#compilation-of-dynamic-blocks","title":"Compilation of dynamic blocks","text":"<p>Note</p> <p>The topic of dynamic Python blocks is covered in separate docs page. To unerstand the content of this section you only need to know that there is a way to define Workflow Blocks in-place in Workflow Definition - specifying both block manifest and Python code in JSON document. This functionality only works if you run Workflows Execution Engine on your hardware and is disabled ad Roboflow hosted platform.</p> <p>The Workflows Compiler can transform Dynamic Python Blocks, defined directly in a Workflow Definition, into  full-fledged Workflow Blocks at runtime. The Compiler generates these block classes dynamically based on the  block's definition, eliminating the need for developers to manually create them as they would in a plugin.</p> <p>Once this process is complete, the dynamic blocks are added to the pool of available Workflow Blocks. These blocks  can then be used in the <code>steps</code> section of your Workflow Definition, just like any other standard block.</p>"},{"location":"workflows/workflows_compiler/#parsing-workflow-definition","title":"Parsing Workflow Definition","text":"<p>Once all Workflow Blocks are loaded, the Compiler retrieves the manifest classes for each block.  These manifests are <code>pydantic</code> data classes that define the structure of step entries in definition. At parsing stage, errors with Workflows Definition are alerted, for example:</p> <ul> <li> <p>usage of non-existing blocks</p> </li> <li> <p>invalid configuration of steps</p> </li> <li> <p>lack of required parameters for steps</p> </li> </ul> <p>Thanks to <code>pydantic</code>, the Workflows Compiler doesn't need its own parser. Additionally, blocks creators use standard  Python library to define block manifests.</p>"},{"location":"workflows/workflows_compiler/#building-workflow-execution-graph","title":"Building Workflow Execution Graph","text":"<p>Building the Workflow Execution graph is the most critical stage of Workflow compilation.  Here's how it works:</p>"},{"location":"workflows/workflows_compiler/#adding-vertices","title":"Adding Vertices","text":"<p>First, each input, step and output are added as vertices in the graph, with each vertex given a special label  for future identification. These vertices also include metadata, like marking input vertices with seeds for data  lineage tracking (more on this later).</p>"},{"location":"workflows/workflows_compiler/#adding-edges","title":"Adding Edges","text":"<p>After placing the vertices, the next step is to create edges between them based on the selectors defined in  the Workflow. The Compiler examines the block manifests to determine which properties can accept selectors  and the expected \"kind\" of those selectors. This enables the Compiler to detect errors in the Workflow  definition, such as:</p> <ul> <li> <p>Providing an output kind from one step that doesn't match the expected input kind of the next step.</p> </li> <li> <p>Referring to non-existent steps or inputs.</p> </li> </ul> <p>Each edge also contains metadata indicating which input property is being fed by the output data, which is  helpful at later stages of compilation and during execution</p> <p>Note</p> <p>Normally, step inputs \"request\" data from step outputs, forming an edge from Step A's output to Step B's input  during Step B's processing. However, control-flow blocks are an exception,  as they both accept data and declare other steps in the manifest, creating a special flow-control edge in the graph.</p>"},{"location":"workflows/workflows_compiler/#structural-validation","title":"Structural Validation","text":"<p>Once the graph is constructed, the Compiler checks for structural issues like cycles to ensure the graph can be  executed properly.</p>"},{"location":"workflows/workflows_compiler/#data-lineage-verification","title":"Data Lineage verification","text":"<p>Finally, data lineage properties are populated from input nodes and carried through the graph. So, what is  data lineage? Lineage is a list of identifiers that track the creation and nesting of batches through the steps,  determining:</p> <ul> <li> <p>the source path of data</p> </li> <li> <p><code>dimensionality level</code> of data</p> </li> <li> <p>compatibility of different pieces of data that may be referred by a step - ensuring that step will only  take corresponding batches elements from multiple sources (such that batch element index <code>example: (1, 2)</code> refers to the exact same piece of data when two batch-oriented inputs are connected into the step and not to some randomly  provided batches with different lineage that does not make sense to process together)</p> </li> </ul> <p>Each time a new nested batch is created by a step, a unique identifier is added to the lineage of the output.  This allows the Compiler to track and verify if the inputs across steps are compatible.</p> <p>Note</p> <p>Fundamental assumption of data lineage is that all batch-oriented inputs are granted the same lineage identifier - so implicitly it enforces all input batches to be fed with  data that has corresponding data-points at corresponding positions in batches. For instance,  if your Workflow compares <code>image_1</code> to <code>image_2</code> (and you declare those two inputs in Wofklow Definition), the Compiler assumes the elements of <code>image_1[3]</code> to correspond with <code>image_2[3]</code>.</p> <p>Thanks to lineage tracking, the Compiler can detect potential mistakes. For example, if you attempt to connect two  dynamic crop outputs to a single step's inputs, the Compiler will notice that the number of crops in each  output may not match. This would result in nested batch elements with mismatched indices, which could lead to  unpredictable results during execution if the situation is not prevented.</p> <p>Example of lineage missmatch</p> <p>Imagine the following scenario:</p> <ul> <li> <p>you declare single image input in your Workflow</p> </li> <li> <p>at first you perform object detection using two different models</p> </li> <li> <p>you use two dynamic crop steps - to crop based on first and second model predictions  respectivelly</p> </li> <li> <p>now you want to use block to compare two images features (using classical Compute Vision methods)</p> </li> </ul> <p>What would you expect to happen when you plug inputs from those two crop steps into comparison block?</p> <ul> <li> <p>Without tracing the lineage you would \"flatten\" and \"zip\" those two batches and pass pairs of images to comparison block - the problem is that in this case you cannot  determine if the comparisons between those elements actually makes sense - probably do not!</p> </li> <li> <p>With lineage tracing - Compiler knows that you attempt to feed two batches with lineages that do not match regarding last nesting level and raises compilation error.</p> </li> </ul> <p>One may ask - \"ok, but maybe I would like to apply secondary classifier on both crops and  merge results at the end to get all results in single output - is that possible?\". The answer is yes - as mentioned above, nested batches differ only at the last lineage level - so when we use  some blocks from \"dimensionality collapse\" category - we will align the results of secondary classifiers  into batches at <code>dimensionality level</code> 1 with matching lineage.</p> <p>As outlined in the section dedicated to blocks development, each block can define  the expected dimensionality of its inputs and outputs. This refers to how the data should be structured.  For example, if a block needs an <code>image</code> input that's one level above a batch of <code>predictions</code>, the Compiler will  check that this requirement is met when verifying the Workflow step. If the connections between steps don\u2019t match  the expected dimensionality, an error will occur. Additionally, each input is also verified to ensure it is compatible  based on data lineage. Once the step passes validation, the output dimensionality is determined and will be used to  check compatibility with subsequent steps.</p> <p>It\u2019s important to note that blocks define dimensionality requirements in relative terms, not absolute. This means  a block specifies the difference (or offset) in dimensionality between its inputs and outputs. This approach allows  blocks to work flexibly at any dimensionality level.</p> <p>Note</p> <p>In version 1, the Workflows Compiler only supports blocks that work across two different <code>dimensionality levels</code>.  This was done to keep the design straightforward. If there's a need for blocks that handle more  <code>dimensionality levels</code> in the future, we will consider expanding this support.</p>"},{"location":"workflows/workflows_compiler/#denoting-flow-control","title":"Denoting flow-control","text":"<p>The Workflows Compiler helps the Execution Engine manage flow-control structures in workflows. It marks specific  attributes that allow the system to understand how flow-control impacts building inputs for certain steps and the  execution of the workflow graph (for more details, see the  Execution Engine docs).</p> <p>To ensure the workflow structure is correct, the Compiler checks data lineage for flow-control steps in a  similar way as described in the section on data-lineage verification.</p> <p>The Compiler assumes flow-control steps can affect other steps if:</p> <ul> <li> <p>The flow-control step operates on non-batch-oriented inputs - in this case, the flow-control step can  either allow or prevent the connected step (and related steps) from running entirely, even if the input  is a batch of data - all batch elements are affected.</p> </li> <li> <p>The flow-control step operates on batch-oriented inputs with compatible lineage - here, the flow-control step  can decide separately for each element in the batch which ones will proceed and which ones will be stopped.</p> </li> </ul>"},{"location":"workflows/workflows_compiler/#initializing-workflow-steps-from-blocks","title":"Initializing Workflow steps from blocks","text":"<p>The documentation often refers to a Workflow Step as an instance of a Workflow Block, which serves as its prototype.  To put it simply, a Workflow Block is a class that implements specific behavior, which can be customized by  configuration\u2014whether it's set by the environment running the Execution Engine, the Workflow definition,  or inputs at runtime.</p> <p>In programming, we create an instance of a class using a constructor, usually requiring initialization parameters.  One the same note, Workflow Blocks are initialized by the Workflows Compiler whenever a step in the Workflow  references that block. Some blocks may need specific initialization parameters, while others won't.</p> <p>When a block requires initialization parameters:</p> <ul> <li> <p>The block must declare the parameters it needs, as explained in detail in  the blocks development guide</p> </li> <li> <p>The values for these parameters must be provided from the environment where the Workflow is being executed.</p> </li> <li> <p>The values for these parameters must be provided from the environment where the Workflow is being executed.</p> </li> </ul> <p>This second part might seem tricky, so let\u2019s look at an example. In the in user guide, under the section showing how to integrate with Workflows using the <code>inference</code> Python package, you might come  across code like this:</p> <pre><code>[...]\n# example init paramaters for blocks - dependent on set of blocks \n# used in your workflow\nworkflow_init_parameters = {\n    \"workflows_core.model_manager\": model_manager,\n    \"workflows_core.api_key\": \"&lt;YOUR-API-KEY&gt;,\n    \"workflows_core.step_execution_mode\": StepExecutionMode.LOCAL,\n}\n\n# instance of Execution Engine - init(...) method invocation triggers\n# the compilation process\nexecution_engine = ExecutionEngine.init(\n    ...,\n    init_parameters=workflow_init_parameters,\n    ...,\n)\n[...]\n</code></pre> <p>In this example, <code>workflow_init_parameters contains</code> values that the Compiler uses when  initializing Workflow steps based on block requests.</p> <p>Initialization parameters (often called \"init parameters\") can be passed to the Compiler in two ways:</p> <ul> <li> <p>Explicitly: You provide specific values (numbers, strings, objects, etc.).</p> </li> <li> <p>Implicitly: Default values are defined within the Workflows plugin, which can either be specific values or  functions (taking no parameters) that generate values dynamically, such as from environmental variables.</p> </li> </ul> <p>The dictionary <code>workflow_init_parameters</code> shows explicitly passed init parameters. The structure of the keys  is important: <code>{plugin_name}.{init_parameter_name}</code>. You can also specify just <code>{init_parameter_name}</code>, but this  changes how parameters are resolved.</p>"},{"location":"workflows/workflows_compiler/#how-parameters-are-resolved","title":"How Parameters Are Resolved?","text":"<p>When the Compiler looks for a block\u2019s required init parameter, it follows this process:</p> <ol> <li> <p>Exact Match: It first checks the explicitly provided parameters for an exact match to  <code>{plugin_name}.{init_parameter_name}</code>.</p> </li> <li> <p>Default Parameters: If no match is found, it checks the plugin\u2019s default parameters.</p> </li> <li> <p>General Match: Finally, it looks for a general match with just <code>{init_parameter_name}</code> in the explicitly  provided parameters.</p> </li> </ol> <p>This mechanism allows flexibility, as some block parameters can have default values while others must be  provided explicitly. Additionally, it lets certain parameters be shared across different plugins.</p>"},{"location":"workflows/workflows_execution_engine/","title":"Workflows Execution Engine in details","text":"<p>The compilation process creates a Workflow Execution graph, which  holds all the necessary details to run a Workflow definition. In this section, we'll explain the details  of the execution process.</p> <p>At a high level, the process does the following:</p> <ol> <li> <p>Validates runtime input: it checks that all required placeholders from the Workflow definition are filled  with data and ensures the data types are correct.</p> </li> <li> <p>Determines execution order: it defines the order in which the steps are executed.</p> </li> <li> <p>Prepares step inputs and caches outputs: it organizes the inputs for each step and saves the outputs  for future use.</p> </li> <li> <p>Builds the final Workflow outputs: it assembles the overall result of the Workflow.</p> </li> </ol>"},{"location":"workflows/workflows_execution_engine/#validation-of-runtime-input","title":"Validation of runtime input","text":"<p>The Workflow definition specifies the expected inputs for Workflow execution. As discussed  earlier, inputs can be either batch-oriented data to be processed by steps or parameters that  configure the step execution. This distinction is crucial to how the Workflow runs and will be explored throughout  this page.</p> <p>Starting with input validation, the Execution Engine has a dedicated component that parses and prepares the input  for use. It recognizes batch-oriented inputs from the Workflow definition and converts them into an internal  representation (e.g., <code>WorkflowImage</code> becomes <code>Batch[WorkflowImageData]</code>). This allows block developers to easily  work with the data. Non-batch-oriented parameters are checked for type consistency against the block manifests  used to create steps that require those parameters. This ensures that type errors are caught early in the  execution process.</p> <p>Note</p> <p>All batch-oriented inputs must have a size of either 1 or n. When a batch contains only a single element, it is  automatically broadcasted across the entire batch.</p>"},{"location":"workflows/workflows_execution_engine/#determining-execution-order","title":"Determining execution order","text":"<p>The Workflow Execution Graph is a directed acyclic graph (DAG),  which allows us to determine the topological order. Topological order refers to a sequence in which Workflow steps are  executed, ensuring that each step's dependencies are met before it runs. In other words, if a step relies on the output  of another step, the Workflow Engine ensures that the dependency step is executed first.</p> <p>Additionally, the topological structure allows us to identify which steps can be executed in parallel without causing  race conditions. Parallel execution is the default mode in the Workflows Execution Engine. This means that multiple  independent steps, such as those used in model ensembling can run simultaneously, resulting in significant  improvements in execution speed compared to sequential processing.</p> <p>Warning</p> <p>Due to the parallel execution mode in the Execution Engine (and to avoid unnecessary data copying when passing  it to each step), we strongly urge all block developers to avoid mutating any data passed to the block's <code>run(...)</code>  method. If modifications are necessary, always make a copy of the input object before making changes!</p>"},{"location":"workflows/workflows_execution_engine/#handling-step-inputs-and-outputs","title":"Handling step inputs and outputs","text":"<p>Handling step inputs and outputs is a complex task for the Execution Engine. This involves:</p> <ul> <li> <p>Differentiating between SIMD (Single Instruction, Multiple Data) and non-SIMD blocks in relation to their inputs.</p> </li> <li> <p>Preparing step inputs while considering conditional execution and the expected input dimensionality.</p> </li> <li> <p>Managing outputs from steps that control the flow of execution.</p> </li> <li> <p>Registering outputs from data-processing steps, ensuring they match the output dimensionality declared by the blocks.</p> </li> </ul> <p>Let\u2019s explore each of these topics in detail.</p>"},{"location":"workflows/workflows_execution_engine/#simd-vs-non-simd-steps","title":"SIMD vs non-SIMD steps","text":"<p>As the definition suggests, a SIMD (Single Instruction, Multiple Data) step processes batch-oriented data, where the  same operation is applied to each data point, potentially using non-batch-oriented parameters for configuration.  The output from such a step is expected to be a batch of elements, preserving the order of the input batch elements.  This applies to both regular processing steps and flow-control steps (see  blocks development guide), where flow-control decisions  affect each batch element individually.</p> <p>In essence, the type of data fed into the step determines whether it's SIMD or non-SIMD. If a step requests any  batch-oriented input, it will be treated as a SIMD step.</p> <p>Non-SIMD steps, by contrast, are expected to deliver a single result for the input data. In the case of non-SIMD  flow-control steps, they affect all downstream steps as a whole, rather than individually for each element in a batch.</p>"},{"location":"workflows/workflows_execution_engine/#preparing-step-inputs","title":"Preparing step inputs","text":"<p>Each requested input element may be batch-oriented or not. Non-batch inputs are relatively easy,  they do not require special treatment. With batch-oriented ones, there is a lot more of a hustle. Execution Engine maintains indices for each batch-oriented datapoints, for instance:</p> <ul> <li> <p>if there is input images batch, each element will achieve their own unique index - let's say there is four input images, the batch indices will be <code>[(0, ), (1, ), (2, ), (3, )]</code>.</p> </li> <li> <p>step output being not-nested batch will also be indexed, for instance predictions from a model for each of image mentioned above will also be indexed <code>[(0, ), (1, ), (2, ), (3, )]</code>.</p> </li> <li> <p>having a block that increases <code>dimensionality level</code> - let's say a Dynamic Crop based on predictions from object-detection model - having 2 crops for first image, 1 for second and three for fourth - output of such step will be indexed in the following way: <code>[(0, 0), (0, 1), (1, 0), (3, 0), (3, 1), (3, 2)]</code>.</p> </li> </ul> <p>Indexing of elements is important while gathering inputs for steps execution. Thanks to them, all batch oriented  inputs may be aligned - such that Execution Engine will always ship prediction <code>(3, )</code> with image <code>(3, )</code> and  crops batch of crops <code>[(3, 0), (3, 1), (3, 2)]</code> when any step requests it. </p> <p>Each requested input element can either be batch-oriented or non-batch. Non-batch inputs are straightforward and don't  require special handling. However, batch-oriented inputs involve more complexity. The Execution Engine tracks indices  for each batch-oriented data point. For example:</p> <ul> <li> <p>If there's a batch of input images, each element receives its own unique index. For a batch of four images, the  indices would be <code>[(0,), (1,), (2,), (3,)]</code>.</p> </li> <li> <p>A step output which do not increase dimensionality will also be indexed similarly. For example, model predictions  for each of the four images would have indices <code>[(0,), (1,), (2,), (3,)]</code>.</p> </li> <li> <p>If a block increases the <code>dimensionality_level</code> (e.g., a dynamic crop based on predictions from an object  detection model), the output will be indexed differently. Suppose there are 2 crops for the first image,  1 for the second, and 3 for the fourth. The indices for this output would be  <code>[(0, 0), (0, 1), (1, 0), (3, 0), (3, 1), (3, 2)]</code>.</p> </li> </ul> <p>Indexing is crucial for aligning inputs during step execution. The Execution Engine ensures that all batch-oriented  inputs are synchronized. For example, it will match prediction <code>(3,)</code> with image <code>(3,)</code> and the corresponding batch  of crops <code>[(3, 0), (3, 1), (3, 2)]</code> when a step requests them.</p> <p>Note</p> <p>Keeping data lineage in order during compilation simplifies execution. The Execution Engine doesn't need to  verify if dynamically created nested batches come from the same source. Its job is to align indices when  preparing step inputs.</p>"},{"location":"workflows/workflows_execution_engine/#additional-considerations","title":"Additional Considerations","text":""},{"location":"workflows/workflows_execution_engine/#input-dimensionality-offsets","title":"Input Dimensionality Offsets","text":"<p>Workflow blocks define how input dimensionality is handled.  If the Execution Engine detects a difference in input dimensionality, it will wrap the larger dimension into a batch.  For example, if a block processes both input images and dynamically cropped images, the latter will be wrapped into a  batch so that each top-level image is processed with its corresponding batch of crops.</p>"},{"location":"workflows/workflows_execution_engine/#deeply-nested-batches-are-flattened-before-step-execution","title":"Deeply nested batches are flattened before step execution","text":"<p>Given the block defines all input at the same dimensionality level, no matter how deep the nesting of input batches is,  step input will be flattened to a single batch and indices in the outputs will be automagically maintained by  Execution Engine.</p>"},{"location":"workflows/workflows_execution_engine/#conditional-execution","title":"Conditional Execution","text":"<p>Flow-control blocks manage which steps should be executed based on certain conditions. During compilation,  steps affected by these conditions are flagged. When constructing their inputs, a mask for flow-control exclusion  (both SIMD- and non-SIMD-oriented) is applied. Based on this mask, specific input elements will be replaced with  <code>None</code>, representing an empty value.</p> <p>By default, blocks don't accept empty values, so any <code>None</code> at index <code>(i,)</code> in a batch will cause that index to be  excluded from processing. This is how flow control is managed within the Execution Engine. Some blocks, however,  are designed to handle empty inputs. In such cases, while the flow-control mask will be applied, empty inputs  won't be eliminated from the input batch.</p>"},{"location":"workflows/workflows_execution_engine/#batch-processing-mode","title":"Batch Processing Mode","text":"<p>Blocks may either process batch inputs all at once or, by default, require the Execution Engine to loop over each  input and repeatedly invoke the block's <code>run(...)</code> method.</p>"},{"location":"workflows/workflows_execution_engine/#managing-flow-control-steps-outputs","title":"Managing flow-control steps outputs","text":"<p>The outputs of flow-control steps are unique because these steps determine which data points should be  passed to subsequent steps which is roughly similar to outcome of this pseudocode:</p> <pre><code>if condition(A):\n    step_1(A)\n    step_2(A)\nelse:\n    step_3(A)\n</code></pre> <p>The Workflows Execution Engine parses the outputs from flow-control steps and creates execution branches.  Each branch has an associated mask:</p> <ul> <li> <p>For SIMD branches, the mask contains a set of indices that will remain active for processing.</p> </li> <li> <p>For non-SIMD branches, the mask is a simple <code>True</code> / <code>False</code> value that determines whether the entire  branch is active.</p> </li> </ul> <p>After a flow-control step executes, this mask is registered and applied to any steps affected by the decision.  This allows the Engine to filter out specific data points from processing in the downstream branch.  If a data point is excluded from the first step in a branch (due to the masking), that data point is automatically  eliminated from the entire branch (as a result of exclusion of empty inputs by default).</p>"},{"location":"workflows/workflows_execution_engine/#caching-steps-outputs","title":"Caching steps outputs","text":"<p>It's not just the outcomes of flow-control steps that need to be managed carefully\u2014data processing steps also require  attention to ensure their results are correctly passed to other steps. The key aspect here is properly indexing  the outputs.</p> <p>In simple cases where all inputs share the same <code>dimensionality level</code> and the output maintains that same  dimensionality, the Execution Engine's main task is to preserve the order of input indices. However, when input  dimensionalities differ, the Workflow block used to create the step determines how indexing should be handled.</p> <p>If the dimensionality changes during processing, the Execution Engine either uses the high-level index or creates  nested dimensions dynamically based on the length of element lists in the output. This ensures proper alignment and  tracking of data across steps.</p>"},{"location":"workflows/workflows_execution_engine/#building-workflow-outputs","title":"Building Workflow outputs","text":"<p>For details on how outputs are constructed, please refer to the information provided on the  Workflows Definitions page and the  Output Construction section of the Workflow Execution  documentation.</p>"},{"location":"workflows/blocks/absolute_static_crop/","title":"Absolute Static Crop","text":""},{"location":"workflows/blocks/absolute_static_crop/#version-v1","title":"Version <code>v1</code>","text":"<p>Crop a Region of Interest (RoI) from an image, using absolute coordinates.</p> <p>This is useful when placed after an ObjectDetection block as part of a multi-stage  workflow. For example, you could use an ObjectDetection block to detect objects, then  the AbsoluteStaticCrop block to crop objects, then an OCR block to run character  recognition on each of the individual cropped regions.</p>"},{"location":"workflows/blocks/absolute_static_crop/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/absolute_static_crop@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/absolute_static_crop/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>x_center</code> <code>int</code> Center X of static crop (absolute coordinate). \u2705 <code>y_center</code> <code>int</code> Center Y of static crop (absolute coordinate). \u2705 <code>width</code> <code>int</code> Width of static crop (absolute value). \u2705 <code>height</code> <code>int</code> Height of static crop (absolute value). \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/absolute_static_crop/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Absolute Static Crop</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Absolute Static Crop</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>x_center</code> (<code>integer</code>): Center X of static crop (absolute coordinate).</li> <li><code>y_center</code> (<code>integer</code>): Center Y of static crop (absolute coordinate).</li> <li><code>width</code> (<code>integer</code>): Width of static crop (absolute value).</li> <li><code>height</code> (<code>integer</code>): Height of static crop (absolute value).</li> </ul> </li> <li> <p>output</p> <ul> <li><code>crops</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Absolute Static Crop</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/absolute_static_crop@v1\",\n    \"images\": \"$inputs.image\",\n    \"x_center\": 40,\n    \"y_center\": 40,\n    \"width\": 40,\n    \"height\": 40\n}\n</code></pre>"},{"location":"workflows/blocks/anthropic_claude/","title":"Anthropic Claude","text":""},{"location":"workflows/blocks/anthropic_claude/#version-v1","title":"Version <code>v1</code>","text":"<p>Ask a question to Anthropic Claude model with vision capabilities.</p> <p>You can specify arbitrary text prompts or predefined ones, the block supports the following types of prompt:</p> <ul> <li> <p>Open Prompt (<code>unconstrained</code>) - Use any prompt to generate a raw response</p> </li> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Visual Question Answering (<code>visual-question-answering</code>) - Model answers the question you submit in the prompt</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Single-Label Classification (<code>classification</code>) - Model classifies the image content as one of the provided classes</p> </li> <li> <p>Multi-Label Classification (<code>multi-label-classification</code>) - Model classifies the image content as one or more of the provided classes</p> </li> <li> <p>Unprompted Object Detection (<code>object-detection</code>) - Model detects and returns the bounding boxes for prominent objects in the image</p> </li> <li> <p>Structured Output Generation (<code>structured-answering</code>) - Model returns a JSON response with the specified fields</p> </li> </ul> <p>You need to provide your Anthropic API key to use the Claude model. </p>"},{"location":"workflows/blocks/anthropic_claude/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/anthropic_claude@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/anthropic_claude/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the Claude model. \u2705 <code>output_structure</code> <code>Dict[str, str]</code> Dictionary with structure of expected JSON response. \u274c <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>api_key</code> <code>str</code> Your Antropic API key. \u2705 <code>model_version</code> <code>str</code> Model to be used. \u2705 <code>max_tokens</code> <code>int</code> Maximum number of tokens the model can generate in it's response.. \u274c <code>temperature</code> <code>float</code> Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are.. \u2705 <code>max_image_size</code> <code>int</code> Maximum size of the image - if input has larger side, it will be downscaled, keeping aspect ratio. \u2705 <code>max_concurrent_requests</code> <code>int</code> Number of concurrent requests that can be executed by block when batch of input images provided. If not given - block defaults to value configured globally in Workflows Execution Engine. Please restrict if you hit ANtropic API limits.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/anthropic_claude/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Anthropic Claude</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Time in zone</code>, <code>JSON Parser</code>, <code>Perspective Correction</code>, <code>Line Counter Visualization</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Path deviation</code>, <code>Reference Path Visualization</code>, <code>Line Counter</code>, <code>VLM as Classifier</code>, <code>Email Notification</code>, <code>Path deviation</code>, <code>VLM as Detector</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Anthropic Claude</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the Claude model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>api_key</code> (<code>string</code>): Your Antropic API key.</li> <li><code>model_version</code> (<code>string</code>): Model to be used.</li> <li><code>temperature</code> (<code>float</code>): Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are..</li> <li><code>max_image_size</code> (<code>integer</code>): Maximum size of the image - if input has larger side, it will be downscaled, keeping aspect ratio.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Anthropic Claude</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/anthropic_claude@v1\",\n    \"images\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"output_structure\": {\n        \"my_key\": \"description\"\n    },\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"api_key\": \"xxx-xxx\",\n    \"model_version\": \"claude-3-5-sonnet\",\n    \"max_tokens\": \"&lt;block_does_not_provide_example&gt;\",\n    \"temperature\": \"&lt;block_does_not_provide_example&gt;\",\n    \"max_image_size\": \"&lt;block_does_not_provide_example&gt;\",\n    \"max_concurrent_requests\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre>"},{"location":"workflows/blocks/background_color_visualization/","title":"Background Color Visualization","text":""},{"location":"workflows/blocks/background_color_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>BackgroundColorVisualization</code> block draws all areas outside of detected regions in an image with a specified color.</p>"},{"location":"workflows/blocks/background_color_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/background_color_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/background_color_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color</code> <code>str</code> Color of the background.. \u2705 <code>opacity</code> <code>float</code> Transparency of the Mask overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/background_color_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Background Color Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Background Color Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color</code> (<code>string</code>): Color of the background..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the Mask overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Background Color Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/background_color_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color\": \"WHITE\",\n    \"opacity\": 0.5\n}\n</code></pre>"},{"location":"workflows/blocks/barcode_detection/","title":"Barcode Detection","text":""},{"location":"workflows/blocks/barcode_detection/#version-v1","title":"Version <code>v1</code>","text":"<p>Detect the location of barcodes in an image.</p> <p>This block is useful for manufacturing and consumer packaged goods projects where you  need to detect a barcode region in an image. You can then apply Crop block to isolate  each barcode then apply further processing (i.e. OCR of the characters on a barcode).</p>"},{"location":"workflows/blocks/barcode_detection/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/barcode_detector@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/barcode_detection/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/barcode_detection/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Barcode Detection</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: None</li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Barcode Detection</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>bar_code_detection</code>): Prediction with barcode detection.</li> </ul> </li> </ul> Example JSON definition of step <code>Barcode Detection</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/barcode_detector@v1\",\n    \"images\": \"$inputs.image\"\n}\n</code></pre>"},{"location":"workflows/blocks/blur_visualization/","title":"Blur Visualization","text":""},{"location":"workflows/blocks/blur_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>BlurVisualization</code> block blurs detected objects in an image using Supervision's <code>sv.BlurAnnotator</code>.</p>"},{"location":"workflows/blocks/blur_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/blur_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/blur_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>kernel_size</code> <code>int</code> Size of the average pooling kernel used for blurring.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/blur_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Blur Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Blur Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>kernel_size</code> (<code>integer</code>): Size of the average pooling kernel used for blurring..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Blur Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/blur_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"kernel_size\": 15\n}\n</code></pre>"},{"location":"workflows/blocks/bounding_box_visualization/","title":"Bounding Box Visualization","text":""},{"location":"workflows/blocks/bounding_box_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>BoundingBoxVisualization</code> block draws a box around detected objects in an image using Supervision's <code>sv.RoundBoxAnnotator</code>.</p>"},{"location":"workflows/blocks/bounding_box_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/bounding_box_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/bounding_box_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>thickness</code> <code>int</code> Thickness of the bounding box in pixels.. \u2705 <code>roundness</code> <code>float</code> Roundness of the corners of the bounding box.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/bounding_box_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Bounding Box Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Bounding Box Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the bounding box in pixels..</li> <li><code>roundness</code> (<code>float_zero_to_one</code>): Roundness of the corners of the bounding box..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Bounding Box Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"thickness\": 2,\n    \"roundness\": 0.0\n}\n</code></pre>"},{"location":"workflows/blocks/bounding_rectangle/","title":"Bounding Rectangle","text":""},{"location":"workflows/blocks/bounding_rectangle/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>BoundingRect</code> is a transformer block designed to simplify polygon to the minimum boundig rectangle. This block is best suited when Zone needs to be created based on shape of detected object (i.e. basketball field, road segment, zebra crossing etc.) Input detections should be filtered beforehand and contain only desired classes of interest. Resulsts are stored in sv.Detections.data</p>"},{"location":"workflows/blocks/bounding_rectangle/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/bounding_rect@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/bounding_rectangle/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/bounding_rectangle/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Bounding Rectangle</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Path deviation</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Path deviation</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Detection Offset</code>, <code>Detections Filter</code>, <code>Bounding Rectangle</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Bounding Rectangle</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): .</li> </ul> </li> <li> <p>output</p> <ul> <li><code>detections_with_rect</code> (<code>instance_segmentation_prediction</code>): Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Bounding Rectangle</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/bounding_rect@v1\",\n    \"predictions\": \"$segmentation.predictions\"\n}\n</code></pre>"},{"location":"workflows/blocks/byte_tracker/","title":"Byte Tracker","text":""},{"location":"workflows/blocks/byte_tracker/#version-v3","title":"Version <code>v3</code>","text":"<p>The <code>ByteTrackerBlock</code> integrates ByteTrack, an advanced object tracking algorithm,  to manage object tracking across sequential video frames within workflows.</p> <p>This block accepts detections and their corresponding video frames as input,  initializing trackers for each detection based on configurable parameters like track  activation threshold, lost track buffer, minimum matching threshold, and frame rate.  These parameters allow fine-tuning of the tracking process to suit specific accuracy  and performance needs.</p> <p>New outputs introduced in <code>v3</code></p> <p>The block has not changed compared to <code>v2</code> apart from the fact that there are two  new outputs added:</p> <ul> <li> <p><code>new_instances</code>: delivers sv.Detections objects with bounding boxes that have  tracker IDs which were first seen - specific tracked instance will only be listed in that output once - when new tracker ID is generated </p> </li> <li> <p><code>already_seen_instances</code>: delivers sv.Detections objects with bounding boxes that have  tracker IDs which were already seen - specific tracked instance will only be listed in that output each time the tracker associates the bounding box with already seen tracker ID </p> </li> </ul>"},{"location":"workflows/blocks/byte_tracker/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/byte_tracker@v3</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/byte_tracker/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>track_activation_threshold</code> <code>float</code> Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability.. \u2705 <code>lost_track_buffer</code> <code>int</code> Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps.. \u2705 <code>minimum_matching_threshold</code> <code>float</code> Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift.. \u2705 <code>minimum_consecutive_frames</code> <code>int</code> Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks.. \u2705 <code>instances_cache_size</code> <code>int</code> Size of the instances cache to decide if specific tracked instance is new or already seen. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/byte_tracker/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Byte Tracker</code> in version <code>v3</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Dot Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Byte Tracker</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>Line Counter</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Byte Tracker</code> in version <code>v3</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Objects to be tracked.</li> <li><code>track_activation_threshold</code> (<code>float_zero_to_one</code>): Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability..</li> <li><code>lost_track_buffer</code> (<code>integer</code>): Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps..</li> <li><code>minimum_matching_threshold</code> (<code>float_zero_to_one</code>): Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift..</li> <li><code>minimum_consecutive_frames</code> (<code>integer</code>): Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>tracked_detections</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>new_instances</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>already_seen_instances</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Byte Tracker</code> in version <code>v3</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/byte_tracker@v3\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"track_activation_threshold\": 0.25,\n    \"lost_track_buffer\": 30,\n    \"minimum_matching_threshold\": 0.8,\n    \"minimum_consecutive_frames\": 1,\n    \"instances_cache_size\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre>"},{"location":"workflows/blocks/byte_tracker/#version-v2","title":"Version <code>v2</code>","text":"<p>The <code>ByteTrackerBlock</code> integrates ByteTrack, an advanced object tracking algorithm,  to manage object tracking across sequential video frames within workflows.</p> <p>This block accepts detections and their corresponding video frames as input,  initializing trackers for each detection based on configurable parameters like track  activation threshold, lost track buffer, minimum matching threshold, and frame rate.  These parameters allow fine-tuning of the tracking process to suit specific accuracy  and performance needs.</p>"},{"location":"workflows/blocks/byte_tracker/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/byte_tracker@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/byte_tracker/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>track_activation_threshold</code> <code>float</code> Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability.. \u2705 <code>lost_track_buffer</code> <code>int</code> Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps.. \u2705 <code>minimum_matching_threshold</code> <code>float</code> Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift.. \u2705 <code>minimum_consecutive_frames</code> <code>int</code> Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/byte_tracker/#available-connections_1","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Byte Tracker</code> in version <code>v2</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Byte Tracker</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Objects to be tracked.</li> <li><code>track_activation_threshold</code> (<code>float_zero_to_one</code>): Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability..</li> <li><code>lost_track_buffer</code> (<code>integer</code>): Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps..</li> <li><code>minimum_matching_threshold</code> (<code>float_zero_to_one</code>): Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift..</li> <li><code>minimum_consecutive_frames</code> (<code>integer</code>): Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>tracked_detections</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Byte Tracker</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/byte_tracker@v2\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"track_activation_threshold\": 0.25,\n    \"lost_track_buffer\": 30,\n    \"minimum_matching_threshold\": 0.8,\n    \"minimum_consecutive_frames\": 1\n}\n</code></pre>"},{"location":"workflows/blocks/byte_tracker/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>ByteTrackerBlock</code> integrates ByteTrack, an advanced object tracking algorithm,  to manage object tracking across sequential video frames within workflows.</p> <p>This block accepts detections and their corresponding video frames as input,  initializing trackers for each detection based on configurable parameters like track  activation threshold, lost track buffer, minimum matching threshold, and frame rate.  These parameters allow fine-tuning of the tracking process to suit specific accuracy  and performance needs.</p>"},{"location":"workflows/blocks/byte_tracker/#type-identifier_2","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/byte_tracker@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/byte_tracker/#properties_2","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>track_activation_threshold</code> <code>float</code> Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability.. \u2705 <code>lost_track_buffer</code> <code>int</code> Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps.. \u2705 <code>minimum_matching_threshold</code> <code>float</code> Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift.. \u2705 <code>minimum_consecutive_frames</code> <code>int</code> Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/byte_tracker/#available-connections_2","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Byte Tracker</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Byte Tracker</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>metadata</code> (<code>video_metadata</code>): not available.</li> <li><code>detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Objects to be tracked.</li> <li><code>track_activation_threshold</code> (<code>float_zero_to_one</code>): Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability..</li> <li><code>lost_track_buffer</code> (<code>integer</code>): Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps..</li> <li><code>minimum_matching_threshold</code> (<code>float_zero_to_one</code>): Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift..</li> <li><code>minimum_consecutive_frames</code> (<code>integer</code>): Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>tracked_detections</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Byte Tracker</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/byte_tracker@v1\",\n    \"metadata\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"track_activation_threshold\": 0.25,\n    \"lost_track_buffer\": 30,\n    \"minimum_matching_threshold\": 0.8,\n    \"minimum_consecutive_frames\": 1\n}\n</code></pre>"},{"location":"workflows/blocks/camera_focus/","title":"Camera Focus","text":""},{"location":"workflows/blocks/camera_focus/#version-v1","title":"Version <code>v1</code>","text":"<p>This block calculate the Brenner function score which is a measure of the texture in the image.  An in-focus image has a high Brenner function score, and contains texture at a smaller scale than  an out-of-focus image. Conversely, an out-of-focus image has a low Brenner function score, and   does not contain small-scale texture.</p>"},{"location":"workflows/blocks/camera_focus/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/camera_focus@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/camera_focus/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/camera_focus/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Camera Focus</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Camera Focus</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> <li><code>focus_measure</code> (<code>float</code>): Float value.</li> </ul> </li> </ul> Example JSON definition of step <code>Camera Focus</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/camera_focus@v1\",\n    \"image\": \"$inputs.image\"\n}\n</code></pre>"},{"location":"workflows/blocks/circle_visualization/","title":"Circle Visualization","text":""},{"location":"workflows/blocks/circle_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>CircleVisualization</code> block draws a circle around detected objects in an image using Supervision's <code>sv.CircleAnnotator</code>.</p>"},{"location":"workflows/blocks/circle_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/circle_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/circle_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>thickness</code> <code>int</code> Thickness of the lines in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/circle_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Circle Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Circle Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the lines in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Circle Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/circle_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"thickness\": 2\n}\n</code></pre>"},{"location":"workflows/blocks/clip_comparison/","title":"Clip Comparison","text":""},{"location":"workflows/blocks/clip_comparison/#version-v2","title":"Version <code>v2</code>","text":"<p>Use the OpenAI CLIP zero-shot classification model to classify images.</p> <p>This block accepts an image and a list of text prompts. The block then returns the  similarity of each text label to the provided image.</p> <p>This block is useful for classifying images without having to train a fine-tuned  classification model. For example, you could use CLIP to classify the type of vehicle  in an image, or if an image contains NSFW material.</p>"},{"location":"workflows/blocks/clip_comparison/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/clip_comparison@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/clip_comparison/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <code>classes</code> <code>List[str]</code> List of classes to calculate similarity against each input image. \u2705 <code>version</code> <code>str</code> Variant of CLIP model. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/clip_comparison/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Clip Comparison</code> in version <code>v2</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Time in zone</code>, <code>Perspective Correction</code>, <code>Detections Classes Replacement</code>, <code>Line Counter Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Roboflow Dataset Upload</code>, <code>Line Counter</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Path deviation</code>, <code>Reference Path Visualization</code>, <code>Line Counter</code>, <code>VLM as Classifier</code>, <code>Email Notification</code>, <code>Path deviation</code>, <code>VLM as Detector</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Clip Comparison</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to calculate similarity against each input image.</li> <li><code>version</code> (<code>string</code>): Variant of CLIP model.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>similarities</code> (<code>list_of_values</code>): List of values of any type.</li> <li><code>max_similarity</code> (<code>float_zero_to_one</code>): <code>float</code> value in range <code>[0.0, 1.0]</code>.</li> <li><code>most_similar_class</code> (<code>string</code>): String value.</li> <li><code>min_similarity</code> (<code>float_zero_to_one</code>): <code>float</code> value in range <code>[0.0, 1.0]</code>.</li> <li><code>least_similar_class</code> (<code>string</code>): String value.</li> <li><code>classification_predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> </ul> </li> </ul> Example JSON definition of step <code>Clip Comparison</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/clip_comparison@v2\",\n    \"images\": \"$inputs.image\",\n    \"classes\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"version\": \"ViT-B-16\"\n}\n</code></pre>"},{"location":"workflows/blocks/clip_comparison/#version-v1","title":"Version <code>v1</code>","text":"<p>Use the OpenAI CLIP zero-shot classification model to classify images.</p> <p>This block accepts an image and a list of text prompts. The block then returns the  similarity of each text label to the provided image.</p> <p>This block is useful for classifying images without having to train a fine-tuned  classification model. For example, you could use CLIP to classify the type of vehicle  in an image, or if an image contains NSFW material.</p>"},{"location":"workflows/blocks/clip_comparison/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/clip_comparison@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/clip_comparison/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <code>texts</code> <code>List[str]</code> List of texts to calculate similarity against each input image. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/clip_comparison/#available-connections_1","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Clip Comparison</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Path deviation</code>, <code>Time in zone</code>, <code>Polygon Zone Visualization</code>, <code>Line Counter</code>, <code>Webhook Sink</code>, <code>Path deviation</code>, <code>Reference Path Visualization</code>, <code>Line Counter</code>, <code>Time in zone</code>, <code>VLM as Classifier</code>, <code>Perspective Correction</code>, <code>Line Counter Visualization</code>, <code>VLM as Detector</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Clip Comparison</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>texts</code> (<code>list_of_values</code>): List of texts to calculate similarity against each input image.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>similarity</code> (<code>list_of_values</code>): List of values of any type.</li> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>prediction_type</code> (<code>prediction_type</code>): String value with type of prediction.</li> </ul> </li> </ul> Example JSON definition of step <code>Clip Comparison</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/clip_comparison@v1\",\n    \"images\": \"$inputs.image\",\n    \"texts\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ]\n}\n</code></pre>"},{"location":"workflows/blocks/cog_vlm/","title":"CogVLM","text":""},{"location":"workflows/blocks/cog_vlm/#version-v1","title":"Version <code>v1</code>","text":"<p>Ask a question to CogVLM, an open source vision-language model.</p> <p>This model requires a GPU and can only be run on self-hosted devices, and is not available on the Roboflow Hosted API.</p> <p>This model was previously part of the LMM block.</p>"},{"location":"workflows/blocks/cog_vlm/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/cog_vlm@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/cog_vlm/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>prompt</code> <code>str</code> Text prompt to the CogVLM model. \u2705 <code>json_output_format</code> <code>Dict[str, str]</code> Holds dictionary that maps name of requested output field into its description. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/cog_vlm/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>CogVLM</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>LMM For Classification</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Circle Visualization</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>Size Measurement</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Email Notification</code>, <code>Dimension Collapse</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Mask Visualization</code>, <code>Perspective Correction</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>JSON Parser</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>CogVLM</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the CogVLM model.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>image</code> (<code>image_metadata</code>): Dictionary with image metadata required by supervision.</li> <li><code>structured_output</code> (<code>dictionary</code>): Dictionary.</li> <li><code>raw_output</code> (<code>string</code>): String value.</li> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>CogVLM</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/cog_vlm@v1\",\n    \"images\": \"$inputs.image\",\n    \"prompt\": \"my prompt\",\n    \"json_output_format\": {\n        \"count\": \"number of cats in the picture\"\n    }\n}\n</code></pre>"},{"location":"workflows/blocks/color_visualization/","title":"Color Visualization","text":""},{"location":"workflows/blocks/color_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>ColorVisualization</code> block paints a solid color on detected objects in an image using Supervision's <code>sv.ColorAnnotator</code>.</p>"},{"location":"workflows/blocks/color_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/color_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/color_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>opacity</code> <code>float</code> Transparency of the color overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/color_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Color Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Color Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the color overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Color Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/color_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"opacity\": 0.5\n}\n</code></pre>"},{"location":"workflows/blocks/continue_if/","title":"Continue If","text":""},{"location":"workflows/blocks/continue_if/#version-v1","title":"Version <code>v1</code>","text":"<p>Based on provided configuration, block decides if it should follow to pointed execution path</p>"},{"location":"workflows/blocks/continue_if/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/continue_if@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/continue_if/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>condition_statement</code> <code>StatementGroup</code> Workflows UQL definition of conditional logic.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/continue_if/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Continue If</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> <li>outputs: None</li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Continue If</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>evaluation_parameters</code> (Union[<code>*</code>, <code>image</code>]): References to additional parameters that may be provided in runtime to parametrise operations.</li> <li><code>next_steps</code> (step): Reference to step which shall be executed if expression evaluates to true.</li> </ul> </li> <li> <p>output</p> </li> </ul> Example JSON definition of step <code>Continue If</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/continue_if@v1\",\n    \"condition_statement\": {\n        \"statements\": [\n            {\n                \"comparator\": {\n                    \"type\": \"(Number) ==\"\n                },\n                \"left_operand\": {\n                    \"operand_name\": \"left\",\n                    \"type\": \"DynamicOperand\"\n                },\n                \"right_operand\": {\n                    \"type\": \"StaticOperand\",\n                    \"value\": 1\n                },\n                \"type\": \"BinaryStatement\"\n            }\n        ],\n        \"type\": \"StatementGroup\"\n    },\n    \"evaluation_parameters\": {\n        \"left\": \"$inputs.some\"\n    },\n    \"next_steps\": [\n        \"$steps.on_true\"\n    ]\n}\n</code></pre>"},{"location":"workflows/blocks/corner_visualization/","title":"Corner Visualization","text":""},{"location":"workflows/blocks/corner_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>CornerVisualization</code> block draws the corners of detected objects in an image using Supervision's <code>sv.BoxCornerAnnotator</code>.</p>"},{"location":"workflows/blocks/corner_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/corner_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/corner_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>thickness</code> <code>int</code> Thickness of the lines in pixels.. \u2705 <code>corner_length</code> <code>int</code> Length of the corner lines in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/corner_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Corner Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Corner Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the lines in pixels..</li> <li><code>corner_length</code> (<code>integer</code>): Length of the corner lines in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Corner Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/corner_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"thickness\": 4,\n    \"corner_length\": 15\n}\n</code></pre>"},{"location":"workflows/blocks/crop_visualization/","title":"Crop Visualization","text":""},{"location":"workflows/blocks/crop_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>CropVisualization</code> block draws scaled up crops of detections on the scene using Supervision's <code>sv.CropAnnotator</code>.</p>"},{"location":"workflows/blocks/crop_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/crop_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/crop_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>position</code> <code>str</code> The anchor position for placing the crop.. \u2705 <code>scale_factor</code> <code>float</code> The factor by which to scale the cropped image part. A factor of 2, for example, would double the size of the cropped area, allowing for a closer view of the detection.. \u2705 <code>border_thickness</code> <code>int</code> Thickness of the outline in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/crop_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Crop Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Crop Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>position</code> (<code>string</code>): The anchor position for placing the crop..</li> <li><code>scale_factor</code> (<code>float</code>): The factor by which to scale the cropped image part. A factor of 2, for example, would double the size of the cropped area, allowing for a closer view of the detection..</li> <li><code>border_thickness</code> (<code>integer</code>): Thickness of the outline in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Crop Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/crop_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"position\": \"CENTER\",\n    \"scale_factor\": 2.0,\n    \"border_thickness\": 2\n}\n</code></pre>"},{"location":"workflows/blocks/csv_formatter/","title":"CSV Formatter","text":""},{"location":"workflows/blocks/csv_formatter/#version-v1","title":"Version <code>v1</code>","text":"<p>The CSV Formatter block prepares structured CSV content based on specified data configurations within  a workflow. It allows users to:</p> <ul> <li> <p>choose which data appears as columns</p> </li> <li> <p>apply operations to transform the data within the block</p> </li> <li> <p>aggregate whole batch of data into single CSV document (see Data Aggregation section)</p> </li> </ul> <p>The generated CSV content can be used as input for other blocks, such as File Sink or Email Notifications.</p>"},{"location":"workflows/blocks/csv_formatter/#defining-columns","title":"Defining columns","text":"<p>Use <code>columns_data</code> property to specify name of the columns and data sources. Defining UQL operations in  <code>columns_operations</code> you can perform specific operation on each column.</p> <p>Timestamp column</p> <p>The block automatically adds <code>timestamp</code> column and this column name is reserved and cannot be used.</p> <p>The value of timestamp would be in the following format: <code>2024-10-18T14:09:57.622297+00:00</code>, values  are scaled to UTC time zone.</p> <p>For example, the following definition <pre><code>columns_data = {\n    \"predictions\": \"$steps.model.predictions\",\n    \"reference\": \"$inputs.reference_class_names\",\n}\ncolumns_operations = {\n    \"predictions\": [\n        {\"type\": \"DetectionsPropertyExtract\", \"property_name\": \"class_name\"}\n    ],\n}\n</code></pre></p> <p>Will generate CSV content: <pre><code>timestamp,predictions,reference\n\"2024-10-16T11:15:15.336322+00:00\",\"['a', 'b', 'c']\",\"['a', 'b']\"\n</code></pre></p> <p>When applied on object detection predictions from a single image, assuming that <code>$inputs.reference_class_names</code> holds a list of reference classes.</p>"},{"location":"workflows/blocks/csv_formatter/#data-aggregation","title":"Data Aggregation","text":"<p>The block may take input from different blocks, hence its behavior may differ depending on context:</p> <ul> <li> <p>data <code>batch_size=1</code>: whenever single input is provided - block will provide the output as in the example above -  CSV header will be placed in the first row, the second row will hold the data</p> </li> <li> <p>data <code>batch_size&gt;1</code>: each datapoint will create one row in CSV document, but only the last batch element will be fed with the aggregated output, leaving other batch elements' outputs empty</p> </li> </ul>"},{"location":"workflows/blocks/csv_formatter/#when-should-i-expect-batch_size1","title":"When should I expect <code>batch_size=1</code>?","text":"<p>You may expect <code>batch_size=1</code> in the following scenarios:</p> <ul> <li> <p>CSV Formatter was connected to the output of block that only operates on one image and produces one prediction</p> </li> <li> <p>CSV Formatter was connected to the output of block that aggregates data for whole batch and produces single  non-empty output (which is exactly the characteristics of CSV Formatter itself)</p> </li> </ul>"},{"location":"workflows/blocks/csv_formatter/#when-should-i-expect-batch_size1_1","title":"When should I expect <code>batch_size&gt;1</code>?","text":"<p>You may expect <code>batch_size=1</code> in the following scenarios:</p> <ul> <li>CSV Formatter was connected to the output of block that produces single prediction for single image, but batch of images were fed - then CSV Formatter will aggregate the CSV content and output it in the position of the last batch element:</li> </ul> <pre><code>--- input_batch[0] ----&gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 ----&gt;  &lt;Empty&gt;\n--- input_batch[1] ----&gt; \u2502                       \u2502 ----&gt;  &lt;Empty&gt;\n        ...              \u2502      CSV Formatter    \u2502 ----&gt;  &lt;Empty&gt;\n        ...              \u2502                       \u2502 ----&gt;  &lt;Empty&gt;           \n--- input_batch[n] ----&gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ----&gt;  {\"csv_content\": \"...\"}\n</code></pre> <p>Format of CSV document for <code>batch_size&gt;1</code></p> <p>If the example presented above is applied for larger input batch sizes - the output document structure  would be as follows:</p> <pre><code>timestamp,predictions,reference\n\"2024-10-16T11:15:15.336322+00:00\",\"['a', 'b', 'c']\",\"['a', 'b']\"\n\"2024-10-16T11:15:15.436322+00:00\",\"['b', 'c']\",\"['a', 'b']\"\n\"2024-10-16T11:15:15.536322+00:00\",\"['a', 'c']\",\"['a', 'b']\"\n</code></pre>"},{"location":"workflows/blocks/csv_formatter/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/csv_formatter@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/csv_formatter/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>columns_data</code> <code>Dict[str, Union[bool, float, int, str]]</code> References data to be used to construct each and every column. \u2705 <code>columns_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data w.r.t. each column. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/csv_formatter/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>CSV Formatter</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> <li>outputs: <code>Webhook Sink</code>, <code>Email Notification</code>, <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Stability AI Inpainting</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>CSV Formatter</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>columns_data</code> (Union[<code>*</code>, <code>image</code>]): References data to be used to construct each and every column.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>csv_content</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>CSV Formatter</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/csv_formatter@v1\",\n    \"columns_data\": {\n        \"predictions\": \"$steps.model.predictions\",\n        \"reference\": \"$inputs.reference_class_names\"\n    },\n    \"columns_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"workflows/blocks/data_aggregator/","title":"Data Aggregator","text":""},{"location":"workflows/blocks/data_aggregator/#version-v1","title":"Version <code>v1</code>","text":"<p>The Data Aggregator block collects and processes data from Workflows to generate time-based statistical  summaries. It allows users to define custom aggregation strategies over specified intervals, making it suitable  for creating analytics on data streams.</p> <p>The block enables:</p> <ul> <li> <p>feeding it with data from other Workflow blocks and applying in-place operations (for instance to extract  desired values out of model predictions)</p> </li> <li> <p>using multiple aggregation modes, including <code>sum</code>, <code>avg</code>, <code>max</code>, <code>min</code>, <code>count</code> and others</p> </li> <li> <p>specifying aggregation interval flexibly</p> </li> </ul>"},{"location":"workflows/blocks/data_aggregator/#feeding-data-aggregator","title":"Feeding Data Aggregator","text":"<p>You can specify the data to aggregate by referencing input sources using the <code>data</code> field. Optionally, for each specified <code>data</code> input you can apply chain of UQL operations with <code>data_operations</code> property.</p> <p>For example, the following configuration:</p> <pre><code>data = {\n    \"predictions_model_a\": \"$steps.model_a.predictions\",\n    \"predictions_model_b\": \"$steps.model_b.predictions\",\n}\ndata_operations = { \n    \"predictions_model_a\": [\n        {\"type\": \"DetectionsPropertyExtract\", \"property_name\": \"class_name\"}\n    ],\n    \"predictions_model_b\": [{\"type\": \"SequenceLength\"}]\n}\n</code></pre> <p>on each step run will at first take <code>predictions_model_a</code> to extract list of detected classes and calculate the number of predicted bounding boxes for <code>predictions_model_b</code>.</p>"},{"location":"workflows/blocks/data_aggregator/#specifying-data-aggregations","title":"Specifying data aggregations","text":"<p>For each input data referenced by <code>data</code> property you can specify list of aggregation operations, that include:</p> <ul> <li> <p><code>sum</code>: Taking the sum of values (requires data to be numeric)</p> </li> <li> <p><code>avg</code>: Taking the average of values (requires data to be numeric)</p> </li> <li> <p><code>max</code>: Taking the max of values (requires data to be numeric)</p> </li> <li> <p><code>min</code>: Taking the min of values (requires data to be numeric)</p> </li> <li> <p><code>count</code>: Counting the values - if provided value is list - operation will add length of the list into  aggregated state</p> </li> <li> <p><code>distinct</code>: deduplication of encountered values - providing list of unique values in the output. If  aggregation data is list - operation will add each element of the list into aggregated state.</p> </li> <li> <p><code>count_distinct</code>: counting occurrences of distinct values - providing number of different values that were  encountered. If aggregation data is list - operation will add each element of the list into aggregated state.</p> </li> <li> <p><code>count_distinct</code>: counting distinct values - providing number of different values that were  encountered. If aggregation data is list - operation will add each element of the list into aggregated state.</p> </li> <li> <p><code>values_counts</code>: counting occurrences of each distinct value - providing dictionary mapping each unique value  encountered into the number of observations. If aggregation data is list - operation will add each element of the list  into aggregated state.</p> </li> <li> <p><code>values_difference</code>: calculates the difference between max and min observed value (requires data to be numeric)</p> </li> </ul> <p>If we take the <code>data</code> and <code>data_operations</code> from the example above and specify <code>aggregation_mode</code> in the following way:</p> <pre><code>aggregation_mode = {\n    \"predictions_model_a\": [\"distinct\", \"count_distinct\"],\n    \"predictions_model_b\": [\"avg\"],\n}\n</code></pre> <p>Our aggregation report will contain the following values:</p> <pre><code>{\n    \"predictions_model_a_distinct\": [\"car\", \"person\", \"dog\"],\n    \"predictions_model_a_count_distinct\": {\"car\": 378, \"person\": 128, \"dog\": 37},\n    \"predictions_model_b_avg\": 7.35,\n}\n</code></pre> <p>where:</p> <ul> <li> <p><code>predictions_model_a_distinct</code> provides distinct classes predicted by model A in aggregation window</p> </li> <li> <p><code>predictions_model_a_count_distinct</code> provides number of classes instances predicted by model A in aggregation  window</p> </li> <li> <p><code>predictions_model_b_avg</code> provides average number of bounding boxes predicted by model B in aggregation window</p> </li> </ul>"},{"location":"workflows/blocks/data_aggregator/#interval-nature-of-the-block","title":"Interval nature of the block","text":"<p>Block behaviour is dictated by internal 'clock'</p> <p>Behaviour of this block differs from other, more classical blocks which output the data for each input. Data Aggregator block maintains its internal state that dictates when the data will be produced,  flushing internal aggregation state of the block. </p> <p>You can expect that most of the times, once fed with data, the block will produce empty outputs, effectively terminating downstream processing:</p> <pre><code>--- input_batch[0] ----&gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 ----&gt;  &lt;Empty&gt;\n--- input_batch[1] ----&gt; \u2502                       \u2502 ----&gt;  &lt;Empty&gt;\n        ...              \u2502     Data Aggregator   \u2502 ----&gt;  &lt;Empty&gt;\n        ...              \u2502                       \u2502 ----&gt;  &lt;Empty&gt;           \n--- input_batch[n] ----&gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ----&gt;  &lt;Empty&gt;\n</code></pre> <p>But once for a while, the block will yield aggregated data and flush its internal state:</p> <pre><code>--- input_batch[0] ----&gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 ----&gt;  &lt;Empty&gt;\n--- input_batch[1] ----&gt; \u2502                       \u2502 ----&gt;  &lt;Empty&gt;\n        ...              \u2502     Data Aggregator   \u2502 ----&gt;  {&lt;aggregated_report&gt;}\n        ...              \u2502                       \u2502 ----&gt;  &lt;Empty&gt; # first datapoint added to new state          \n--- input_batch[n] ----&gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ----&gt;  &lt;Empty&gt;\n</code></pre> <p>Setting the aggregation interval is possible with <code>interval</code> and <code>interval_unit</code> property. <code>interval</code> specifies the length of aggregation window and <code>interval_unit</code> bounds the <code>interval</code> value  into units. You can specify the interval based on:</p> <ul> <li> <p>time elapse: using <code>[\"seconds\", \"minutes\", \"hours\"]</code> as <code>interval_unit</code> will make the  Data Aggregator to yield the aggregated report based on time that elapsed since last report  was released - this setting is relevant for processing of video streams.</p> </li> <li> <p>number of runs: using <code>runs</code> as <code>interval_unit</code> - this setting is relevant for  processing of video files, as in this context wall-clock time elapse is not the proper way of getting meaningful reports.</p> </li> </ul>"},{"location":"workflows/blocks/data_aggregator/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/data_aggregator@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/data_aggregator/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>data_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data w.r.t. element of the data. \u274c <code>aggregation_mode</code> <code>Dict[str, List[str]]</code> Lists of aggregation operations to apply on each input data. \u274c <code>interval_unit</code> <code>str</code> Unit to measure <code>interval</code>. \u274c <code>interval</code> <code>int</code> Length of aggregation interval. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/data_aggregator/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Data Aggregator</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Circle Visualization</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Mask Visualization</code>, <code>Perspective Correction</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Data Aggregator</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (Union[<code>*</code>, <code>image</code>]): References data to be used to construct each and every column.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>Data Aggregator</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/data_aggregator@v1\",\n    \"data\": {\n        \"predictions\": \"$steps.model.predictions\",\n        \"reference\": \"$inputs.reference_class_names\"\n    },\n    \"data_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"aggregation_mode\": {\n        \"predictions\": [\n            \"distinct\",\n            \"count_distinct\"\n        ]\n    },\n    \"interval_unit\": \"seconds\",\n    \"interval\": 10\n}\n</code></pre>"},{"location":"workflows/blocks/detection_offset/","title":"Detection Offset","text":""},{"location":"workflows/blocks/detection_offset/#version-v1","title":"Version <code>v1</code>","text":"<p>Apply a fixed offset to the width and height of a detection.</p> <p>You can use this block to add padding around the result of a detection. This is useful  to ensure that you can analyze bounding boxes that may be within the region of an  object instead of being around an object.</p>"},{"location":"workflows/blocks/detection_offset/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detection_offset@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detection_offset/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>offset_width</code> <code>int</code> Offset for boxes width. \u2705 <code>offset_height</code> <code>int</code> Offset for boxes height. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detection_offset/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Detection Offset</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detection Offset</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Reference to detection-like predictions.</li> <li><code>offset_width</code> (<code>integer</code>): Offset for boxes width.</li> <li><code>offset_height</code> (<code>integer</code>): Offset for boxes height.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code> or Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object if <code>keypoint_detection_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detection Offset</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detection_offset@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"offset_width\": 10,\n    \"offset_height\": 10\n}\n</code></pre>"},{"location":"workflows/blocks/detections_classes_replacement/","title":"Detections Classes Replacement","text":""},{"location":"workflows/blocks/detections_classes_replacement/#version-v1","title":"Version <code>v1</code>","text":"<p>Combine results of detection model with classification results performed separately for  each and every bounding box. </p> <p>Bounding boxes without top class predicted by classification model are discarded,  for multi-label classification results, most confident label is taken as bounding box class.  </p>"},{"location":"workflows/blocks/detections_classes_replacement/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_classes_replacement@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_classes_replacement/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_classes_replacement/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Detections Classes Replacement</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Multi-Label Classification Model</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>VLM as Classifier</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Single-Label Classification Model</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Classes Replacement</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>object_detection_predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): The output of a detection model describing the bounding boxes that will have classes replaced..</li> <li><code>classification_predictions</code> (<code>classification_prediction</code>): The output of classification model for crops taken based on RoIs pointed as the other parameter.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code> or Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object if <code>keypoint_detection_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Classes Replacement</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n    \"object_detection_predictions\": \"$steps.my_object_detection_model.predictions\",\n    \"classification_predictions\": \"$steps.my_classification_model.predictions\"\n}\n</code></pre>"},{"location":"workflows/blocks/detections_consensus/","title":"Detections Consensus","text":""},{"location":"workflows/blocks/detections_consensus/#version-v1","title":"Version <code>v1</code>","text":"<p>Combine detections from multiple detection-based models based on a majority vote  strategy.</p> <p>This block is useful if you have multiple specialized models that you want to consult  to determine whether a certain object is present in an image.</p> <p>See the table below to explore the values you can use to configure the consensus block.</p>"},{"location":"workflows/blocks/detections_consensus/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_consensus@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_consensus/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>required_votes</code> <code>int</code> Required number of votes for single detection from different models to accept detection as output detection. \u2705 <code>class_aware</code> <code>bool</code> Flag to decide if merging detections is class-aware or only bounding boxes aware. \u2705 <code>iou_threshold</code> <code>float</code> IoU threshold to consider detections from different models as matching (increasing votes for region). \u2705 <code>confidence</code> <code>float</code> Confidence threshold for merged detections. \u2705 <code>classes_to_consider</code> <code>List[str]</code> Optional list of classes to consider in consensus procedure.. \u2705 <code>required_objects</code> <code>Optional[Dict[str, int], int]</code> If given, it holds the number of objects that must be present in merged results, to assume that object presence is reached. Can be selector to <code>InferenceParameter</code>, integer value or dictionary with mapping of class name into minimal number of merged detections of given class to assume consensus.. \u2705 <code>presence_confidence_aggregation</code> <code>AggregationMode</code> Mode dictating aggregation of confidence scores and classes both in case of object presence deduction procedure.. \u274c <code>detections_merge_confidence_aggregation</code> <code>AggregationMode</code> Mode dictating aggregation of confidence scores and classes both in case of boxes consensus procedure. One of <code>average</code>, <code>max</code>, <code>min</code>. Default: <code>average</code>. While using for merging overlapping boxes, against classes - <code>average</code> equals to majority vote, <code>max</code> - for the class of detection with max confidence, <code>min</code> - for the class of detection with min confidence.. \u274c <code>detections_merge_coordinates_aggregation</code> <code>AggregationMode</code> Mode dictating aggregation of bounding boxes. One of <code>average</code>, <code>max</code>, <code>min</code>. Default: <code>average</code>. <code>average</code> means taking mean from all boxes coordinates, <code>min</code> - taking smallest box, <code>max</code> - taking largest box.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_consensus/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Detections Consensus</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Webhook Sink</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Consensus</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions_batches</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Reference to detection-like model predictions made against single image to agree on model consensus.</li> <li><code>required_votes</code> (<code>integer</code>): Required number of votes for single detection from different models to accept detection as output detection.</li> <li><code>class_aware</code> (<code>boolean</code>): Flag to decide if merging detections is class-aware or only bounding boxes aware.</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): IoU threshold to consider detections from different models as matching (increasing votes for region).</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for merged detections.</li> <li><code>classes_to_consider</code> (<code>list_of_values</code>): Optional list of classes to consider in consensus procedure..</li> <li><code>required_objects</code> (Union[<code>integer</code>, <code>dictionary</code>]): If given, it holds the number of objects that must be present in merged results, to assume that object presence is reached. Can be selector to <code>InferenceParameter</code>, integer value or dictionary with mapping of class name into minimal number of merged detections of given class to assume consensus..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>object_present</code> (Union[<code>boolean</code>, <code>dictionary</code>]): Boolean flag if <code>boolean</code> or Dictionary if <code>dictionary</code>.</li> <li><code>presence_confidence</code> (Union[<code>float_zero_to_one</code>, <code>dictionary</code>]): <code>float</code> value in range <code>[0.0, 1.0]</code> if <code>float_zero_to_one</code> or Dictionary if <code>dictionary</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Consensus</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_consensus@v1\",\n    \"predictions_batches\": [\n        \"$steps.a.predictions\",\n        \"$steps.b.predictions\"\n    ],\n    \"required_votes\": 2,\n    \"class_aware\": true,\n    \"iou_threshold\": 0.3,\n    \"confidence\": 0.1,\n    \"classes_to_consider\": [\n        \"a\",\n        \"b\"\n    ],\n    \"required_objects\": 3,\n    \"presence_confidence_aggregation\": \"max\",\n    \"detections_merge_confidence_aggregation\": \"min\",\n    \"detections_merge_coordinates_aggregation\": \"min\"\n}\n</code></pre>"},{"location":"workflows/blocks/detections_filter/","title":"Detections Filter","text":""},{"location":"workflows/blocks/detections_filter/#version-v1","title":"Version <code>v1</code>","text":"<p>Conditionally filter out model predictions.</p>"},{"location":"workflows/blocks/detections_filter/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_filter@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_filter/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>operations</code> <code>List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]</code> Definition of filtering operations. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_filter/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Detections Filter</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>LMM For Classification</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Clip Comparison</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>Bounding Rectangle</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Size Measurement</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>JSON Parser</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Filter</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Reference to detection-like predictions.</li> <li><code>operations_parameters</code> (Union[<code>*</code>, <code>image</code>]): References to additional parameters that may be provided in runtime to parametrise operations.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code> or Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object if <code>keypoint_detection_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Filter</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_filter@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"operations\": [\n        {\n            \"filter_operation\": {\n                \"statements\": [\n                    {\n                        \"comparator\": {\n                            \"type\": \"in (Sequence)\"\n                        },\n                        \"left_operand\": {\n                            \"operations\": [\n                                {\n                                    \"property_name\": \"class_name\",\n                                    \"type\": \"ExtractDetectionProperty\"\n                                }\n                            ],\n                            \"type\": \"DynamicOperand\"\n                        },\n                        \"right_operand\": {\n                            \"operand_name\": \"classes\",\n                            \"type\": \"DynamicOperand\"\n                        },\n                        \"type\": \"BinaryStatement\"\n                    }\n                ],\n                \"type\": \"StatementGroup\"\n            },\n            \"type\": \"DetectionsFilter\"\n        }\n    ],\n    \"operations_parameters\": {\n        \"classes\": \"$inputs.classes\"\n    }\n}\n</code></pre>"},{"location":"workflows/blocks/detections_stabilizer/","title":"Detections Stabilizer","text":""},{"location":"workflows/blocks/detections_stabilizer/#version-v1","title":"Version <code>v1</code>","text":"<p>This block stores last known position for each bounding box If box disappears then this block will bring it back so short gaps are filled with last known box position The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames) WARNING: this block will produce many short-lived bounding boxes for unstable trackers!</p>"},{"location":"workflows/blocks/detections_stabilizer/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/stabilize_detections@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_stabilizer/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>smoothing_window_size</code> <code>int</code> Predicted movement of detection will be smoothed based on historical measurements of velocity, this parameter controls number of historical measurements taken under account when calculating smoothed velocity. Detections will be removed from generating smoothed predictions if they had been missing for longer than this number of frames.. \u2705 <code>bbox_smoothing_coefficient</code> <code>float</code> Bounding box smoothing coefficient applied when given tracker_id is present on current frame. This parameter must be initialized with value between 0 and 1. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_stabilizer/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Detections Stabilizer</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Stabilizer</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Tracked detections.</li> <li><code>smoothing_window_size</code> (<code>integer</code>): Predicted movement of detection will be smoothed based on historical measurements of velocity, this parameter controls number of historical measurements taken under account when calculating smoothed velocity. Detections will be removed from generating smoothed predictions if they had been missing for longer than this number of frames..</li> <li><code>bbox_smoothing_coefficient</code> (<code>float_zero_to_one</code>): Bounding box smoothing coefficient applied when given tracker_id is present on current frame. This parameter must be initialized with value between 0 and 1.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>tracked_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Stabilizer</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/stabilize_detections@v1\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"smoothing_window_size\": 5,\n    \"bbox_smoothing_coefficient\": 0.2\n}\n</code></pre>"},{"location":"workflows/blocks/detections_stitch/","title":"Detections Stitch","text":""},{"location":"workflows/blocks/detections_stitch/#version-v1","title":"Version <code>v1</code>","text":"<p>This block merges detections that were inferred for multiple sub-parts of the same input image into single detection. </p> <p>Block may be helpful in the following scenarios: * to apply Slicing Adaptive Inference (SAHI) technique,  as a final step of procedure, which involves Image Slicer block and model block at previous stages. * to merge together detections performed by precise, high-resolution model applied as secondary model after coarse detection is performed in the first stage and Dynamic Crop is applied later. </p>"},{"location":"workflows/blocks/detections_stitch/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_stitch@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_stitch/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>overlap_filtering_strategy</code> <code>str</code> Which strategy to employ when filtering overlapping boxes. None does nothing, NMS discards surplus detections, NMM merges them.. \u2705 <code>iou_threshold</code> <code>float</code> Parameter of overlap filtering strategy. If box intersection over union is above this  ratio, discard or merge the lower confidence box.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_stitch/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Detections Stitch</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Stitch</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>reference_image</code> (<code>image</code>): Image that was origin to take crops that yielded predictions..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): The output of a detection model describing the bounding boxes to be merged..</li> <li><code>overlap_filtering_strategy</code> (<code>string</code>): Which strategy to employ when filtering overlapping boxes. None does nothing, NMS discards surplus detections, NMM merges them..</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Parameter of overlap filtering strategy. If box intersection over union is above this  ratio, discard or merge the lower confidence box..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Stitch</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_stitch@v1\",\n    \"reference_image\": \"$inputs.image\",\n    \"predictions\": \"$steps.my_object_detection_model.predictions\",\n    \"overlap_filtering_strategy\": \"nms\",\n    \"iou_threshold\": 0.4\n}\n</code></pre>"},{"location":"workflows/blocks/detections_transformation/","title":"Detections Transformation","text":""},{"location":"workflows/blocks/detections_transformation/#version-v1","title":"Version <code>v1</code>","text":"<p>Block changes detected Bounding Boxes in a way specified in configuration.</p> <p>It supports such operations as changing the size of Bounding Boxes. </p>"},{"location":"workflows/blocks/detections_transformation/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/detections_transformation@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/detections_transformation/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>operations</code> <code>List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]</code> Definition of transformations to be applied on detections. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/detections_transformation/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Detections Transformation</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>LMM For Classification</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Clip Comparison</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>Bounding Rectangle</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Size Measurement</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>JSON Parser</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Detections Transformation</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Reference to detection-like predictions.</li> <li><code>operations_parameters</code> (Union[<code>*</code>, <code>image</code>]): References to additional parameters that may be provided in runtime to parameterize operations.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code> or Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object if <code>keypoint_detection_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Detections Transformation</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/detections_transformation@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"operations\": [\n        {\n            \"filter_operation\": {\n                \"statements\": [\n                    {\n                        \"comparator\": {\n                            \"type\": \"in (Sequence)\"\n                        },\n                        \"left_operand\": {\n                            \"operations\": [\n                                {\n                                    \"property_name\": \"class_name\",\n                                    \"type\": \"ExtractDetectionProperty\"\n                                }\n                            ],\n                            \"type\": \"DynamicOperand\"\n                        },\n                        \"right_operand\": {\n                            \"operand_name\": \"classes\",\n                            \"type\": \"DynamicOperand\"\n                        },\n                        \"type\": \"BinaryStatement\"\n                    }\n                ],\n                \"type\": \"StatementGroup\"\n            },\n            \"type\": \"DetectionsFilter\"\n        }\n    ],\n    \"operations_parameters\": {\n        \"classes\": \"$inputs.classes\"\n    }\n}\n</code></pre>"},{"location":"workflows/blocks/dimension_collapse/","title":"Dimension Collapse","text":""},{"location":"workflows/blocks/dimension_collapse/#version-v1","title":"Version <code>v1</code>","text":"<p>Takes multiple step outputs at data depth level n, concatenate them into list and reduce dimensionality to level n-1.</p> <p>Useful in scenarios like: * aggregation of classification results for dynamically cropped images * aggregation of OCR results for dynamically cropped images</p>"},{"location":"workflows/blocks/dimension_collapse/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/dimension_collapse@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/dimension_collapse/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/dimension_collapse/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Dimension Collapse</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> <li>outputs: <code>Path deviation</code>, <code>Time in zone</code>, <code>Polygon Zone Visualization</code>, <code>Line Counter</code>, <code>Webhook Sink</code>, <code>Path deviation</code>, <code>Reference Path Visualization</code>, <code>Line Counter</code>, <code>Time in zone</code>, <code>VLM as Classifier</code>, <code>Perspective Correction</code>, <code>Line Counter Visualization</code>, <code>VLM as Detector</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Dimension Collapse</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (<code>*</code>): Reference to step outputs at depth level n to be concatenated and moved into level n-1..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Dimension Collapse</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/dimension_collapse@v1\",\n    \"data\": \"$steps.ocr_step.results\"\n}\n</code></pre>"},{"location":"workflows/blocks/distance_measurement/","title":"Distance Measurement","text":""},{"location":"workflows/blocks/distance_measurement/#version-v1","title":"Version <code>v1</code>","text":"<p>Measure the distance between two bounding boxes on a 2D plane using a camera positioned perpendicular to the plane. This method requires footage from this specific perspective, along with either a reference object of known dimensions placed in the same plane as the bounding boxes or a pixel-to-centimeter ratio that defines how many pixels correspond to one centimeter.</p>"},{"location":"workflows/blocks/distance_measurement/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/distance_measurement@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/distance_measurement/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>object_1_class_name</code> <code>str</code> The class name of the first object.. \u274c <code>object_2_class_name</code> <code>str</code> The class name of the second object.. \u274c <code>reference_axis</code> <code>str</code> The axis along which the distance will be measured.. \u274c <code>calibration_method</code> <code>str</code> Select how to calibrate the measurement of distance between objects.. \u274c <code>reference_object_class_name</code> <code>str</code> The class name of the reference object.. \u2705 <code>reference_width</code> <code>float</code> Width of the reference object in centimeters. \u2705 <code>reference_height</code> <code>float</code> Height of the reference object in centimeters. \u2705 <code>pixel_ratio</code> <code>float</code> The pixel-to-centimeter ratio of the input image, i.e. 1 centimeter = 100 pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/distance_measurement/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Distance Measurement</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Line Counter Visualization</code>, <code>Webhook Sink</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Distance Measurement</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): The output of a detection model describing the bounding boxes that will be used to measure the objects..</li> <li><code>reference_object_class_name</code> (<code>string</code>): The class name of the reference object..</li> <li><code>reference_width</code> (<code>float</code>): Width of the reference object in centimeters.</li> <li><code>reference_height</code> (<code>float</code>): Height of the reference object in centimeters.</li> <li><code>pixel_ratio</code> (<code>float</code>): The pixel-to-centimeter ratio of the input image, i.e. 1 centimeter = 100 pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>distance_cm</code> (<code>integer</code>): Integer value.</li> <li><code>distance_pixel</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Distance Measurement</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/distance_measurement@v1\",\n    \"predictions\": \"$steps.model.predictions\",\n    \"object_1_class_name\": \"car\",\n    \"object_2_class_name\": \"person\",\n    \"reference_axis\": \"vertical\",\n    \"calibration_method\": \"&lt;block_does_not_provide_example&gt;\",\n    \"reference_object_class_name\": \"reference-object\",\n    \"reference_width\": 2.5,\n    \"reference_height\": 2.5,\n    \"pixel_ratio\": 100\n}\n</code></pre>"},{"location":"workflows/blocks/dominant_color/","title":"Dominant Color","text":""},{"location":"workflows/blocks/dominant_color/#version-v1","title":"Version <code>v1</code>","text":"<p>Extract the dominant color from an input image using K-means clustering.</p> <p>This block identifies the most prevalent color in an image. Processing time is dependant on color complexity and image size. Most images should complete in under half a second.</p> <p>The output is a list of RGB values representing the dominant color, making it easy  to use in further processing or visualization tasks.</p> <p>Note: The block operates on the assumption that the input image is in RGB format. </p>"},{"location":"workflows/blocks/dominant_color/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/dominant_color@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/dominant_color/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>color_clusters</code> <code>int</code> Number of dominant colors to identify. Higher values increase precision but may slow processing.. \u2705 <code>max_iterations</code> <code>int</code> Max number of iterations to perform. Higher values increase precision but may slow processing.. \u2705 <code>target_size</code> <code>int</code> Sets target for the smallest dimension of the downsampled image in pixels. Lower values increase speed but may reduce precision.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/dominant_color/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Dominant Color</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Dynamic Crop</code>, <code>Pixel Color Count</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Dominant Color</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>color_clusters</code> (<code>integer</code>): Number of dominant colors to identify. Higher values increase precision but may slow processing..</li> <li><code>max_iterations</code> (<code>integer</code>): Max number of iterations to perform. Higher values increase precision but may slow processing..</li> <li><code>target_size</code> (<code>integer</code>): Sets target for the smallest dimension of the downsampled image in pixels. Lower values increase speed but may reduce precision..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>rgb_color</code> (<code>rgb_color</code>): RGB color.</li> </ul> </li> </ul> Example JSON definition of step <code>Dominant Color</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/dominant_color@v1\",\n    \"image\": \"$inputs.image\",\n    \"color_clusters\": 4,\n    \"max_iterations\": 100,\n    \"target_size\": 100\n}\n</code></pre>"},{"location":"workflows/blocks/dot_visualization/","title":"Dot Visualization","text":""},{"location":"workflows/blocks/dot_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>DotVisualization</code> block draws dots on an image at specific coordinates based on provided detections using Supervision's <code>sv.DotAnnotator</code>.</p>"},{"location":"workflows/blocks/dot_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/dot_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/dot_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>position</code> <code>str</code> The anchor position for placing the dot.. \u2705 <code>radius</code> <code>int</code> Radius of the dot in pixels.. \u2705 <code>outline_thickness</code> <code>int</code> Thickness of the outline of the dot in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/dot_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Dot Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Dot Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>position</code> (<code>string</code>): The anchor position for placing the dot..</li> <li><code>radius</code> (<code>integer</code>): Radius of the dot in pixels..</li> <li><code>outline_thickness</code> (<code>integer</code>): Thickness of the outline of the dot in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Dot Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/dot_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"position\": \"CENTER\",\n    \"radius\": 4,\n    \"outline_thickness\": 2\n}\n</code></pre>"},{"location":"workflows/blocks/dynamic_crop/","title":"Dynamic Crop","text":""},{"location":"workflows/blocks/dynamic_crop/#version-v1","title":"Version <code>v1</code>","text":"<p>Create dynamic crops from an image based on detections from detections-based model.</p> <p>This is useful when placed after an ObjectDetection block as part of a multi-stage  workflow. For example, you could use an ObjectDetection block to detect objects, then  the DynamicCropBlock block to crop objects, then an OCR block to run character recognition on  each of the individual cropped regions.</p> <p>In addition, for instance segmentation predictions (which provide segmentation mask for each  bounding box) it is possible to remove background in the crops, outside of detected instances. To enable that functionality, set <code>mask_opacity</code> to positive value and optionally tune  <code>background_color</code>.</p>"},{"location":"workflows/blocks/dynamic_crop/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/dynamic_crop@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/dynamic_crop/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>mask_opacity</code> <code>float</code> For instance segmentation, mask_opacity can be used to control background removal. Opacity 1.0 removes the background, while 0.0 leaves the crop unchanged.. \u2705 <code>background_color</code> <code>Union[Any, str]</code> For background removal based on segmentation mask, new background color can be selected. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67)).. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/dynamic_crop/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Dynamic Crop</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Dominant Color</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Dynamic Crop</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The input image for this step..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): The output of a detection model describing the bounding boxes that will be used to crop the image..</li> <li><code>mask_opacity</code> (<code>float_zero_to_one</code>): For instance segmentation, mask_opacity can be used to control background removal. Opacity 1.0 removes the background, while 0.0 leaves the crop unchanged..</li> <li><code>background_color</code> (Union[<code>rgb_color</code>, <code>string</code>]): For background removal based on segmentation mask, new background color can be selected. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67))..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>crops</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Dynamic Crop</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/dynamic_crop@v1\",\n    \"images\": \"$inputs.image\",\n    \"predictions\": \"$steps.my_object_detection_model.predictions\",\n    \"mask_opacity\": \"&lt;block_does_not_provide_example&gt;\",\n    \"background_color\": \"#431112\"\n}\n</code></pre>"},{"location":"workflows/blocks/dynamic_zone/","title":"Dynamic Zone","text":""},{"location":"workflows/blocks/dynamic_zone/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>DynamicZoneBlock</code> is a transformer block designed to simplify polygon so it's geometrically convex and then reduce number of vertices to requested amount. This block is best suited when Zone needs to be created based on shape of detected object (i.e. basketball field, road segment, zebra crossing etc.) Input detections should be filtered and contain only desired classes of interest.</p>"},{"location":"workflows/blocks/dynamic_zone/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/dynamic_zone@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/dynamic_zone/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>required_number_of_vertices</code> <code>int</code> Keep simplifying polygon until number of vertices matches this number. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/dynamic_zone/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Dynamic Zone</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Path deviation</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Path deviation</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Detection Offset</code>, <code>Detections Filter</code>, <code>Bounding Rectangle</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code></li> <li>outputs: <code>Path deviation</code>, <code>Time in zone</code>, <code>Polygon Zone Visualization</code>, <code>Line Counter</code>, <code>Webhook Sink</code>, <code>Path deviation</code>, <code>Reference Path Visualization</code>, <code>Line Counter</code>, <code>Time in zone</code>, <code>VLM as Classifier</code>, <code>Perspective Correction</code>, <code>Line Counter Visualization</code>, <code>VLM as Detector</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Dynamic Zone</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): .</li> <li><code>required_number_of_vertices</code> (<code>integer</code>): Keep simplifying polygon until number of vertices matches this number.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>zones</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Dynamic Zone</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/dynamic_zone@v1\",\n    \"predictions\": \"$segmentation.predictions\",\n    \"required_number_of_vertices\": 4\n}\n</code></pre>"},{"location":"workflows/blocks/ellipse_visualization/","title":"Ellipse Visualization","text":""},{"location":"workflows/blocks/ellipse_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>EllipseVisualization</code> block draws ellipses that highlight detected objects in an image using Supervision's <code>sv.EllipseAnnotator</code>.</p>"},{"location":"workflows/blocks/ellipse_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/ellipse_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/ellipse_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>thickness</code> <code>int</code> Thickness of the lines in pixels.. \u2705 <code>start_angle</code> <code>int</code> Starting angle of the ellipse in degrees.. \u2705 <code>end_angle</code> <code>int</code> Ending angle of the ellipse in degrees.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/ellipse_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Ellipse Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Ellipse Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the lines in pixels..</li> <li><code>start_angle</code> (<code>integer</code>): Starting angle of the ellipse in degrees..</li> <li><code>end_angle</code> (<code>integer</code>): Ending angle of the ellipse in degrees..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Ellipse Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/ellipse_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"thickness\": 2,\n    \"start_angle\": -45,\n    \"end_angle\": 235\n}\n</code></pre>"},{"location":"workflows/blocks/email_notification/","title":"Macro Syntax Error","text":"<p>File: <code>workflows/blocks/email_notification.md</code></p> <p>Line 26 in Markdown file: unexpected char '$' at 916 <pre><code>message = \"This is example notification. Predicted classes: {{ $parameters.predicted_classes }}\"\n</code></pre></p>"},{"location":"workflows/blocks/expression/","title":"Expression","text":""},{"location":"workflows/blocks/expression/#version-v1","title":"Version <code>v1</code>","text":"<p>Creates specific output based on defined input variables and configured rules - which is useful while creating business logic in workflows.</p> <p>Based on configuration, block takes input data, optionally performs operation on data,  save it as variables and evaluate switch-case like statements to get the final result.</p>"},{"location":"workflows/blocks/expression/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/expression@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/expression/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>data_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data before switch-case instruction. \u274c <code>switch</code> <code>CasesDefinition</code> Definition of switch-case statement. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/expression/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Expression</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Circle Visualization</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Mask Visualization</code>, <code>Perspective Correction</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Expression</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (Union[<code>*</code>, <code>image</code>]): References data to be used to construct results.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>Expression</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/expression@v1\",\n    \"data\": {\n        \"predictions\": \"$steps.model.predictions\",\n        \"reference\": \"$inputs.reference_class_names\"\n    },\n    \"data_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"switch\": {\n        \"cases\": [\n            {\n                \"condition\": {\n                    \"statements\": [\n                        {\n                            \"comparator\": {\n                                \"type\": \"==\"\n                            },\n                            \"left_operand\": {\n                                \"operand_name\": \"class_name\",\n                                \"type\": \"DynamicOperand\"\n                            },\n                            \"right_operand\": {\n                                \"operand_name\": \"reference\",\n                                \"type\": \"DynamicOperand\"\n                            },\n                            \"type\": \"BinaryStatement\"\n                        }\n                    ],\n                    \"type\": \"StatementGroup\"\n                },\n                \"result\": {\n                    \"type\": \"StaticCaseResult\",\n                    \"value\": \"PASS\"\n                },\n                \"type\": \"CaseDefinition\"\n            }\n        ],\n        \"default\": {\n            \"type\": \"StaticCaseResult\",\n            \"value\": \"FAIL\"\n        },\n        \"type\": \"CasesDefinition\"\n    }\n}\n</code></pre>"},{"location":"workflows/blocks/first_non_empty_or_default/","title":"First Non Empty Or Default","text":""},{"location":"workflows/blocks/first_non_empty_or_default/#version-v1","title":"Version <code>v1</code>","text":"<p>Takes input data which may not be present due to filtering or conditional execution and fills with default value to make it compliant with further processing.</p>"},{"location":"workflows/blocks/first_non_empty_or_default/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/first_non_empty_or_default@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/first_non_empty_or_default/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>default</code> <code>Any</code> Default value that will be placed whenever there is no data found. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/first_non_empty_or_default/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>First Non Empty Or Default</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Circle Visualization</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Mask Visualization</code>, <code>Perspective Correction</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>First Non Empty Or Default</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (<code>*</code>): Reference data to replace empty values.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>First Non Empty Or Default</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/first_non_empty_or_default@v1\",\n    \"data\": \"$steps.my_step.predictions\",\n    \"default\": \"empty\"\n}\n</code></pre>"},{"location":"workflows/blocks/florence2_model/","title":"Florence-2 Model","text":""},{"location":"workflows/blocks/florence2_model/#version-v1","title":"Version <code>v1</code>","text":"<p>Dedicated inference server required (GPU recommended) - you may want to use dedicated deployment</p> <p>This Workflow block introduces Florence 2, a Visual Language Model (VLM) capable of performing a  wide range of tasks, including:</p> <ul> <li> <p>Object Detection</p> </li> <li> <p>Instance Segmentation</p> </li> <li> <p>Image Captioning</p> </li> <li> <p>Optical Character Recognition (OCR)</p> </li> <li> <p>and more...</p> </li> </ul> <p>Below is a comprehensive list of tasks supported by the model, along with descriptions on  how to utilize their outputs within the Workflows ecosystem:</p> <p>Task Descriptions:</p> <ul> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Text Detection &amp; Recognition (OCR) (<code>ocr-with-text-detection</code>) - Model detects text regions in the image, and then performs OCR on each detected region</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Captioning (long) (<code>more-detailed-caption</code>) - Model provides a very long description of the image</p> </li> <li> <p>Unprompted Object Detection (<code>object-detection</code>) - Model detects and returns the bounding boxes for prominent objects in the image</p> </li> <li> <p>Object Detection (<code>open-vocabulary-object-detection</code>) - Model detects and returns the bounding boxes for the provided classes</p> </li> <li> <p>Detection &amp; Captioning (<code>object-detection-and-caption</code>) - Model detects prominent objects and captions them</p> </li> <li> <p>Prompted Object Detection (<code>phrase-grounded-object-detection</code>) - Based on the textual prompt, model detects objects matching the descriptions</p> </li> <li> <p>Prompted Instance Segmentation (<code>phrase-grounded-instance-segmentation</code>) - Based on the textual prompt, model segments objects matching the descriptions</p> </li> <li> <p>Segment Bounding Box (<code>detection-grounded-instance-segmentation</code>) - Model segments the object in the provided bounding box into a polygon</p> </li> <li> <p>Classification of Bounding Box (<code>detection-grounded-classification</code>) - Model classifies the object inside the provided bounding box</p> </li> <li> <p>Captioning of Bounding Box (<code>detection-grounded-caption</code>) - Model captions the object in the provided bounding box</p> </li> <li> <p>Text Recognition (OCR) for Bounding Box (<code>detection-grounded-ocr</code>) - Model performs OCR on the text inside the provided bounding box</p> </li> <li> <p>Regions of Interest proposal (<code>region-proposal</code>) - Model proposes Regions of Interest (Bounding Boxes) in the image</p> </li> </ul>"},{"location":"workflows/blocks/florence2_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/florence_2@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/florence2_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>model_version</code> <code>str</code> Model to be used. \u2705 <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the Florence-2 model. \u2705 <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>grounding_detection</code> <code>Optional[List[float], List[int]]</code> Detection to ground Florence-2 model. May be statically provided bounding box <code>[left_top_x, left_top_y, right_bottom_x, right_bottom_y]</code> or result of object-detection model. If the latter is true, one box will be selected based on <code>grounding_selection_mode</code>.. \u2705 <code>grounding_selection_mode</code> <code>str</code> . \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/florence2_model/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Florence-2 Model</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Time in zone</code>, <code>JSON Parser</code>, <code>Perspective Correction</code>, <code>Line Counter Visualization</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Path deviation</code>, <code>Reference Path Visualization</code>, <code>Line Counter</code>, <code>VLM as Classifier</code>, <code>Email Notification</code>, <code>Path deviation</code>, <code>VLM as Detector</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Florence-2 Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>model_version</code> (<code>string</code>): Model to be used.</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the Florence-2 model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>grounding_detection</code> (Union[<code>list_of_values</code>, <code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Detection to ground Florence-2 model. May be statically provided bounding box <code>[left_top_x, left_top_y, right_bottom_x, right_bottom_y]</code> or result of object-detection model. If the latter is true, one box will be selected based on <code>grounding_selection_mode</code>..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>raw_output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>parsed_output</code> (<code>dictionary</code>): Dictionary.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Florence-2 Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/florence_2@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_version\": \"florence-2-base\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"grounding_detection\": \"$steps.detection.predictions\",\n    \"grounding_selection_mode\": \"first\"\n}\n</code></pre>"},{"location":"workflows/blocks/google_gemini/","title":"Google Gemini","text":""},{"location":"workflows/blocks/google_gemini/#version-v1","title":"Version <code>v1</code>","text":"<p>Ask a question to Google's Gemini model with vision capabilities.</p> <p>You can specify arbitrary text prompts or predefined ones, the block supports the following types of prompt:</p> <ul> <li> <p>Open Prompt (<code>unconstrained</code>) - Use any prompt to generate a raw response</p> </li> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Visual Question Answering (<code>visual-question-answering</code>) - Model answers the question you submit in the prompt</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Single-Label Classification (<code>classification</code>) - Model classifies the image content as one of the provided classes</p> </li> <li> <p>Multi-Label Classification (<code>multi-label-classification</code>) - Model classifies the image content as one or more of the provided classes</p> </li> <li> <p>Unprompted Object Detection (<code>object-detection</code>) - Model detects and returns the bounding boxes for prominent objects in the image</p> </li> <li> <p>Structured Output Generation (<code>structured-answering</code>) - Model returns a JSON response with the specified fields</p> </li> </ul> <p>You need to provide your Google AI API key to use the Gemini model. </p> <p>WARNING!</p> <p>This block makes use of <code>/v1beta</code> API of Google Gemini model - the implementation may change  in the future, without guarantee of backward compatibility.</p>"},{"location":"workflows/blocks/google_gemini/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/google_gemini@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/google_gemini/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the Gemini model. \u2705 <code>output_structure</code> <code>Dict[str, str]</code> Dictionary with structure of expected JSON response. \u274c <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>api_key</code> <code>str</code> Your Google AI API key. \u2705 <code>model_version</code> <code>str</code> Model to be used. \u2705 <code>max_tokens</code> <code>int</code> Maximum number of tokens the model can generate in it's response.. \u274c <code>temperature</code> <code>float</code> Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are.. \u2705 <code>max_concurrent_requests</code> <code>int</code> Number of concurrent requests that can be executed by block when batch of input images provided. If not given - block defaults to value configured globally in Workflows Execution Engine. Please restrict if you hit Google Gemini API limits.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/google_gemini/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Google Gemini</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Time in zone</code>, <code>JSON Parser</code>, <code>Perspective Correction</code>, <code>Line Counter Visualization</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Path deviation</code>, <code>Reference Path Visualization</code>, <code>Line Counter</code>, <code>VLM as Classifier</code>, <code>Email Notification</code>, <code>Path deviation</code>, <code>VLM as Detector</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Google Gemini</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the Gemini model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>api_key</code> (<code>string</code>): Your Google AI API key.</li> <li><code>model_version</code> (<code>string</code>): Model to be used.</li> <li><code>temperature</code> (<code>float</code>): Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Google Gemini</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/google_gemini@v1\",\n    \"images\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"output_structure\": {\n        \"my_key\": \"description\"\n    },\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"api_key\": \"xxx-xxx\",\n    \"model_version\": \"gemini-1.5-flash\",\n    \"max_tokens\": \"&lt;block_does_not_provide_example&gt;\",\n    \"temperature\": \"&lt;block_does_not_provide_example&gt;\",\n    \"max_concurrent_requests\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre>"},{"location":"workflows/blocks/google_vision_ocr/","title":"Google Vision OCR","text":""},{"location":"workflows/blocks/google_vision_ocr/#version-v1","title":"Version <code>v1</code>","text":"<p>Detect text in images using Google Vision OCR.</p> <p>Supported types of text detection:</p> <ul> <li><code>text_detection</code>: optimized for areas of text within a larger image.</li> <li><code>ocr_text_detection</code>: optimized for dense text documents.</li> </ul> <p>You need to provide your Google Vision API key to use this block.</p>"},{"location":"workflows/blocks/google_vision_ocr/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/google_vision_ocr@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/google_vision_ocr/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>ocr_type</code> <code>str</code> Type of OCR to use. \u274c <code>api_key</code> <code>str</code> Your Google Vision API key. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/google_vision_ocr/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Google Vision OCR</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Google Vision OCR</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): Image to run OCR.</li> <li><code>api_key</code> (<code>string</code>): Your Google Vision API key.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>text</code> (<code>string</code>): String value.</li> <li><code>language</code> (<code>string</code>): String value.</li> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Google Vision OCR</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/google_vision_ocr@v1\",\n    \"image\": \"$inputs.image\",\n    \"ocr_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"api_key\": \"xxx-xxx\"\n}\n</code></pre>"},{"location":"workflows/blocks/halo_visualization/","title":"Halo Visualization","text":""},{"location":"workflows/blocks/halo_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>HaloVisualization</code> block uses a detected polygon from an instance segmentation to draw a halo using <code>sv.HaloAnnotator</code>.</p>"},{"location":"workflows/blocks/halo_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/halo_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/halo_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>opacity</code> <code>float</code> Transparency of the halo overlay.. \u2705 <code>kernel_size</code> <code>int</code> Size of the average pooling kernel used for creating the halo.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/halo_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Halo Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Halo Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the halo overlay..</li> <li><code>kernel_size</code> (<code>integer</code>): Size of the average pooling kernel used for creating the halo..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Halo Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/halo_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.instance_segmentation_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"opacity\": 0.8,\n    \"kernel_size\": 40\n}\n</code></pre>"},{"location":"workflows/blocks/image_blur/","title":"Image Blur","text":""},{"location":"workflows/blocks/image_blur/#version-v1","title":"Version <code>v1</code>","text":"<p>Apply a blur to an image.  The blur type and kernel size can be specified.</p>"},{"location":"workflows/blocks/image_blur/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/image_blur@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_blur/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>blur_type</code> <code>str</code> Type of Blur to perform on image.. \u2705 <code>kernel_size</code> <code>int</code> Size of the average pooling kernel used for blurring.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_blur/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Image Blur</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Blur</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>blur_type</code> (<code>string</code>): Type of Blur to perform on image..</li> <li><code>kernel_size</code> (<code>integer</code>): Size of the average pooling kernel used for blurring..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Blur</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/image_blur@v1\",\n    \"image\": \"$inputs.image\",\n    \"blur_type\": \"average\",\n    \"kernel_size\": 5\n}\n</code></pre>"},{"location":"workflows/blocks/image_contours/","title":"Image Contours","text":""},{"location":"workflows/blocks/image_contours/#version-v1","title":"Version <code>v1</code>","text":"<p>Finds the contours in an image. It returns the contours and number of contours. The input image should be thresholded before using this block.</p>"},{"location":"workflows/blocks/image_contours/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/contours_detection@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_contours/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>line_thickness</code> <code>int</code> Line thickness for drawing contours.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_contours/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Image Contours</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Contours</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>line_thickness</code> (<code>integer</code>): Line thickness for drawing contours..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> <li><code>contours</code> (<code>contours</code>): List of numpy arrays where each array represents contour points.</li> <li><code>hierarchy</code> (<code>numpy_array</code>): Numpy array.</li> <li><code>number_contours</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Contours</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/contours_detection@v1\",\n    \"image\": \"$inputs.image\",\n    \"line_thickness\": 3\n}\n</code></pre>"},{"location":"workflows/blocks/image_convert_grayscale/","title":"Image Convert Grayscale","text":""},{"location":"workflows/blocks/image_convert_grayscale/#version-v1","title":"Version <code>v1</code>","text":"<p>Block to convert an RGB image to grayscale. The output image will have only one channel.</p>"},{"location":"workflows/blocks/image_convert_grayscale/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/convert_grayscale@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_convert_grayscale/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_convert_grayscale/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Image Convert Grayscale</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Convert Grayscale</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Convert Grayscale</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/convert_grayscale@v1\",\n    \"image\": \"$inputs.image\"\n}\n</code></pre>"},{"location":"workflows/blocks/image_preprocessing/","title":"Image Preprocessing","text":""},{"location":"workflows/blocks/image_preprocessing/#version-v1","title":"Version <code>v1</code>","text":"<p>Apply a resize, flip, or rotation step to an image. </p> <p>Width and height are required for resizing. Degrees are required for rotating. Flip type is required for flipping.</p>"},{"location":"workflows/blocks/image_preprocessing/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/image_preprocessing@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_preprocessing/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>task_type</code> <code>str</code> Preprocessing task to be applied to the image.. \u274c <code>width</code> <code>int</code> Width of the image to be resized to.. \u2705 <code>height</code> <code>int</code> Height of the image to be resized to.. \u2705 <code>rotation_degrees</code> <code>int</code> Positive value to rotate clockwise, negative value to rotate counterclockwise. \u2705 <code>flip_type</code> <code>str</code> Type of flip to be applied to the image.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_preprocessing/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Image Preprocessing</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Preprocessing</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>width</code> (<code>integer</code>): Width of the image to be resized to..</li> <li><code>height</code> (<code>integer</code>): Height of the image to be resized to..</li> <li><code>rotation_degrees</code> (<code>integer</code>): Positive value to rotate clockwise, negative value to rotate counterclockwise.</li> <li><code>flip_type</code> (<code>string</code>): Type of flip to be applied to the image..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Preprocessing</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/image_preprocessing@v1\",\n    \"image\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"width\": 640,\n    \"height\": 640,\n    \"rotation_degrees\": 90,\n    \"flip_type\": \"vertical\"\n}\n</code></pre>"},{"location":"workflows/blocks/image_slicer/","title":"Image Slicer","text":""},{"location":"workflows/blocks/image_slicer/#version-v1","title":"Version <code>v1</code>","text":"<p>This block enables Slicing Adaptive Inference (SAHI) technique in  Workflows providing implementation for first step of procedure - making slices out of input image.</p> <p>To use the block effectively, it must be paired with detection model (object-detection or  instance segmentation) running against output images from this block. At the end -  Detections Stitch block must be applied on top of predictions to merge them as if  the prediction was made against input image, not its slices.</p> <p>We recommend adjusting the size of slices to match the model's input size and the scale of objects in the dataset  the model was trained on. Models generally perform best on data that is similar to what they encountered during  training. The default size of slices is 640, but this might not be optimal if the model's input size is 320, as each  slice would be downsized by a factor of two during inference. Similarly, if the model's input size is 1280, each slice  will be artificially up-scaled. The best setup should be determined experimentally based on the specific data and model  you are using.</p> <p>To learn more about SAHI please visit Roboflow blog which describes the technique in details, yet not in context of Roboflow workflows.</p>"},{"location":"workflows/blocks/image_slicer/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/image_slicer@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_slicer/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>slice_width</code> <code>int</code> Width of each slice, in pixels. \u2705 <code>slice_height</code> <code>int</code> Height of each slice, in pixels. \u2705 <code>overlap_ratio_width</code> <code>float</code> Overlap ratio between consecutive slices in the width dimension. \u2705 <code>overlap_ratio_height</code> <code>float</code> Overlap ratio between consecutive slices in the height dimension. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_slicer/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Image Slicer</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Slicer</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>slice_width</code> (<code>integer</code>): Width of each slice, in pixels.</li> <li><code>slice_height</code> (<code>integer</code>): Height of each slice, in pixels.</li> <li><code>overlap_ratio_width</code> (<code>float_zero_to_one</code>): Overlap ratio between consecutive slices in the width dimension.</li> <li><code>overlap_ratio_height</code> (<code>float_zero_to_one</code>): Overlap ratio between consecutive slices in the height dimension.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>slices</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Slicer</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/image_slicer@v1\",\n    \"image\": \"$inputs.image\",\n    \"slice_width\": 320,\n    \"slice_height\": 320,\n    \"overlap_ratio_width\": 0.2,\n    \"overlap_ratio_height\": 0.2\n}\n</code></pre>"},{"location":"workflows/blocks/image_threshold/","title":"Image Threshold","text":""},{"location":"workflows/blocks/image_threshold/#version-v1","title":"Version <code>v1</code>","text":"<p>Apply a threshold to an image. The image must be in grayscale.</p>"},{"location":"workflows/blocks/image_threshold/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/threshold@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/image_threshold/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>threshold_type</code> <code>str</code> Type of Edge Detection to perform.. \u2705 <code>thresh_value</code> <code>int</code> Threshold value.. \u2705 <code>max_value</code> <code>int</code> Maximum value for thresholding. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/image_threshold/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Image Threshold</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Image Threshold</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>threshold_type</code> (<code>string</code>): Type of Edge Detection to perform..</li> <li><code>thresh_value</code> (<code>integer</code>): Threshold value..</li> <li><code>max_value</code> (<code>integer</code>): Maximum value for thresholding.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Image Threshold</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/threshold@v1\",\n    \"image\": \"$inputs.image\",\n    \"threshold_type\": \"binary\",\n    \"thresh_value\": 127,\n    \"max_value\": 255\n}\n</code></pre>"},{"location":"workflows/blocks/instance_segmentation_model/","title":"Instance Segmentation Model","text":""},{"location":"workflows/blocks/instance_segmentation_model/#version-v1","title":"Version <code>v1</code>","text":"<p>Run inference on an instance segmentation model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/instance_segmentation_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_instance_segmentation_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/instance_segmentation_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier. \u2705 <code>class_agnostic_nms</code> <code>bool</code> Value to decide if NMS is to be used in class-agnostic mode.. \u2705 <code>class_filter</code> <code>List[str]</code> List of classes to retrieve from predictions (to define subset of those which was used while model training). \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions. \u2705 <code>iou_threshold</code> <code>float</code> Parameter of NMS, to decide on minimum box intersection over union to merge boxes. \u2705 <code>max_detections</code> <code>int</code> Maximum number of detections to return. \u2705 <code>max_candidates</code> <code>int</code> Maximum number of candidates as NMS input to be taken into account.. \u2705 <code>mask_decode_mode</code> <code>str</code> Parameter of mask decoding in prediction post-processing.. \u2705 <code>tradeoff_factor</code> <code>float</code> Post-processing parameter to dictate tradeoff between fast and accurate. \u2705 <code>disable_active_learning</code> <code>bool</code> Parameter to decide if Active Learning data sampling is disabled for the model. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/instance_segmentation_model/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Instance Segmentation Model</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Instance Segmentation Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier.</li> <li><code>class_agnostic_nms</code> (<code>boolean</code>): Value to decide if NMS is to be used in class-agnostic mode..</li> <li><code>class_filter</code> (<code>list_of_values</code>): List of classes to retrieve from predictions (to define subset of those which was used while model training).</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions.</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Parameter of NMS, to decide on minimum box intersection over union to merge boxes.</li> <li><code>max_detections</code> (<code>integer</code>): Maximum number of detections to return.</li> <li><code>max_candidates</code> (<code>integer</code>): Maximum number of candidates as NMS input to be taken into account..</li> <li><code>mask_decode_mode</code> (<code>string</code>): Parameter of mask decoding in prediction post-processing..</li> <li><code>tradeoff_factor</code> (<code>float_zero_to_one</code>): Post-processing parameter to dictate tradeoff between fast and accurate.</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Parameter to decide if Active Learning data sampling is disabled for the model.</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>inference_id</code> (<code>string</code>): String value.</li> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Instance Segmentation Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_instance_segmentation_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"class_agnostic_nms\": true,\n    \"class_filter\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"confidence\": 0.3,\n    \"iou_threshold\": 0.4,\n    \"max_detections\": 300,\n    \"max_candidates\": 3000,\n    \"mask_decode_mode\": \"accurate\",\n    \"tradeoff_factor\": 0.3,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre>"},{"location":"workflows/blocks/json_parser/","title":"JSON Parser","text":""},{"location":"workflows/blocks/json_parser/#version-v1","title":"Version <code>v1</code>","text":"<p>The block expects string input that would be produced by blocks exposing Large Language Models (LLMs) and  Visual Language Models (VLMs). Input is parsed to JSON, and its keys are exposed as block outputs.</p> <p>Accepted formats: - valid JSON strings - JSON documents wrapped with Markdown tags (very common for GPT responses) <pre><code>{\"my\": \"json\"}\n</code></pre></p> <p>Details regarding block behavior:</p> <ul> <li> <p><code>error_status</code> is set <code>True</code> whenever at least one of <code>expected_fields</code> cannot be retrieved from input</p> </li> <li> <p>in case of multiple markdown blocks with raw JSON content - only first will be parsed and returned, while <code>error_status</code> will remain <code>False</code></p> </li> </ul>"},{"location":"workflows/blocks/json_parser/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/json_parser@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/json_parser/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>expected_fields</code> <code>List[str]</code> List of expected JSON fields. <code>error_status</code> field name is reserved and cannot be used.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/json_parser/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>JSON Parser</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>OpenAI</code>, <code>Google Gemini</code>, <code>Anthropic Claude</code>, <code>Florence-2 Model</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>LMM For Classification</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Circle Visualization</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>Size Measurement</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Mask Visualization</code>, <code>Perspective Correction</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>JSON Parser</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>JSON Parser</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>raw_json</code> (<code>language_model_output</code>): The string with raw JSON to parse..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>JSON Parser</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/json_parser@v1\",\n    \"raw_json\": [\n        \"$steps.lmm.output\"\n    ],\n    \"expected_fields\": [\n        \"field_a\",\n        \"field_b\"\n    ]\n}\n</code></pre>"},{"location":"workflows/blocks/keypoint_detection_model/","title":"Keypoint Detection Model","text":""},{"location":"workflows/blocks/keypoint_detection_model/#version-v1","title":"Version <code>v1</code>","text":"<p>Run inference on a keypoint detection model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/keypoint_detection_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_keypoint_detection_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/keypoint_detection_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier. \u2705 <code>class_agnostic_nms</code> <code>bool</code> Value to decide if NMS is to be used in class-agnostic mode.. \u2705 <code>class_filter</code> <code>List[str]</code> List of classes to retrieve from predictions (to define subset of those which was used while model training). \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions. \u2705 <code>iou_threshold</code> <code>float</code> Parameter of NMS, to decide on minimum box intersection over union to merge boxes. \u2705 <code>max_detections</code> <code>int</code> Maximum number of detections to return. \u2705 <code>max_candidates</code> <code>int</code> Maximum number of candidates as NMS input to be taken into account.. \u2705 <code>keypoint_confidence</code> <code>float</code> Confidence threshold to predict keypoint as visible.. \u2705 <code>disable_active_learning</code> <code>bool</code> Parameter to decide if Active Learning data sampling is disabled for the model. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/keypoint_detection_model/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Keypoint Detection Model</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Detections Transformation</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Label Visualization</code>, <code>Crop Visualization</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Dot Visualization</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Detections Classes Replacement</code>, <code>Roboflow Dataset Upload</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Segment Anything 2 Model</code>, <code>Webhook Sink</code>, <code>Triangle Visualization</code>, <code>Stability AI Inpainting</code>, <code>Keypoint Visualization</code>, <code>Trace Visualization</code>, <code>Corner Visualization</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Pixelate Visualization</code>, <code>Model Comparison Visualization</code>, <code>Email Notification</code>, <code>Florence-2 Model</code>, <code>Blur Visualization</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Keypoint Detection Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier.</li> <li><code>class_agnostic_nms</code> (<code>boolean</code>): Value to decide if NMS is to be used in class-agnostic mode..</li> <li><code>class_filter</code> (<code>list_of_values</code>): List of classes to retrieve from predictions (to define subset of those which was used while model training).</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions.</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Parameter of NMS, to decide on minimum box intersection over union to merge boxes.</li> <li><code>max_detections</code> (<code>integer</code>): Maximum number of detections to return.</li> <li><code>max_candidates</code> (<code>integer</code>): Maximum number of candidates as NMS input to be taken into account..</li> <li><code>keypoint_confidence</code> (<code>float_zero_to_one</code>): Confidence threshold to predict keypoint as visible..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Parameter to decide if Active Learning data sampling is disabled for the model.</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>inference_id</code> (<code>string</code>): String value.</li> <li><code>predictions</code> (<code>keypoint_detection_prediction</code>): Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Keypoint Detection Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_keypoint_detection_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"class_agnostic_nms\": true,\n    \"class_filter\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"confidence\": 0.3,\n    \"iou_threshold\": 0.4,\n    \"max_detections\": 300,\n    \"max_candidates\": 3000,\n    \"keypoint_confidence\": 0.3,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre>"},{"location":"workflows/blocks/keypoint_visualization/","title":"Keypoint Visualization","text":""},{"location":"workflows/blocks/keypoint_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>KeypointVisualization</code> block uses a detections from an keypoint detection model to draw keypoints on objects using <code>sv.VertexAnnotator</code>.</p>"},{"location":"workflows/blocks/keypoint_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/keypoint_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/keypoint_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>annotator_type</code> <code>str</code> Type of annotator to be used for keypoint visualization.. \u274c <code>color</code> <code>str</code> Color of the keypoint.. \u2705 <code>text_color</code> <code>str</code> Text color of the keypoint.. \u2705 <code>text_scale</code> <code>float</code> Scale of the text.. \u2705 <code>text_thickness</code> <code>int</code> Thickness of the text characters.. \u2705 <code>text_padding</code> <code>int</code> Padding around the text in pixels.. \u2705 <code>thickness</code> <code>int</code> Thickness of the outline in pixels.. \u2705 <code>radius</code> <code>int</code> Radius of the keypoint in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/keypoint_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Keypoint Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Keypoint Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (<code>keypoint_detection_prediction</code>): Predictions.</li> <li><code>color</code> (<code>string</code>): Color of the keypoint..</li> <li><code>text_color</code> (<code>string</code>): Text color of the keypoint..</li> <li><code>text_scale</code> (<code>float</code>): Scale of the text..</li> <li><code>text_thickness</code> (<code>integer</code>): Thickness of the text characters..</li> <li><code>text_padding</code> (<code>integer</code>): Padding around the text in pixels..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the outline in pixels..</li> <li><code>radius</code> (<code>integer</code>): Radius of the keypoint in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Keypoint Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/keypoint_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.keypoint_detection_model.predictions\",\n    \"annotator_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"color\": \"#A351FB\",\n    \"text_color\": \"black\",\n    \"text_scale\": 0.5,\n    \"text_thickness\": 1,\n    \"text_padding\": 10,\n    \"thickness\": 2,\n    \"radius\": 10\n}\n</code></pre>"},{"location":"workflows/blocks/label_visualization/","title":"Label Visualization","text":""},{"location":"workflows/blocks/label_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>LabelVisualization</code> block draws labels on an image at specific coordinates based on provided detections using Supervision's <code>sv.LabelAnnotator</code>.</p>"},{"location":"workflows/blocks/label_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/label_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/label_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>text</code> <code>str</code> The type of text to display.. \u2705 <code>text_position</code> <code>str</code> The anchor position for placing the label.. \u2705 <code>text_color</code> <code>str</code> Color of the text.. \u2705 <code>text_scale</code> <code>float</code> Scale of the text.. \u2705 <code>text_thickness</code> <code>int</code> Thickness of the text characters.. \u2705 <code>text_padding</code> <code>int</code> Padding around the text in pixels.. \u2705 <code>border_radius</code> <code>int</code> Radius of the label in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/label_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Label Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Label Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>text</code> (<code>string</code>): The type of text to display..</li> <li><code>text_position</code> (<code>string</code>): The anchor position for placing the label..</li> <li><code>text_color</code> (<code>string</code>): Color of the text..</li> <li><code>text_scale</code> (<code>float</code>): Scale of the text..</li> <li><code>text_thickness</code> (<code>integer</code>): Thickness of the text characters..</li> <li><code>text_padding</code> (<code>integer</code>): Padding around the text in pixels..</li> <li><code>border_radius</code> (<code>integer</code>): Radius of the label in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Label Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/label_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"text\": \"LABEL\",\n    \"text_position\": \"CENTER\",\n    \"text_color\": \"WHITE\",\n    \"text_scale\": 1.0,\n    \"text_thickness\": 1,\n    \"text_padding\": 10,\n    \"border_radius\": 0\n}\n</code></pre>"},{"location":"workflows/blocks/line_counter/","title":"Line Counter","text":""},{"location":"workflows/blocks/line_counter/#version-v2","title":"Version <code>v2</code>","text":"<p>The <code>LineCounter</code> is an analytics block designed to count objects passing the line. The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames)</p>"},{"location":"workflows/blocks/line_counter/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/line_counter@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/line_counter/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>line_segment</code> <code>List[Any]</code> Line in the format [[x1, y1], [x2, y2]] consisting of exactly two points. For line [[0, 100], [100, 100]] line will count objects entering from the bottom as IN. \u2705 <code>triggering_anchor</code> <code>str</code> Point from the detection for triggering line crossing.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/line_counter/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Line Counter</code> in version <code>v2</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Anthropic Claude</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Size Measurement</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Clip Comparison</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Line Counter Visualization</code>, <code>Webhook Sink</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Line Counter</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Predictions.</li> <li><code>line_segment</code> (<code>list_of_values</code>): Line in the format [[x1, y1], [x2, y2]] consisting of exactly two points. For line [[0, 100], [100, 100]] line will count objects entering from the bottom as IN.</li> <li><code>triggering_anchor</code> (<code>string</code>): Point from the detection for triggering line crossing..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>count_in</code> (<code>integer</code>): Integer value.</li> <li><code>count_out</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Line Counter</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/line_counter@v2\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"line_segment\": [\n        [\n            0,\n            50\n        ],\n        [\n            500,\n            50\n        ]\n    ],\n    \"triggering_anchor\": \"CENTER\"\n}\n</code></pre>"},{"location":"workflows/blocks/line_counter/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>LineCounter</code> is an analytics block designed to count objects passing the line. The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames)</p>"},{"location":"workflows/blocks/line_counter/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/line_counter@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/line_counter/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>line_segment</code> <code>List[Any]</code> Line in the format [[x1, y1], [x2, y2]] consisting of exactly two points. For line [[0, 100], [100, 100]] line will count objects entering from the bottom as IN. \u2705 <code>triggering_anchor</code> <code>str</code> Point from the detection for triggering line crossing.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/line_counter/#available-connections_1","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Line Counter</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Anthropic Claude</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Size Measurement</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Clip Comparison</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Line Counter Visualization</code>, <code>Webhook Sink</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Line Counter</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>metadata</code> (<code>video_metadata</code>): not available.</li> <li><code>detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Predictions.</li> <li><code>line_segment</code> (<code>list_of_values</code>): Line in the format [[x1, y1], [x2, y2]] consisting of exactly two points. For line [[0, 100], [100, 100]] line will count objects entering from the bottom as IN.</li> <li><code>triggering_anchor</code> (<code>string</code>): Point from the detection for triggering line crossing..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>count_in</code> (<code>integer</code>): Integer value.</li> <li><code>count_out</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Line Counter</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/line_counter@v1\",\n    \"metadata\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"line_segment\": [\n        [\n            0,\n            50\n        ],\n        [\n            500,\n            50\n        ]\n    ],\n    \"triggering_anchor\": \"CENTER\"\n}\n</code></pre>"},{"location":"workflows/blocks/line_counter_visualization/","title":"Line Counter Visualization","text":""},{"location":"workflows/blocks/line_counter_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>LineCounterZoneVisualization</code> block draws line in an image with a specified color and opacity. Please note: line zone will be drawn on top of image passed to this block, this block should be placed before other visualization blocks in the workflow.</p>"},{"location":"workflows/blocks/line_counter_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/line_counter_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/line_counter_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>zone</code> <code>List[Any]</code> Line in the format [[x1, y1], [x2, y2]] consisting of exactly two points.. \u2705 <code>color</code> <code>str</code> Color of the zone.. \u2705 <code>thickness</code> <code>int</code> Thickness of the lines in pixels.. \u2705 <code>text_thickness</code> <code>int</code> Thickness of the text in pixels.. \u2705 <code>text_scale</code> <code>float</code> Scale of the text.. \u2705 <code>count_in</code> <code>int</code> Reference to the number of objects that crossed into the line zone.. \u2705 <code>count_out</code> <code>int</code> Reference to the number of objects that crossed out of the line zone.. \u2705 <code>opacity</code> <code>float</code> Transparency of the Mask overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/line_counter_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Line Counter Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Anthropic Claude</code>, <code>Crop Visualization</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Pixel Color Count</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Line Counter Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>zone</code> (<code>list_of_values</code>): Line in the format [[x1, y1], [x2, y2]] consisting of exactly two points..</li> <li><code>color</code> (<code>string</code>): Color of the zone..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the lines in pixels..</li> <li><code>text_thickness</code> (<code>integer</code>): Thickness of the text in pixels..</li> <li><code>text_scale</code> (<code>float</code>): Scale of the text..</li> <li><code>count_in</code> (<code>integer</code>): Reference to the number of objects that crossed into the line zone..</li> <li><code>count_out</code> (<code>integer</code>): Reference to the number of objects that crossed out of the line zone..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the Mask overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Line Counter Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/line_counter_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"zone\": [\n        [\n            0,\n            50\n        ],\n        [\n            500,\n            50\n        ]\n    ],\n    \"color\": \"WHITE\",\n    \"thickness\": 2,\n    \"text_thickness\": 1,\n    \"text_scale\": 1.0,\n    \"count_in\": \"$steps.line_counter.count_in\",\n    \"count_out\": \"$steps.line_counter.count_out\",\n    \"opacity\": 0.3\n}\n</code></pre>"},{"location":"workflows/blocks/lmm/","title":"LMM","text":""},{"location":"workflows/blocks/lmm/#version-v1","title":"Version <code>v1</code>","text":"<p>Ask a question to a Large Multimodal Model (LMM) with an image and text.</p> <p>You can specify arbitrary text prompts to an LMMBlock.</p> <p>The LLMBlock supports two LMMs:</p> <ul> <li>OpenAI's GPT-4 with Vision, and;</li> <li>CogVLM.</li> </ul> <p>You need to provide your OpenAI API key to use the GPT-4 with Vision model. You do not  need to provide an API key to use CogVLM.</p> <p>If you want to classify an image into one or more categories, we recommend using the  dedicated LMMForClassificationBlock.</p>"},{"location":"workflows/blocks/lmm/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/lmm@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/lmm/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>prompt</code> <code>str</code> Holds unconstrained text prompt to LMM mode. \u2705 <code>lmm_type</code> <code>str</code> Type of LMM to be used. \u2705 <code>lmm_config</code> <code>LMMConfig</code> Configuration of LMM. \u274c <code>remote_api_key</code> <code>str</code> Holds API key required to call LMM model - in current state of development, we require OpenAI key when <code>lmm_type=gpt_4v</code> and do not require additional API key for CogVLM calls.. \u2705 <code>json_output</code> <code>Dict[str, str]</code> Holds dictionary that maps name of requested output field into its description. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/lmm/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>LMM</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>LMM For Classification</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Circle Visualization</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>Size Measurement</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Email Notification</code>, <code>Dimension Collapse</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Mask Visualization</code>, <code>Perspective Correction</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>JSON Parser</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>LMM</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>prompt</code> (<code>string</code>): Holds unconstrained text prompt to LMM mode.</li> <li><code>lmm_type</code> (<code>string</code>): Type of LMM to be used.</li> <li><code>remote_api_key</code> (<code>string</code>): Holds API key required to call LMM model - in current state of development, we require OpenAI key when <code>lmm_type=gpt_4v</code> and do not require additional API key for CogVLM calls..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>image</code> (<code>image_metadata</code>): Dictionary with image metadata required by supervision.</li> <li><code>structured_output</code> (<code>dictionary</code>): Dictionary.</li> <li><code>raw_output</code> (<code>string</code>): String value.</li> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>LMM</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/lmm@v1\",\n    \"images\": \"$inputs.image\",\n    \"prompt\": \"my prompt\",\n    \"lmm_type\": \"gpt_4v\",\n    \"lmm_config\": {\n        \"gpt_image_detail\": \"low\",\n        \"gpt_model_version\": \"gpt-4o\",\n        \"max_tokens\": 200\n    },\n    \"remote_api_key\": \"xxx-xxx\",\n    \"json_output\": {\n        \"count\": \"number of cats in the picture\"\n    }\n}\n</code></pre>"},{"location":"workflows/blocks/lmm_for_classification/","title":"LMM For Classification","text":""},{"location":"workflows/blocks/lmm_for_classification/#version-v1","title":"Version <code>v1</code>","text":"<p>Classify an image into one or more categories using a Large Multimodal Model (LMM).</p> <p>You can specify arbitrary classes to an LMMBlock.</p> <p>The LLMBlock supports two LMMs:</p> <ul> <li>OpenAI's GPT-4 with Vision, and;</li> <li>CogVLM.</li> </ul> <p>You need to provide your OpenAI API key to use the GPT-4 with Vision model. You do not  need to provide an API key to use CogVLM.</p>"},{"location":"workflows/blocks/lmm_for_classification/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/lmm_for_classification@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/lmm_for_classification/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>lmm_type</code> <code>str</code> Type of LMM to be used. \u2705 <code>classes</code> <code>List[str]</code> List of classes that LMM shall classify against. \u2705 <code>lmm_config</code> <code>LMMConfig</code> Configuration of LMM. \u274c <code>remote_api_key</code> <code>str</code> Holds API key required to call LMM model - in current state of development, we require OpenAI key when <code>lmm_type=gpt_4v</code> and do not require additional API key for CogVLM calls.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/lmm_for_classification/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>LMM For Classification</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Webhook Sink</code>, <code>Email Notification</code>, <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Stability AI Inpainting</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>LMM For Classification</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>lmm_type</code> (<code>string</code>): Type of LMM to be used.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes that LMM shall classify against.</li> <li><code>remote_api_key</code> (<code>string</code>): Holds API key required to call LMM model - in current state of development, we require OpenAI key when <code>lmm_type=gpt_4v</code> and do not require additional API key for CogVLM calls..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>raw_output</code> (<code>string</code>): String value.</li> <li><code>top</code> (<code>top_class</code>): String value representing top class predicted by classification model.</li> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>image</code> (<code>image_metadata</code>): Dictionary with image metadata required by supervision.</li> <li><code>prediction_type</code> (<code>prediction_type</code>): String value with type of prediction.</li> </ul> </li> </ul> Example JSON definition of step <code>LMM For Classification</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/lmm_for_classification@v1\",\n    \"images\": \"$inputs.image\",\n    \"lmm_type\": \"gpt_4v\",\n    \"classes\": [\n        \"a\",\n        \"b\"\n    ],\n    \"lmm_config\": {\n        \"gpt_image_detail\": \"low\",\n        \"gpt_model_version\": \"gpt-4o\",\n        \"max_tokens\": 200\n    },\n    \"remote_api_key\": \"xxx-xxx\"\n}\n</code></pre>"},{"location":"workflows/blocks/local_file_sink/","title":"Local File Sink","text":""},{"location":"workflows/blocks/local_file_sink/#version-v1","title":"Version <code>v1</code>","text":"<p>The Local File Sink block saves workflow data as files on a local file system. It allows users to configure how  the data is stored, either:</p> <ul> <li> <p>aggregating multiple entries into a single file</p> </li> <li> <p>or saving each entry as a separate file. </p> </li> </ul> <p>This block is useful for logging, data export, or preparing files for subsequent processing.</p>"},{"location":"workflows/blocks/local_file_sink/#file-content-file-type-and-output-mode","title":"File Content, File Type and Output Mode","text":"<p><code>content</code> is expected to be the output from another block producing string values of specific types denoted by <code>file_type</code>.</p> <p><code>output_mode</code> set into <code>append_log</code> will make the block appending single file with consecutive entries passed to <code>content</code> input up to <code>max_entries_per_file</code>. In this mode it is important that </p> <p><code>file_type</code> in <code>append_log</code> mode</p> <p>Contrary to <code>separate_files</code> output mode, <code>append_log</code> mode may introduce subtle changes into the structure of the <code>content</code> to properly append it into existing file, hence setting proper <code>file_type</code> is crucial:</p> <ul> <li> <p><code>file_type=json</code>: in <code>append_log</code> mode, the block will create <code>*.jsonl</code> file in  JSON Lines format - for that to be possible, each JSON document will be parsed and dumped to ensure that it will fit into single line.</p> </li> <li> <p><code>file_type=csv</code>: in <code>append_log</code> mode, the block will deduct the first line from the  content (making it required for CSV content to always be shipped with header row) of  consecutive updates into the content of already created file.</p> </li> </ul> <p>Security considerations</p> <p>The block has an ability to write to the file system. If you find this unintended in your system,  you can disable the block setting environmental variable <code>ALLOW_WORKFLOW_BLOCKS_ACCESSING_LOCAL_STORAGE=False</code> in the environment which host Workflows Execution Engine.</p> <p>If you want to restrict the directory which may be used to write data - set  environmental variable <code>WORKFLOW_BLOCKS_WRITE_DIRECTORY</code> to the absolute path of directory which you allow to be used.</p>"},{"location":"workflows/blocks/local_file_sink/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/local_file_sink@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/local_file_sink/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>file_type</code> <code>str</code> Type of the file. \u274c <code>output_mode</code> <code>str</code> Decides how to organise the content of the file. \u274c <code>target_directory</code> <code>str</code> Target directory. \u2705 <code>file_name_prefix</code> <code>str</code> File name prefix. \u2705 <code>max_entries_per_file</code> <code>int</code> Defines how many datapoints can be appended to a single file. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/local_file_sink/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Local File Sink</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Roboflow Custom Metadata</code>, <code>Google Gemini</code>, <code>Anthropic Claude</code>, <code>Clip Comparison</code>, <code>Local File Sink</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>LMM For Classification</code>, <code>CSV Formatter</code>, <code>Object Detection Model</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Roboflow Dataset Upload</code>, <code>Roboflow Dataset Upload</code>, <code>OpenAI</code>, <code>Keypoint Detection Model</code>, <code>Webhook Sink</code>, <code>Instance Segmentation Model</code>, <code>CogVLM</code>, <code>OCR Model</code>, <code>VLM as Classifier</code>, <code>Stitch OCR Detections</code>, <code>Florence-2 Model</code>, <code>Email Notification</code>, <code>Single-Label Classification Model</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Webhook Sink</code>, <code>Email Notification</code>, <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Stability AI Inpainting</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Local File Sink</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>content</code> (<code>string</code>): Content of the file to save.</li> <li><code>target_directory</code> (<code>string</code>): Target directory.</li> <li><code>file_name_prefix</code> (<code>string</code>): File name prefix.</li> <li><code>max_entries_per_file</code> (<code>string</code>): Defines how many datapoints can be appended to a single file.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Local File Sink</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/local_file_sink@v1\",\n    \"content\": \"$steps.csv_formatter.csv_content\",\n    \"file_type\": \"csv\",\n    \"output_mode\": \"append_log\",\n    \"target_directory\": \"some/location\",\n    \"file_name_prefix\": \"my_file\",\n    \"max_entries_per_file\": 1024\n}\n</code></pre>"},{"location":"workflows/blocks/mask_visualization/","title":"Mask Visualization","text":""},{"location":"workflows/blocks/mask_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>MaskVisualization</code> block uses a detected polygon from an instance segmentation to draw a mask using <code>sv.MaskAnnotator</code>.</p>"},{"location":"workflows/blocks/mask_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/mask_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/mask_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>opacity</code> <code>float</code> Transparency of the Mask overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/mask_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Mask Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Mask Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the Mask overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Mask Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/mask_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.instance_segmentation_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"opacity\": 0.5\n}\n</code></pre>"},{"location":"workflows/blocks/model_comparison_visualization/","title":"Model Comparison Visualization","text":""},{"location":"workflows/blocks/model_comparison_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>ModelComparisonVisualization</code> block draws all areas predicted by neither model with a specified color, lets overlapping areas of the predictions shine through, and colors areas predicted by only one model with a distinct color.</p>"},{"location":"workflows/blocks/model_comparison_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/model_comparison_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/model_comparison_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_a</code> <code>str</code> Color of the areas Model A predicted that Model B did not... \u2705 <code>color_b</code> <code>str</code> Color of the areas Model B predicted that Model A did not.. \u2705 <code>background_color</code> <code>str</code> Color of the areas neither model predicted.. \u2705 <code>opacity</code> <code>float</code> Transparency of the overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/model_comparison_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Model Comparison Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>YOLO-World Model</code>, <code>Halo Visualization</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>Trace Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Model Comparison Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions_a</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_a</code> (<code>string</code>): Color of the areas Model A predicted that Model B did not...</li> <li><code>predictions_b</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_b</code> (<code>string</code>): Color of the areas Model B predicted that Model A did not..</li> <li><code>background_color</code> (<code>string</code>): Color of the areas neither model predicted..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Model Comparison Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/model_comparison_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions_a\": \"$steps.object_detection_model.predictions\",\n    \"color_a\": \"GREEN\",\n    \"predictions_b\": \"$steps.object_detection_model.predictions\",\n    \"color_b\": \"RED\",\n    \"background_color\": \"BLACK\",\n    \"opacity\": 0.7\n}\n</code></pre>"},{"location":"workflows/blocks/multi_label_classification_model/","title":"Multi-Label Classification Model","text":""},{"location":"workflows/blocks/multi_label_classification_model/#version-v1","title":"Version <code>v1</code>","text":"<p>Run inference on a multi-label classification model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/multi_label_classification_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_multi_label_classification_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/multi_label_classification_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions. \u2705 <code>disable_active_learning</code> <code>bool</code> Parameter to decide if Active Learning data sampling is disabled for the model. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/multi_label_classification_model/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Multi-Label Classification Model</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Roboflow Dataset Upload</code>, <code>Roboflow Dataset Upload</code>, <code>Webhook Sink</code>, <code>Roboflow Custom Metadata</code>, <code>Stability AI Inpainting</code>, <code>Local File Sink</code>, <code>Email Notification</code>, <code>Detections Classes Replacement</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Multi-Label Classification Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier.</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions.</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Parameter to decide if Active Learning data sampling is disabled for the model.</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>inference_id</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Multi-Label Classification Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_multi_label_classification_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre>"},{"location":"workflows/blocks/object_detection_model/","title":"Object Detection Model","text":""},{"location":"workflows/blocks/object_detection_model/#version-v1","title":"Version <code>v1</code>","text":"<p>Run inference on a object-detection model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/object_detection_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_object_detection_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/object_detection_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier. \u2705 <code>class_agnostic_nms</code> <code>bool</code> Value to decide if NMS is to be used in class-agnostic mode.. \u2705 <code>class_filter</code> <code>List[str]</code> List of classes to retrieve from predictions (to define subset of those which was used while model training). \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions. \u2705 <code>iou_threshold</code> <code>float</code> Parameter of NMS, to decide on minimum box intersection over union to merge boxes. \u2705 <code>max_detections</code> <code>int</code> Maximum number of detections to return. \u2705 <code>max_candidates</code> <code>int</code> Maximum number of candidates as NMS input to be taken into account.. \u2705 <code>disable_active_learning</code> <code>bool</code> Parameter to decide if Active Learning data sampling is disabled for the model. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/object_detection_model/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Object Detection Model</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Object Detection Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier.</li> <li><code>class_agnostic_nms</code> (<code>boolean</code>): Value to decide if NMS is to be used in class-agnostic mode..</li> <li><code>class_filter</code> (<code>list_of_values</code>): List of classes to retrieve from predictions (to define subset of those which was used while model training).</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions.</li> <li><code>iou_threshold</code> (<code>float_zero_to_one</code>): Parameter of NMS, to decide on minimum box intersection over union to merge boxes.</li> <li><code>max_detections</code> (<code>integer</code>): Maximum number of detections to return.</li> <li><code>max_candidates</code> (<code>integer</code>): Maximum number of candidates as NMS input to be taken into account..</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Parameter to decide if Active Learning data sampling is disabled for the model.</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>inference_id</code> (<code>string</code>): String value.</li> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Object Detection Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"class_agnostic_nms\": true,\n    \"class_filter\": [\n        \"a\",\n        \"b\",\n        \"c\"\n    ],\n    \"confidence\": 0.3,\n    \"iou_threshold\": 0.4,\n    \"max_detections\": 300,\n    \"max_candidates\": 3000,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre>"},{"location":"workflows/blocks/ocr_model/","title":"OCR Model","text":""},{"location":"workflows/blocks/ocr_model/#version-v1","title":"Version <code>v1</code>","text":"<p>Retrieve the characters in an image using Optical Character Recognition (OCR).</p> <p>This block returns the text within an image.</p> <p>You may want to use this block in combination with a detections-based block (i.e.  ObjectDetectionBlock). An object detection model could isolate specific regions from an  image (i.e. a shipping container ID in a logistics use case) for further processing.  You can then use a DynamicCropBlock to crop the region of interest before running OCR.</p> <p>Using a detections model then cropping detections allows you to isolate your analysis  on particular regions of an image.</p>"},{"location":"workflows/blocks/ocr_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/ocr_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/ocr_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> Unique name of step in workflows. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/ocr_model/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>OCR Model</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Webhook Sink</code>, <code>Email Notification</code>, <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Stability AI Inpainting</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>OCR Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>result</code> (<code>string</code>): String value.</li> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>prediction_type</code> (<code>prediction_type</code>): String value with type of prediction.</li> </ul> </li> </ul> Example JSON definition of step <code>OCR Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/ocr_model@v1\",\n    \"images\": \"$inputs.image\"\n}\n</code></pre>"},{"location":"workflows/blocks/open_ai/","title":"OpenAI","text":""},{"location":"workflows/blocks/open_ai/#version-v2","title":"Version <code>v2</code>","text":"<p>Ask a question to OpenAI's GPT-4 with Vision model.</p> <p>You can specify arbitrary text prompts or predefined ones, the block supports the following types of prompt:</p> <ul> <li> <p>Open Prompt (<code>unconstrained</code>) - Use any prompt to generate a raw response</p> </li> <li> <p>Text Recognition (OCR) (<code>ocr</code>) - Model recognizes text in the image</p> </li> <li> <p>Visual Question Answering (<code>visual-question-answering</code>) - Model answers the question you submit in the prompt</p> </li> <li> <p>Captioning (short) (<code>caption</code>) - Model provides a short description of the image</p> </li> <li> <p>Captioning (<code>detailed-caption</code>) - Model provides a long description of the image</p> </li> <li> <p>Single-Label Classification (<code>classification</code>) - Model classifies the image content as one of the provided classes</p> </li> <li> <p>Multi-Label Classification (<code>multi-label-classification</code>) - Model classifies the image content as one or more of the provided classes</p> </li> <li> <p>Structured Output Generation (<code>structured-answering</code>) - Model returns a JSON response with the specified fields</p> </li> </ul> <p>You need to provide your OpenAI API key to use the GPT-4 with Vision model. </p>"},{"location":"workflows/blocks/open_ai/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/open_ai@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/open_ai/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>task_type</code> <code>str</code> Task type to be performed by model. Value determines required parameters and output response.. \u274c <code>prompt</code> <code>str</code> Text prompt to the OpenAI model. \u2705 <code>output_structure</code> <code>Dict[str, str]</code> Dictionary with structure of expected JSON response. \u274c <code>classes</code> <code>List[str]</code> List of classes to be used. \u2705 <code>api_key</code> <code>str</code> Your OpenAI API key. \u2705 <code>model_version</code> <code>str</code> Model to be used. \u2705 <code>image_detail</code> <code>str</code> Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity.. \u2705 <code>max_tokens</code> <code>int</code> Maximum number of tokens the model can generate in it's response.. \u274c <code>temperature</code> <code>float</code> Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are.. \u2705 <code>max_concurrent_requests</code> <code>int</code> Number of concurrent requests that can be executed by block when batch of input images provided. If not given - block defaults to value configured globally in Workflows Execution Engine. Please restrict if you hit OpenAI limits.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/open_ai/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>OpenAI</code> in version <code>v2</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Time in zone</code>, <code>JSON Parser</code>, <code>Perspective Correction</code>, <code>Line Counter Visualization</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Path deviation</code>, <code>Reference Path Visualization</code>, <code>Line Counter</code>, <code>VLM as Classifier</code>, <code>Email Notification</code>, <code>Path deviation</code>, <code>VLM as Detector</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>OpenAI</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the OpenAI model.</li> <li><code>classes</code> (<code>list_of_values</code>): List of classes to be used.</li> <li><code>api_key</code> (<code>string</code>): Your OpenAI API key.</li> <li><code>model_version</code> (<code>string</code>): Model to be used.</li> <li><code>image_detail</code> (<code>string</code>): Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity..</li> <li><code>temperature</code> (<code>float</code>): Temperature to sample from the model - value in range 0.0-2.0, the higher - the more random / \"creative\" the generations are..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (Union[<code>string</code>, <code>language_model_output</code>]): String value if <code>string</code> or LLM / VLM output if <code>language_model_output</code>.</li> <li><code>classes</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>OpenAI</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/open_ai@v2\",\n    \"images\": \"$inputs.image\",\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\",\n    \"prompt\": \"my prompt\",\n    \"output_structure\": {\n        \"my_key\": \"description\"\n    },\n    \"classes\": [\n        \"class-a\",\n        \"class-b\"\n    ],\n    \"api_key\": \"xxx-xxx\",\n    \"model_version\": \"gpt-4o\",\n    \"image_detail\": \"auto\",\n    \"max_tokens\": \"&lt;block_does_not_provide_example&gt;\",\n    \"temperature\": \"&lt;block_does_not_provide_example&gt;\",\n    \"max_concurrent_requests\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre>"},{"location":"workflows/blocks/open_ai/#version-v1","title":"Version <code>v1</code>","text":"<p>Ask a question to OpenAI's GPT-4 with Vision model.</p> <p>You can specify arbitrary text prompts to the OpenAIBlock.</p> <p>You need to provide your OpenAI API key to use the GPT-4 with Vision model. </p> <p>This model was previously part of the LMM block.</p>"},{"location":"workflows/blocks/open_ai/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/open_ai@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/open_ai/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>prompt</code> <code>str</code> Text prompt to the OpenAI model. \u2705 <code>openai_api_key</code> <code>str</code> Your OpenAI API key. \u2705 <code>openai_model</code> <code>str</code> Model to be used. \u2705 <code>json_output_format</code> <code>Dict[str, str]</code> Holds dictionary that maps name of requested output field into its description. \u274c <code>image_detail</code> <code>str</code> Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity.. \u2705 <code>max_tokens</code> <code>int</code> Maximum number of tokens the model can generate in it's response.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/open_ai/#available-connections_1","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>OpenAI</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>LMM For Classification</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Circle Visualization</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>Size Measurement</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Email Notification</code>, <code>Dimension Collapse</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Mask Visualization</code>, <code>Perspective Correction</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>JSON Parser</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>OpenAI</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>prompt</code> (<code>string</code>): Text prompt to the OpenAI model.</li> <li><code>openai_api_key</code> (<code>string</code>): Your OpenAI API key.</li> <li><code>openai_model</code> (<code>string</code>): Model to be used.</li> <li><code>image_detail</code> (<code>string</code>): Indicates the image's quality, with 'high' suggesting it is of high resolution and should be processed or displayed with high fidelity..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>root_parent_id</code> (<code>parent_id</code>): Identifier of parent for step output.</li> <li><code>image</code> (<code>image_metadata</code>): Dictionary with image metadata required by supervision.</li> <li><code>structured_output</code> (<code>dictionary</code>): Dictionary.</li> <li><code>raw_output</code> (<code>string</code>): String value.</li> <li><code>*</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>OpenAI</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/open_ai@v1\",\n    \"images\": \"$inputs.image\",\n    \"prompt\": \"my prompt\",\n    \"openai_api_key\": \"xxx-xxx\",\n    \"openai_model\": \"gpt-4o\",\n    \"json_output_format\": {\n        \"count\": \"number of cats in the picture\"\n    },\n    \"image_detail\": \"auto\",\n    \"max_tokens\": 450\n}\n</code></pre>"},{"location":"workflows/blocks/pathdeviation/","title":"Path deviation","text":""},{"location":"workflows/blocks/pathdeviation/#version-v2","title":"Version <code>v2</code>","text":"<p>The <code>PathDeviationAnalyticsBlock</code> is an analytics block designed to measure the Frechet distance of tracked objects from a user-defined reference path. The block requires detections to be tracked (i.e. each object must have a unique tracker_id assigned, which persists between frames).</p>"},{"location":"workflows/blocks/pathdeviation/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/path_deviation_analytics@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/pathdeviation/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>triggering_anchor</code> <code>str</code> Triggering anchor. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS. \u2705 <code>reference_path</code> <code>List[Any]</code> Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...]. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/pathdeviation/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Path deviation</code> in version <code>v2</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Anthropic Claude</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Size Measurement</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Clip Comparison</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Path deviation</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): not available.</li> <li><code>detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Predictions.</li> <li><code>triggering_anchor</code> (<code>string</code>): Triggering anchor. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS.</li> <li><code>reference_path</code> (<code>list_of_values</code>): Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...].</li> </ul> </li> <li> <p>output</p> <ul> <li><code>path_deviation_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Path deviation</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/path_deviation_analytics@v2\",\n    \"image\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"triggering_anchor\": \"CENTER\",\n    \"reference_path\": \"$inputs.expected_path\"\n}\n</code></pre>"},{"location":"workflows/blocks/pathdeviation/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>PathDeviationAnalyticsBlock</code> is an analytics block designed to measure the Frechet distance of tracked objects from a user-defined reference path. The block requires detections to be tracked (i.e. each object must have a unique tracker_id assigned, which persists between frames).</p>"},{"location":"workflows/blocks/pathdeviation/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/path_deviation_analytics@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/pathdeviation/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>triggering_anchor</code> <code>str</code> Triggering anchor. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS. \u2705 <code>reference_path</code> <code>List[Any]</code> Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...]. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/pathdeviation/#available-connections_1","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Path deviation</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Anthropic Claude</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Size Measurement</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Clip Comparison</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Path deviation</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>metadata</code> (<code>video_metadata</code>): not available.</li> <li><code>detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Predictions.</li> <li><code>triggering_anchor</code> (<code>string</code>): Triggering anchor. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS.</li> <li><code>reference_path</code> (<code>list_of_values</code>): Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...].</li> </ul> </li> <li> <p>output</p> <ul> <li><code>path_deviation_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Path deviation</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/path_deviation_analytics@v1\",\n    \"metadata\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"triggering_anchor\": \"CENTER\",\n    \"reference_path\": \"$inputs.expected_path\"\n}\n</code></pre>"},{"location":"workflows/blocks/perspective_correction/","title":"Perspective Correction","text":""},{"location":"workflows/blocks/perspective_correction/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>PerspectiveCorrectionBlock</code> is a transformer block designed to correct coordinates of detections based on transformation defined by two polygons. This block is best suited when produced coordinates should be considered as if camera was placed directly above the scene and was not introducing distortions.</p>"},{"location":"workflows/blocks/perspective_correction/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/perspective_correction@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/perspective_correction/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>perspective_polygons</code> <code>List[Any]</code> Perspective polygons (for each batch at least one must be consisting of 4 vertices). \u2705 <code>transformed_rect_width</code> <code>int</code> Transformed rect width. \u2705 <code>transformed_rect_height</code> <code>int</code> Transformed rect height. \u2705 <code>extend_perspective_polygon_by_detections_anchor</code> <code>str</code> If set, perspective polygons will be extended to contain all bounding boxes. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS. \u2705 <code>warp_image</code> <code>bool</code> If set to True, image will be warped into transformed rect. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/perspective_correction/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Perspective Correction</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Anthropic Claude</code>, <code>Crop Visualization</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Byte Tracker</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>YOLO-World Model</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Size Measurement</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Image Slicer</code>, <code>Time in zone</code>, <code>Image Contours</code>, <code>Segment Anything 2 Model</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>OpenAI</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>Dynamic Crop</code>, <code>LMM</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Line Counter</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Perspective Correction</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Predictions.</li> <li><code>images</code> (<code>image</code>): The input image for this step..</li> <li><code>perspective_polygons</code> (<code>list_of_values</code>): Perspective polygons (for each batch at least one must be consisting of 4 vertices).</li> <li><code>transformed_rect_width</code> (<code>integer</code>): Transformed rect width.</li> <li><code>transformed_rect_height</code> (<code>integer</code>): Transformed rect height.</li> <li><code>extend_perspective_polygon_by_detections_anchor</code> (<code>string</code>): If set, perspective polygons will be extended to contain all bounding boxes. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS.</li> <li><code>warp_image</code> (<code>boolean</code>): If set to True, image will be warped into transformed rect.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>corrected_coordinates</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> <li><code>warped_image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Perspective Correction</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/perspective_correction@v1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"images\": \"$inputs.image\",\n    \"perspective_polygons\": \"$steps.perspective_wrap.zones\",\n    \"transformed_rect_width\": 1000,\n    \"transformed_rect_height\": 1000,\n    \"extend_perspective_polygon_by_detections_anchor\": \"CENTER\",\n    \"warp_image\": false\n}\n</code></pre>"},{"location":"workflows/blocks/pixel_color_count/","title":"Pixel Color Count","text":""},{"location":"workflows/blocks/pixel_color_count/#version-v1","title":"Version <code>v1</code>","text":"<p>Count the number of pixels that match a specific color within a given tolerance.</p>"},{"location":"workflows/blocks/pixel_color_count/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/pixel_color_count@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/pixel_color_count/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>target_color</code> <code>Union[Any, str]</code> Target color to count in the image. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67)).. \u2705 <code>tolerance</code> <code>int</code> Tolerance for color matching.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/pixel_color_count/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Pixel Color Count</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Dominant Color</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Line Counter Visualization</code>, <code>Webhook Sink</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Pixel Color Count</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>target_color</code> (Union[<code>rgb_color</code>, <code>string</code>]): Target color to count in the image. Can be a hex string (like '#431112') RGB string (like '(128, 32, 64)') or a RGB tuple (like (18, 17, 67))..</li> <li><code>tolerance</code> (<code>integer</code>): Tolerance for color matching..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>matching_pixels_count</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Pixel Color Count</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/pixel_color_count@v1\",\n    \"image\": \"$inputs.image\",\n    \"target_color\": \"#431112\",\n    \"tolerance\": 10\n}\n</code></pre>"},{"location":"workflows/blocks/pixelate_visualization/","title":"Pixelate Visualization","text":""},{"location":"workflows/blocks/pixelate_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>PixelateVisualization</code> block pixelates detected objects in an image using Supervision's <code>sv.PixelateAnnotator</code>.</p>"},{"location":"workflows/blocks/pixelate_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/pixelate_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/pixelate_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>pixel_size</code> <code>int</code> Size of the pixelation.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/pixelate_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Pixelate Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Pixelate Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>pixel_size</code> (<code>integer</code>): Size of the pixelation..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Pixelate Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/pixelate_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"pixel_size\": 20\n}\n</code></pre>"},{"location":"workflows/blocks/polygon_visualization/","title":"Polygon Visualization","text":""},{"location":"workflows/blocks/polygon_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>PolygonVisualization</code> block uses a detections from an instance segmentation to draw polygons around objects using <code>sv.PolygonAnnotator</code>.</p>"},{"location":"workflows/blocks/polygon_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/polygon_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/polygon_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>thickness</code> <code>int</code> Thickness of the outline in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/polygon_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Polygon Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Polygon Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the outline in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Polygon Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/polygon_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.instance_segmentation_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"thickness\": 2\n}\n</code></pre>"},{"location":"workflows/blocks/polygon_zone_visualization/","title":"Polygon Zone Visualization","text":""},{"location":"workflows/blocks/polygon_zone_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>PolygonZoneVisualization</code> block draws polygon zone in an image with a specified color and opacity. Please note: zones will be drawn on top of image passed to this block, this block should be placed before other visualization blocks in the workflow.</p>"},{"location":"workflows/blocks/polygon_zone_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/polygon_zone_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/polygon_zone_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>zone</code> <code>List[Any]</code> Polygon zones (one for each batch) in a format [[(x1, y1), (x2, y2), (x3, y3), ...], ...]; each zone must consist of more than 2 points. \u2705 <code>color</code> <code>str</code> Color of the zone.. \u2705 <code>opacity</code> <code>float</code> Transparency of the Mask overlay.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/polygon_zone_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Polygon Zone Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Anthropic Claude</code>, <code>Crop Visualization</code>, <code>Clip Comparison</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Dynamic Zone</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Polygon Zone Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>zone</code> (<code>list_of_values</code>): Polygon zones (one for each batch) in a format [[(x1, y1), (x2, y2), (x3, y3), ...], ...]; each zone must consist of more than 2 points.</li> <li><code>color</code> (<code>string</code>): Color of the zone..</li> <li><code>opacity</code> (<code>float_zero_to_one</code>): Transparency of the Mask overlay..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Polygon Zone Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/polygon_zone_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"zone\": \"$inputs.zones\",\n    \"color\": \"WHITE\",\n    \"opacity\": 0.3\n}\n</code></pre>"},{"location":"workflows/blocks/property_definition/","title":"Property Definition","text":""},{"location":"workflows/blocks/property_definition/#version-v1","title":"Version <code>v1</code>","text":"<p>Define a field using properties from previous workflow steps.</p> <p>Example use-cases: * extraction of all class names for object-detection predictions * extraction of class / confidence from classification result * extraction ocr text from OCR result</p>"},{"location":"workflows/blocks/property_definition/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/property_definition@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/property_definition/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>operations</code> <code>List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]</code> List of operations to perform on data to generate output. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/property_definition/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Property Definition</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Circle Visualization</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Mask Visualization</code>, <code>Perspective Correction</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Property Definition</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>data</code> (Union[<code>*</code>, <code>image</code>]): Reference data to extract property from.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>output</code> (<code>*</code>): Equivalent of any element.</li> </ul> </li> </ul> Example JSON definition of step <code>Property Definition</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/property_definition@v1\",\n    \"data\": \"$steps.my_step.predictions\",\n    \"operations\": [\n        {\n            \"property_name\": \"class_name\",\n            \"type\": \"DetectionsPropertyExtract\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/blocks/qr_code_detection/","title":"QR Code Detection","text":""},{"location":"workflows/blocks/qr_code_detection/#version-v1","title":"Version <code>v1</code>","text":"<p>Detect the location of a QR code.</p> <p>This block is useful for manufacturing and consumer packaged goods projects where you  need to detect a QR code region in an image. You can then apply Crop block to isolate  each QR code then apply further processing (i.e. read a QR code with a custom block).</p>"},{"location":"workflows/blocks/qr_code_detection/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/qr_code_detector@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/qr_code_detection/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/qr_code_detection/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>QR Code Detection</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: None</li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>QR Code Detection</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>qr_code_detection</code>): Prediction with QR code detection.</li> </ul> </li> </ul> Example JSON definition of step <code>QR Code Detection</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/qr_code_detector@v1\",\n    \"images\": \"$inputs.image\"\n}\n</code></pre>"},{"location":"workflows/blocks/rate_limiter/","title":"Rate Limiter","text":""},{"location":"workflows/blocks/rate_limiter/#version-v1","title":"Version <code>v1</code>","text":"<p>The Rate Limiter block controls the execution frequency of a branch within a Workflow by enforcing a  cooldown period. It ensures that the connected steps do not run more frequently than a specified interval,  helping to manage resource usage and prevent over-execution.</p>"},{"location":"workflows/blocks/rate_limiter/#block-usage","title":"Block usage","text":"<p>Rate Limiter is useful when you have two blocks that are directly connected, as shown below:</p> <p>--- input_a --&gt; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 --- input_b --&gt; \u2502   step_1  \u2502 --&gt;  output_a --&gt;  \u2502   step_2  \u2502 --- input_c --&gt; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518</p> <p>If you want to throttle the Step 2 execution rate - you should apply rate limiter in between:</p> <ul> <li> <p>keep the existing blocks configuration as is (do not change connections)</p> </li> <li> <p>set <code>depends_on</code> reference of Rate Limiter into <code>output_a</code></p> </li> <li> <p>set <code>next_steps</code> reference to be a list referring to <code>[$steps.step_2]</code></p> </li> <li> <p>adjust <code>cooldown_seconds</code> to specify what is the number of seconds that must be awaited before next time when <code>step_2</code> is fired </p> </li> </ul>"},{"location":"workflows/blocks/rate_limiter/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/rate_limiter@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/rate_limiter/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>cooldown_seconds</code> <code>float</code> The minimum number of seconds between allowed executions.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/rate_limiter/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Rate Limiter</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Relative Static Crop</code>, <code>Detections Consensus</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Data Aggregator</code>, <code>JSON Parser</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Expression</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> <li>outputs: None</li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Rate Limiter</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>depends_on</code> (Union[<code>*</code>, <code>image</code>]): Reference to any output of the the step which immediately preceeds this branch..</li> <li><code>next_steps</code> (step): Reference to steps which shall be executed if rate limit allows..</li> </ul> </li> <li> <p>output</p> </li> </ul> Example JSON definition of step <code>Rate Limiter</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/rate_limiter@v1\",\n    \"cooldown_seconds\": 1.0,\n    \"depends_on\": \"$steps.model\",\n    \"next_steps\": [\n        \"$steps.upload\"\n    ]\n}\n</code></pre>"},{"location":"workflows/blocks/reference_path_visualization/","title":"Reference Path Visualization","text":""},{"location":"workflows/blocks/reference_path_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The Reference Path Visualization block draws reference path in the image. To be used in combination with Path deviation block - to display the reference path.</p>"},{"location":"workflows/blocks/reference_path_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/reference_path_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/reference_path_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>reference_path</code> <code>List[Any]</code> Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...]. \u2705 <code>color</code> <code>str</code> Color of the zone.. \u2705 <code>thickness</code> <code>int</code> Thickness of the lines in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/reference_path_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Reference Path Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Anthropic Claude</code>, <code>Crop Visualization</code>, <code>Clip Comparison</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Dynamic Zone</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Reference Path Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>reference_path</code> (<code>list_of_values</code>): Reference path in a format [(x1, y1), (x2, y2), (x3, y3), ...].</li> <li><code>color</code> (<code>string</code>): Color of the zone..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the lines in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Reference Path Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/reference_path_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"reference_path\": \"$inputs.expected_path\",\n    \"color\": \"WHITE\",\n    \"thickness\": 2\n}\n</code></pre>"},{"location":"workflows/blocks/relative_static_crop/","title":"Relative Static Crop","text":""},{"location":"workflows/blocks/relative_static_crop/#version-v1","title":"Version <code>v1</code>","text":"<p>Crop a Region of Interest (RoI) from an image, using relative coordinates.</p> <p>This is useful when placed after an ObjectDetection block as part of a multi-stage  workflow. For example, you could use an ObjectDetection block to detect objects, then  the RelativeStaticCrop block to crop objects, then an OCR block to run character  recognition on each of the individual cropped regions.</p>"},{"location":"workflows/blocks/relative_static_crop/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/relative_statoic_crop@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/relative_static_crop/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>x_center</code> <code>float</code> Center X of static crop (relative coordinate 0.0-1.0). \u2705 <code>y_center</code> <code>float</code> Center Y of static crop (relative coordinate 0.0-1.0). \u2705 <code>width</code> <code>float</code> Width of static crop (relative value 0.0-1.0). \u2705 <code>height</code> <code>float</code> Height of static crop (relative value 0.0-1.0). \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/relative_static_crop/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Relative Static Crop</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Relative Static Crop</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>x_center</code> (<code>float_zero_to_one</code>): Center X of static crop (relative coordinate 0.0-1.0).</li> <li><code>y_center</code> (<code>float_zero_to_one</code>): Center Y of static crop (relative coordinate 0.0-1.0).</li> <li><code>width</code> (<code>float_zero_to_one</code>): Width of static crop (relative value 0.0-1.0).</li> <li><code>height</code> (<code>float_zero_to_one</code>): Height of static crop (relative value 0.0-1.0).</li> </ul> </li> <li> <p>output</p> <ul> <li><code>crops</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Relative Static Crop</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/relative_statoic_crop@v1\",\n    \"images\": \"$inputs.image\",\n    \"x_center\": 0.3,\n    \"y_center\": 0.3,\n    \"width\": 0.3,\n    \"height\": 0.3\n}\n</code></pre>"},{"location":"workflows/blocks/roboflow_custom_metadata/","title":"Roboflow Custom Metadata","text":""},{"location":"workflows/blocks/roboflow_custom_metadata/#version-v1","title":"Version <code>v1</code>","text":"<p>Block allows users to add custom metadata to each inference result in Roboflow Model Monitoring dashboard.</p> <p>This is useful for adding information specific to your use case. For example, if you want to be able to filter inferences by a specific label such as location, you can attach those labels to each inference with this block.</p> <p>For more information on Model Monitoring at Roboflow, see https://docs.roboflow.com/deploy/model-monitoring.</p>"},{"location":"workflows/blocks/roboflow_custom_metadata/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_custom_metadata@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/roboflow_custom_metadata/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>field_value</code> <code>str</code> This is the name of the metadata field you are creating. \u2705 <code>field_name</code> <code>str</code> Name of the field to be updated in Roboflow Customer Metadata. \u274c <code>fire_and_forget</code> <code>bool</code> Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use <code>True</code> if best-effort registration is needed, use <code>False</code> while debugging and if error handling is needed. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/roboflow_custom_metadata/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Roboflow Custom Metadata</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Roboflow Custom Metadata</code>, <code>Anthropic Claude</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Roboflow Dataset Upload</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Webhook Sink</code>, <code>Path deviation</code>, <code>OCR Model</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Bounding Rectangle</code>, <code>Detections Filter</code>, <code>CSV Formatter</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>OpenAI</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code>, <code>CogVLM</code>, <code>VLM as Classifier</code>, <code>Florence-2 Model</code>, <code>Email Notification</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> <li>outputs: <code>Webhook Sink</code>, <code>Email Notification</code>, <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Stability AI Inpainting</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Roboflow Custom Metadata</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (Union[<code>classification_prediction</code>, <code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Reference data to extract property from.</li> <li><code>field_value</code> (<code>string</code>): This is the name of the metadata field you are creating.</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use <code>True</code> if best-effort registration is needed, use <code>False</code> while debugging and if error handling is needed.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Roboflow Custom Metadata</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_custom_metadata@v1\",\n    \"predictions\": \"$steps.my_step.predictions\",\n    \"field_value\": \"toronto\",\n    \"field_name\": \"The name of the value of the field\",\n    \"fire_and_forget\": true\n}\n</code></pre>"},{"location":"workflows/blocks/roboflow_dataset_upload/","title":"Roboflow Dataset Upload","text":""},{"location":"workflows/blocks/roboflow_dataset_upload/#version-v2","title":"Version <code>v2</code>","text":"<p>Block let users save their images and predictions into Roboflow Dataset. Persisting data from production environments helps iteratively building more robust models. </p> <p>Block provides configuration options to decide how data should be stored and what are the limits  to be applied. We advice using this block in combination with rate limiter blocks to effectively  collect data that the model struggle with.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_dataset_upload@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>target_project</code> <code>str</code> name of Roboflow dataset / project to be used as target for collected data. \u2705 <code>usage_quota_name</code> <code>str</code> Unique name for Roboflow project pointed by <code>target_project</code> parameter, that identifies usage quota applied for this block.. \u274c <code>data_percentage</code> <code>float</code> Percent of data that will be saved (in range [0.0, 100.0]). \u2705 <code>persist_predictions</code> <code>bool</code> Boolean flag to decide if predictions should be registered along with images. \u2705 <code>minutely_usage_limit</code> <code>int</code> Maximum number of data registration requests per minute accounted in scope of single server or whole Roboflow platform, depending on context of usage.. \u274c <code>hourly_usage_limit</code> <code>int</code> Maximum number of data registration requests per hour accounted in scope of single server or whole Roboflow platform, depending on context of usage.. \u274c <code>daily_usage_limit</code> <code>int</code> Maximum number of data registration requests per day accounted in scope of single server or whole Roboflow platform, depending on context of usage.. \u274c <code>max_image_size</code> <code>Any</code> Maximum size of the image to be registered - bigger images will be downsized preserving aspect ratio. Format of data: <code>(width, height)</code>. \u274c <code>compression_level</code> <code>int</code> Compression level for images registered. \u274c <code>registration_tags</code> <code>List[str]</code> Tags to be attached to registered datapoints. \u2705 <code>disable_sink</code> <code>bool</code> boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request. \u2705 <code>fire_and_forget</code> <code>bool</code> Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use <code>True</code> if best-effort registration is needed, use <code>False</code> while debugging and if error handling is needed. \u2705 <code>labeling_batch_prefix</code> <code>str</code> Prefix of the name for labeling batches that will be registered in Roboflow app. \u2705 <code>labeling_batches_recreation_frequency</code> <code>str</code> Frequency in which new labeling batches are created in Roboflow app. New batches are created with name prefix provided in <code>labeling_batch_prefix</code> in given time intervals.Useful in organising labeling flow.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Roboflow Dataset Upload</code> in version <code>v2</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code>, <code>Single-Label Classification Model</code></li> <li>outputs: <code>Webhook Sink</code>, <code>Email Notification</code>, <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Stability AI Inpainting</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Roboflow Dataset Upload</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>target_project</code> (<code>roboflow_project</code>): name of Roboflow dataset / project to be used as target for collected data.</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>keypoint_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>classification_prediction</code>]): Model predictions to be saved.</li> <li><code>data_percentage</code> (<code>float</code>): Percent of data that will be saved (in range [0.0, 100.0]).</li> <li><code>persist_predictions</code> (<code>boolean</code>): Boolean flag to decide if predictions should be registered along with images.</li> <li><code>registration_tags</code> (<code>string</code>): Tags to be attached to registered datapoints.</li> <li><code>disable_sink</code> (<code>boolean</code>): boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request.</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use <code>True</code> if best-effort registration is needed, use <code>False</code> while debugging and if error handling is needed.</li> <li><code>labeling_batch_prefix</code> (<code>string</code>): Prefix of the name for labeling batches that will be registered in Roboflow app.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Roboflow Dataset Upload</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_dataset_upload@v2\",\n    \"images\": \"$inputs.image\",\n    \"target_project\": \"my_dataset\",\n    \"usage_quota_name\": \"quota-for-data-sampling-1\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"data_percentage\": true,\n    \"persist_predictions\": true,\n    \"minutely_usage_limit\": 10,\n    \"hourly_usage_limit\": 10,\n    \"daily_usage_limit\": 10,\n    \"max_image_size\": [\n        1920,\n        1080\n    ],\n    \"compression_level\": 95,\n    \"registration_tags\": [\n        \"location-florida\",\n        \"factory-name\",\n        \"$inputs.dynamic_tag\"\n    ],\n    \"disable_sink\": true,\n    \"fire_and_forget\": \"&lt;block_does_not_provide_example&gt;\",\n    \"labeling_batch_prefix\": \"my_labeling_batch_name\",\n    \"labeling_batches_recreation_frequency\": \"never\"\n}\n</code></pre>"},{"location":"workflows/blocks/roboflow_dataset_upload/#version-v1","title":"Version <code>v1</code>","text":"<p>Block let users save their images and predictions into Roboflow Dataset. Persisting data from production environments helps iteratively building more robust models. </p> <p>Block provides configuration options to decide how data should be stored and what are the limits  to be applied. We advice using this block in combination with rate limiter blocks to effectively  collect data that the model struggle with.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_dataset_upload@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>target_project</code> <code>str</code> name of Roboflow dataset / project to be used as target for collected data. \u2705 <code>usage_quota_name</code> <code>str</code> Unique name for Roboflow project pointed by <code>target_project</code> parameter, that identifies usage quota applied for this block.. \u274c <code>persist_predictions</code> <code>bool</code> Boolean flag to decide if predictions should be registered along with images. \u274c <code>minutely_usage_limit</code> <code>int</code> Maximum number of data registration requests per minute accounted in scope of single server or whole Roboflow platform, depending on context of usage.. \u274c <code>hourly_usage_limit</code> <code>int</code> Maximum number of data registration requests per hour accounted in scope of single server or whole Roboflow platform, depending on context of usage.. \u274c <code>daily_usage_limit</code> <code>int</code> Maximum number of data registration requests per day accounted in scope of single server or whole Roboflow platform, depending on context of usage.. \u274c <code>max_image_size</code> <code>Any</code> Maximum size of the image to be registered - bigger images will be downsized preserving aspect ratio. Format of data: <code>(width, height)</code>. \u274c <code>compression_level</code> <code>int</code> Compression level for images registered. \u274c <code>registration_tags</code> <code>List[str]</code> Tags to be attached to registered datapoints. \u2705 <code>disable_sink</code> <code>bool</code> boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request. \u2705 <code>fire_and_forget</code> <code>bool</code> Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use <code>True</code> if best-effort registration is needed, use <code>False</code> while debugging and if error handling is needed. \u2705 <code>labeling_batch_prefix</code> <code>str</code> Prefix of the name for labeling batches that will be registered in Roboflow app. \u2705 <code>labeling_batches_recreation_frequency</code> <code>str</code> Frequency in which new labeling batches are created in Roboflow app. New batches are created with name prefix provided in <code>labeling_batch_prefix</code> in given time intervals.Useful in organising labeling flow.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/roboflow_dataset_upload/#available-connections_1","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Roboflow Dataset Upload</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code>, <code>Single-Label Classification Model</code></li> <li>outputs: <code>Webhook Sink</code>, <code>Email Notification</code>, <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Stability AI Inpainting</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Roboflow Dataset Upload</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>keypoint_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>classification_prediction</code>]): Reference q detection-like predictions.</li> <li><code>target_project</code> (<code>roboflow_project</code>): name of Roboflow dataset / project to be used as target for collected data.</li> <li><code>registration_tags</code> (<code>string</code>): Tags to be attached to registered datapoints.</li> <li><code>disable_sink</code> (<code>boolean</code>): boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request.</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use <code>True</code> if best-effort registration is needed, use <code>False</code> while debugging and if error handling is needed.</li> <li><code>labeling_batch_prefix</code> (<code>string</code>): Prefix of the name for labeling batches that will be registered in Roboflow app.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Roboflow Dataset Upload</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_dataset_upload@v1\",\n    \"images\": \"$inputs.image\",\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"target_project\": \"my_dataset\",\n    \"usage_quota_name\": \"quota-for-data-sampling-1\",\n    \"persist_predictions\": true,\n    \"minutely_usage_limit\": 10,\n    \"hourly_usage_limit\": 10,\n    \"daily_usage_limit\": 10,\n    \"max_image_size\": [\n        512,\n        512\n    ],\n    \"compression_level\": 75,\n    \"registration_tags\": [\n        \"location-florida\",\n        \"factory-name\",\n        \"$inputs.dynamic_tag\"\n    ],\n    \"disable_sink\": true,\n    \"fire_and_forget\": true,\n    \"labeling_batch_prefix\": \"my_labeling_batch_name\",\n    \"labeling_batches_recreation_frequency\": \"never\"\n}\n</code></pre>"},{"location":"workflows/blocks/segment_anything2_model/","title":"Segment Anything 2 Model","text":""},{"location":"workflows/blocks/segment_anything2_model/#version-v1","title":"Version <code>v1</code>","text":"<p>Run Segment Anything 2, a zero-shot instance segmentation model, on an image.</p> <p>** Dedicated inference server required (GPU recomended) **</p> <p>You can use pass in boxes/predictions from other models to Segment Anything 2 to use as prompts for the model. If you pass in box detections from another model, the class names of the boxes will be forwarded to the predicted masks.  If using the model unprompted, the model will assign integers as class names / ids.</p>"},{"location":"workflows/blocks/segment_anything2_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/segment_anything@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/segment_anything2_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>version</code> <code>str</code> Model to be used.  One of hiera_large, hiera_small, hiera_tiny, hiera_b_plus. \u2705 <code>threshold</code> <code>float</code> Threshold for predicted masks scores. \u2705 <code>multimask_output</code> <code>bool</code> Flag to determine whether to use sam2 internal multimask or single mask mode. For ambiguous prompts setting to True is recomended.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/segment_anything2_model/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Segment Anything 2 Model</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Segment Anything 2 Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>boxes</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Bounding boxes (from another model) to convert to polygons.</li> <li><code>version</code> (<code>string</code>): Model to be used.  One of hiera_large, hiera_small, hiera_tiny, hiera_b_plus.</li> <li><code>threshold</code> (<code>float</code>): Threshold for predicted masks scores.</li> <li><code>multimask_output</code> (<code>boolean</code>): Flag to determine whether to use sam2 internal multimask or single mask mode. For ambiguous prompts setting to True is recomended..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>instance_segmentation_prediction</code>): Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>Segment Anything 2 Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/segment_anything@v1\",\n    \"images\": \"$inputs.image\",\n    \"boxes\": \"$steps.object_detection_model.predictions\",\n    \"version\": \"hiera_large\",\n    \"threshold\": 0.3,\n    \"multimask_output\": true\n}\n</code></pre>"},{"location":"workflows/blocks/sift/","title":"SIFT","text":""},{"location":"workflows/blocks/sift/#version-v1","title":"Version <code>v1</code>","text":"<p>The Scale-Invariant Feature Transform (SIFT) algorithm is a popular method in computer vision for detecting  and describing features (interesting parts) in images. SIFT is used to find key points in an image and  describe them in a way that allows for recognizing the same objects or features in different images,  even if the images are taken from different angles, distances, or lighting conditions.</p> <p>Read more: https://en.wikipedia.org/wiki/Scale-invariant_feature_transform</p>"},{"location":"workflows/blocks/sift/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/sift@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/sift/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/sift/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>SIFT</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>SIFT</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> <li><code>keypoints</code> (<code>image_keypoints</code>): Image keypoints detected by classical Computer Vision method.</li> <li><code>descriptors</code> (<code>numpy_array</code>): Numpy array.</li> </ul> </li> </ul> Example JSON definition of step <code>SIFT</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/sift@v1\",\n    \"image\": \"$inputs.image\"\n}\n</code></pre>"},{"location":"workflows/blocks/sift_comparison/","title":"SIFT Comparison","text":""},{"location":"workflows/blocks/sift_comparison/#version-v2","title":"Version <code>v2</code>","text":"<p>Compare SIFT descriptors from multiple images using FLANN-based matcher.</p> <p>This block is useful for determining if multiple images match based on their SIFT descriptors.</p>"},{"location":"workflows/blocks/sift_comparison/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/sift_comparison@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/sift_comparison/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>good_matches_threshold</code> <code>int</code> Threshold for the number of good matches to consider the images as matching. \u2705 <code>ratio_threshold</code> <code>float</code> Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering.. \u2705 <code>matcher</code> <code>str</code> Matcher to use for comparing the SIFT descriptors. \u2705 <code>visualize</code> <code>bool</code> Whether to visualize the keypoints and matches between the two images. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/sift_comparison/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>SIFT Comparison</code> in version <code>v2</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>SIFT Comparison</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>SIFT Comparison</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>input_1</code> (Union[<code>image</code>, <code>numpy_array</code>]): Reference to Image or SIFT descriptors from the first image to compare.</li> <li><code>input_2</code> (Union[<code>image</code>, <code>numpy_array</code>]): Reference to Image or SIFT descriptors from the second image to compare.</li> <li><code>good_matches_threshold</code> (<code>integer</code>): Threshold for the number of good matches to consider the images as matching.</li> <li><code>ratio_threshold</code> (<code>float_zero_to_one</code>): Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering..</li> <li><code>matcher</code> (<code>string</code>): Matcher to use for comparing the SIFT descriptors.</li> <li><code>visualize</code> (<code>boolean</code>): Whether to visualize the keypoints and matches between the two images.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>images_match</code> (<code>boolean</code>): Boolean flag.</li> <li><code>good_matches_count</code> (<code>integer</code>): Integer value.</li> <li><code>keypoints_1</code> (<code>image_keypoints</code>): Image keypoints detected by classical Computer Vision method.</li> <li><code>descriptors_1</code> (<code>numpy_array</code>): Numpy array.</li> <li><code>keypoints_2</code> (<code>image_keypoints</code>): Image keypoints detected by classical Computer Vision method.</li> <li><code>descriptors_2</code> (<code>numpy_array</code>): Numpy array.</li> <li><code>visualization_1</code> (<code>image</code>): Image in workflows.</li> <li><code>visualization_2</code> (<code>image</code>): Image in workflows.</li> <li><code>visualization_matches</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>SIFT Comparison</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/sift_comparison@v2\",\n    \"input_1\": \"$inputs.image1\",\n    \"input_2\": \"$inputs.image2\",\n    \"good_matches_threshold\": 50,\n    \"ratio_threshold\": 0.7,\n    \"matcher\": \"FlannBasedMatcher\",\n    \"visualize\": true\n}\n</code></pre>"},{"location":"workflows/blocks/sift_comparison/#version-v1","title":"Version <code>v1</code>","text":"<p>Compare SIFT descriptors from multiple images using FLANN-based matcher.</p> <p>This block is useful for determining if multiple images match based on their SIFT descriptors.</p>"},{"location":"workflows/blocks/sift_comparison/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/sift_comparison@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/sift_comparison/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>good_matches_threshold</code> <code>int</code> Threshold for the number of good matches to consider the images as matching. \u2705 <code>ratio_threshold</code> <code>float</code> Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/sift_comparison/#available-connections_1","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>SIFT Comparison</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Image Contours</code>, <code>SIFT Comparison</code>, <code>SIFT</code></li> <li>outputs: <code>Line Counter Visualization</code>, <code>Webhook Sink</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>SIFT Comparison</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>descriptor_1</code> (<code>numpy_array</code>): Reference to SIFT descriptors from the first image to compare.</li> <li><code>descriptor_2</code> (<code>numpy_array</code>): Reference to SIFT descriptors from the second image to compare.</li> <li><code>good_matches_threshold</code> (<code>integer</code>): Threshold for the number of good matches to consider the images as matching.</li> <li><code>ratio_threshold</code> (<code>integer</code>): Ratio threshold for the ratio test, which is used to filter out poor matches by comparing the distance of the closest match to the distance of the second closest match. A lower ratio indicates stricter filtering..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>good_matches_count</code> (<code>integer</code>): Integer value.</li> <li><code>images_match</code> (<code>boolean</code>): Boolean flag.</li> </ul> </li> </ul> Example JSON definition of step <code>SIFT Comparison</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/sift_comparison@v1\",\n    \"descriptor_1\": \"$steps.sift.descriptors\",\n    \"descriptor_2\": \"$steps.sift.descriptors\",\n    \"good_matches_threshold\": 50,\n    \"ratio_threshold\": 0.7\n}\n</code></pre>"},{"location":"workflows/blocks/single_label_classification_model/","title":"Single-Label Classification Model","text":""},{"location":"workflows/blocks/single_label_classification_model/#version-v1","title":"Version <code>v1</code>","text":"<p>Run inference on a multi-class classification model hosted on or uploaded to Roboflow.</p> <p>You can query any model that is private to your account, or any public model available  on Roboflow Universe.</p> <p>You will need to set your Roboflow API key in your Inference environment to use this  block. To learn more about setting your Roboflow API key, refer to the Inference  documentation.</p>"},{"location":"workflows/blocks/single_label_classification_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/roboflow_classification_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/single_label_classification_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>model_id</code> <code>str</code> Roboflow model identifier. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for predictions. \u2705 <code>disable_active_learning</code> <code>bool</code> Parameter to decide if Active Learning data sampling is disabled for the model. \u2705 <code>active_learning_target_dataset</code> <code>str</code> Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/single_label_classification_model/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Single-Label Classification Model</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Roboflow Dataset Upload</code>, <code>Roboflow Dataset Upload</code>, <code>Webhook Sink</code>, <code>Roboflow Custom Metadata</code>, <code>Stability AI Inpainting</code>, <code>Local File Sink</code>, <code>Email Notification</code>, <code>Detections Classes Replacement</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Single-Label Classification Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>model_id</code> (<code>roboflow_model_id</code>): Roboflow model identifier.</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for predictions.</li> <li><code>disable_active_learning</code> (<code>boolean</code>): Parameter to decide if Active Learning data sampling is disabled for the model.</li> <li><code>active_learning_target_dataset</code> (<code>roboflow_project</code>): Target dataset for Active Learning data sampling - see Roboflow Active Learning docs for more information.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>inference_id</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Single-Label Classification Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/roboflow_classification_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"model_id\": \"my_project/3\",\n    \"confidence\": 0.3,\n    \"disable_active_learning\": true,\n    \"active_learning_target_dataset\": \"my_project\"\n}\n</code></pre>"},{"location":"workflows/blocks/size_measurement/","title":"Size Measurement","text":""},{"location":"workflows/blocks/size_measurement/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>SizeMeasurementBlock</code> is a transformer block designed to measure the dimensions of objects in relation to a reference object. The reference object is detected using one model, and the object to be measured is detected using another model. The block outputs the dimensions of the objects to be measured in terms of the reference object. Note: if reference_predictions provides multiple boxes, the most confident one will be selected. In order to achieve different behavior you can use Detection Transformation block with custom filter and also continue_if block if no reference detection meets expectations.</p>"},{"location":"workflows/blocks/size_measurement/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/size_measurement@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/size_measurement/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>reference_dimensions</code> <code>Union[Any, List[float], str]</code> Dimensions of the reference object (width, height) in desired units (e.g., inches) as a string in the format 'width,height' or as a tuple (width, height). \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/size_measurement/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Size Measurement</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Object Detection Model</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>YOLO-World Model</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Bounding Rectangle</code>, <code>Detections Filter</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Byte Tracker</code></li> <li>outputs: <code>Path deviation</code>, <code>Time in zone</code>, <code>Polygon Zone Visualization</code>, <code>Line Counter</code>, <code>Webhook Sink</code>, <code>Path deviation</code>, <code>Reference Path Visualization</code>, <code>Line Counter</code>, <code>Time in zone</code>, <code>VLM as Classifier</code>, <code>Perspective Correction</code>, <code>Line Counter Visualization</code>, <code>VLM as Detector</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Size Measurement</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>reference_predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Predictions from the reference object model.</li> <li><code>object_predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Predictions from the model that detects the object to measure.</li> <li><code>reference_dimensions</code> (Union[<code>list_of_values</code>, <code>string</code>]): Dimensions of the reference object (width, height) in desired units (e.g., inches) as a string in the format 'width,height' or as a tuple (width, height).</li> </ul> </li> <li> <p>output</p> <ul> <li><code>dimensions</code> (<code>list_of_values</code>): List of values of any type.</li> </ul> </li> </ul> Example JSON definition of step <code>Size Measurement</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/size_measurement@v1\",\n    \"reference_predictions\": \"$segmentation.reference_predictions\",\n    \"object_predictions\": \"$segmentation.object_predictions\",\n    \"reference_dimensions\": \"5.0,5.0\"\n}\n</code></pre>"},{"location":"workflows/blocks/stability_ai_inpainting/","title":"Stability AI Inpainting","text":""},{"location":"workflows/blocks/stability_ai_inpainting/#version-v1","title":"Version <code>v1</code>","text":"<p>The block wraps  Stability AI inpainting API and  let users use instance segmentation results to change the content of images in a creative way.</p>"},{"location":"workflows/blocks/stability_ai_inpainting/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/stability_ai_inpainting@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/stability_ai_inpainting/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>prompt</code> <code>str</code> Prompt to inpainting model (what you wish to see). \u2705 <code>negative_prompt</code> <code>str</code> Negative prompt to inpainting model (what you do not wish to see). \u2705 <code>api_key</code> <code>str</code> Your Stability AI API key. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/stability_ai_inpainting/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Stability AI Inpainting</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Keypoint Detection Model</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Stitch OCR Detections</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>CSV Formatter</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>OpenAI</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>Clip Comparison</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Local File Sink</code>, <code>Multi-Label Classification Model</code>, <code>Dynamic Crop</code>, <code>LMM</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Stability AI Inpainting</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image which was the base to generate VLM prediction.</li> <li><code>segmentation_mask</code> (<code>instance_segmentation_prediction</code>): Segmentation masks.</li> <li><code>prompt</code> (<code>string</code>): Prompt to inpainting model (what you wish to see).</li> <li><code>negative_prompt</code> (<code>string</code>): Negative prompt to inpainting model (what you do not wish to see).</li> <li><code>api_key</code> (<code>string</code>): Your Stability AI API key.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Stability AI Inpainting</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/stability_ai_inpainting@v1\",\n    \"image\": \"$inputs.image\",\n    \"segmentation_mask\": \"$steps.model.predictions\",\n    \"prompt\": \"my prompt\",\n    \"negative_prompt\": \"my prompt\",\n    \"api_key\": \"xxx-xxx\"\n}\n</code></pre>"},{"location":"workflows/blocks/stitch_images/","title":"Stitch Images","text":""},{"location":"workflows/blocks/stitch_images/#version-v1","title":"Version <code>v1</code>","text":"<p>This block combines two related scenes both containing fair amount of details. Block is utilizing Scale Invariant Feature Transform (SIFT)</p>"},{"location":"workflows/blocks/stitch_images/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/stitch_images@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/stitch_images/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>max_allowed_reprojection_error</code> <code>float</code> Advanced parameter overwriting cv.findHomography ransacReprojThreshold parameter. Maximum allowed reprojection error to treat a point pair as an inlier. Increasing value of this parameter for low details photo may yield better results.. \u2705 <code>count_of_best_matches_per_query_descriptor</code> <code>int</code> Advanced parameter overwriting cv.BFMatcher.knnMatch <code>k</code> parameter. Count of best matches found per each query descriptor or less if a query descriptor has less than k possible matches in total.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/stitch_images/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Stitch Images</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Stitch Images</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image1</code> (<code>image</code>): First input image for this step..</li> <li><code>image2</code> (<code>image</code>): Second input image for this step..</li> <li><code>max_allowed_reprojection_error</code> (<code>float_zero_to_one</code>): Advanced parameter overwriting cv.findHomography ransacReprojThreshold parameter. Maximum allowed reprojection error to treat a point pair as an inlier. Increasing value of this parameter for low details photo may yield better results..</li> <li><code>count_of_best_matches_per_query_descriptor</code> (<code>integer</code>): Advanced parameter overwriting cv.BFMatcher.knnMatch <code>k</code> parameter. Count of best matches found per each query descriptor or less if a query descriptor has less than k possible matches in total..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>stitched_image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Stitch Images</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/stitch_images@v1\",\n    \"image1\": \"$inputs.image1\",\n    \"image2\": \"$inputs.image2\",\n    \"max_allowed_reprojection_error\": 3,\n    \"count_of_best_matches_per_query_descriptor\": 2\n}\n</code></pre>"},{"location":"workflows/blocks/stitch_ocr_detections/","title":"Stitch OCR Detections","text":""},{"location":"workflows/blocks/stitch_ocr_detections/#version-v1","title":"Version <code>v1</code>","text":"<p>Combines OCR detection results into a coherent text string by organizing detections spatially.  This transformation is perfect for turning individual OCR results into structured, readable text!</p>"},{"location":"workflows/blocks/stitch_ocr_detections/#how-it-works","title":"How It Works","text":"<p>This transformation reconstructs the original text from OCR detection results by:</p> <ol> <li> <p>\ud83d\udcd0 Grouping text detections into rows based on their vertical (<code>y</code>) positions</p> </li> <li> <p>\ud83d\udccf Sorting detections within each row by horizontal (<code>x</code>) position</p> </li> <li> <p>\ud83d\udcdc Concatenating the text in reading order (left-to-right, top-to-bottom)</p> </li> </ol>"},{"location":"workflows/blocks/stitch_ocr_detections/#parameters","title":"Parameters","text":"<ul> <li> <p><code>tolerance</code>: Controls how close detections need to be vertically to be considered part of the same line of text.  A higher tolerance will group detections that are further apart vertically.</p> </li> <li> <p><code>reading_direction</code>: Determines the order in which text is read. Available options:</p> <ul> <li> <p>\"left_to_right\": Standard left-to-right reading (e.g., English) \u27a1\ufe0f</p> </li> <li> <p>\"right_to_left\": Right-to-left reading (e.g., Arabic) \u2b05\ufe0f</p> </li> <li> <p>\"vertical_top_to_bottom\": Vertical reading from top to bottom \u2b07\ufe0f</p> </li> <li> <p>\"vertical_bottom_to_top\": Vertical reading from bottom to top \u2b06\ufe0f</p> </li> </ul> </li> </ul>"},{"location":"workflows/blocks/stitch_ocr_detections/#why-use-this-transformation","title":"Why Use This Transformation?","text":"<p>This is especially useful for:</p> <ul> <li> <p>\ud83d\udcd6 Converting individual character/word detections into a readable text block</p> </li> <li> <p>\ud83d\udcdd Reconstructing multi-line text from OCR results</p> </li> <li> <p>\ud83d\udd00 Maintaining proper reading order for detected text elements</p> </li> <li> <p>\ud83c\udf0f Supporting different writing systems and text orientations</p> </li> </ul>"},{"location":"workflows/blocks/stitch_ocr_detections/#example-usage","title":"Example Usage","text":"<p>Use this transformation after an OCR model that outputs individual words or characters, so you can reconstruct the  original text layout in its intended format.</p>"},{"location":"workflows/blocks/stitch_ocr_detections/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/stitch_ocr_detections@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/stitch_ocr_detections/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>reading_direction</code> <code>str</code> The direction of the text in the image.. \u274c <code>tolerance</code> <code>int</code> The tolerance for grouping detections into the same line of text.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/stitch_ocr_detections/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Stitch OCR Detections</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>Time in zone</code>, <code>YOLO-World Model</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Webhook Sink</code>, <code>Email Notification</code>, <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Stability AI Inpainting</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Stitch OCR Detections</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>predictions</code> (<code>object_detection_prediction</code>): The output of an OCR detection model..</li> <li><code>tolerance</code> (<code>integer</code>): The tolerance for grouping detections into the same line of text..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>ocr_text</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Stitch OCR Detections</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/stitch_ocr_detections@v1\",\n    \"predictions\": \"$steps.my_ocr_detection_model.predictions\",\n    \"reading_direction\": \"right_to_left\",\n    \"tolerance\": 10\n}\n</code></pre>"},{"location":"workflows/blocks/template_matching/","title":"Template Matching","text":""},{"location":"workflows/blocks/template_matching/#version-v1","title":"Version <code>v1</code>","text":"<p>Apply Template Matching to an image. Block is based on OpenCV library function called <code>cv2.matchTemplate(...)</code> that searches for a template image within a larger image. This is often used in computer vision tasks where  you need to find a specific object or pattern in a scene, like detecting logos, objects, or  specific regions in an image.</p> <p>Please take into account the following characteristics of block: * it tends to produce overlapping and duplicated predictions, hence we added NMS which can be disabled * block may find very large number of matches in some cases due to simplicity of methods being used -  in that cases NMS may be computationally intractable and should be disabled</p> <p>Output from the block is in a form of sv.Detections objects which can be nicely paired with other blocks accepting this kind of input (like visualization blocks).</p>"},{"location":"workflows/blocks/template_matching/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/template_matching@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/template_matching/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>matching_threshold</code> <code>float</code> The threshold value for template matching.. \u2705 <code>apply_nms</code> <code>bool</code> Flag to decide if NMS should be applied at the output detections.. \u2705 <code>nms_threshold</code> <code>float</code> The threshold value NMS procedure (if to be applied).. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/template_matching/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Template Matching</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Webhook Sink</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Line Counter Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Template Matching</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>template</code> (<code>image</code>): The template image for this step..</li> <li><code>matching_threshold</code> (<code>float</code>): The threshold value for template matching..</li> <li><code>apply_nms</code> (<code>boolean</code>): Flag to decide if NMS should be applied at the output detections..</li> <li><code>nms_threshold</code> (<code>float_zero_to_one</code>): The threshold value NMS procedure (if to be applied)..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>number_of_matches</code> (<code>integer</code>): Integer value.</li> </ul> </li> </ul> Example JSON definition of step <code>Template Matching</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/template_matching@v1\",\n    \"image\": \"$inputs.image\",\n    \"template\": \"$inputs.template\",\n    \"matching_threshold\": 0.8,\n    \"apply_nms\": \"$inputs.apply_nms\",\n    \"nms_threshold\": \"$inputs.nms_threshold\"\n}\n</code></pre>"},{"location":"workflows/blocks/timeinzone/","title":"Time in zone","text":""},{"location":"workflows/blocks/timeinzone/#version-v2","title":"Version <code>v2</code>","text":"<p>The <code>TimeInZoneBlock</code> is an analytics block designed to measure time spent by objects in a zone. The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames)</p>"},{"location":"workflows/blocks/timeinzone/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/time_in_zone@v2</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/timeinzone/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>zone</code> <code>List[Any]</code> Zones (one for each batch) in a format [(x1, y1), (x2, y2), (x3, y3), ...]. \u2705 <code>triggering_anchor</code> <code>str</code> Triggering anchor. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS. \u2705 <code>remove_out_of_zone_detections</code> <code>bool</code> If true, detections found outside of zone will be filtered out. \u2705 <code>reset_out_of_zone_detections</code> <code>bool</code> If true, detections found outside of zone will have time reset. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/timeinzone/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Time in zone</code> in version <code>v2</code>.</p> <ul> <li>inputs: <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Anthropic Claude</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Bounding Rectangle</code>, <code>Byte Tracker</code>, <code>Detections Filter</code>, <code>Size Measurement</code>, <code>Object Detection Model</code>, <code>Detections Stitch</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Clip Comparison</code>, <code>Segment Anything 2 Model</code>, <code>YOLO-World Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Time in zone</code> in version <code>v2</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Predictions.</li> <li><code>zone</code> (<code>list_of_values</code>): Zones (one for each batch) in a format [(x1, y1), (x2, y2), (x3, y3), ...].</li> <li><code>triggering_anchor</code> (<code>string</code>): Triggering anchor. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS.</li> <li><code>remove_out_of_zone_detections</code> (<code>boolean</code>): If true, detections found outside of zone will be filtered out.</li> <li><code>reset_out_of_zone_detections</code> (<code>boolean</code>): If true, detections found outside of zone will have time reset.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>timed_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Time in zone</code> in version <code>v2</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/time_in_zone@v2\",\n    \"image\": \"$inputs.image\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"zone\": \"$inputs.zones\",\n    \"triggering_anchor\": \"CENTER\",\n    \"remove_out_of_zone_detections\": true,\n    \"reset_out_of_zone_detections\": true\n}\n</code></pre>"},{"location":"workflows/blocks/timeinzone/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>TimeInZoneBlock</code> is an analytics block designed to measure time spent by objects in a zone. The block requires detections to be tracked (i.e. each object must have unique tracker_id assigned, which persists between frames)</p>"},{"location":"workflows/blocks/timeinzone/#type-identifier_1","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/time_in_zone@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/timeinzone/#properties_1","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>zone</code> <code>List[Any]</code> Zones (one for each batch) in a format [(x1, y1), (x2, y2), (x3, y3), ...]. \u2705 <code>triggering_anchor</code> <code>str</code> Triggering anchor. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS. \u2705 <code>remove_out_of_zone_detections</code> <code>bool</code> If true, detections found outside of zone will be filtered out. \u2705 <code>reset_out_of_zone_detections</code> <code>bool</code> If true, detections found outside of zone will have time reset. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/timeinzone/#available-connections_1","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Time in zone</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Anthropic Claude</code>, <code>Crop Visualization</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Size Measurement</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Dynamic Zone</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Polygon Visualization</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Bounding Rectangle</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Trace Visualization</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Time in zone</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>metadata</code> (<code>video_metadata</code>): not available.</li> <li><code>detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Predictions.</li> <li><code>zone</code> (<code>list_of_values</code>): Zones (one for each batch) in a format [(x1, y1), (x2, y2), (x3, y3), ...].</li> <li><code>triggering_anchor</code> (<code>string</code>): Triggering anchor. Allowed values: CENTER, CENTER_LEFT, CENTER_RIGHT, TOP_CENTER, TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_CENTER, BOTTOM_RIGHT, CENTER_OF_MASS.</li> <li><code>remove_out_of_zone_detections</code> (<code>boolean</code>): If true, detections found outside of zone will be filtered out.</li> <li><code>reset_out_of_zone_detections</code> (<code>boolean</code>): If true, detections found outside of zone will have time reset.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>timed_detections</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>]): Prediction with detected bounding boxes in form of sv.Detections(...) object if <code>object_detection_prediction</code> or Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object if <code>instance_segmentation_prediction</code>.</li> </ul> </li> </ul> Example JSON definition of step <code>Time in zone</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/time_in_zone@v1\",\n    \"image\": \"$inputs.image\",\n    \"metadata\": \"&lt;block_does_not_provide_example&gt;\",\n    \"detections\": \"$steps.object_detection_model.predictions\",\n    \"zone\": \"$inputs.zones\",\n    \"triggering_anchor\": \"CENTER\",\n    \"remove_out_of_zone_detections\": true,\n    \"reset_out_of_zone_detections\": true\n}\n</code></pre>"},{"location":"workflows/blocks/trace_visualization/","title":"Trace Visualization","text":""},{"location":"workflows/blocks/trace_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>TraceVisualization</code> block draws tracker results on an image using Supervision's <code>sv.TraceAnnotator</code>.</p>"},{"location":"workflows/blocks/trace_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/trace_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/trace_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>position</code> <code>str</code> The anchor position for placing the label.. \u2705 <code>trace_length</code> <code>int</code> Maximum number of historical tracked objects positions to display.. \u2705 <code>thickness</code> <code>int</code> Thickness of the track visualization line.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/trace_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Trace Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Trace Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>position</code> (<code>string</code>): The anchor position for placing the label..</li> <li><code>trace_length</code> (<code>integer</code>): Maximum number of historical tracked objects positions to display..</li> <li><code>thickness</code> (<code>integer</code>): Thickness of the track visualization line..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Trace Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/trace_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"position\": \"CENTER\",\n    \"trace_length\": 30,\n    \"thickness\": 1\n}\n</code></pre>"},{"location":"workflows/blocks/triangle_visualization/","title":"Triangle Visualization","text":""},{"location":"workflows/blocks/triangle_visualization/#version-v1","title":"Version <code>v1</code>","text":"<p>The <code>TriangleVisualization</code> block draws triangle markers on an image at specific coordinates based on provided detections using Supervision's <code>sv.TriangleAnnotator</code>.</p>"},{"location":"workflows/blocks/triangle_visualization/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/triangle_visualization@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/triangle_visualization/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>copy_image</code> <code>bool</code> Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed.. \u2705 <code>color_palette</code> <code>str</code> Color palette to use for annotations.. \u2705 <code>palette_size</code> <code>int</code> Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>.. \u2705 <code>custom_colors</code> <code>List[str]</code> List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\".. \u2705 <code>color_axis</code> <code>str</code> Strategy to use for mapping colors to annotations.. \u2705 <code>position</code> <code>str</code> The anchor position for placing the triangle.. \u2705 <code>base</code> <code>int</code> Base width of the triangle in pixels.. \u2705 <code>height</code> <code>int</code> Height of the triangle in pixels.. \u2705 <code>outline_thickness</code> <code>int</code> Thickness of the outline of the triangle in pixels.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/triangle_visualization/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Triangle Visualization</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Template Matching</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Detections Classes Replacement</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Keypoint Detection Model</code>, <code>Relative Static Crop</code>, <code>Path deviation</code>, <code>Image Convert Grayscale</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Bounding Rectangle</code>, <code>Color Visualization</code>, <code>Detections Filter</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Time in zone</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Instance Segmentation Model</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>Clip Comparison</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>Keypoint Visualization</code>, <code>Halo Visualization</code>, <code>YOLO-World Model</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Polygon Zone Visualization</code>, <code>Image Contours</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Triangle Visualization</code>, <code>Segment Anything 2 Model</code>, <code>Image Slicer</code>, <code>Instance Segmentation Model</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Image Blur</code>, <code>Object Detection Model</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Stability AI Inpainting</code>, <code>Image Convert Grayscale</code>, <code>OCR Model</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>LMM</code>, <code>Dynamic Crop</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Trace Visualization</code>, <code>CogVLM</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Triangle Visualization</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The input image for this step..</li> <li><code>copy_image</code> (<code>boolean</code>): Duplicate the image contents (vs overwriting the image in place). Deselect for chained visualizations that should stack on previous ones where the intermediate state is not needed..</li> <li><code>predictions</code> (Union[<code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code>]): Predictions.</li> <li><code>color_palette</code> (<code>string</code>): Color palette to use for annotations..</li> <li><code>palette_size</code> (<code>integer</code>): Number of colors in the color palette. Applies when using a matplotlib <code>color_palette</code>..</li> <li><code>custom_colors</code> (<code>list_of_values</code>): List of colors to use for annotations when <code>color_palette</code> is set to \"CUSTOM\"..</li> <li><code>color_axis</code> (<code>string</code>): Strategy to use for mapping colors to annotations..</li> <li><code>position</code> (<code>string</code>): The anchor position for placing the triangle..</li> <li><code>base</code> (<code>integer</code>): Base width of the triangle in pixels..</li> <li><code>height</code> (<code>integer</code>): Height of the triangle in pixels..</li> <li><code>outline_thickness</code> (<code>integer</code>): Thickness of the outline of the triangle in pixels..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>image</code> (<code>image</code>): Image in workflows.</li> </ul> </li> </ul> Example JSON definition of step <code>Triangle Visualization</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/triangle_visualization@v1\",\n    \"image\": \"$inputs.image\",\n    \"copy_image\": true,\n    \"predictions\": \"$steps.object_detection_model.predictions\",\n    \"color_palette\": \"DEFAULT\",\n    \"palette_size\": 10,\n    \"custom_colors\": [\n        \"#FF0000\",\n        \"#00FF00\",\n        \"#0000FF\"\n    ],\n    \"color_axis\": \"CLASS\",\n    \"position\": \"CENTER\",\n    \"base\": 10,\n    \"height\": 10,\n    \"outline_thickness\": 2\n}\n</code></pre>"},{"location":"workflows/blocks/vl_mas_classifier/","title":"VLM as Classifier","text":""},{"location":"workflows/blocks/vl_mas_classifier/#version-v1","title":"Version <code>v1</code>","text":"<p>The block expects string input that would be produced by blocks exposing Large Language Models (LLMs) and  Visual Language Models (VLMs). Input is parsed to classification prediction and returned as block output.</p> <p>Accepted formats:</p> <ul> <li> <p>valid JSON strings</p> </li> <li> <p>JSON documents wrapped with Markdown tags (very common for GPT responses)</p> </li> </ul> <p>Example: <pre><code>{\"my\": \"json\"}\n</code></pre></p> <p>Details regarding block behavior:</p> <ul> <li> <p><code>error_status</code> is set <code>True</code> whenever parsing cannot be completed</p> </li> <li> <p>in case of multiple markdown blocks with raw JSON content - only first will be parsed</p> </li> </ul>"},{"location":"workflows/blocks/vl_mas_classifier/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/vlm_as_classifier@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/vl_mas_classifier/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>classes</code> <code>List[str]</code> List of all classes used by the model, required to generate mapping between class name and class id.. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/vl_mas_classifier/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>VLM as Classifier</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Anthropic Claude</code>, <code>Crop Visualization</code>, <code>Clip Comparison</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Dynamic Zone</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Roboflow Dataset Upload</code>, <code>Roboflow Dataset Upload</code>, <code>Webhook Sink</code>, <code>Roboflow Custom Metadata</code>, <code>Stability AI Inpainting</code>, <code>Local File Sink</code>, <code>Email Notification</code>, <code>Detections Classes Replacement</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>VLM as Classifier</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image which was the base to generate VLM prediction.</li> <li><code>vlm_output</code> (<code>language_model_output</code>): The string with raw classification prediction to parse..</li> <li><code>classes</code> (<code>list_of_values</code>): List of all classes used by the model, required to generate mapping between class name and class id..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>predictions</code> (<code>classification_prediction</code>): Predictions from classifier.</li> <li><code>inference_id</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>VLM as Classifier</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n    \"image\": \"$inputs.image\",\n    \"vlm_output\": [\n        \"$steps.lmm.output\"\n    ],\n    \"classes\": [\n        \"$steps.lmm.classes\",\n        \"$inputs.classes\",\n        [\n            \"class_a\",\n            \"class_b\"\n        ]\n    ]\n}\n</code></pre>"},{"location":"workflows/blocks/vl_mas_detector/","title":"VLM as Detector","text":""},{"location":"workflows/blocks/vl_mas_detector/#version-v1","title":"Version <code>v1</code>","text":"<p>The block expects string input that would be produced by blocks exposing Large Language Models (LLMs) and  Visual Language Models (VLMs). Input is parsed to object-detection prediction and returned as block output.</p> <p>Accepted formats:</p> <ul> <li> <p>valid JSON strings</p> </li> <li> <p>JSON documents wrapped with Markdown tags</p> </li> </ul> <p>Example <pre><code>{\"my\": \"json\"}\n</code></pre></p> <p>Details regarding block behavior:</p> <ul> <li> <p><code>error_status</code> is set <code>True</code> whenever parsing cannot be completed</p> </li> <li> <p>in case of multiple markdown blocks with raw JSON content - only first will be parsed</p> </li> </ul>"},{"location":"workflows/blocks/vl_mas_detector/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/vlm_as_detector@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/vl_mas_detector/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>classes</code> <code>List[str]</code> List of all classes used by the model, required to generate mapping between class name and class id.. \u2705 <code>model_type</code> <code>str</code> Type of the model that generated prediction. \u274c <code>task_type</code> <code>str</code> Task type to performed by model.. \u274c <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/vl_mas_detector/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>VLM as Detector</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Anthropic Claude</code>, <code>Crop Visualization</code>, <code>Clip Comparison</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Clip Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Google Gemini</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>OpenAI</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Dynamic Zone</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Local File Sink</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>VLM as Detector</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>image</code> (<code>image</code>): The image which was the base to generate VLM prediction.</li> <li><code>vlm_output</code> (<code>language_model_output</code>): The string with raw classification prediction to parse..</li> <li><code>classes</code> (<code>list_of_values</code>): List of all classes used by the model, required to generate mapping between class name and class id..</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> <li><code>inference_id</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>VLM as Detector</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/vlm_as_detector@v1\",\n    \"image\": \"$inputs.image\",\n    \"vlm_output\": [\n        \"$steps.lmm.output\"\n    ],\n    \"classes\": [\n        \"$steps.lmm.classes\",\n        \"$inputs.classes\",\n        [\n            \"class_a\",\n            \"class_b\"\n        ]\n    ],\n    \"model_type\": [\n        \"google-gemini\",\n        \"anthropic-claude\",\n        \"florence-2\"\n    ],\n    \"task_type\": \"&lt;block_does_not_provide_example&gt;\"\n}\n</code></pre>"},{"location":"workflows/blocks/webhook_sink/","title":"Webhook Sink","text":""},{"location":"workflows/blocks/webhook_sink/#version-v1","title":"Version <code>v1</code>","text":"<p>The Webhook Sink block enables sending a data from Workflow into external APIs  by sending HTTP requests containing workflow results. It supports multiple HTTP methods  (GET, POST, PUT) and can be configured to send:</p> <ul> <li> <p>JSON payloads</p> </li> <li> <p>query parameters</p> </li> <li> <p>multipart-encoded files </p> </li> </ul> <p>This block is designed to provide flexibility for integrating workflows with remote systems  for data exchange, notifications, or other integrations.</p>"},{"location":"workflows/blocks/webhook_sink/#setting-query-parameters","title":"Setting Query Parameters","text":"<p>You can easily set query parameters for your request:</p> <pre><code>query_parameters = {\n    \"api_key\": \"$inputs.api_key\",\n}\n</code></pre> <p>will send request into the following URL: <code>https://your-host/some/resource?api_key=&lt;API_KEY_VALUE&gt;</code></p>"},{"location":"workflows/blocks/webhook_sink/#setting-headers","title":"Setting headers","text":"<p>Setting headers is as easy as setting query parameters:</p> <pre><code>headers = {\n    \"api_key\": \"$inputs.api_key\",\n}\n</code></pre>"},{"location":"workflows/blocks/webhook_sink/#sending-json-payloads","title":"Sending JSON payloads","text":"<p>You can set the body of your message to be JSON document that you construct specifying <code>json_payload</code>  and <code>json_payload_operations</code> properties. <code>json_payload</code> works similarly to <code>query_parameters</code> and  <code>headers</code>, but you can optionally apply UQL operations on top of JSON body elements.</p> <p>Let's assume that you want to send number of bounding boxes predicted by object detection model in body field named <code>detections_number</code>, then you need to specify configuration similar to the  following:</p> <pre><code>json_payload = {\n    \"detections_number\": \"$steps.model.predictions\",\n}\njson_payload_operations = {\n    \"detections_number\": [{\"type\": \"SequenceLength\"}]\n}\n</code></pre>"},{"location":"workflows/blocks/webhook_sink/#multipart-encoded-files-in-post-requests","title":"Multipart-Encoded Files in POST requests","text":"<p>Your endpoint may also accept multipart requests. You can form them in a way which is similar to  JSON payloads - setting <code>multi_part_encoded_files</code> and <code>multi_part_encoded_files_operations</code>.</p> <p>Let's assume you want to send the image in the request, then your configuration may be the following:</p> <pre><code>multi_part_encoded_files = {\n    \"image\": \"$inputs.image\",\n}\nmulti_part_encoded_files_operations = {\n    \"image\": [{\"type\": \"ConvertImageToJPEG\"}]\n}\n</code></pre>"},{"location":"workflows/blocks/webhook_sink/#cooldown","title":"Cooldown","text":"<p>The block accepts <code>cooldown_seconds</code> (which defaults to <code>5</code> seconds) to prevent unintended bursts of  notifications. Please adjust it according to your needs, setting <code>0</code> indicate no cooldown. </p> <p>During cooldown period, consecutive runs of the step will cause <code>throttling_status</code> output to be set <code>True</code> and no notification will be sent.</p>"},{"location":"workflows/blocks/webhook_sink/#async-execution","title":"Async execution","text":"<p>Configure the <code>fire_and_forget</code> property. Set it to True if you want the request to be sent in the background,  allowing the Workflow to proceed without waiting on data to be sent. In this case you will not be able to rely on  <code>error_status</code> output which will always be set to <code>False</code>, so we recommend setting the <code>fire_and_forget=False</code> for debugging purposes.</p>"},{"location":"workflows/blocks/webhook_sink/#disabling-notifications-based-on-runtime-parameter","title":"Disabling notifications based on runtime parameter","text":"<p>Sometimes it would be convenient to manually disable the Webhook sink block. This is possible  setting <code>disable_sink</code> flag to hold reference to Workflow input. with such setup, caller would be able to disable the sink when needed sending agreed input parameter.</p>"},{"location":"workflows/blocks/webhook_sink/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/webhook_sink@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/webhook_sink/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>url</code> <code>str</code> URL of the resource to make request. \u2705 <code>method</code> <code>str</code> HTTP method to be used. \u274c <code>query_parameters</code> <code>Dict[str, Union[List[Union[bool, float, int, str]], bool, float, int, str]]</code> Request query parameters. \u2705 <code>headers</code> <code>Dict[str, Union[bool, float, int, str]]</code> Request headers. \u2705 <code>json_payload</code> <code>Dict[str, Union[Dict[Any, Any], List[Any], bool, float, int, str]]</code> Fields to put into JSON payload. \u2705 <code>json_payload_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data w.r.t. each value of <code>json_payload</code> parameter. \u274c <code>multi_part_encoded_files</code> <code>Dict[str, Union[Dict[Any, Any], List[Any], bool, float, int, str]]</code> Data to POST as Multipart-Encoded File. \u2705 <code>multi_part_encoded_files_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data w.r.t. each value of <code>multi_part_encoded_files</code> parameter. \u274c <code>form_data</code> <code>Dict[str, Union[Dict[Any, Any], List[Any], bool, float, int, str]]</code> Fields to put into form-data. \u2705 <code>form_data_operations</code> <code>Dict[str, List[Union[ClassificationPropertyExtract, ConvertDictionaryToJSON, ConvertImageToBase64, ConvertImageToJPEG, DetectionsFilter, DetectionsOffset, DetectionsPropertyExtract, DetectionsRename, DetectionsSelection, DetectionsShift, DetectionsToDictionary, Divide, ExtractDetectionProperty, ExtractImageProperty, LookupTable, Multiply, NumberRound, NumericSequenceAggregate, RandomNumber, SequenceAggregate, SequenceApply, SequenceLength, SequenceMap, SortDetections, StringMatches, StringSubSequence, StringToLowerCase, StringToUpperCase, ToBoolean, ToNumber, ToString]]]</code> UQL definitions of operations to be performed on defined data w.r.t. each value of <code>form_data</code> parameter. \u274c <code>request_timeout</code> <code>int</code> Number of seconds to wait for remote API response. \u2705 <code>fire_and_forget</code> <code>bool</code> Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use <code>True</code> if best-effort registration is needed, use <code>False</code> while debugging and if error handling is needed. \u2705 <code>disable_sink</code> <code>bool</code> boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request. \u2705 <code>cooldown_seconds</code> <code>int</code> Number of seconds to wait until follow-up notification can be sent. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/webhook_sink/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>Webhook Sink</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Detections Stabilizer</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Polygon Visualization</code>, <code>Image Threshold</code>, <code>SIFT Comparison</code>, <code>Roboflow Dataset Upload</code>, <code>Camera Focus</code>, <code>Keypoint Detection Model</code>, <code>YOLO-World Model</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Blur Visualization</code>, <code>Detections Stitch</code>, <code>SIFT</code>, <code>Image Preprocessing</code>, <code>Data Aggregator</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Dynamic Zone</code>, <code>Byte Tracker</code>, <code>Reference Path Visualization</code>, <code>Rate Limiter</code>, <code>VLM as Classifier</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Dimension Collapse</code>, <code>Anthropic Claude</code>, <code>QR Code Detection</code>, <code>Clip Comparison</code>, <code>Template Matching</code>, <code>Time in zone</code>, <code>Byte Tracker</code>, <code>Image Blur</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Webhook Sink</code>, <code>Stability AI Inpainting</code>, <code>OCR Model</code>, <code>Label Visualization</code>, <code>VLM as Detector</code>, <code>Detections Transformation</code>, <code>Google Gemini</code>, <code>Local File Sink</code>, <code>LMM</code>, <code>Stitch Images</code>, <code>Google Vision OCR</code>, <code>OpenAI</code>, <code>Time in zone</code>, <code>Trace Visualization</code>, <code>Continue If</code>, <code>Line Counter</code>, <code>Property Definition</code>, <code>Dominant Color</code>, <code>Single-Label Classification Model</code>, <code>LMM For Classification</code>, <code>JSON Parser</code>, <code>Distance Measurement</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Clip Comparison</code>, <code>Circle Visualization</code>, <code>Keypoint Visualization</code>, <code>First Non Empty Or Default</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Pixel Color Count</code>, <code>Stitch OCR Detections</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>SIFT Comparison</code>, <code>Size Measurement</code>, <code>Bounding Rectangle</code>, <code>CSV Formatter</code>, <code>Polygon Zone Visualization</code>, <code>Barcode Detection</code>, <code>OpenAI</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Instance Segmentation Model</code>, <code>Corner Visualization</code>, <code>Email Notification</code>, <code>Ellipse Visualization</code>, <code>Object Detection Model</code>, <code>Image Convert Grayscale</code>, <code>Detection Offset</code>, <code>Model Comparison Visualization</code>, <code>Expression</code>, <code>Background Color Visualization</code>, <code>Multi-Label Classification Model</code>, <code>Dynamic Crop</code>, <code>Line Counter Visualization</code>, <code>Line Counter</code>, <code>CogVLM</code></li> <li>outputs: <code>Webhook Sink</code>, <code>Email Notification</code>, <code>Roboflow Custom Metadata</code>, <code>Local File Sink</code>, <code>Stability AI Inpainting</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>Webhook Sink</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>url</code> (<code>string</code>): URL of the resource to make request.</li> <li><code>query_parameters</code> (Union[<code>string</code>, <code>top_class</code>, <code>roboflow_api_key</code>, <code>roboflow_project</code>, <code>list_of_values</code>, <code>float</code>, <code>boolean</code>, <code>roboflow_model_id</code>, <code>integer</code>, <code>float_zero_to_one</code>]): Request query parameters.</li> <li><code>headers</code> (Union[<code>string</code>, <code>top_class</code>, <code>roboflow_api_key</code>, <code>roboflow_project</code>, <code>float</code>, <code>boolean</code>, <code>roboflow_model_id</code>, <code>integer</code>, <code>float_zero_to_one</code>]): Request headers.</li> <li><code>json_payload</code> (<code>*</code>): Fields to put into JSON payload.</li> <li><code>multi_part_encoded_files</code> (<code>*</code>): Data to POST as Multipart-Encoded File.</li> <li><code>form_data</code> (<code>*</code>): Fields to put into form-data.</li> <li><code>request_timeout</code> (<code>integer</code>): Number of seconds to wait for remote API response.</li> <li><code>fire_and_forget</code> (<code>boolean</code>): Boolean flag dictating if sink is supposed to be executed in the background, not waiting on status of registration before end of workflow run. Use <code>True</code> if best-effort registration is needed, use <code>False</code> while debugging and if error handling is needed.</li> <li><code>disable_sink</code> (<code>boolean</code>): boolean flag that can be also reference to input - to arbitrarily disable data collection for specific request.</li> <li><code>cooldown_seconds</code> (<code>integer</code>): Number of seconds to wait until follow-up notification can be sent.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>error_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>throttling_status</code> (<code>boolean</code>): Boolean flag.</li> <li><code>message</code> (<code>string</code>): String value.</li> </ul> </li> </ul> Example JSON definition of step <code>Webhook Sink</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/webhook_sink@v1\",\n    \"url\": \"&lt;block_does_not_provide_example&gt;\",\n    \"method\": \"&lt;block_does_not_provide_example&gt;\",\n    \"query_parameters\": {\n        \"api_key\": \"$inputs.api_key\"\n    },\n    \"headers\": {\n        \"api_key\": \"$inputs.api_key\"\n    },\n    \"json_payload\": {\n        \"field\": \"$steps.model.predictions\"\n    },\n    \"json_payload_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"multi_part_encoded_files\": {\n        \"image\": \"$steps.visualization.image\"\n    },\n    \"multi_part_encoded_files_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"form_data\": {\n        \"field\": \"$inputs.field_value\"\n    },\n    \"form_data_operations\": {\n        \"predictions\": [\n            {\n                \"property_name\": \"class_name\",\n                \"type\": \"DetectionsPropertyExtract\"\n            }\n        ]\n    },\n    \"request_timeout\": \"$inputs.request_timeout\",\n    \"fire_and_forget\": \"$inputs.fire_and_forget\",\n    \"disable_sink\": false,\n    \"cooldown_seconds\": \"$inputs.cooldown_seconds\"\n}\n</code></pre>"},{"location":"workflows/blocks/yolo_world_model/","title":"YOLO-World Model","text":""},{"location":"workflows/blocks/yolo_world_model/#version-v1","title":"Version <code>v1</code>","text":"<p>Run YOLO-World, a zero-shot object detection model, on an image.</p> <p>YOLO-World accepts one or more text classes you want to identify in an image. The model  returns the location of objects that meet the specified class, if YOLO-World is able to  identify objects of that class.</p> <p>We recommend experimenting with YOLO-World to evaluate the model on your use case  before using this block in production. For example on how to effectively prompt  YOLO-World, refer to the Roboflow YOLO-World prompting  guide.</p>"},{"location":"workflows/blocks/yolo_world_model/#type-identifier","title":"Type identifier","text":"<p>Use the following identifier in step <code>\"type\"</code> field: <code>roboflow_core/yolo_world_model@v1</code>to add the block as as step in your workflow.</p>"},{"location":"workflows/blocks/yolo_world_model/#properties","title":"Properties","text":"Name Type Description Refs <code>name</code> <code>str</code> The unique name of this step.. \u274c <code>class_names</code> <code>List[str]</code> One or more classes that you want YOLO-World to detect. The model accepts any string as an input, though does best with short descriptions of common objects.. \u2705 <code>version</code> <code>str</code> Variant of YoloWorld model. \u2705 <code>confidence</code> <code>float</code> Confidence threshold for detections. \u2705 <p>The Refs column marks possibility to parametrise the property with dynamic values available  in <code>workflow</code> runtime. See Bindings for more info.</p>"},{"location":"workflows/blocks/yolo_world_model/#available-connections","title":"Available Connections","text":"<p>Check what blocks you can connect to <code>YOLO-World Model</code> in version <code>v1</code>.</p> <ul> <li>inputs: <code>Bounding Box Visualization</code>, <code>Crop Visualization</code>, <code>Absolute Static Crop</code>, <code>Dot Visualization</code>, <code>Image Blur</code>, <code>Polygon Visualization</code>, <code>Perspective Correction</code>, <code>Mask Visualization</code>, <code>Image Threshold</code>, <code>Circle Visualization</code>, <code>SIFT Comparison</code>, <code>Camera Focus</code>, <code>Keypoint Visualization</code>, <code>Stability AI Inpainting</code>, <code>Halo Visualization</code>, <code>Relative Static Crop</code>, <code>Image Convert Grayscale</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>SIFT</code>, <code>Background Color Visualization</code>, <code>Image Preprocessing</code>, <code>Dynamic Crop</code>, <code>Color Visualization</code>, <code>Stitch Images</code>, <code>Line Counter Visualization</code>, <code>Image Slicer</code>, <code>Image Contours</code>, <code>Polygon Zone Visualization</code>, <code>Triangle Visualization</code>, <code>Trace Visualization</code>, <code>Reference Path Visualization</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Ellipse Visualization</code></li> <li>outputs: <code>Distance Measurement</code>, <code>Bounding Box Visualization</code>, <code>Roboflow Custom Metadata</code>, <code>Crop Visualization</code>, <code>Detections Stabilizer</code>, <code>Time in zone</code>, <code>Dot Visualization</code>, <code>Byte Tracker</code>, <code>Detections Classes Replacement</code>, <code>Perspective Correction</code>, <code>Circle Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Path deviation</code>, <code>Detections Consensus</code>, <code>Detection Offset</code>, <code>Stitch OCR Detections</code>, <code>Model Comparison Visualization</code>, <code>Label Visualization</code>, <code>Blur Visualization</code>, <code>Path deviation</code>, <code>Byte Tracker</code>, <code>Detections Stitch</code>, <code>Detections Transformation</code>, <code>Background Color Visualization</code>, <code>Dynamic Crop</code>, <code>Size Measurement</code>, <code>Detections Filter</code>, <code>Color Visualization</code>, <code>Roboflow Dataset Upload</code>, <code>Time in zone</code>, <code>Segment Anything 2 Model</code>, <code>Triangle Visualization</code>, <code>Line Counter</code>, <code>Trace Visualization</code>, <code>Byte Tracker</code>, <code>Line Counter</code>, <code>Corner Visualization</code>, <code>Pixelate Visualization</code>, <code>Florence-2 Model</code>, <code>Ellipse Visualization</code></li> </ul> <p>The available connections depend on its binding kinds. Check what binding kinds  <code>YOLO-World Model</code> in version <code>v1</code>  has.</p> Bindings <ul> <li> <p>input</p> <ul> <li><code>images</code> (<code>image</code>): The image to infer on.</li> <li><code>class_names</code> (<code>list_of_values</code>): One or more classes that you want YOLO-World to detect. The model accepts any string as an input, though does best with short descriptions of common objects..</li> <li><code>version</code> (<code>string</code>): Variant of YoloWorld model.</li> <li><code>confidence</code> (<code>float_zero_to_one</code>): Confidence threshold for detections.</li> </ul> </li> <li> <p>output</p> <ul> <li><code>predictions</code> (<code>object_detection_prediction</code>): Prediction with detected bounding boxes in form of sv.Detections(...) object.</li> </ul> </li> </ul> Example JSON definition of step <code>YOLO-World Model</code> in version <code>v1</code> <pre><code>{\n    \"name\": \"&lt;your_step_name_here&gt;\",\n    \"type\": \"roboflow_core/yolo_world_model@v1\",\n    \"images\": \"$inputs.image\",\n    \"class_names\": [\n        \"person\",\n        \"car\",\n        \"license plate\"\n    ],\n    \"version\": \"v2-s\",\n    \"confidence\": 0.005\n}\n</code></pre>"},{"location":"workflows/gallery/advanced_inference_techniques/","title":"Example Workflows - Advanced inference techniques","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/advanced_inference_techniques/#sahi-in-workflows-object-detection","title":"SAHI in workflows - object detection","text":"<p>This example illustrates usage of SAHI  technique in workflows.</p> <p>Workflows implementation requires three blocks:</p> <ul> <li> <p>Image Slicer - which runs a sliding window over image and for each image prepares batch of crops </p> </li> <li> <p>detection model block (in our scenario Roboflow Object Detection model) - which is responsible  for making predictions on each crop</p> </li> <li> <p>Detections stitch - which combines partial predictions for each slice of the image into a single prediction</p> </li> </ul> Workflow definition <pre><code>{\n    \"version\": \"1.0.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"overlap_filtering_strategy\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/image_slicer@v1\",\n            \"name\": \"image_slicer\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"detection\",\n            \"image\": \"$steps.image_slicer.slices\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_stitch@v1\",\n            \"name\": \"stitch\",\n            \"reference_image\": \"$inputs.image\",\n            \"predictions\": \"$steps.detection.predictions\",\n            \"overlap_filtering_strategy\": \"$inputs.overlap_filtering_strategy\"\n        },\n        {\n            \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n            \"name\": \"bbox_visualiser\",\n            \"predictions\": \"$steps.stitch.predictions\",\n            \"image\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.stitch.predictions\",\n            \"coordinates_system\": \"own\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"visualisation\",\n            \"selector\": \"$steps.bbox_visualiser.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/advanced_inference_techniques/#sahi-in-workflows-instance-segmentation","title":"SAHI in workflows - instance segmentation","text":"<p>This example illustrates usage of SAHI  technique in workflows.</p> <p>Workflows implementation requires three blocks:</p> <ul> <li> <p>Image Slicer - which runs a sliding window over image and for each image prepares batch of crops </p> </li> <li> <p>detection model block (in our scenario Roboflow Instance Segmentation model) - which is responsible  for making predictions on each crop</p> </li> <li> <p>Detections stitch - which combines partial predictions for each slice of the image into a single prediction</p> </li> </ul> Workflow definition <pre><code>{\n    \"version\": \"1.0.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"overlap_filtering_strategy\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/image_slicer@v1\",\n            \"name\": \"image_slicer\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"detection\",\n            \"image\": \"$steps.image_slicer.slices\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_stitch@v1\",\n            \"name\": \"stitch\",\n            \"reference_image\": \"$inputs.image\",\n            \"predictions\": \"$steps.detection.predictions\",\n            \"overlap_filtering_strategy\": \"$inputs.overlap_filtering_strategy\"\n        },\n        {\n            \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n            \"name\": \"bbox_visualiser\",\n            \"predictions\": \"$steps.stitch.predictions\",\n            \"image\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.stitch.predictions\",\n            \"coordinates_system\": \"own\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"visualisation\",\n            \"selector\": \"$steps.bbox_visualiser.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/","title":"Example Workflows - Basic Workflows","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/basic_workflows/#workflow-with-bounding-rect","title":"Workflow with bounding rect","text":"<p>This is the basic workflow that only contains a single object detection model and bounding rectangle extraction.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"InstanceSegmentationModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-seg-640\"\n        },\n        {\n            \"type\": \"roboflow_core/bounding_rect@v1\",\n            \"name\": \"bounding_rect\",\n            \"predictions\": \"$steps.detection.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.bounding_rect.detections_with_rect\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/#workflow-with-clip-model","title":"Workflow with CLIP model","text":"<p>This is the basic workflow that only contains a single CLIP model block. </p> <p>Please take a look at how batch-oriented WorkflowImage data is plugged to  detection step via input selector (<code>$inputs.image</code>) and how non-batch parameters  (reference set of texts that the each image in batch will be compared to) is dynamically specified - via <code>$inputs.reference</code> selector.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"reference\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ClipComparison\",\n            \"name\": \"comparison\",\n            \"images\": \"$inputs.image\",\n            \"texts\": \"$inputs.reference\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"similarity\",\n            \"selector\": \"$steps.comparison.similarity\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/#workflow-with-static-crop-and-object-detection-model","title":"Workflow with static crop and object detection model","text":"<p>This is the basic workflow that contains single transformation (static crop) followed by object detection model. This example may be inspiration for anyone who would like to run specific model only on specific part of the image. The Region of Interest does not necessarily have to be defined statically -  please note that coordinates of static crops are referred via input selectors,  which means that each time you run the workflow (for instance in each different physical location, where RoI for static crop is location-dependent) you may  provide different RoI coordinates.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.7\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"x_center\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"y_center\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"width\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"height\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"AbsoluteStaticCrop\",\n            \"name\": \"crop\",\n            \"image\": \"$inputs.image\",\n            \"x_center\": \"$inputs.x_center\",\n            \"y_center\": \"$inputs.y_center\",\n            \"width\": \"$inputs.width\",\n            \"height\": \"$inputs.height\"\n        },\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$steps.crop.crops\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"crop\",\n            \"selector\": \"$steps.crop.crops\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.detection.*\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result_in_own_coordinates\",\n            \"selector\": \"$steps.detection.*\",\n            \"coordinates_system\": \"own\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/basic_workflows/#workflow-with-single-object-detection-model","title":"Workflow with single object detection model","text":"<p>This is the basic workflow that only contains a single object detection model. </p> <p>Please take a look at how batch-oriented WorkflowImage data is plugged to  detection step via input selector (<code>$inputs.image</code>) and how non-batch parameters are dynamically specified - via <code>$inputs.model_id</code> and <code>$inputs.confidence</code> selectors.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.3\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.detection.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/data_analytics_in_workflows/","title":"Example Workflows - Data analytics in Workflows","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/data_analytics_in_workflows/#workflow-producing-csv","title":"Workflow producing CSV","text":"<p>This example showcases how to export CSV file out of Workflow. Object detection results are  processed with CSV Formatter block to produce aggregated results.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"additional_column_value\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/csv_formatter@v1\",\n            \"name\": \"csv_formatter\",\n            \"columns_data\": {\n                \"predicted_classes\": \"$steps.model.predictions\",\n                \"number_of_bounding_boxes\": \"$steps.model.predictions\",\n                \"additional_column\": \"$inputs.additional_column_value\"\n            },\n            \"columns_operations\": {\n                \"predicted_classes\": [\n                    {\n                        \"type\": \"DetectionsPropertyExtract\",\n                        \"property_name\": \"class_name\"\n                    }\n                ],\n                \"number_of_bounding_boxes\": [\n                    {\n                        \"type\": \"SequenceLength\"\n                    }\n                ]\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"csv\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.csv_formatter.csv_content\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/data_analytics_in_workflows/#aggregation-of-results-over-time","title":"Aggregation of results over time","text":"<p>This example shows how to aggregate and analyse predictions using Workflows.</p> <p>The key for data analytics in this example is Data Aggregator block which is fed with model  predictions and perform the following aggregations on each 6 consecutive predictions:</p> <ul> <li> <p>taking classes names from  bounding boxes, it outputs unique classes names, number of unique classes and number of bounding boxes for each class</p> </li> <li> <p>taking the number of detected bounding boxes in each prediction, it outputs minimum, maximum and total number  of bounding boxes per prediction in aggregated time window </p> </li> </ul> <p>Run on video to produce meaningful results</p> <p>This workflow will not work using the docs preview. You must run it on video file. Copy the template into your Roboflow app, start <code>inference</code> server and use video preview  to get the results.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\"\n        },\n        {\n            \"type\": \"roboflow_core/data_aggregator@v1\",\n            \"name\": \"data_aggregation\",\n            \"data\": {\n                \"predicted_classes\": \"$steps.model.predictions\",\n                \"number_of_predictions\": \"$steps.model.predictions\"\n            },\n            \"data_operations\": {\n                \"predicted_classes\": [\n                    {\n                        \"type\": \"DetectionsPropertyExtract\",\n                        \"property_name\": \"class_name\"\n                    }\n                ],\n                \"number_of_predictions\": [\n                    {\n                        \"type\": \"SequenceLength\"\n                    }\n                ]\n            },\n            \"aggregation_mode\": {\n                \"predicted_classes\": [\n                    \"distinct\",\n                    \"count_distinct\",\n                    \"values_counts\"\n                ],\n                \"number_of_predictions\": [\n                    \"min\",\n                    \"max\",\n                    \"sum\"\n                ]\n            },\n            \"interval\": 6,\n            \"interval_unit\": \"runs\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"aggregation_results\",\n            \"selector\": \"$steps.data_aggregation.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/data_analytics_in_workflows/#saving-workflow-results-into-file","title":"Saving Workflow results into file","text":"<p>This Workflow was created to achieve few ends:</p> <ul> <li> <p>getting predictions from object detection model and returning them to the caller</p> </li> <li> <p>persisting the predictions - each one in separate JSON file</p> </li> <li> <p>aggregating the predictions data - producing report on each 6th input image</p> </li> <li> <p>saving the results in CSV file, appending rows until file size is exceeded </p> </li> </ul> <p>Run on video to produce meaningful results</p> <p>This workflow will not work using the docs preview. You must run it on video file. Copy the template into your Roboflow app, start <code>inference</code> server and use video preview  to get the results.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"target_directory\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\"\n        },\n        {\n            \"type\": \"roboflow_core/expression@v1\",\n            \"name\": \"json_formatter\",\n            \"data\": {\n                \"predictions\": \"$steps.model.predictions\"\n            },\n            \"switch\": {\n                \"type\": \"CasesDefinition\",\n                \"cases\": [],\n                \"default\": {\n                    \"type\": \"DynamicCaseResult\",\n                    \"parameter_name\": \"predictions\",\n                    \"operations\": [\n                        {\n                            \"type\": \"DetectionsToDictionary\"\n                        },\n                        {\n                            \"type\": \"ConvertDictionaryToJSON\"\n                        }\n                    ]\n                }\n            }\n        },\n        {\n            \"type\": \"roboflow_core/local_file_sink@v1\",\n            \"name\": \"predictions_sink\",\n            \"content\": \"$steps.json_formatter.output\",\n            \"file_type\": \"json\",\n            \"output_mode\": \"separate_files\",\n            \"target_directory\": \"$inputs.target_directory\",\n            \"file_name_prefix\": \"prediction\"\n        },\n        {\n            \"type\": \"roboflow_core/data_aggregator@v1\",\n            \"name\": \"data_aggregation\",\n            \"data\": {\n                \"predicted_classes\": \"$steps.model.predictions\",\n                \"number_of_predictions\": \"$steps.model.predictions\"\n            },\n            \"data_operations\": {\n                \"predicted_classes\": [\n                    {\n                        \"type\": \"DetectionsPropertyExtract\",\n                        \"property_name\": \"class_name\"\n                    }\n                ],\n                \"number_of_predictions\": [\n                    {\n                        \"type\": \"SequenceLength\"\n                    }\n                ]\n            },\n            \"aggregation_mode\": {\n                \"predicted_classes\": [\n                    \"count_distinct\"\n                ],\n                \"number_of_predictions\": [\n                    \"min\",\n                    \"max\",\n                    \"sum\"\n                ]\n            },\n            \"interval\": 6,\n            \"interval_unit\": \"runs\"\n        },\n        {\n            \"type\": \"roboflow_core/csv_formatter@v1\",\n            \"name\": \"csv_formatter\",\n            \"columns_data\": {\n                \"number_of_distinct_classes\": \"$steps.data_aggregation.predicted_classes_count_distinct\",\n                \"min_number_of_bounding_boxes\": \"$steps.data_aggregation.number_of_predictions_min\",\n                \"max_number_of_bounding_boxes\": \"$steps.data_aggregation.number_of_predictions_max\",\n                \"total_number_of_bounding_boxes\": \"$steps.data_aggregation.number_of_predictions_sum\"\n            }\n        },\n        {\n            \"type\": \"roboflow_core/local_file_sink@v1\",\n            \"name\": \"reports_sink\",\n            \"content\": \"$steps.csv_formatter.csv_content\",\n            \"file_type\": \"csv\",\n            \"output_mode\": \"append_log\",\n            \"target_directory\": \"$inputs.target_directory\",\n            \"file_name_prefix\": \"aggregation_report\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.model.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_enhanced_by_roboflow_platform/","title":"Example Workflows - Workflows enhanced by Roboflow Platform","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_enhanced_by_roboflow_platform/#data-collection-for-active-learning","title":"Data Collection for Active Learning","text":"<p>This example showcases how to stack models on top of each other - in this particular case, we detect objects using object detection models, requesting only \"dogs\" bounding boxes in the output of prediction. Additionally, we register cropped images in Roboflow dataset.</p> <p>Thanks to this setup, we are able to collect production data and continuously train better models over time.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"data_percentage\",\n            \"default_value\": 50.0\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"persist_predictions\",\n            \"default_value\": true\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"tag\",\n            \"default_value\": \"my_tag\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"disable_sink\",\n            \"default_value\": false\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"fire_and_forget\",\n            \"default_value\": true\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"labeling_batch_prefix\",\n            \"default_value\": \"some\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/dynamic_crop@v1\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_classification_model@v1\",\n            \"name\": \"breds_classification\",\n            \"image\": \"$steps.cropping.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_dataset_upload@v2\",\n            \"name\": \"data_collection\",\n            \"images\": \"$steps.cropping.crops\",\n            \"predictions\": \"$steps.breds_classification.predictions\",\n            \"target_project\": \"my_project\",\n            \"usage_quota_name\": \"my_quota\",\n            \"data_percentage\": \"$inputs.data_percentage\",\n            \"persist_predictions\": \"$inputs.persist_predictions\",\n            \"minutely_usage_limit\": 10,\n            \"hourly_usage_limit\": 100,\n            \"daily_usage_limit\": 1000,\n            \"max_image_size\": [\n                100,\n                200\n            ],\n            \"compression_level\": 85,\n            \"registration_tags\": [\n                \"a\",\n                \"b\",\n                \"$inputs.tag\"\n            ],\n            \"disable_sink\": \"$inputs.disable_sink\",\n            \"fire_and_forget\": \"$inputs.fire_and_forget\",\n            \"labeling_batch_prefix\": \"$inputs.labeling_batch_prefix\",\n            \"labeling_batches_recreation_frequency\": \"never\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.breds_classification.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"registration_message\",\n            \"selector\": \"$steps.data_collection.message\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_for_ocr/","title":"Example Workflows - Workflows for OCR","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_for_ocr/#workflow-with-doctr-model","title":"Workflow with DocTR model","text":"<p>This example showcases quite sophisticated workflows usage scenario that assume the following:</p> <ul> <li> <p>we have generic object detection model capable of recognising cars</p> </li> <li> <p>we have specialised object detection model trained to detect license plates in the images depicting single car only</p> </li> <li> <p>we have generic OCR model capable of recognising lines of texts from images</p> </li> </ul> <p>Our goal is to read license plates of every car we detect in the picture. We can achieve that goal with  workflow from this example. In the definition we can see that generic object detection model is applied first,  to make the job easier for the secondary (plates detection) model we enlarge bounding boxes, slightly  offsetting its dimensions with Detections Offset block - later we apply cropping to be able to run license plate detection for every detected car instance (increasing the depth of the batch). Once secondary model runs and we have bounding boxes for license plates - we crop previously cropped cars images to extract plates. Once this is done, plates crops are passed to OCR step which turns images of plates into text.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"car\"\n            ]\n        },\n        {\n            \"type\": \"DetectionOffset\",\n            \"name\": \"offset\",\n            \"predictions\": \"$steps.detection.predictions\",\n            \"image_metadata\": \"$steps.detection.image\",\n            \"prediction_type\": \"$steps.detection.prediction_type\",\n            \"offset_width\": 10,\n            \"offset_height\": 10\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cars_crops\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.offset.predictions\"\n        },\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"plates_detection\",\n            \"image\": \"$steps.cars_crops.crops\",\n            \"model_id\": \"vehicle-registration-plates-trudk/2\"\n        },\n        {\n            \"type\": \"DetectionOffset\",\n            \"name\": \"plates_offset\",\n            \"predictions\": \"$steps.plates_detection.predictions\",\n            \"image_metadata\": \"$steps.plates_detection.image\",\n            \"prediction_type\": \"$steps.plates_detection.prediction_type\",\n            \"offset_width\": 50,\n            \"offset_height\": 50\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"plates_crops\",\n            \"image\": \"$steps.cars_crops.crops\",\n            \"predictions\": \"$steps.plates_offset.predictions\"\n        },\n        {\n            \"type\": \"OCRModel\",\n            \"name\": \"ocr\",\n            \"image\": \"$steps.plates_crops.crops\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"cars_crops\",\n            \"selector\": \"$steps.cars_crops.crops\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"plates_crops\",\n            \"selector\": \"$steps.plates_crops.crops\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"plates_ocr\",\n            \"selector\": \"$steps.ocr.result\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_for_ocr/#google-vision-ocr","title":"Google Vision OCR","text":"<p>In this example, Google Vision OCR is used to extract text from input image. Additionally, example presents how to combine structured output of Google API with visualisation blocks.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_vision_ocr@v1\",\n            \"name\": \"google_vision_ocr\",\n            \"image\": \"$inputs.image\",\n            \"ocr_type\": \"text_detection\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n            \"name\": \"bounding_box_visualization\",\n            \"predictions\": \"$steps.google_vision_ocr.predictions\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/label_visualization@v1\",\n            \"name\": \"label_visualization\",\n            \"predictions\": \"$steps.google_vision_ocr.predictions\",\n            \"image\": \"$steps.bounding_box_visualization.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"extracted_text\",\n            \"selector\": \"$steps.google_vision_ocr.text\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"text_detections\",\n            \"selector\": \"$steps.google_vision_ocr.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"text_visualised\",\n            \"selector\": \"$steps.label_visualization.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_for_ocr/#workflow-with-model-detecting-individual-characters-and-text-stitching","title":"Workflow with model detecting individual characters and text stitching","text":"<p>This workflow extracts and organizes text from an image using OCR. It begins by analyzing the image with detection  model to detect individual characters or words and their positions. </p> <p>Then, it groups nearby text into lines based on a specified <code>tolerance</code> for spacing and arranges them in  reading order (<code>left-to-right</code>). </p> <p>The final output is a JSON field containing the structured text in readable, logical order, accurately reflecting  the layout of the original image.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"ocr-oy9a7/1\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"tolerance\",\n            \"default_value\": 10\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.4\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"ocr_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"roboflow_core/stitch_ocr_detections@v1\",\n            \"name\": \"detections_stitch\",\n            \"predictions\": \"$steps.ocr_detection.predictions\",\n            \"reading_direction\": \"left_to_right\",\n            \"tolerance\": \"$inputs.tolerance\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"ocr_text\",\n            \"selector\": \"$steps.detections_stitch.ocr_text\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_business_logic/","title":"Example Workflows - Workflows with business logic","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_business_logic/#workflow-with-extraction-of-classes-for-detections-1","title":"Workflow with extraction of classes for detections (1)","text":"<p>In practical use-cases you may find the need to inject pieces of business logic inside  your Workflow, such that it is easier to integrate with app created in Workflows ecosystem.</p> <p>Translation of model predictions into domain-specific language of your business is possible  with specialised blocks that let you parametrise such programming constructs  as switch-case statements.</p> <p>In this example, our goal is to:</p> <ul> <li> <p>tell how many objects are detected</p> </li> <li> <p>verify that the picture presents exactly two dogs</p> </li> </ul> <p>To achieve that goal, we run generic object detection model as first step, then we use special block called Property Definition that is capable of executing various operations to transform input data into desired output. We have two such blocks:</p> <ul> <li> <p><code>instances_counter</code> which takes object detection predictions and apply operation to extract sequence length -  effectively calculating number of instances of objects that were predicted</p> </li> <li> <p><code>property_extraction</code> which extracts class names from all detected bounding boxes</p> </li> </ul> <p><code>instances_counter</code> basically completes first goal of the workflow, but to satisfy the second one we need to  build evaluation logic that will tell \"PASS\" / \"FAIL\" based on comparison of extracted class names with  reference parameter (provided via Workflow input <code>$inputs.reference</code>). We can use Expression block to achieve  that goal - building custom case statements (checking if class names being list of classes  extracted from object detection prediction matches reference passed in the input).</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"reference\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"PropertyDefinition\",\n            \"name\": \"property_extraction\",\n            \"data\": \"$steps.general_detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsPropertyExtract\",\n                    \"property_name\": \"class_name\"\n                }\n            ]\n        },\n        {\n            \"type\": \"PropertyDefinition\",\n            \"name\": \"instances_counter\",\n            \"data\": \"$steps.general_detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"SequenceLength\"\n                }\n            ]\n        },\n        {\n            \"type\": \"Expression\",\n            \"name\": \"expression\",\n            \"data\": {\n                \"class_names\": \"$steps.property_extraction.output\",\n                \"reference\": \"$inputs.reference\"\n            },\n            \"switch\": {\n                \"type\": \"CasesDefinition\",\n                \"cases\": [\n                    {\n                        \"type\": \"CaseDefinition\",\n                        \"condition\": {\n                            \"type\": \"StatementGroup\",\n                            \"statements\": [\n                                {\n                                    \"type\": \"BinaryStatement\",\n                                    \"left_operand\": {\n                                        \"type\": \"DynamicOperand\",\n                                        \"operand_name\": \"class_names\"\n                                    },\n                                    \"comparator\": {\n                                        \"type\": \"==\"\n                                    },\n                                    \"right_operand\": {\n                                        \"type\": \"DynamicOperand\",\n                                        \"operand_name\": \"reference\"\n                                    }\n                                }\n                            ]\n                        },\n                        \"result\": {\n                            \"type\": \"StaticCaseResult\",\n                            \"value\": \"PASS\"\n                        }\n                    }\n                ],\n                \"default\": {\n                    \"type\": \"StaticCaseResult\",\n                    \"value\": \"FAIL\"\n                }\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"detected_classes\",\n            \"selector\": \"$steps.property_extraction.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"number_of_detected_boxes\",\n            \"selector\": \"$steps.instances_counter.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"verdict\",\n            \"selector\": \"$steps.expression.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_business_logic/#workflow-with-extraction-of-classes-for-detections-2","title":"Workflow with extraction of classes for detections (2)","text":"<p>In practical use-cases you may find the need to inject pieces of business logic inside  your Workflow, such that it is easier to integrate with app created in Workflows ecosystem.</p> <p>Translation of model predictions into domain-specific language of your business is possible  with specialised blocks that let you parametrise such programming constructs  as switch-case statements.</p> <p>In this example, our goal is to:</p> <ul> <li> <p>run generic object detection model to find instances of dogs</p> </li> <li> <p>crop dogs detection</p> </li> <li> <p>run specialised dogs breed classifier to assign granular label for each dog</p> </li> <li> <p>compare predicted dogs breeds to verify if detected labels matches exactly reverence value passed in input.</p> </li> </ul> <p>This example is quite complex as it requires quite deep understanding of Workflows ecosystem. Let's start from the beginning - we run object detection model, crop its detections according to dogs class to perform  classification. This is quite typical for workflows (you may find such pattern in remaining examples). </p> <p>The complexity increases when we try to handle classification output. We need to have a list of classes for each input image, but for now we have complex objects with all classification predictions details provided by <code>breds_classification</code> step - what is more - we have batch of such predictions for each input image (as we created dogs crops based on object detection model predictions). To solve the  problem, at first we apply Property Definition step taking classifier predictions and turning them into strings representing predicted classes. We still have batch of class names at dimensionality level 2,  which needs to be brought into dimensionality level 1 to make a single comparison against reference  value for each input image. To achieve that effect we use Dimension Collapse block which does nothing else but grabs the batch of classes and turns it into list of classes at dimensionality level 1 - one  list for each input image.</p> <p>That would solve our problems, apart from one nuance that must be taken into account. First-stage model is not guaranteed to detect any dogs - and if that happens we do not execute cropping and further  processing for that image, leaving all outputs derived from downstream computations <code>None</code> which is suboptimal. To compensate for that, we may use First Non Empty Or Default block which will take  <code>outputs_concatenation</code> step output and replace missing values with empty list (as effectively this is  equivalent of not detecting any dog).</p> <p>Such prepared output of <code>empty_values_replacement</code> step may be now plugged into Expression block,  performing switch-case like logic to deduce if breeds of detected dogs match with reference value  passed to workflow execution.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"reference\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"ClassificationModel\",\n            \"name\": \"breds_classification\",\n            \"image\": \"$steps.cropping.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\"\n        },\n        {\n            \"type\": \"PropertyDefinition\",\n            \"name\": \"property_extraction\",\n            \"data\": \"$steps.breds_classification.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ]\n        },\n        {\n            \"type\": \"DimensionCollapse\",\n            \"name\": \"outputs_concatenation\",\n            \"data\": \"$steps.property_extraction.output\"\n        },\n        {\n            \"type\": \"FirstNonEmptyOrDefault\",\n            \"name\": \"empty_values_replacement\",\n            \"data\": [\n                \"$steps.outputs_concatenation.output\"\n            ],\n            \"default\": []\n        },\n        {\n            \"type\": \"Expression\",\n            \"name\": \"expression\",\n            \"data\": {\n                \"detected_classes\": \"$steps.empty_values_replacement.output\",\n                \"reference\": \"$inputs.reference\"\n            },\n            \"switch\": {\n                \"type\": \"CasesDefinition\",\n                \"cases\": [\n                    {\n                        \"type\": \"CaseDefinition\",\n                        \"condition\": {\n                            \"type\": \"StatementGroup\",\n                            \"statements\": [\n                                {\n                                    \"type\": \"BinaryStatement\",\n                                    \"left_operand\": {\n                                        \"type\": \"DynamicOperand\",\n                                        \"operand_name\": \"detected_classes\"\n                                    },\n                                    \"comparator\": {\n                                        \"type\": \"==\"\n                                    },\n                                    \"right_operand\": {\n                                        \"type\": \"DynamicOperand\",\n                                        \"operand_name\": \"reference\"\n                                    }\n                                }\n                            ]\n                        },\n                        \"result\": {\n                            \"type\": \"StaticCaseResult\",\n                            \"value\": \"PASS\"\n                        }\n                    }\n                ],\n                \"default\": {\n                    \"type\": \"StaticCaseResult\",\n                    \"value\": \"FAIL\"\n                }\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"detected_classes\",\n            \"selector\": \"$steps.property_extraction.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"wrapped_classes\",\n            \"selector\": \"$steps.empty_values_replacement.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"verdict\",\n            \"selector\": \"$steps.expression.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/","title":"Example Workflows - Workflows with classical Computer Vision methods","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-generating-camera-focus-measure","title":"Workflow generating camera focus measure","text":"<p>In this example, we demonstrate how to evaluate camera focus using a specific block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/camera_focus@v1\",\n            \"name\": \"camera_focus\",\n            \"image\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"camera_focus_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.camera_focus.image\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"camera_focus_measure\",\n            \"selector\": \"$steps.camera_focus.focus_measure\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-detecting-contours","title":"Workflow detecting contours","text":"<p>In this example we show how classical contour detection works in cooperation with blocks performing its pre-processing (conversion to gray and blur).</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/convert_grayscale@v1\",\n            \"name\": \"image_convert_grayscale\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/image_blur@v1\",\n            \"name\": \"image_blur\",\n            \"image\": \"$steps.image_convert_grayscale.image\"\n        },\n        {\n            \"type\": \"roboflow_core/threshold@v1\",\n            \"name\": \"image_threshold\",\n            \"image\": \"$steps.image_blur.image\",\n            \"thresh_value\": 200,\n            \"threshold_type\": \"binary_inv\"\n        },\n        {\n            \"type\": \"roboflow_core/contours_detection@v1\",\n            \"name\": \"image_contours\",\n            \"image\": \"$steps.image_threshold.image\",\n            \"raw_image\": \"$inputs.image\",\n            \"line_thickness\": 5\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"number_contours\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_contours.number_contours\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"contour_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_contours.image\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"contours\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_contours.contours\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"grayscale_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_convert_grayscale.image\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"blurred_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_blur.image\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"thresholded_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.image_threshold.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-calculating-pixels-with-dominant-color","title":"Workflow calculating pixels with dominant color","text":"<p>This example shows how Dominant Color block and Pixel Color Count block can be used together.</p> <p>First, dominant color gets detected and then number of pixels with that color is calculated.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/dominant_color@v1\",\n            \"name\": \"dominant_color\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/pixel_color_count@v1\",\n            \"name\": \"pixelation\",\n            \"image\": \"$inputs.image\",\n            \"target_color\": \"$steps.dominant_color.rgb_color\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"matching_pixels_count\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.pixelation.matching_pixels_count\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-calculating-dominant-color","title":"Workflow calculating dominant color","text":"<p>This example shows how Dominant Color block can be used against input image.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/dominant_color@v1\",\n            \"name\": \"dominant_color\",\n            \"image\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"color\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.dominant_color.rgb_color\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-resizing-the-input-image","title":"Workflow resizing the input image","text":"<p>This example shows how the Image Preprocessing block can be used to resize an input image.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/image_preprocessing@v1\",\n            \"name\": \"resize_image\",\n            \"image\": \"$inputs.image\",\n            \"task_type\": \"resize\",\n            \"width\": 1000,\n            \"height\": 800\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"resized_image\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.resize_image.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#sift-in-workflows","title":"SIFT in Workflows","text":"<p>In this example we check how SIFT-based pattern matching works in cooperation with expression block.</p> <p>The Workflow first calculates SIFT features for input image and reference template,  then image features are compared to template features. At the end - switch-case  statement is built with Expression block to produce output. </p> <p>Important detail: If there is empty output from SIFT descriptors calculation for (which is a valid output if no feature gets recognised) the sift comparison won't  execute - hence First Non Empty Or Default block is used to provide default outcome  for <code>images_match</code> output of SIFT comparison block.</p> <p>Please note that a single image can be passed as template, and batch of images are passed as images to look for template. This workflow does also validate Execution Engine capabilities to broadcast batch-oriented inputs properly.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"template\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/sift@v1\",\n            \"name\": \"image_sift\",\n            \"image\": \"$inputs.image\"\n        },\n        {\n            \"type\": \"roboflow_core/sift@v1\",\n            \"name\": \"template_sift\",\n            \"image\": \"$inputs.template\"\n        },\n        {\n            \"type\": \"roboflow_core/sift_comparison@v1\",\n            \"name\": \"sift_comparison\",\n            \"descriptor_1\": \"$steps.image_sift.descriptors\",\n            \"descriptor_2\": \"$steps.template_sift.descriptors\",\n            \"good_matches_threshold\": 50\n        },\n        {\n            \"type\": \"roboflow_core/first_non_empty_or_default@v1\",\n            \"name\": \"empty_values_replacement\",\n            \"data\": [\n                \"$steps.sift_comparison.images_match\"\n            ],\n            \"default\": false\n        },\n        {\n            \"type\": \"roboflow_core/expression@v1\",\n            \"name\": \"is_match_expression\",\n            \"data\": {\n                \"images_match\": \"$steps.empty_values_replacement.output\"\n            },\n            \"switch\": {\n                \"type\": \"CasesDefinition\",\n                \"cases\": [\n                    {\n                        \"type\": \"CaseDefinition\",\n                        \"condition\": {\n                            \"type\": \"StatementGroup\",\n                            \"statements\": [\n                                {\n                                    \"type\": \"UnaryStatement\",\n                                    \"operand\": {\n                                        \"type\": \"DynamicOperand\",\n                                        \"operand_name\": \"images_match\"\n                                    },\n                                    \"operator\": {\n                                        \"type\": \"(Boolean) is True\"\n                                    }\n                                }\n                            ]\n                        },\n                        \"result\": {\n                            \"type\": \"StaticCaseResult\",\n                            \"value\": \"MATCH\"\n                        }\n                    }\n                ],\n                \"default\": {\n                    \"type\": \"StaticCaseResult\",\n                    \"value\": \"NO MATCH\"\n                }\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.is_match_expression.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_classical_computer_vision_methods/#workflow-stitching-images","title":"Workflow stitching images","text":"<p>In this example two images of the same scene are stitched together. Given enough shared details order of the images does not influence final result.</p> <p>Please note that images need to have enough common details for the algorithm to stitch them properly.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image1\"\n        },\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image2\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"count_of_best_matches_per_query_descriptor\"\n        },\n        {\n            \"type\": \"InferenceParameter\",\n            \"name\": \"max_allowed_reprojection_error\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/stitch_images@v1\",\n            \"name\": \"stitch_images\",\n            \"image1\": \"$inputs.image1\",\n            \"image2\": \"$inputs.image2\",\n            \"count_of_best_matches_per_query_descriptor\": \"$inputs.count_of_best_matches_per_query_descriptor\",\n            \"max_allowed_reprojection_error\": \"$inputs.max_allowed_reprojection_error\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"stitched_image\",\n            \"selector\": \"$steps.stitch_images.stitched_image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_data_transformations/","title":"Example Workflows - Workflows with data transformations","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_data_transformations/#workflow-with-detections-class-remapping","title":"Workflow with detections class remapping","text":"<p>This workflow presents how to use Detections Transformation block that is going to  change the name of the following classes: <code>apple</code>, <code>banana</code> into <code>fruit</code>.</p> <p>In this example, we use non-strict mapping, causing new class <code>fruit</code> to be added to pool of classes - you can see that if <code>banana</code> or <code>apple</code> is detected, the class name changes to <code>fruit</code> and class id is 1024.</p> <p>You can test the execution submitting image like  this.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.4\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"model\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"DetectionsTransformation\",\n            \"name\": \"class_rename\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsRename\",\n                    \"strict\": false,\n                    \"class_map\": {\n                        \"apple\": \"fruit\",\n                        \"banana\": \"fruit\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"original_predictions\",\n            \"selector\": \"$steps.model.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"renamed_predictions\",\n            \"selector\": \"$steps.class_rename.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_data_transformations/#workflow-with-detections-filtering","title":"Workflow with detections filtering","text":"<p>This example presents how to use Detections Transformation block to build workflow that is going to filter predictions based on:</p> <ul> <li> <p>predicted classes</p> </li> <li> <p>size of predicted bounding box relative to size of input image</p> </li> </ul> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.3\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"DetectionsTransformation\",\n            \"name\": \"filtering\",\n            \"predictions\": \"$steps.detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsFilter\",\n                    \"filter_operation\": {\n                        \"type\": \"StatementGroup\",\n                        \"operator\": \"and\",\n                        \"statements\": [\n                            {\n                                \"type\": \"BinaryStatement\",\n                                \"left_operand\": {\n                                    \"type\": \"DynamicOperand\",\n                                    \"operations\": [\n                                        {\n                                            \"type\": \"ExtractDetectionProperty\",\n                                            \"property_name\": \"class_name\"\n                                        }\n                                    ]\n                                },\n                                \"comparator\": {\n                                    \"type\": \"in (Sequence)\"\n                                },\n                                \"right_operand\": {\n                                    \"type\": \"DynamicOperand\",\n                                    \"operand_name\": \"classes\"\n                                }\n                            },\n                            {\n                                \"type\": \"BinaryStatement\",\n                                \"left_operand\": {\n                                    \"type\": \"DynamicOperand\",\n                                    \"operations\": [\n                                        {\n                                            \"type\": \"ExtractDetectionProperty\",\n                                            \"property_name\": \"size\"\n                                        }\n                                    ]\n                                },\n                                \"comparator\": {\n                                    \"type\": \"(Number) &gt;=\"\n                                },\n                                \"right_operand\": {\n                                    \"type\": \"DynamicOperand\",\n                                    \"operand_name\": \"image\",\n                                    \"operations\": [\n                                        {\n                                            \"type\": \"ExtractImageProperty\",\n                                            \"property_name\": \"size\"\n                                        },\n                                        {\n                                            \"type\": \"Multiply\",\n                                            \"other\": 0.02\n                                        }\n                                    ]\n                                }\n                            }\n                        ]\n                    }\n                }\n            ],\n            \"operations_parameters\": {\n                \"image\": \"$inputs.image\",\n                \"classes\": \"$inputs.classes\"\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.filtering.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_data_transformations/#instance-segmentation-results-with-background-subtracted","title":"Instance Segmentation results with background subtracted","text":"<p>This example showcases how to extract all instances detected by instance segmentation model as separate crops without background.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-seg-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.4\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_instance_segmentation_model@v1\",\n            \"name\": \"segmentation\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"roboflow_core/dynamic_crop@v1\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.segmentation.predictions\",\n            \"mask_opacity\": 1.0\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"crops\",\n            \"selector\": \"$steps.cropping.crops\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.segmentation.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_data_transformations/#workflow-with-detections-sorting","title":"Workflow with detections sorting","text":"<p>This workflow presents how to use Detections Transformation block that is going to  align predictions from object detection model such that results are sorted  ascending regarding confidence.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.75\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"DetectionsTransformation\",\n            \"name\": \"sorting\",\n            \"predictions\": \"$steps.detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"SortDetections\",\n                    \"mode\": \"confidence\",\n                    \"ascending\": true\n                }\n            ],\n            \"operations_parameters\": {\n                \"image\": \"$inputs.image\",\n                \"classes\": \"$inputs.classes\"\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.sorting.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_dynamic_python_blocks/","title":"Example Workflows - Workflows with dynamic Python Blocks","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_dynamic_python_blocks/#workflow-measuring-bounding-boxes-overlap","title":"Workflow measuring bounding boxes overlap","text":"<p>In real world use-cases you may not be able to find all pieces of functionalities required to complete  your workflow within existing blocks. </p> <p>In such cases you may create piece of python code and put it in workflow as a dynamic block. Specifically  here, we define two dynamic blocks:</p> <ul> <li> <p><code>OverlapMeasurement</code> which will accept object detection predictions and provide for boxes  of specific class matrix of overlap with all boxes of another class.</p> </li> <li> <p><code>MaximumOverlap</code> that will take overlap matrix produced by <code>OverlapMeasurement</code> and calculate maximum overlap.</p> </li> </ul> <p>Dynamic block may be used to create steps, exactly as if those blocks were standard Workflow blocks  existing in ecosystem. The workflow presented in the example predicts from object detection model and  calculates overlap matrix. Later, only if more than one object is detected, maximum overlap is calculated.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"dynamic_blocks_definitions\": [\n        {\n            \"type\": \"DynamicBlockDefinition\",\n            \"manifest\": {\n                \"type\": \"ManifestDescription\",\n                \"block_type\": \"OverlapMeasurement\",\n                \"inputs\": {\n                    \"predictions\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"selector_types\": [\n                            \"step_output\"\n                        ]\n                    },\n                    \"class_x\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"value_types\": [\n                            \"string\"\n                        ]\n                    },\n                    \"class_y\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"value_types\": [\n                            \"string\"\n                        ]\n                    }\n                },\n                \"outputs\": {\n                    \"overlap\": {\n                        \"type\": \"DynamicOutputDefinition\",\n                        \"kind\": []\n                    }\n                }\n            },\n            \"code\": {\n                \"type\": \"PythonCode\",\n                \"run_function_code\": \"\\ndef run(self, predictions: sv.Detections, class_x: str, class_y: str) -&gt; BlockResult:\\n    bboxes_class_x = predictions[predictions.data[\\\"class_name\\\"] == class_x]\\n    bboxes_class_y = predictions[predictions.data[\\\"class_name\\\"] == class_y]\\n    overlap = []\\n    for bbox_x in bboxes_class_x:\\n        bbox_x_coords = bbox_x[0]\\n        bbox_overlaps = []\\n        for bbox_y in bboxes_class_y:\\n            if bbox_y[-1][\\\"detection_id\\\"] == bbox_x[-1][\\\"detection_id\\\"]:\\n                continue\\n            bbox_y_coords = bbox_y[0]\\n            x_min = max(bbox_x_coords[0], bbox_y_coords[0])\\n            y_min = max(bbox_x_coords[1], bbox_y_coords[1])\\n            x_max = min(bbox_x_coords[2], bbox_y_coords[2])\\n            y_max = min(bbox_x_coords[3], bbox_y_coords[3])\\n            # compute the area of intersection rectangle\\n            intersection_area = max(0, x_max - x_min + 1) * max(0, y_max - y_min + 1)\\n            box_x_area = (bbox_x_coords[2] - bbox_x_coords[0] + 1) * (bbox_x_coords[3] - bbox_x_coords[1] + 1)\\n            local_overlap = intersection_area / (box_x_area + 1e-5)\\n            bbox_overlaps.append(local_overlap)\\n        overlap.append(bbox_overlaps)\\n    return  {\\\"overlap\\\": overlap}\\n\"\n            }\n        },\n        {\n            \"type\": \"DynamicBlockDefinition\",\n            \"manifest\": {\n                \"type\": \"ManifestDescription\",\n                \"block_type\": \"MaximumOverlap\",\n                \"inputs\": {\n                    \"overlaps\": {\n                        \"type\": \"DynamicInputDefinition\",\n                        \"selector_types\": [\n                            \"step_output\"\n                        ]\n                    }\n                },\n                \"outputs\": {\n                    \"max_value\": {\n                        \"type\": \"DynamicOutputDefinition\",\n                        \"kind\": []\n                    }\n                }\n            },\n            \"code\": {\n                \"type\": \"PythonCode\",\n                \"run_function_code\": \"\\ndef run(self, overlaps: List[List[float]]) -&gt; BlockResult:\\n    max_value = -1\\n    for overlap in overlaps:\\n        for overlap_value in overlap:\\n            if not max_value:\\n                max_value = overlap_value\\n            else:\\n                max_value = max(max_value, overlap_value)\\n    return {\\\"max_value\\\": max_value}\\n\"\n            }\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"model\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"OverlapMeasurement\",\n            \"name\": \"overlap_measurement\",\n            \"predictions\": \"$steps.model.predictions\",\n            \"class_x\": \"dog\",\n            \"class_y\": \"dog\"\n        },\n        {\n            \"type\": \"ContinueIf\",\n            \"name\": \"continue_if\",\n            \"condition_statement\": {\n                \"type\": \"StatementGroup\",\n                \"statements\": [\n                    {\n                        \"type\": \"BinaryStatement\",\n                        \"left_operand\": {\n                            \"type\": \"DynamicOperand\",\n                            \"operand_name\": \"overlaps\",\n                            \"operations\": [\n                                {\n                                    \"type\": \"SequenceLength\"\n                                }\n                            ]\n                        },\n                        \"comparator\": {\n                            \"type\": \"(Number) &gt;=\"\n                        },\n                        \"right_operand\": {\n                            \"type\": \"StaticOperand\",\n                            \"value\": 1\n                        }\n                    }\n                ]\n            },\n            \"evaluation_parameters\": {\n                \"overlaps\": \"$steps.overlap_measurement.overlap\"\n            },\n            \"next_steps\": [\n                \"$steps.maximum_overlap\"\n            ]\n        },\n        {\n            \"type\": \"MaximumOverlap\",\n            \"name\": \"maximum_overlap\",\n            \"overlaps\": \"$steps.overlap_measurement.overlap\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"overlaps\",\n            \"selector\": \"$steps.overlap_measurement.overlap\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"max_overlap\",\n            \"selector\": \"$steps.maximum_overlap.max_value\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_flow_control/","title":"Example Workflows - Workflows with flow control","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_flow_control/#workflow-with-if-statement-applied-on-nested-batches","title":"Workflow with if statement applied on nested batches","text":"<p>In this test scenario we verify if we can successfully apply conditional branching when data dimensionality increases. We first make detections on input images and perform crop increasing dimensionality to 2. Then we make another detections on cropped images and check if inside crop we only see one instance of class dog (very naive way of making sure that bboxes contain only single objects). Only if that condition is true, we run classification model - to classify dog breed.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"first_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"DetectionsTransformation\",\n            \"name\": \"enlarging_boxes\",\n            \"predictions\": \"$steps.first_detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsOffset\",\n                    \"offset_x\": 50,\n                    \"offset_y\": 50\n                }\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"first_crop\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.enlarging_boxes.predictions\"\n        },\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"second_detection\",\n            \"image\": \"$steps.first_crop.crops\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"ContinueIf\",\n            \"name\": \"continue_if\",\n            \"condition_statement\": {\n                \"type\": \"StatementGroup\",\n                \"statements\": [\n                    {\n                        \"type\": \"BinaryStatement\",\n                        \"left_operand\": {\n                            \"type\": \"DynamicOperand\",\n                            \"operand_name\": \"prediction\",\n                            \"operations\": [\n                                {\n                                    \"type\": \"SequenceLength\"\n                                }\n                            ]\n                        },\n                        \"comparator\": {\n                            \"type\": \"(Number) ==\"\n                        },\n                        \"right_operand\": {\n                            \"type\": \"StaticOperand\",\n                            \"value\": 1\n                        }\n                    }\n                ]\n            },\n            \"evaluation_parameters\": {\n                \"prediction\": \"$steps.second_detection.predictions\"\n            },\n            \"next_steps\": [\n                \"$steps.classification\"\n            ]\n        },\n        {\n            \"type\": \"ClassificationModel\",\n            \"name\": \"classification\",\n            \"image\": \"$steps.first_crop.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"dog_classification\",\n            \"selector\": \"$steps.classification.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_flow_control/#workflow-with-if-statement-applied-on-non-batch-oriented-input","title":"Workflow with if statement applied on non batch-oriented input","text":"<p>In this test scenario we show that we can use non-batch oriented conditioning (ContinueIf block).</p> <p>If statement is effectively applied on input parameter that would determine path of execution for all data passed in <code>image</code> input. When the value matches expectation - all dependent steps will be executed, otherwise only the independent ones.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"first_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"DetectionsTransformation\",\n            \"name\": \"enlarging_boxes\",\n            \"predictions\": \"$steps.first_detection.predictions\",\n            \"operations\": [\n                {\n                    \"type\": \"DetectionsOffset\",\n                    \"offset_x\": 50,\n                    \"offset_y\": 50\n                }\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"first_crop\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.enlarging_boxes.predictions\"\n        },\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"second_detection\",\n            \"image\": \"$steps.first_crop.crops\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"ContinueIf\",\n            \"name\": \"continue_if\",\n            \"condition_statement\": {\n                \"type\": \"StatementGroup\",\n                \"statements\": [\n                    {\n                        \"type\": \"BinaryStatement\",\n                        \"left_operand\": {\n                            \"type\": \"DynamicOperand\",\n                            \"operand_name\": \"prediction\",\n                            \"operations\": [\n                                {\n                                    \"type\": \"SequenceLength\"\n                                }\n                            ]\n                        },\n                        \"comparator\": {\n                            \"type\": \"(Number) ==\"\n                        },\n                        \"right_operand\": {\n                            \"type\": \"StaticOperand\",\n                            \"value\": 1\n                        }\n                    }\n                ]\n            },\n            \"evaluation_parameters\": {\n                \"prediction\": \"$steps.second_detection.predictions\"\n            },\n            \"next_steps\": [\n                \"$steps.classification\"\n            ]\n        },\n        {\n            \"type\": \"ClassificationModel\",\n            \"name\": \"classification\",\n            \"image\": \"$steps.first_crop.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"dog_classification\",\n            \"selector\": \"$steps.classification.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_foundation_models/","title":"Example Workflows - Workflows with foundation models","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_foundation_models/#workflow-with-segment-anything-2-model","title":"Workflow with Segment Anything 2 model","text":"<p>Meta AI introduced very capable segmentation model called SAM 2 which has capabilities of producing segmentation masks for instances of objects. </p> <p>EXAMPLE REQUIRES DEDICATED DEPLOYMENT and will not run in preview!</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"mask_threshold\",\n            \"default_value\": 0.0\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"version\",\n            \"default_value\": \"hiera_tiny\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/segment_anything@v1\",\n            \"name\": \"segment_anything\",\n            \"images\": \"$inputs.image\",\n            \"threshold\": \"$inputs.mask_threshold\",\n            \"version\": \"$inputs.version\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.segment_anything.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_multiple_models/","title":"Example Workflows - Workflows with multiple models","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_multiple_models/#workflow-detection-model-followed-by-classifier","title":"Workflow detection model followed by classifier","text":"<p>This example showcases how to stack models on top of each other - in this particular case, we detect objects using object detection models, requesting only \"dogs\" bounding boxes in the output of prediction. </p> <p>Based on the model predictions, we take each bounding box with dog and apply dynamic cropping to be able to run classification model for each and every instance of dog separately. Please note that for each inserted image we will have nested batch of crops (with size  dynamically determined in runtime, based on first model predictions) and for each crop we apply secondary model.</p> <p>Secondary model is supposed to make prediction from dogs breed classifier model  to assign detailed class for each dog instance.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"ClassificationModel\",\n            \"name\": \"breds_classification\",\n            \"image\": \"$steps.cropping.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.breds_classification.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_multiple_models/#workflow-with-classifier-providing-detailed-labels-for-detected-objects","title":"Workflow with classifier providing detailed labels for detected objects","text":"<p>This example illustrates how helpful Workflows could be when you have generic object detection model  (capable of detecting common classes - like dogs) and specific classifier (capable of providing granular  predictions for narrow high-level classes of objects - like dogs breed classifier). Having list of classifier predictions for each detected dog is not handy way of dealing with output -  as you kind of loose the information about location of specific dog. To avoid this problem, you may want to replace class labels of original bounding boxes (from the first model localising dogs) with classes predicted by classifier.</p> <p>In this example, we use Detections Classes Replacement block which is also interesting from the  perspective of difference of its inputs dimensionality levels. <code>object_detection_predictions</code> input has level 1 (there is one prediction with bboxes for each input image) and <code>classification_predictions</code> has level 2 (there are bunch of classification results for each input image). The block combines that two inputs and produces result at dimensionality level 1 - exactly the same as predictions from  object detection model.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"ClassificationModel\",\n            \"name\": \"breds_classification\",\n            \"image\": \"$steps.cropping.crops\",\n            \"model_id\": \"dog-breed-xpaq6/1\"\n        },\n        {\n            \"type\": \"DetectionsClassesReplacement\",\n            \"name\": \"classes_replacement\",\n            \"object_detection_predictions\": \"$steps.general_detection.predictions\",\n            \"classification_predictions\": \"$steps.breds_classification.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"original_predictions\",\n            \"selector\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions_with_replaced_classes\",\n            \"selector\": \"$steps.classes_replacement.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_multiple_models/#workflow-presenting-models-ensemble","title":"Workflow presenting models ensemble","text":"<p>This workflow presents how to combine predictions from multiple models running against the same  input image with the block called Detections Consensus. </p> <p>First, we run two object detections models steps and we combine their predictions. Fusion may be  performed in different scenarios based on Detections Consensus step configuration:</p> <ul> <li> <p>you may combine predictions from models detecting different objects and then require only single  model vote to add predicted bounding box to the output prediction</p> </li> <li> <p>you may combine predictions from models detecting the same objects and expect multiple positive  votes to accept bounding box to the output prediction - this way you may improve the quality of  predictions</p> </li> </ul> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_id\",\n            \"default_value\": \"yolov8n-640\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection_1\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": 0.3\n        },\n        {\n            \"type\": \"RoboflowObjectDetectionModel\",\n            \"name\": \"detection_2\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_id\",\n            \"confidence\": 0.83\n        },\n        {\n            \"type\": \"DetectionsConsensus\",\n            \"name\": \"consensus\",\n            \"predictions_batches\": [\n                \"$steps.detection_1.predictions\",\n                \"$steps.detection_2.predictions\"\n            ],\n            \"required_votes\": 2,\n            \"required_objects\": {\n                \"person\": 2\n            }\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.consensus.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_multiple_models/#comparison-of-detection-models-predictions","title":"Comparison of detection models predictions","text":"<p>This example showcases how to compare predictions from two different models using Workflows and  Model Comparison Visualization block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_1\",\n            \"default_value\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"model_2\",\n            \"default_value\": \"yolov8n-1280\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_1\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"model_1\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"$inputs.model_2\"\n        },\n        {\n            \"type\": \"roboflow_core/model_comparison_visualization@v1\",\n            \"name\": \"model_comparison_visualization\",\n            \"image\": \"$inputs.image\",\n            \"predictions_a\": \"$steps.model_1.predictions\",\n            \"predictions_b\": \"$steps.model.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_1_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_2_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model_1.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"visualization\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model_comparison_visualization.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/","title":"Example Workflows - Workflows with Visual Language Models","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#prompting-anthropic-claude-with-arbitrary-prompt","title":"Prompting Anthropic Claude with arbitrary prompt","text":"<p>In this example, Anthropic Claude model is prompted with arbitrary text from user</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"unconstrained\",\n            \"prompt\": \"Give me dominant color of the image\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.claude.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-ocr-model","title":"Using Anthropic Claude as OCR model","text":"<p>In this example, Anthropic Claude model is used as OCR system. User just points task type and do not need to provide any prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"ocr\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.claude.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-visual-question-answering-system","title":"Using Anthropic Claude as Visual Question Answering system","text":"<p>In this example, Anthropic Claude model is used as VQA system. User provides question via prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"visual-question-answering\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.claude.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-image-captioning-system","title":"Using Anthropic Claude as Image Captioning system","text":"<p>In this example, Anthropic Claude model is used as Image Captioning system.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"caption\",\n            \"api_key\": \"$inputs.api_key\",\n            \"temperature\": 1.0\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.claude.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-multi-class-classifier","title":"Using Anthropic Claude as multi-class classifier","text":"<p>In this example, Anthropic Claude model is used as classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v1</code> block which turns model output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.claude.output\",\n            \"classes\": \"$steps.claude.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"claude_result\",\n            \"selector\": \"$steps.claude.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"top_class\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-multi-label-classifier","title":"Using Anthropic Claude as multi-label classifier","text":"<p>In this example, Anthropic Claude model is used as multi-label classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v1</code> block which turns model output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"multi-label-classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.claude.output\",\n            \"classes\": \"$steps.claude.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-to-provide-structured-json","title":"Using Anthropic Claude to provide structured JSON","text":"<p>In this example, Anthropic Claude model is expected to provide structured output in JSON, which can later be parsed by dedicated <code>roboflow_core/json_parser@v1</code> block which transforms string into dictionary  and expose it's keys to other blocks for further processing. In this case, parsed output is transformed using <code>roboflow_core/property_definition@v1</code> block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"structured-answering\",\n            \"output_structure\": {\n                \"dogs_count\": \"count of dogs instances in the image\",\n                \"cats_count\": \"count of cats instances in the image\"\n            },\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/json_parser@v1\",\n            \"name\": \"parser\",\n            \"raw_json\": \"$steps.claude.output\",\n            \"expected_fields\": [\n                \"dogs_count\",\n                \"cats_count\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"property_definition\",\n            \"operations\": [\n                {\n                    \"type\": \"ToString\"\n                }\n            ],\n            \"data\": \"$steps.parser.dogs_count\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.property_definition.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-object-detection-model","title":"Using Anthropic Claude as object-detection model","text":"<p>In this example, Anthropic Claude model is expected to provide output, which can later be parsed by dedicated <code>roboflow_core/vlm_as_detector@v1</code> block which transforms string into <code>sv.Detections</code>,  which can later be used by other blocks processing object-detection predictions.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"object-detection\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_detector@v1\",\n            \"name\": \"parser\",\n            \"vlm_output\": \"$steps.claude.output\",\n            \"image\": \"$inputs.image\",\n            \"classes\": \"$steps.claude.classes\",\n            \"model_type\": \"anthropic-claude\",\n            \"task_type\": \"object-detection\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"claude_result\",\n            \"selector\": \"$steps.claude.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-anthropic-claude-as-secondary-classifier","title":"Using Anthropic Claude as secondary classifier","text":"<p>In this example, Anthropic Claude model is used as secondary classifier - first, YOLO model detects dogs, then for each dog we run classification with VLM and at the end we replace  detections classes to have bounding boxes with dogs breeds labels.</p> <p>Breeds that we classify: <code>russell-terrier</code>, <code>wirehaired-pointing-griffon</code>, <code>beagle</code></p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\",\n            \"default_value\": [\n                \"russell-terrier\",\n                \"wirehaired-pointing-griffon\",\n                \"beagle\"\n            ]\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/anthropic_claude@v1\",\n            \"name\": \"claude\",\n            \"images\": \"$steps.cropping.crops\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n            \"name\": \"parser\",\n            \"image\": \"$steps.cropping.crops\",\n            \"vlm_output\": \"$steps.claude.output\",\n            \"classes\": \"$steps.claude.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n            \"name\": \"classes_replacement\",\n            \"object_detection_predictions\": \"$steps.general_detection.predictions\",\n            \"classification_predictions\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.classes_replacement.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#florence-2-grounded-classification","title":"Florence 2 - grounded classification","text":"<p>THIS EXAMPLE CAN ONLY BE RUN LOCALLY OR USING DEDICATED DEPLOYMENT</p> <p>In this example, we use object detection model to find regions of interest in the  input image, which are later classified by Florence 2 model. With Workflows it is possible  to pass <code>grounding_detection</code> as an input for all of the tasks named <code>detection-grounded-*</code>.</p> <p>Grounding detection can either be input parameter or output of detection model. If the  latter is true, one should choose <code>grounding_selection_mode</code> - as Florence do only support  a single bounding box as grounding - when multiple detections can be provided, block will select one based on parameter.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"confidence\",\n            \"default_value\": 0.4\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"model_1\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"confidence\": \"$inputs.confidence\"\n        },\n        {\n            \"type\": \"roboflow_core/florence_2@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"detection-grounded-classification\",\n            \"grounding_detection\": \"$steps.model_1.predictions\",\n            \"grounding_selection_mode\": \"most-confident\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#florence-2-grounded-segmentation","title":"Florence 2 - grounded segmentation","text":"<p>THIS EXAMPLE CAN ONLY BE RUN LOCALLY OR USING DEDICATED DEPLOYMENT</p> <p>In this example, we use object detection model to find regions of interest in the  input image and run segmentation of selected region with Florence 2. With Workflows it is  possible to pass <code>grounding_detection</code> as an input for all of the tasks named  <code>detection-grounded-*</code>.</p> <p>Grounding detection can either be input parameter or output of detection model. If the  latter is true, one should choose <code>grounding_selection_mode</code> - as Florence do only support  a single bounding box as grounding - when multiple detections can be provided, block will select one based on parameter.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"model_1\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/florence_2@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"detection-grounded-instance-segmentation\",\n            \"grounding_detection\": \"$steps.model_1.predictions\",\n            \"grounding_selection_mode\": \"most-confident\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#florence-2-grounded-captioning","title":"Florence 2 - grounded captioning","text":"<p>THIS EXAMPLE CAN ONLY BE RUN LOCALLY OR USING DEDICATED DEPLOYMENT</p> <p>In this example, we use object detection model to find regions of interest in the  input image and run captioning of selected region with Florence 2. With Workflows it is  possible to pass <code>grounding_detection</code> as an input for all of the tasks named  <code>detection-grounded-*</code>.</p> <p>Grounding detection can either be input parameter or output of detection model. If the  latter is true, one should choose <code>grounding_selection_mode</code> - as Florence do only support  a single bounding box as grounding - when multiple detections can be provided, block will select one based on parameter.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"model_1\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/florence_2@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"detection-grounded-instance-segmentation\",\n            \"grounding_detection\": \"$steps.model_1.predictions\",\n            \"grounding_selection_mode\": \"most-confident\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#florence-2-object-detection","title":"Florence 2 - object detection","text":"<p>THIS EXAMPLE CAN ONLY BE RUN LOCALLY OR USING DEDICATED DEPLOYMENT</p> <p>In this example, we use Florence 2 as zero-shot object detection model, specifically  performing open-vocabulary detection. Input parameter <code>classes</code> can be used to provide list of objects that model should find. Beware that Florence 2 is prone to  seek for all of the classes provided in your list - so if you select class which is not visible in the image, you can expect either big bounding box covering whole image,  or multiple bounding boxes over one of detected instance, with auxiliary boxes providing not meaningful labels for all of the objects you specified in class list.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"InferenceImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"model_1\",\n            \"images\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\"\n        },\n        {\n            \"type\": \"roboflow_core/florence_2@v1\",\n            \"name\": \"model\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"detection-grounded-instance-segmentation\",\n            \"grounding_detection\": \"$steps.model_1.predictions\",\n            \"grounding_selection_mode\": \"most-confident\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"model_predictions\",\n            \"coordinates_system\": \"own\",\n            \"selector\": \"$steps.model.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#prompting-googles-gemini-with-arbitrary-prompt","title":"Prompting Google's Gemini with arbitrary prompt","text":"<p>In this example, Google's Gemini model is prompted with arbitrary text from user</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"unconstrained\",\n            \"prompt\": \"Give me dominant color of the image\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gemini.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-ocr-model","title":"Using Google's Gemini as OCR model","text":"<p>In this example, Google's Gemini model is used as OCR system. User just points task type and do not need to provide any prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"ocr\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gemini.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-visual-question-answering-system","title":"Using Google's Gemini as Visual Question Answering system","text":"<p>In this example, Google's Gemini model is used as VQA system. User provides question via prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"visual-question-answering\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gemini.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-image-captioning-system","title":"Using Google's Gemini as Image Captioning system","text":"<p>In this example, Google's Gemini model is used as Image Captioning system.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"caption\",\n            \"api_key\": \"$inputs.api_key\",\n            \"temperature\": 1.0\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gemini.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-multi-class-classifier","title":"Using Google's Gemini as multi-class classifier","text":"<p>In this example, Google's Gemini model is used as classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v1</code> block which turns model output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.gemini.output\",\n            \"classes\": \"$steps.gemini.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"gemini_result\",\n            \"selector\": \"$steps.gemini.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"top_class\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-multi-label-classifier","title":"Using Google's Gemini as multi-label classifier","text":"<p>In this example, Google's Gemini model is used as multi-label classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v1</code> block which turns model output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"multi-label-classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.gemini.output\",\n            \"classes\": \"$steps.gemini.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-to-provide-structured-json","title":"Using Google's Gemini to provide structured JSON","text":"<p>In this example, Google's Gemini model is expected to provide structured output in JSON, which can later be parsed by dedicated <code>roboflow_core/json_parser@v1</code> block which transforms string into dictionary  and expose it's keys to other blocks for further processing. In this case, parsed output is transformed using <code>roboflow_core/property_definition@v1</code> block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"structured-answering\",\n            \"output_structure\": {\n                \"dogs_count\": \"count of dogs instances in the image\",\n                \"cats_count\": \"count of cats instances in the image\"\n            },\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/json_parser@v1\",\n            \"name\": \"parser\",\n            \"raw_json\": \"$steps.gemini.output\",\n            \"expected_fields\": [\n                \"dogs_count\",\n                \"cats_count\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"property_definition\",\n            \"operations\": [\n                {\n                    \"type\": \"ToString\"\n                }\n            ],\n            \"data\": \"$steps.parser.dogs_count\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.property_definition.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-object-detection-model","title":"Using Google's Gemini as object-detection model","text":"<p>In this example, Google's Gemini model is expected to provide output, which can later be parsed by dedicated <code>roboflow_core/vlm_as_detector@v1</code> block which transforms string into <code>sv.Detections</code>,  which can later be used by other blocks processing object-detection predictions.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"object-detection\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_detector@v1\",\n            \"name\": \"parser\",\n            \"vlm_output\": \"$steps.gemini.output\",\n            \"image\": \"$inputs.image\",\n            \"classes\": \"$steps.gemini.classes\",\n            \"model_type\": \"google-gemini\",\n            \"task_type\": \"object-detection\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"gemini_result\",\n            \"selector\": \"$steps.gemini.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-googles-gemini-as-secondary-classifier","title":"Using Google's Gemini as secondary classifier","text":"<p>In this example, Google's Gemini model is used as secondary classifier - first, YOLO model detects dogs, then for each dog we run classification with VLM and at the end we replace  detections classes to have bounding boxes with dogs breeds labels.</p> <p>Breeds that we classify: <code>russell-terrier</code>, <code>wirehaired-pointing-griffon</code>, <code>beagle</code></p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\",\n            \"default_value\": [\n                \"russell-terrier\",\n                \"wirehaired-pointing-griffon\",\n                \"beagle\"\n            ]\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/google_gemini@v1\",\n            \"name\": \"gemini\",\n            \"images\": \"$steps.cropping.crops\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n            \"name\": \"parser\",\n            \"image\": \"$steps.cropping.crops\",\n            \"vlm_output\": \"$steps.gemini.output\",\n            \"classes\": \"$steps.gemini.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n            \"name\": \"classes_replacement\",\n            \"object_detection_predictions\": \"$steps.general_detection.predictions\",\n            \"classification_predictions\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.classes_replacement.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#prompting-gpt-with-arbitrary-prompt","title":"Prompting GPT with arbitrary prompt","text":"<p>In this example, GPT model is prompted with arbitrary text from user</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"unconstrained\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gpt.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-ocr-model","title":"Using GPT as OCR model","text":"<p>In this example, GPT model is used as OCR system. User just points task type and do not need to provide any prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"ocr\",\n            \"api_key\": \"$inputs.api_key\",\n            \"model_version\": \"gpt-4o-mini\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gpt.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-visual-question-answering-system","title":"Using GPT as Visual Question Answering system","text":"<p>In this example, GPT model is used as VQA system. User provides question via prompt.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"prompt\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"visual-question-answering\",\n            \"prompt\": \"$inputs.prompt\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gpt.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-image-captioning-system","title":"Using GPT as Image Captioning system","text":"<p>In this example, GPT model is used as Image Captioning system.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"caption\",\n            \"api_key\": \"$inputs.api_key\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.gpt.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-multi-class-classifier","title":"Using GPT as multi-class classifier","text":"<p>In this example, GPT model is used as classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v1</code> block which turns GPT output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.gpt.output\",\n            \"classes\": \"$steps.gpt.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"gpt_result\",\n            \"selector\": \"$steps.gpt.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"top_class\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-multi-label-classifier","title":"Using GPT as multi-label classifier","text":"<p>In this example, GPT model is used as multi-label classifier. Output from the model is parsed by special <code>roboflow_core/vlm_as_classifier@v1</code> block which turns GPT output text into full-blown prediction, which can later be used by other blocks compatible with  classification predictions - in this case we extract top-class property.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"multi-label-classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n            \"name\": \"parser\",\n            \"image\": \"$inputs.image\",\n            \"vlm_output\": \"$steps.gpt.output\",\n            \"classes\": \"$steps.gpt.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"top_class\",\n            \"operations\": [\n                {\n                    \"type\": \"ClassificationPropertyExtract\",\n                    \"property_name\": \"top_class\"\n                }\n            ],\n            \"data\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.top_class.output\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"parsed_prediction\",\n            \"selector\": \"$steps.parser.*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-to-provide-structured-json","title":"Using GPT to provide structured JSON","text":"<p>In this example, GPT model is expected to provide structured output in JSON, which can later be parsed by dedicated <code>roboflow_core/json_parser@v1</code> block which transforms string into dictionary  and expose it's keys to other blocks for further processing. In this case, parsed output is transformed using <code>roboflow_core/property_definition@v1</code> block.</p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$inputs.image\",\n            \"task_type\": \"structured-answering\",\n            \"output_structure\": {\n                \"dogs_count\": \"count of dogs instances in the image\",\n                \"cats_count\": \"count of cats instances in the image\"\n            },\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/json_parser@v1\",\n            \"name\": \"parser\",\n            \"raw_json\": \"$steps.gpt.output\",\n            \"expected_fields\": [\n                \"dogs_count\",\n                \"cats_count\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/property_definition@v1\",\n            \"name\": \"property_definition\",\n            \"operations\": [\n                {\n                    \"type\": \"ToString\"\n                }\n            ],\n            \"data\": \"$steps.parser.dogs_count\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"result\",\n            \"selector\": \"$steps.property_definition.output\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visual_language_models/#using-gpt-as-secondary-classifier","title":"Using GPT as secondary classifier","text":"<p>In this example, GPT model is used as secondary classifier - first, YOLO model detects dogs, then for each dog we run classification with VLM and at the end we replace  detections classes to have bounding boxes with dogs breeds labels.</p> <p>Breeds that we classify: <code>russell-terrier</code>, <code>wirehaired-pointing-griffon</code>, <code>beagle</code></p> Workflow definition <pre><code>{\n    \"version\": \"1.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"api_key\"\n        },\n        {\n            \"type\": \"WorkflowParameter\",\n            \"name\": \"classes\",\n            \"default_value\": [\n                \"russell-terrier\",\n                \"wirehaired-pointing-griffon\",\n                \"beagle\"\n            ]\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"ObjectDetectionModel\",\n            \"name\": \"general_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"dog\"\n            ]\n        },\n        {\n            \"type\": \"Crop\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.general_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/open_ai@v2\",\n            \"name\": \"gpt\",\n            \"images\": \"$steps.cropping.crops\",\n            \"task_type\": \"classification\",\n            \"classes\": \"$inputs.classes\",\n            \"api_key\": \"$inputs.api_key\"\n        },\n        {\n            \"type\": \"roboflow_core/vlm_as_classifier@v1\",\n            \"name\": \"parser\",\n            \"image\": \"$steps.cropping.crops\",\n            \"vlm_output\": \"$steps.gpt.output\",\n            \"classes\": \"$steps.gpt.classes\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_classes_replacement@v1\",\n            \"name\": \"classes_replacement\",\n            \"object_detection_predictions\": \"$steps.general_detection.predictions\",\n            \"classification_predictions\": \"$steps.parser.predictions\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.classes_replacement.predictions\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/gallery/workflows_with_visualization_blocks/","title":"Example Workflows - Workflows with visualization blocks","text":"<p>Below you can find example workflows you can use as inspiration to build your apps.</p>"},{"location":"workflows/gallery/workflows_with_visualization_blocks/#predictions-from-different-models-visualised-together","title":"Predictions from different models visualised together","text":"<p>This workflow showcases how predictions from different models (even from nested  batches created from input images) may be visualised together.</p> <p>Our scenario covers:</p> <ul> <li> <p>Detecting cars using YOLOv8 model</p> </li> <li> <p>Dynamically cropping input images to run secondary model (license plates detector) for each  car instance</p> </li> <li> <p>Stitching together all predictions for licence plates into single prediction</p> </li> <li> <p>Fusing cars detections and license plates detections into single prediction</p> </li> <li> <p>Visualizing final predictions</p> </li> </ul> Workflow definition <pre><code>{\n    \"version\": \"1.0.0\",\n    \"inputs\": [\n        {\n            \"type\": \"WorkflowImage\",\n            \"name\": \"image\"\n        }\n    ],\n    \"steps\": [\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"car_detection\",\n            \"image\": \"$inputs.image\",\n            \"model_id\": \"yolov8n-640\",\n            \"class_filter\": [\n                \"car\"\n            ]\n        },\n        {\n            \"type\": \"roboflow_core/dynamic_crop@v1\",\n            \"name\": \"cropping\",\n            \"image\": \"$inputs.image\",\n            \"predictions\": \"$steps.car_detection.predictions\"\n        },\n        {\n            \"type\": \"roboflow_core/roboflow_object_detection_model@v1\",\n            \"name\": \"plates_detection\",\n            \"image\": \"$steps.cropping.crops\",\n            \"model_id\": \"vehicle-registration-plates-trudk/2\"\n        },\n        {\n            \"type\": \"roboflow_core/detections_stitch@v1\",\n            \"name\": \"stitch\",\n            \"reference_image\": \"$inputs.image\",\n            \"predictions\": \"$steps.plates_detection.predictions\",\n            \"overlap_filtering_strategy\": \"nms\"\n        },\n        {\n            \"type\": \"DetectionsConsensus\",\n            \"name\": \"consensus\",\n            \"predictions_batches\": [\n                \"$steps.car_detection.predictions\",\n                \"$steps.stitch.predictions\"\n            ],\n            \"required_votes\": 1\n        },\n        {\n            \"type\": \"roboflow_core/bounding_box_visualization@v1\",\n            \"name\": \"bbox_visualiser\",\n            \"predictions\": \"$steps.consensus.predictions\",\n            \"image\": \"$inputs.image\"\n        }\n    ],\n    \"outputs\": [\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"predictions\",\n            \"selector\": \"$steps.consensus.predictions\"\n        },\n        {\n            \"type\": \"JsonField\",\n            \"name\": \"visualisation\",\n            \"selector\": \"$steps.bbox_visualiser.image\"\n        }\n    ]\n}\n</code></pre>"},{"location":"workflows/kinds/%2A/","title":"Kind <code>*</code>","text":"<p>Equivalent of any element</p>"},{"location":"workflows/kinds/%2A/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/%2A/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>Any</code></p>"},{"location":"workflows/kinds/%2A/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>Any</code></p>"},{"location":"workflows/kinds/%2A/#details","title":"Details","text":"<p>This is a special kind that represents Any value - which is to be used by default if  kind annotation is not specified. It will not tell the compiler what is to be expected as a specific value in runtime, but at the same time, it makes it possible to run the  workflow when we do not know or do not care about types. </p> <p>Important note: Usage of this kind reduces execution engine capabilities to predict  problems with workflow and make those problems to be visible while running the workflow.</p>"},{"location":"workflows/kinds/bar_code_detection/","title":"Kind <code>bar_code_detection</code>","text":"<p>Prediction with barcode detection</p>"},{"location":"workflows/kinds/bar_code_detection/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/bar_code_detection/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/bar_code_detection/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>sv.Detections</code></p>"},{"location":"workflows/kinds/bar_code_detection/#details","title":"Details","text":"<p>This kind represents batch of predictions regarding barcodes location and data their provide.</p> <p>Example: <pre><code>sv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    1.0, 1.0, 1.0, 1.0]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={\n        'class_name': array(['barcode', 'barcode', 'barcode', 'barcode'], dtype='&lt;U13')\n        'detection_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'image_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'inference_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'prediction_type': array([\n            'barcode-detection', 'barcode-detection', \n            'barcode-detection', 'barcode-detection'\n        ], dtype='&lt;U16'),\n        'root_parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'root_parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'root_parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'scaling_relative_to_parent': array([1, 1, 1, 1]),\n        'scaling_relative_to_root_parent': array([1, 1, 1, 1]),\n        'data': np.array(['qr-code-1-data', 'qr-code-2-data', 'qr-code-3-data', 'qr-code-4-data'])\n    }\n)\n</code></pre></p> <p>As you can see, we have extended the standard set of metadata for predictions maintained by <code>supervision</code>. Adding this metadata is needed to ensure compatibility with blocks from <code>roboflow_core</code> plugin.</p> <p>The design of metadata is suboptimal (as metadata regarding whole image is duplicated across all  bounding boxes and there is no way on how to save metadata for empty predictions). We have GH issue to communicate around this problem.</p> <p>Details of additional fields:</p> <ul> <li> <p><code>detection_id</code> - unique identifier for each detection, to be used for when dependent elements  are created based on specific detection (example: Dynamic Crop takes this value as parent id for new image)</p> </li> <li> <p><code>parent_id</code> - identifier of image that generated prediction (to be fetched from <code>WorkflowImageData</code> object)</p> </li> <li> <p><code>image_dimensions</code> - dimensions of image that was basis for prediction - format: <code>(height, width)</code></p> </li> <li> <p><code>inference_id</code> - identifier of inference request (optional, relevant for Roboflow models)</p> </li> <li> <p><code>prediction_type</code> - type of prediction</p> </li> <li> <p><code>root_parent_id</code> - identifier of primary Workflow input that was responsible for downstream prediction  (to be fetched from <code>WorkflowImageData</code> object) - usually identifier of Workflow input placeholder </p> </li> <li> <p><code>root_parent_coordinates</code> - offset regarding origin input - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>root_parent_dimensions</code> - dimensions of origin input image <code>(height, width)</code></p> </li> <li> <p><code>parent_coordinates</code> - offset regarding parent - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>parent_dimensions</code> - dimensions of parent image <code>(height, width)</code></p> </li> <li> <p><code>scaling_relative_to_parent</code> - scaling factor regarding parent image</p> </li> <li> <p><code>scaling_relative_to_root_parent</code> - scaling factor regarding origin input image</p> </li> <li> <p><code>data</code> - extracted barcode</p> </li> </ul> <p>SERIALISATION: Execution Engine behind API will serialise underlying data once selector of this kind is declared as Workflow output - serialisation will be executed such that <code>sv.Detections.from_inference(...)</code> can decode the output. Entity details: ObjectDetectionInferenceResponse</p>"},{"location":"workflows/kinds/boolean/","title":"Kind <code>boolean</code>","text":"<p>Boolean flag</p>"},{"location":"workflows/kinds/boolean/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/boolean/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>bool</code></p>"},{"location":"workflows/kinds/boolean/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>bool</code></p>"},{"location":"workflows/kinds/boolean/#details","title":"Details","text":"<p>This kind represents boolean value - <code>True</code> or <code>False</code></p>"},{"location":"workflows/kinds/bytes/","title":"Kind <code>bytes</code>","text":"<p>This kind represent bytes</p>"},{"location":"workflows/kinds/bytes/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/bytes/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/bytes/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>bytes</code></p>"},{"location":"workflows/kinds/bytes/#details","title":"Details","text":"<p>This kind represent bytes. Default serializer turns bytes into base64-encoded string and this is the source of different data representation.</p>"},{"location":"workflows/kinds/classification_prediction/","title":"Kind <code>classification_prediction</code>","text":"<p>Predictions from classifier</p>"},{"location":"workflows/kinds/classification_prediction/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/classification_prediction/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/classification_prediction/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/classification_prediction/#details","title":"Details","text":"<p>This kind represent predictions from Classification Models.</p> <p>Examples: <pre><code># in case of multi-class classification\n{\n    \"image\": {\"height\": 128, \"width\": 256},\n    \"predictions\": [{\"class_name\": \"A\", \"class_id\": 0, \"confidence\": 0.3}],\n    \"top\": \"A\",\n    \"confidence\": 0.3,\n    \"parent_id\": \"some\",\n    \"prediction_type\": \"classification\",\n    \"inference_id\": \"some\",\n    \"root_parent_id\": \"some\",\n}\n\n# in case of multi-label classification\n{\n    \"image\": {\"height\": 128, \"width\": 256},\n    \"predictions\": {\n        \"a\": {\"confidence\": 0.3, \"class_id\": 0},\n        \"b\": {\"confidence\": 0.3, \"class_id\": 1},\n    }\n    \"predicted_classes\": [\"a\", \"b\"],\n    \"parent_id\": \"some\",\n    \"prediction_type\": \"classification\",\n    \"inference_id\": \"some\",\n    \"root_parent_id\": \"some\",\n}\n</code></pre></p>"},{"location":"workflows/kinds/contours/","title":"Kind <code>contours</code>","text":"<p>List of numpy arrays where each array represents contour points</p>"},{"location":"workflows/kinds/contours/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/contours/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>List[list]</code></p>"},{"location":"workflows/kinds/contours/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>List[np.ndarray]</code></p>"},{"location":"workflows/kinds/contours/#details","title":"Details","text":"<p>This kind represents a value of a list of numpy arrays where each array represents contour points.</p> <p>Example: <pre><code>[\n    np.array([[10, 10],\n              [20, 20],\n              [30, 30]], dtype=np.int32),\n    np.array([[50, 50],\n              [60, 60],\n              [70, 70]], dtype=np.int32)\n]\n</code></pre></p>"},{"location":"workflows/kinds/detection/","title":"Kind <code>detection</code>","text":"<p>Single element of detections-based prediction (like <code>object_detection_prediction</code>)</p>"},{"location":"workflows/kinds/detection/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/detection/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>Tuple[list, Optional[list], Optional[float], Optional[float], Optional[int], dict]</code></p>"},{"location":"workflows/kinds/detection/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>Tuple[np.ndarray, Optional[np.ndarray], Optional[float], Optional[float], Optional[int], dict]</code></p>"},{"location":"workflows/kinds/detection/#details","title":"Details","text":"<p>This kind represents single detection in prediction from a model that detects multiple elements (like object detection or instance segmentation model). It is represented as a tuple that is created from <code>sv.Detections(...)</code> object while iterating over its content. <code>workflows</code> utilises <code>data</code> property of <code>sv.Detections(...)</code> to keep additional metadata which will be available in the tuple. Some properties may not always be present. Take a look at documentation of  <code>object_detection_prediction</code>, <code>instance_segmentation_prediction</code>, <code>keypoint_detection_prediction</code> kinds to discover which additional metadata are available.</p> <p>More technical details about  iterating over <code>sv.Detections(...)</code></p>"},{"location":"workflows/kinds/dictionary/","title":"Kind <code>dictionary</code>","text":"<p>Dictionary</p>"},{"location":"workflows/kinds/dictionary/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/dictionary/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/dictionary/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/dictionary/#details","title":"Details","text":"<p>Not available.</p>"},{"location":"workflows/kinds/float/","title":"Kind <code>float</code>","text":"<p>Float value</p>"},{"location":"workflows/kinds/float/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/float/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>float</code></p>"},{"location":"workflows/kinds/float/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>float</code></p>"},{"location":"workflows/kinds/float/#details","title":"Details","text":"<p>Example: <pre><code>1.3\n2.7\n</code></pre></p>"},{"location":"workflows/kinds/float_zero_to_one/","title":"Kind <code>float_zero_to_one</code>","text":"<p><code>float</code> value in range <code>[0.0, 1.0]</code></p>"},{"location":"workflows/kinds/float_zero_to_one/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/float_zero_to_one/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>float</code></p>"},{"location":"workflows/kinds/float_zero_to_one/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>float</code></p>"},{"location":"workflows/kinds/float_zero_to_one/#details","title":"Details","text":"<p>This kind represents float value from 0.0 to 1.0. </p> <p>Examples: <pre><code>0.1\n0.4\n0.999\n</code></pre></p>"},{"location":"workflows/kinds/image/","title":"Kind <code>image</code>","text":"<p>Image in workflows</p>"},{"location":"workflows/kinds/image/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/image/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/image/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>WorkflowImageData</code></p>"},{"location":"workflows/kinds/image/#details","title":"Details","text":"<p>This is the representation of image in <code>workflows</code>. Underlying data type has different internal and external representation. As an input we support:</p> <p>Update added in Execution Engine <code>v1.2.0</code></p> <p><code>video_metadata</code> added as optional property - should be injected in context of video processing to  provide necessary context for blocks dedicated to video processing.</p> <ul> <li> <p><code>np.ndarray</code> image when Workflows Execution Engine is used directly in <code>inference</code> python package (array can be provided in a form of dictionary presented below, if <code>video_metadata</code> is intended to be injected)</p> </li> <li> <p>dictionary compatible with inference image utils:</p> </li> </ul> <pre><code>{\n    \"type\": \"url\",   # there are different types supported, including np arrays and PIL images\n    \"value\": \"...\"   # value depends on `type`,\n    \"video_metadata\": {  \n        # optional - can be added in context of video processing - introduced in \n        # Execution Engine `v1.2.0` - released in inference `v0.23.0`\n        \"video_identifier\": \"rtsp://some.com/stream1\",\n        \"comes_from_video_file\": False,\n        \"fps\": 23.99,\n        \"frame_number\": 24,\n        \"frame_timestamp\": \"2024-08-21T11:13:44.313999\", \n    }  \n}\n</code></pre> <p>Whe using Workflows Execution Engine exposed behind <code>inference</code> server, two most common <code>type</code> values are <code>base64</code> and  <code>url</code>.</p> <p>Internally, <code>WorkflowImageData</code> is used. If you are a Workflow block developer, we advise checking out usage guide.</p>"},{"location":"workflows/kinds/image_keypoints/","title":"Kind <code>image_keypoints</code>","text":"<p>Image keypoints detected by classical Computer Vision method</p>"},{"location":"workflows/kinds/image_keypoints/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/image_keypoints/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/image_keypoints/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/image_keypoints/#details","title":"Details","text":"<p>The kind represents image keypoints that are detected by classical Computer Vision methods. Underlying representation is serialised OpenCV KeyPoint object.</p> <p>Examples: <pre><code>{\n    \"pt\": (2.429290294647217, 1197.7939453125),\n    \"size\": 1.9633429050445557,\n    \"angle\": 183.4322509765625,\n    \"response\": 0.03325376659631729,\n    \"octave\": 6423039,\n    \"class_id\": -1\n}\n</code></pre></p>"},{"location":"workflows/kinds/image_metadata/","title":"Kind <code>image_metadata</code>","text":"<p>Dictionary with image metadata required by supervision</p>"},{"location":"workflows/kinds/image_metadata/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/image_metadata/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/image_metadata/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/image_metadata/#details","title":"Details","text":"<p>This kind represent batch of prediction metadata providing information about the image that prediction was made against.</p> <p>Examples: <pre><code>[{\"width\": 1280, \"height\": 720}, {\"width\": 1920, \"height\": 1080}]\n[{\"width\": 1280, \"height\": 720}]\n</code></pre></p>"},{"location":"workflows/kinds/instance_segmentation_prediction/","title":"Kind <code>instance_segmentation_prediction</code>","text":"<p>Prediction with detected bounding boxes and segmentation masks in form of sv.Detections(...) object</p>"},{"location":"workflows/kinds/instance_segmentation_prediction/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/instance_segmentation_prediction/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/instance_segmentation_prediction/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>sv.Detections</code></p>"},{"location":"workflows/kinds/instance_segmentation_prediction/#details","title":"Details","text":"<p>This kind represents single instance segmentation prediction in form of  <code>sv.Detections(...)</code> object.</p> <p>Example: <pre><code>sv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=array([\n        [[False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        ...,\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False],\n        [False, False, False, ..., False, False, False]]\n    ]), \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='&lt;U1'),\n        'detection_id': array(['51dfa8d5-261c-4dcb-ab30-9aafe9b52379'], dtype='&lt;U36'),\n        'parent_id': array(['image.[0]'], dtype='&lt;U9'),\n        'image_dimensions': array([[425, 640]]),\n        'inference_id': array(['51dfa8d5-261c-4dcb-ab30-9aafe9b52379'], dtype='&lt;U36'),\n        'prediction_type': array(['instance-segmentation'], dtype='&lt;U16'),\n        'root_parent_id': array(['image.[0]'], dtype='&lt;U9'),\n        'root_parent_coordinates': array([[0, 0]]),\n        'root_parent_dimensions': array([[425, 640]]),\n        'parent_coordinates': array([[0, 0]]),\n        'parent_dimensions': array([[425, 640]]),\n        'scaling_relative_to_parent': array([1]),\n        'scaling_relative_to_root_parent': array([1]),\n    }\n)\n</code></pre></p> <p>As you can see, we have extended the standard set of metadata for predictions maintained by <code>supervision</code>. Adding this metadata is needed to ensure compatibility with blocks from <code>roboflow_core</code> plugin.</p> <p>The design of metadata is suboptimal (as metadata regarding whole image is duplicated across all  bounding boxes and there is no way on how to save metadata for empty predictions). We have GH issue to communicate around this problem.</p> <p>Details of additional fields:</p> <ul> <li> <p><code>detection_id</code> - unique identifier for each detection, to be used for when dependent elements  are created based on specific detection (example: Dynamic Crop takes this value as parent id for new image)</p> </li> <li> <p><code>parent_id</code> - identifier of image that generated prediction (to be fetched from <code>WorkflowImageData</code> object)</p> </li> <li> <p><code>image_dimensions</code> - dimensions of image that was basis for prediction - format: <code>(height, width)</code></p> </li> <li> <p><code>inference_id</code> - identifier of inference request (optional, relevant for Roboflow models)</p> </li> <li> <p><code>prediction_type</code> - type of prediction</p> </li> <li> <p><code>root_parent_id</code> - identifier of primary Workflow input that was responsible for downstream prediction  (to be fetched from <code>WorkflowImageData</code> object) - usually identifier of Workflow input placeholder </p> </li> <li> <p><code>root_parent_coordinates</code> - offset regarding origin input - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>root_parent_dimensions</code> - dimensions of origin input image <code>(height, width)</code></p> </li> <li> <p><code>parent_coordinates</code> - offset regarding parent - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>parent_dimensions</code> - dimensions of parent image <code>(height, width)</code></p> </li> <li> <p><code>scaling_relative_to_parent</code> - scaling factor regarding parent image</p> </li> <li> <p><code>scaling_relative_to_root_parent</code> - scaling factor regarding origin input image</p> </li> </ul> <p>SERIALISATION:</p> <p>Execution Engine behind API will serialise underlying data once selector of this kind is declared as Workflow output - serialisation will be executed such that <code>sv.Detections.from_inference(...)</code> can decode the output. Entity details: InstanceSegmentationInferenceResponse</p>"},{"location":"workflows/kinds/integer/","title":"Kind <code>integer</code>","text":"<p>Integer value</p>"},{"location":"workflows/kinds/integer/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/integer/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>int</code></p>"},{"location":"workflows/kinds/integer/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>int</code></p>"},{"location":"workflows/kinds/integer/#details","title":"Details","text":"<p>Examples: <pre><code>1\n2\n</code></pre></p>"},{"location":"workflows/kinds/keypoint_detection_prediction/","title":"Kind <code>keypoint_detection_prediction</code>","text":"<p>Prediction with detected bounding boxes and detected keypoints in form of sv.Detections(...) object</p>"},{"location":"workflows/kinds/keypoint_detection_prediction/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/keypoint_detection_prediction/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/keypoint_detection_prediction/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>sv.Detections</code></p>"},{"location":"workflows/kinds/keypoint_detection_prediction/#details","title":"Details","text":"<p>This kind represents single keypoints prediction in form of  <code>sv.Detections(...)</code> object.</p> <p>Example: <pre><code>sv.Detections(\n    xyxy=array([[        127,         189,         322,         303]]), \n    mask=None, \n    confidence=array([    0.95898]), \n    class_id=array([6]), \n    tracker_id=None, \n    data={\n        'class_name': array(['G'], dtype='&lt;U1'),\n        'detection_id': array(['51dfa8d5-261c-4dcb-ab30-9aafe9b52379'], dtype='&lt;U36'),\n        'parent_id': array(['image.[0]'], dtype='&lt;U9'),\n        'image_dimensions': array([[425, 640]]),\n        'inference_id': array(['51dfa8d5-261c-4dcb-ab30-9aafe9b52379'], dtype='&lt;U36'),\n        'prediction_type': array(['instance-segmentation'], dtype='&lt;U16'),\n        'root_parent_id': array(['image.[0]'], dtype='&lt;U9'),\n        'root_parent_coordinates': array([[0, 0]]),\n        'root_parent_dimensions': array([[425, 640]]),\n        'parent_coordinates': array([[0, 0]]),\n        'parent_dimensions': array([[425, 640]]),\n        'scaling_relative_to_parent': array([1]),\n        'scaling_relative_to_root_parent': array([1]),\n        'keypoints_class_name': array(),  # variable length array of type object - one 1D array of str for each box\n        'keypoints_class_id': array(),  # variable length array of type object - one 1D array of int for each box\n        'keypoints_confidence': array(),  # variable length array of type object - one 1D array of float for each box\n        'keypoints_xy': array(),  # variable length array of type object - one 2D array for bbox with (x, y) coords\n    }\n)\n</code></pre></p> <p>Prior to sv.Keypoints(...) we introduced  keypoints detection based on <code>sv.Detections(...)</code> object. The decision was suboptimal so we would need to revert in the future, but for now this is the format of data for keypoints detection. </p> <p>The design of metadata is also suboptimal (as metadata regarding whole image is duplicated across all  bounding boxes and there is no way on how to save metadata for empty predictions). We have GH issue to communicate around this problem.</p> <p>Details of additional fields:</p> <ul> <li> <p><code>detection_id</code> - unique identifier for each detection, to be used for when dependent elements  are created based on specific detection (example: Dynamic Crop takes this value as parent id for new image)</p> </li> <li> <p><code>parent_id</code> - identifier of image that generated prediction (to be fetched from <code>WorkflowImageData</code> object)</p> </li> <li> <p><code>image_dimensions</code> - dimensions of image that was basis for prediction - format: <code>(height, width)</code></p> </li> <li> <p><code>inference_id</code> - identifier of inference request (optional, relevant for Roboflow models)</p> </li> <li> <p><code>prediction_type</code> - type of prediction</p> </li> <li> <p><code>root_parent_id</code> - identifier of primary Workflow input that was responsible for downstream prediction  (to be fetched from <code>WorkflowImageData</code> object) - usually identifier of Workflow input placeholder </p> </li> <li> <p><code>root_parent_coordinates</code> - offset regarding origin input - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>root_parent_dimensions</code> - dimensions of origin input image <code>(height, width)</code></p> </li> <li> <p><code>parent_coordinates</code> - offset regarding parent - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>parent_dimensions</code> - dimensions of parent image <code>(height, width)</code></p> </li> <li> <p><code>scaling_relative_to_parent</code> - scaling factor regarding parent image</p> </li> <li> <p><code>scaling_relative_to_root_parent</code> - scaling factor regarding origin input image</p> </li> <li> <p><code>keypoints_class_name</code> array of variable size 1D arrays of string with key points class names</p> </li> <li> <p><code>keypoints_class_id</code> array of variable size 1D arrays of int with key points class ids</p> </li> <li> <p><code>keypoints_confidence</code> array of variable size 1D arrays of float with key points confidence</p> </li> <li> <p><code>keypoints_xy</code> array of variable size 2D arrays of coordinates of keypoints in <code>(x, y)</code> format</p> </li> </ul> <p>SERIALISATION:</p> <p>Execution Engine behind API will serialise underlying data once selector of this kind is declared as Workflow output - serialisation will be executed such that <code>sv.Detections.from_inference(...)</code> can decode the output, but loosing keypoints details - which can be recovered if output  JSON field is parsed. Entity details: KeypointsDetectionInferenceResponse</p>"},{"location":"workflows/kinds/language_model_output/","title":"Kind <code>language_model_output</code>","text":"<p>LLM / VLM output</p>"},{"location":"workflows/kinds/language_model_output/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/language_model_output/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/language_model_output/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/language_model_output/#details","title":"Details","text":"<p>This kind represents output generated by language model. It is Python string, which can be processed  by blocks transforming LLMs / VLMs output into structured form.</p> <p>Examples: <pre><code>{\"predicted_class\": \"car\", \"confidence\": 0.7}  # which is example JSON with classification prediction\n\"The is A.\"  # which is example unstructured generation for VQA task \n</code></pre></p>"},{"location":"workflows/kinds/list_of_values/","title":"Kind <code>list_of_values</code>","text":"<p>List of values of any type</p>"},{"location":"workflows/kinds/list_of_values/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/list_of_values/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>List[Any]</code></p>"},{"location":"workflows/kinds/list_of_values/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>List[Any]</code></p>"},{"location":"workflows/kinds/list_of_values/#details","title":"Details","text":"<p>This kind represents Python list of Any values.</p> <p>Examples: <pre><code>[\"a\", 1, 1.0]\n[\"a\", \"b\", \"c\"]\n</code></pre></p>"},{"location":"workflows/kinds/numpy_array/","title":"Kind <code>numpy_array</code>","text":"<p>Numpy array</p>"},{"location":"workflows/kinds/numpy_array/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/numpy_array/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>list</code></p>"},{"location":"workflows/kinds/numpy_array/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>np.ndarray</code></p>"},{"location":"workflows/kinds/numpy_array/#details","title":"Details","text":"<p>Any np.ndarray object</p>"},{"location":"workflows/kinds/object_detection_prediction/","title":"Kind <code>object_detection_prediction</code>","text":"<p>Prediction with detected bounding boxes in form of sv.Detections(...) object</p>"},{"location":"workflows/kinds/object_detection_prediction/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/object_detection_prediction/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/object_detection_prediction/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>sv.Detections</code></p>"},{"location":"workflows/kinds/object_detection_prediction/#details","title":"Details","text":"<p>This kind represents single object detection prediction in form of  <code>sv.Detections(...)</code> object.</p> <p>Example: <pre><code>sv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    0.84955,     0.74344,     0.45636,     0.86537]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={\n        'class_name': array(['car', 'truck', 'car', 'car'], dtype='&lt;U13')\n        'detection_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'image_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'inference_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'prediction_type': array([\n            'object-detection', 'object-detection', \n            'object-detection', 'object-detection'\n        ], dtype='&lt;U16'),\n        'root_parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'root_parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'root_parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'scaling_relative_to_parent': array([1, 1, 1, 1]),\n        'scaling_relative_to_root_parent': array([1, 1, 1, 1]),\n    }\n)\n</code></pre></p> <p>As you can see, we have extended the standard set of metadata for predictions maintained by <code>supervision</code>. Adding this metadata is needed to ensure compatibility with blocks from <code>roboflow_core</code> plugin.</p> <p>The design of metadata is suboptimal (as metadata regarding whole image is duplicated across all  bounding boxes and there is no way on how to save metadata for empty predictions). We have GH issue to communicate around this problem.</p> <p>Details of additional fields:</p> <ul> <li> <p><code>detection_id</code> - unique identifier for each detection, to be used for when dependent elements  are created based on specific detection (example: Dynamic Crop takes this value as parent id for new image)</p> </li> <li> <p><code>parent_id</code> - identifier of image that generated prediction (to be fetched from <code>WorkflowImageData</code> object)</p> </li> <li> <p><code>image_dimensions</code> - dimensions of image that was basis for prediction - format: <code>(height, width)</code></p> </li> <li> <p><code>inference_id</code> - identifier of inference request (optional, relevant for Roboflow models)</p> </li> <li> <p><code>prediction_type</code> - type of prediction</p> </li> <li> <p><code>root_parent_id</code> - identifier of primary Workflow input that was responsible for downstream prediction  (to be fetched from <code>WorkflowImageData</code> object) - usually identifier of Workflow input placeholder </p> </li> <li> <p><code>root_parent_coordinates</code> - offset regarding origin input - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>root_parent_dimensions</code> - dimensions of origin input image <code>(height, width)</code></p> </li> <li> <p><code>parent_coordinates</code> - offset regarding parent - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>parent_dimensions</code> - dimensions of parent image <code>(height, width)</code></p> </li> <li> <p><code>scaling_relative_to_parent</code> - scaling factor regarding parent image</p> </li> <li> <p><code>scaling_relative_to_root_parent</code> - scaling factor regarding origin input image</p> </li> </ul> <p>SERIALISATION:</p> <p>Execution Engine behind API will serialise underlying data once selector of this kind is declared as Workflow output - serialisation will be executed such that <code>sv.Detections.from_inference(...)</code> can decode the output. Entity details: ObjectDetectionInferenceResponse</p>"},{"location":"workflows/kinds/parent_id/","title":"Kind <code>parent_id</code>","text":"<p>Identifier of parent for step output</p>"},{"location":"workflows/kinds/parent_id/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/parent_id/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/parent_id/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/parent_id/#details","title":"Details","text":"<p>This kind represent batch of prediction metadata providing information about the context of prediction. For example - whenever there is a workflow with multiple models - such that first model detect objects  and then other models make their predictions based on crops from first model detections - <code>parent_id</code> helps to figure out which detection of the first model is associated to which downstream predictions.</p> <p>Examples: <pre><code>\"uuid-1\"\n\"uuid-2\"\n</code></pre></p>"},{"location":"workflows/kinds/point/","title":"Kind <code>point</code>","text":"<p>Single point in 2D</p>"},{"location":"workflows/kinds/point/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/point/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>Tuple[int, int]</code></p>"},{"location":"workflows/kinds/point/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>Tuple[int, int]</code></p>"},{"location":"workflows/kinds/point/#details","title":"Details","text":"<p>Not available.</p>"},{"location":"workflows/kinds/prediction_type/","title":"Kind <code>prediction_type</code>","text":"<p>String value with type of prediction</p>"},{"location":"workflows/kinds/prediction_type/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/prediction_type/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/prediction_type/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/prediction_type/#details","title":"Details","text":"<p>This kind represent batch of prediction metadata providing information about the type of prediction.</p> <p>Examples: <pre><code>\"object-detection\"\n\"instance-segmentation\"\n</code></pre></p>"},{"location":"workflows/kinds/qr_code_detection/","title":"Kind <code>qr_code_detection</code>","text":"<p>Prediction with QR code detection</p>"},{"location":"workflows/kinds/qr_code_detection/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/qr_code_detection/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/qr_code_detection/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>sv.Detections</code></p>"},{"location":"workflows/kinds/qr_code_detection/#details","title":"Details","text":"<p>This kind represents batch of predictions regarding QR codes location and data their provide.</p> <p>Example: <pre><code>sv.Detections(\n    xyxy=array([\n       [        865,       153.5,        1189,       422.5],\n       [      192.5,        77.5,       995.5,       722.5],\n       [        194,          82,         996,         726],\n       [        460,         333,         704,         389]]\n    ), \n    mask=None, \n    confidence=array([    1.0, 1.0, 1.0, 1.0]), \n    class_id=array([2, 7, 2, 0]), \n    tracker_id=None, \n    data={\n        'class_name': array(['qr_code', 'qr_code', 'qr_code', 'qr_code'], dtype='&lt;U13')\n        'detection_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'image_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'inference_id': array([\n            '51dfa8d5-261c-4dcb-ab30-9aafe9b52379', 'c0c684d1-1e30-4880-aedd-29e67e417264'\n            '8cfc543b-9cfe-493b-b5ad-77afed7bee83', 'c0c684d1-1e30-4880-aedd-38e67e441454'\n        ], dtype='&lt;U36'),\n        'prediction_type': array([\n            'qrcode-detection', 'qrcode-detection', \n            'qrcode-detection', 'qrcode-detection'\n        ], dtype='&lt;U16'),\n        'root_parent_id': array(['image.[0]', 'image.[0]', 'image.[0]', 'image.[0]'], dtype='&lt;U9'),\n        'root_parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'root_parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'parent_coordinates': array([[0, 0], [0, 0], [0, 0], [0, 0]]),\n        'parent_dimensions': array([[425, 640], [425, 640], [425, 640], [425, 640]]),\n        'scaling_relative_to_parent': array([1, 1, 1, 1]),\n        'scaling_relative_to_root_parent': array([1, 1, 1, 1]),\n        'data': np.array(['qr-code-1-data', 'qr-code-2-data', 'qr-code-3-data', 'qr-code-4-data'])\n    }\n)\n</code></pre></p> <p>As you can see, we have extended the standard set of metadata for predictions maintained by <code>supervision</code>. Adding this metadata is needed to ensure compatibility with blocks from <code>roboflow_core</code> plugin.</p> <p>The design of metadata is suboptimal (as metadata regarding whole image is duplicated across all  bounding boxes and there is no way on how to save metadata for empty predictions). We have GH issue to communicate around this problem.</p> <p>Details of additional fields:</p> <ul> <li> <p><code>detection_id</code> - unique identifier for each detection, to be used for when dependent elements  are created based on specific detection (example: Dynamic Crop takes this value as parent id for new image)</p> </li> <li> <p><code>parent_id</code> - identifier of image that generated prediction (to be fetched from <code>WorkflowImageData</code> object)</p> </li> <li> <p><code>image_dimensions</code> - dimensions of image that was basis for prediction - format: <code>(height, width)</code></p> </li> <li> <p><code>inference_id</code> - identifier of inference request (optional, relevant for Roboflow models)</p> </li> <li> <p><code>prediction_type</code> - type of prediction</p> </li> <li> <p><code>root_parent_id</code> - identifier of primary Workflow input that was responsible for downstream prediction  (to be fetched from <code>WorkflowImageData</code> object) - usually identifier of Workflow input placeholder </p> </li> <li> <p><code>root_parent_coordinates</code> - offset regarding origin input - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>root_parent_dimensions</code> - dimensions of origin input image <code>(height, width)</code></p> </li> <li> <p><code>parent_coordinates</code> - offset regarding parent - format (<code>offset_x</code>, <code>offset_y</code>)</p> </li> <li> <p><code>parent_dimensions</code> - dimensions of parent image <code>(height, width)</code></p> </li> <li> <p><code>scaling_relative_to_parent</code> - scaling factor regarding parent image</p> </li> <li> <p><code>scaling_relative_to_root_parent</code> - scaling factor regarding origin input image</p> </li> <li> <p><code>data</code> - extracted QR code</p> </li> </ul> <p>SERIALISATION: Execution Engine behind API will serialise underlying data once selector of this kind is declared as Workflow output - serialisation will be executed such that <code>sv.Detections.from_inference(...)</code> can decode the output. Entity details: ObjectDetectionInferenceResponse</p>"},{"location":"workflows/kinds/rgb_color/","title":"Kind <code>rgb_color</code>","text":"<p>RGB color</p>"},{"location":"workflows/kinds/rgb_color/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/rgb_color/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>Tuple[int, int, int]</code></p>"},{"location":"workflows/kinds/rgb_color/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>Tuple[int, int, int]</code></p>"},{"location":"workflows/kinds/rgb_color/#details","title":"Details","text":"<p>This kind represents RGB color as a tuple (R, G, B).</p> <p>Examples: <pre><code>(128, 32, 64)\n(255, 255, 255)\n</code></pre></p>"},{"location":"workflows/kinds/roboflow_api_key/","title":"Kind <code>roboflow_api_key</code>","text":"<p>Roboflow API key</p>"},{"location":"workflows/kinds/roboflow_api_key/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/roboflow_api_key/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_api_key/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_api_key/#details","title":"Details","text":"<p>This kind represents API key that grants access to Roboflow platform. To learn more about Roboflow API keys visit this  page.</p>"},{"location":"workflows/kinds/roboflow_model_id/","title":"Kind <code>roboflow_model_id</code>","text":"<p>Roboflow model id</p>"},{"location":"workflows/kinds/roboflow_model_id/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/roboflow_model_id/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_model_id/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_model_id/#details","title":"Details","text":"<p>This kind represents value specific for Roboflow platform. At the platform, models are identified with special strings in the format: <code>&lt;project_name&gt;/&lt;version&gt;</code>. You should expect this value to be provided once this kind is used. In some special cases, Roboflow  platform accepts model alias as model id which will not conform provided schema. List of aliases can be found here.  </p>"},{"location":"workflows/kinds/roboflow_project/","title":"Kind <code>roboflow_project</code>","text":"<p>Roboflow project name</p>"},{"location":"workflows/kinds/roboflow_project/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/roboflow_project/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_project/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/roboflow_project/#details","title":"Details","text":"<p>This kind represents value specific for Roboflow platform. At the platform, each project has  unique name and the value behind this kind represent this name. To learn more  on how to kick-off with Roboflow project - visit this page. </p>"},{"location":"workflows/kinds/serialised_payloads/","title":"Kind <code>serialised_payloads</code>","text":"<p>Serialised element that is usually accepted by sink</p>"},{"location":"workflows/kinds/serialised_payloads/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/serialised_payloads/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>List[Union[str, dict]]</code></p>"},{"location":"workflows/kinds/serialised_payloads/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>List[Union[str, bytes, dict]]</code></p>"},{"location":"workflows/kinds/serialised_payloads/#details","title":"Details","text":"<p>This value represents list of serialised values. Each serialised value is either string or bytes - if something else is provided - it will be attempted to be serialised  to JSON.</p> <p>Examples: <pre><code>[\"some\", b\"other\", {\"my\": \"dictionary\"}]\n</code></pre></p> <p>This kind is to be used in combination with sinks blocks and serializers blocks. Serializer should output value of this kind which shall then be accepted by sink.</p>"},{"location":"workflows/kinds/string/","title":"Kind <code>string</code>","text":"<p>String value</p>"},{"location":"workflows/kinds/string/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/string/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/string/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/string/#details","title":"Details","text":"<p>Examples: <pre><code>\"my string value\"\n</code></pre></p>"},{"location":"workflows/kinds/top_class/","title":"Kind <code>top_class</code>","text":"<p>String value representing top class predicted by classification model</p>"},{"location":"workflows/kinds/top_class/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/top_class/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/top_class/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>str</code></p>"},{"location":"workflows/kinds/top_class/#details","title":"Details","text":"<p>The kind represent top classes predicted by classification model.</p> <p>Example: <pre><code>\"car\"\n</code></pre></p>"},{"location":"workflows/kinds/video_metadata/","title":"Kind <code>video_metadata</code>","text":"<p>Video image metadata</p>"},{"location":"workflows/kinds/video_metadata/#data-representation","title":"Data representation","text":"<p>Data representation</p> <p>This kind has a different internal and external representation. External representation is relevant for  integration with your workflow, whereas internal one is an implementation detail useful for Workflows blocks development.</p>"},{"location":"workflows/kinds/video_metadata/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>dict</code></p>"},{"location":"workflows/kinds/video_metadata/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>VideoMetadata</code></p>"},{"location":"workflows/kinds/video_metadata/#details","title":"Details","text":"<p>Deprecated since Execution Engine <code>v1.2.0</code></p> <p><code>inference</code> maintainers decided to sunset <code>video_metadata</code> kind in favour of auxiliary metadata added to <code>image</code> kind. </p> <p>This is representation of metadata that describe images that come from videos. It is helpful in cases of stateful video processing, as the metadata may bring  pieces of information that are required by specific blocks.</p> <p>The kind has different internal end external representation. As input we support: <pre><code>{\n    \"video_identifier\": \"rtsp://some.com/stream1\",\n    \"comes_from_video_file\": False,\n    \"fps\": 23.99,\n    \"frame_number\": 24,\n    \"frame_timestamp\": \"2024-08-21T11:13:44.313999\", \n}   \n</code></pre> Internally, <code>VideoMetadata</code> is used. If you are a Workflow block developer, we advise checking out usage guide.</p>"},{"location":"workflows/kinds/zone/","title":"Kind <code>zone</code>","text":"<p>Definition of polygon zone</p>"},{"location":"workflows/kinds/zone/#data-representation","title":"Data representation","text":""},{"location":"workflows/kinds/zone/#external","title":"External","text":"<p>External data representation is relevant for Workflows clients - it dictates what is the input and output format of data.</p> <p>Type: <code>List[Tuple[int, int]]</code></p>"},{"location":"workflows/kinds/zone/#internal","title":"Internal","text":"<p>Internal data representation is relevant for Workflows blocks creators - this is the type that will be provided by Execution Engine in runtime to the block that consumes input of this kind.</p> <p>Type: <code>List[Tuple[int, int]]</code></p>"},{"location":"workflows/kinds/zone/#details","title":"Details","text":"<p>List of points defining polygon zone in format [(x, y)]</p>"},{"location":"workflows/video_processing/overview/","title":"Video Processing with Workflows","text":"<p>We've begun our journey into video processing using Workflows. Over time, we've expanded the number of  video-specific blocks (e.g., the ByteTracker block) and continue to dedicate efforts toward improving  their performance and robustness. The current state of this work is as follows:</p> <ul> <li> <p>We've introduced the <code>WorkflowVideoMetadata</code> input to store metadata related to video frames,  including FPS, timestamp, video source identifier, and file/stream flags. While this may not be the final approach  for handling video metadata, it allows us to build stateful video-processing blocks at this stage.  If your Workflow includes any blocks requiring input of kind <code>video_metadata</code>, you must define this input in  your Workflow. The metadata functions as a batch-oriented parameter, treated by the Execution Engine in the same way as <code>WorkflowImage</code>.</p> </li> <li> <p>The <code>InferencePipeline</code> supports  video processing with Workflows  by automatically injecting <code>WorkflowVideoMetadata</code> into the <code>video_metadata</code> field. This allows you to seamlessly run your Workflow using the <code>InferencePipeline</code> within the <code>inference</code> Python package.</p> </li> <li> <p>In the <code>0.21.0</code> release, we've initiated efforts to enable video processing management via the <code>inference</code> server API.  This means that eventually, no custom scripts will be required to process video using Workflows and <code>InferencePipeline</code>. You'll simply call an endpoint, specify the video source and the workflow, and the server will handle the rest\u2014allowing you to focus on consuming the results.</p> </li> </ul>"},{"location":"workflows/video_processing/overview/#video-management-api-comments-and-status-update","title":"Video management API - comments and status update","text":"<p>This is experimental feature, breaking changes may be introduced over time. There is a list of  known issues. Please visit the page to raise new issues or comment on existing ones.</p>"},{"location":"workflows/video_processing/overview/#release-0210","title":"Release <code>0.21.0</code>","text":"<ul> <li> <p>Added basic endpoints to <code>list</code>, <code>start</code>, <code>pause</code>, <code>resume</code>, <code>terminate</code> and <code>consume</code> results of <code>InferencePipelines</code> running under control of <code>inference</code> server. Endpoints are enabled in <code>inference</code> server docker images for CPU, GPU and Jetson devices. Running inference server there would let you call  <code>http://127.0.0.1:9001/docs</code> to retrieve OpenAPI schemas for endpoints.</p> </li> <li> <p>Added HTTP client for new endpoints into <code>InferenceHTTPClient</code> from <code>inference_sdk</code>. Here you may find examples on how to use the client and API to start processing videos today:</p> </li> </ul> <p>Note</p> <p>Package in version <code>inference~=0.21.0</code> used in examlpe, due to experimental nature of the feature, the code may evolve over time.</p> <pre><code>from inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://192.168.0.115:9001\",\n    api_key=\"&lt;ROBOFLOW-API-KEY&gt;\"\n)\n\n# to list active pipelines\nclient.list_inference_pipelines()\n\n# start processing - single stream\nclient.start_inference_pipeline_with_workflow(\n    video_reference=[\"rtsp://192.168.0.73:8554/live0.stream\"],\n    workspace_name=\"&lt;YOUR-WORKSPACE&gt;\",\n    workflow_id=\"&lt;YOUR-WORKFLOW-ID&gt;\",\n    results_buffer_size=5,  # results are consumed from in-memory buffer - optionally you can control its size\n)\n\n# start processing - one RTSP stream and one camera\n# USB camera cannot be passed easily to docker running on MacBook, but on Jetson devices it works :)\n\nclient.start_inference_pipeline_with_workflow(\n    video_reference=[\"rtsp://192.168.0.73:8554/live0.stream\", 0],\n    workspace_name=\"&lt;YOUR-WORKSPACE&gt;\",\n    workflow_id=\"&lt;YOUR-WORKFLOW-ID&gt;\",\n    batch_collection_timeout=0.05,  # for consumption of multiple video sources it is ADVISED to \n    # set batch collection timeout (defined as fraction of seconds - 0.05 = 50ms)\n)\n\n# start_inference_pipeline_with_workflow(...) will provide you pipeline_id which may be used to:\n\n# * get pipeline status\nclient.get_inference_pipeline_status(\n    pipeline_id=\"182452f4-a2c1-4537-92e1-ec64d1e42de1\",\n)\n\n# * pause pipeline\nclient.pause_inference_pipeline(\n    pipeline_id=\"182452f4-a2c1-4537-92e1-ec64d1e42de1\",\n)\n\n# * resume pipeline\nclient.resume_inference_pipeline(\n    pipeline_id=\"182452f4-a2c1-4537-92e1-ec64d1e42de1\",\n)\n\n# * terminate pipeline\nclient.terminate_inference_pipeline(\n    pipeline_id=\"182452f4-a2c1-4537-92e1-ec64d1e42de1\",\n)\n\n# * consume pipeline results\nclient.consume_inference_pipeline_result(\n    pipeline_id=\"182452f4-a2c1-4537-92e1-ec64d1e42de1\",\n    excluded_fields=[\"workflow_output_field_to_exclude\"]  # this is optional\n    # if you wanted to get rid of some outputs to save bandwidth - feel free to discard them\n)\n</code></pre> <p>The client presented above, may be used preview workflow outputs in a very naive way. Let's assume that the Workflow you defined runs object-detection model and renders it's output using Workflows visualisation  blocks registering output image in <code>preview</code> field. You can use the following script to pool and display processed video frames:</p> <pre><code>import cv2\nfrom inference_sdk import InferenceHTTPClient\nfrom inference.core.utils.image_utils import load_image\n\n\nclient = InferenceHTTPClient(\n    api_url=f\"http://127.0.0.1:9001\",\n    api_key=\"&lt;YOUR-API-KEY&gt;\",\n)\n\nwhile True:\n    result = client.consume_inference_pipeline_result(pipeline_id=\"&lt;PIPELINE-ID&gt;\")\n    if not result[\"outputs\"] or not result[\"outputs\"][0]:\n        # \"outputs\" key contains list of workflow results - why list? InferencePipeline can \n        # run on multiple video sources at the same time - each \"round\" it attempts to \n        # grab single frame from all sources and run through Workflows Execution Engine\n        # * when sources are inactive that may not be possible, hence empty list can be returned\n        # * when one of the source do not provide frame (for instance due to batch collection timeout)\n        # outputs list may contain `None` values!\n        continue\n    # let's assume single source\n    source_result = result[\"outputs\"][0]\n    image, _ = load_image(source_result[\"preview\"])  # \"preview\" is the name of workflow output with image\n    cv2.imshow(\"frame\", image)\n    cv2.waitKey(1)\n</code></pre>"}]}